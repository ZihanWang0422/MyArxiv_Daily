{"2025-09-04T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2509.04443v1","updated":"2025-09-04T17:59:10Z","published":"2025-09-04T17:59:10Z","title":"EMMA: Scaling Mobile Manipulation via Egocentric Human Data","summary":"  Scaling mobile manipulation imitation learning is bottlenecked by expensive\nmobile robot teleoperation. We present Egocentric Mobile MAnipulation (EMMA),\nan end-to-end framework training mobile manipulation policies from human mobile\nmanipulation data with static robot data, sidestepping mobile teleoperation. To\naccomplish this, we co-train human full-body motion data with static robot\ndata. In our experiments across three real-world tasks, EMMA demonstrates\ncomparable performance to baselines trained on teleoperated mobile robot data\n(Mobile ALOHA), achieving higher or equivalent task performance in full task\nsuccess. We find that EMMA is able to generalize to new spatial configurations\nand scenes, and we observe positive performance scaling as we increase the\nhours of human data, opening new avenues for scalable robotic learning in\nreal-world environments. Details of this project can be found at\nhttps://ego-moma.github.io/.\n","authors":["Lawrence Y. Zhu","Pranav Kuppili","Ryan Punamiya","Patcharapong Aphiwetsa","Dhruv Patel","Simar Kareer","Sehoon Ha","Danfei Xu"],"pdf_url":"https://arxiv.org/pdf/2509.04443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04441v1","updated":"2025-09-04T17:57:13Z","published":"2025-09-04T17:57:13Z","title":"DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation","summary":"  We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.\n","authors":["Hao-Shu Fang","Branden Romero","Yichen Xie","Arthur Hu","Bo-Ruei Huang","Juan Alvarez","Matthew Kim","Gabriel Margolis","Kavya Anbarasu","Masayoshi Tomizuka","Edward Adelson","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2509.04441v1.pdf","comment":"project page: https://dex-op.github.io"},{"id":"http://arxiv.org/abs/2509.04413v1","updated":"2025-09-04T17:34:59Z","published":"2025-09-04T17:34:59Z","title":"SAFE--MA--RRT: Multi-Agent Motion Planning with Data-Driven Safety\n  Certificates","summary":"  This paper proposes a fully data-driven motion-planning framework for\nhomogeneous linear multi-agent systems that operate in shared, obstacle-filled\nworkspaces without access to explicit system models. Each agent independently\nlearns its closed-loop behavior from experimental data by solving convex\nsemidefinite programs that generate locally invariant ellipsoids and\ncorresponding state-feedback gains. These ellipsoids, centered along grid-based\nwaypoints, certify the dynamic feasibility of short-range transitions and\ndefine safe regions of operation. A sampling-based planner constructs a tree of\nsuch waypoints, where transitions are allowed only when adjacent ellipsoids\noverlap, ensuring invariant-to-invariant transitions and continuous safety. All\nagents expand their trees simultaneously and are coordinated through a\nspace-time reservation table that guarantees inter-agent safety by preventing\nsimultaneous occupancy and head-on collisions. Each successful edge in the tree\nis equipped with its own local controller, enabling execution without\nre-solving optimization problems at runtime. The resulting trajectories are not\nonly dynamically feasible but also provably safe with respect to both\nenvironmental constraints and inter-agent collisions. Simulation results\ndemonstrate the effectiveness of the approach in synthesizing synchronized,\nsafe trajectories for multiple agents under shared dynamics and constraints,\nusing only data and convex optimization tools.\n","authors":["Babak Esmaeili","Hamidreza Modares"],"pdf_url":"https://arxiv.org/pdf/2509.04413v1.pdf","comment":"Submitted to IEEE Transactions on Automation Science and Engineering"},{"id":"http://arxiv.org/abs/2504.03157v2","updated":"2025-09-04T17:21:58Z","published":"2025-04-04T04:30:55Z","title":"Taming High-Dimensional Dynamics: Learning Optimal Projections onto\n  Spectral Submanifolds","summary":"  High-dimensional nonlinear systems pose considerable challenges for modeling\nand control across many domains, from fluid mechanics to advanced robotics.\nSuch systems are typically approximated with reduced-order models, which often\nrely on orthogonal projections, a simplification that may lead to large\nprediction errors. In this work, we derive optimality of fiber-aligned\nprojections onto spectral submanifolds, preserving the nonlinear geometric\nstructure and minimizing long-term prediction error. We propose a data-driven\nprocedure to learn these projections from trajectories and demonstrate its\neffectiveness through a 180-dimensional robotic system. Our reduced-order\nmodels achieve up to fivefold improvement in trajectory tracking accuracy under\nmodel predictive control compared to the state of the art.\n","authors":["Hugo Buurmeijer","Luis A. Pabon","John Irvin Alora","Roshan S. Kaundinya","George Haller","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2504.03157v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04399v1","updated":"2025-09-04T17:10:15Z","published":"2025-09-04T17:10:15Z","title":"Leveraging Equivariances and Symmetries in the Control Barrier Function\n  Synthesis","summary":"  The synthesis of Control Barrier Functions (CBFs) often involves demanding\ncomputations or a meticulous construction. However, structural properties of\nthe system dynamics and constraints have the potential to mitigate these\nchallenges. In this paper, we explore how equivariances in the dynamics,\nloosely speaking a form of symmetry, can be leveraged in the CBF synthesis.\nAlthough CBFs are generally not inherently symmetric, we show how equivariances\nin the dynamics and symmetries in the constraints induce symmetries in CBFs\nderived through reachability analysis. This insight allows us to infer their\nCBF values across the entire domain from their values on a subset, leading to\nsignificant computational savings. Interestingly, equivariances can be even\nleveraged to the CBF synthesis for non-symmetric constraints. Specifically, we\nshow how a partially known CBF can be leveraged together with equivariances to\nconstruct a CBF for various new constraints. Throughout the paper, we provide\nexamples illustrating the theoretical findings. Furthermore, a numerical study\ninvestigates the computational gains from invoking equivariances into the CBF\nsynthesis.\n","authors":["Adrian Wiltz","Dimos V. Dimarogonas"],"pdf_url":"https://arxiv.org/pdf/2509.04399v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2508.17449v2","updated":"2025-09-04T16:46:34Z","published":"2025-08-24T17:01:15Z","title":"Robotic Manipulation via Imitation Learning: Taxonomy, Evolution,\n  Benchmark, and Challenges","summary":"  Robotic Manipulation (RM) is central to the advancement of autonomous robots,\nenabling them to interact with and manipulate objects in real-world\nenvironments. This survey focuses on RM methodologies that leverage imitation\nlearning, a powerful technique that allows robots to learn complex manipulation\nskills by mimicking human demonstrations. We identify and analyze the most\ninfluential studies in this domain, selected based on community impact and\nintrinsic quality. For each paper, we provide a structured summary, covering\nthe research purpose, technical implementation, hierarchical classification,\ninput formats, key priors, strengths and limitations, and citation metrics.\nAdditionally, we trace the chronological development of imitation learning\ntechniques within RM policy (RMP), offering a timeline of key technological\nadvancements. Where available, we report benchmark results and perform\nquantitative evaluations to compare existing methods. By synthesizing these\ninsights, this review provides a comprehensive resource for researchers and\npractitioners, highlighting both the state of the art and the challenges that\nlie ahead in the field of robotic manipulation through imitation learning.\n","authors":["Zezeng Li","Alexandre Chapin","Enda Xiang","Rui Yang","Bruno Machado","Na Lei","Emmanuel Dellandrea","Di Huang","Liming Chen"],"pdf_url":"https://arxiv.org/pdf/2508.17449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04358v1","updated":"2025-09-04T16:19:24Z","published":"2025-09-04T16:19:24Z","title":"Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the\n  Roles of Information Transparency, User Control, and Proactivity","summary":"  Social robots are increasingly recognized as valuable supporters in the field\nof well-being coaching. They can function as independent coaches or provide\nsupport alongside human coaches, and healthcare professionals. In coaching\ninteractions, these robots often handle sensitive information shared by users,\nmaking privacy a relevant issue. Despite this, little is known about the\nfactors that shape users' privacy perceptions. This research aims to examine\nthree key factors systematically: (1) the transparency about information usage,\n(2) the level of specific user control over how the robot uses their\ninformation, and (3) the robot's behavioral approach - whether it acts\nproactively or only responds on demand. Our results from an online study (N =\n200) show that even when users grant the robot general access to personal data,\nthey additionally expect the ability to explicitly control how that information\nis interpreted and shared during sessions. Experimental conditions that\nprovided such control received significantly higher ratings for perceived\nprivacy appropriateness and trust. Compared to user control, the effects of\ntransparency and proactivity on privacy appropriateness perception were low,\nand we found no significant impact. The results suggest that merely informing\nusers or proactive sharing is insufficient without accompanying user control.\nThese insights underscore the need for further research on mechanisms that\nallow users to manage robots' information processing and sharing, especially\nwhen social robots take on more proactive roles alongside humans.\n","authors":["Atikkhan Faridkhan Nilgar","Manuel Dietrich","Kristof Van Laerhoven"],"pdf_url":"https://arxiv.org/pdf/2509.04358v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04356v1","updated":"2025-09-04T16:18:04Z","published":"2025-09-04T16:18:04Z","title":"SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic\n  Avatars","summary":"  We present SRWToolkit, an open-source Wizard of Oz toolkit designed to\nfacilitate the rapid prototyping of social robotic avatars powered by local\nlarge language models (LLMs). Our web-based toolkit enables multimodal\ninteraction through text input, button-activated speech, and wake-word command.\nThe toolkit offers real-time configuration of avatar appearance, behavior,\nlanguage, and voice via an intuitive control panel. In contrast to prior works\nthat rely on cloud-based LLM services, SRWToolkit emphasizes modularity and\nensures on-device functionality through local LLM inference. In our small-scale\nuser study ($n=11$), participants created and interacted with diverse robotic\nroles (hospital receptionist, mathematics teacher, and driving assistant),\nwhich demonstrated positive outcomes in the toolkit's usability, trust, and\nuser experience. The toolkit enables rapid and efficient development of robot\ncharacters customized to researchers' needs, supporting scalable research in\nhuman-robot interaction.\n","authors":["Atikkhan Faridkhan Nilgar","Kristof Van Laerhoven","Ayub Kinoti"],"pdf_url":"https://arxiv.org/pdf/2509.04356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04324v1","updated":"2025-09-04T15:42:36Z","published":"2025-09-04T15:42:36Z","title":"OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent\n  Detection","summary":"  Grasping assistance is essential for restoring autonomy in individuals with\nmotor impairments, particularly in unstructured environments where object\ncategories and user intentions are diverse and unpredictable. We present\nOVGrasp, a hierarchical control framework for soft exoskeleton-based grasp\nassistance that integrates RGB-D vision, open-vocabulary prompts, and voice\ncommands to enable robust multimodal interaction. To enhance generalization in\nopen environments, OVGrasp incorporates a vision-language foundation model with\nan open-vocabulary mechanism, allowing zero-shot detection of previously unseen\nobjects without retraining. A multimodal decision-maker further fuses spatial\nand linguistic cues to infer user intent, such as grasp or release, in\nmulti-object scenarios. We deploy the complete framework on a custom\negocentric-view wearable exoskeleton and conduct systematic evaluations on 15\nobjects across three grasp types. Experimental results with ten participants\ndemonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,\noutperforming state-of-the-art baselines and achieving improved kinematic\nalignment with natural hand motion.\n","authors":["Chen Hu","Shan Luo","Letizia Gionfrida"],"pdf_url":"https://arxiv.org/pdf/2509.04324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.05997v2","updated":"2025-09-04T14:30:12Z","published":"2025-06-06T11:35:48Z","title":"Spatially-Enhanced Recurrent Memory for Long-Range Mapless Navigation\n  via End-to-End Reinforcement Learning","summary":"  Recent advancements in robot navigation, particularly with end-to-end\nlearning approaches such as reinforcement learning (RL), have demonstrated\nstrong performance. However, successful navigation still depends on two key\ncapabilities: mapping and planning (explicitly or implicitly). Classical\napproaches rely on explicit mapping pipelines to register egocentric\nobservations into a coherent map. In contrast, end-to-end learning often\nachieves this implicitly -- through recurrent neural networks (RNNs) that fuse\ncurrent and historical observations into a latent space for planning. While\nexisting architectures, such as LSTM and GRU, can capture temporal\ndependencies, our findings reveal a critical limitation: their inability to\neffectively perform spatial memorization. This capability is essential for\nintegrating sequential observations from varying perspectives to build spatial\nrepresentations that support planning. To address this, we propose\nSpatially-Enhanced Recurrent Units (SRUs) -- a simple yet effective\nmodification to existing RNNs -- that enhance spatial memorization. We further\nintroduce an attention-based network architecture integrated with SRUs,\nenabling long-range mapless navigation using a single forward-facing stereo\ncamera. We also employ regularization techniques to facilitate robust\nend-to-end recurrent training via RL. Experimental results show 23.5% overall\nimprovement in long-range navigation compared to existing RNNs. With SRU\nmemory, our method outperforms RL baselines -- one relying on explicit mapping\nand the other on stacked historical observations -- by 29.6% and 105.0%,\nrespectively, across diverse environments requiring long-horizon mapping and\nmemorization. Finally, we address the sim-to-real gap by leveraging large-scale\npretraining on synthetic depth data, enabling zero-shot transfer for deployment\nacross diverse and complex real-world environments.\n","authors":["Fan Yang","Per Frivik","David Hoeller","Chen Wang","Cesar Cadena","Marco Hutter"],"pdf_url":"https://arxiv.org/pdf/2506.05997v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2407.15161v4","updated":"2025-09-04T14:07:56Z","published":"2024-07-21T13:33:08Z","title":"FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via\n  Flow Variational Inference","summary":"  Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from\npartial observations remains a critical challenge in robot learning. Prior\ngenerative methods struggle to model the intricate grasp distribution of\ndexterous hands and often fail to reason about shape uncertainty inherent in\npartial point clouds, leading to unreliable or overly conservative grasps. We\npropose FFHFlow, a flow-based variational framework that generates diverse,\nrobust multi-finger grasps while explicitly quantifying perceptual uncertainty\nin the partial point clouds. Our approach leverages a normalizing flow-based\ndeep latent variable model to learn a hierarchical grasp manifold, overcoming\nthe mode collapse and rigid prior limitations of conditional Variational\nAutoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of\nflows, FFHFlow introspects shape uncertainty in partial observations and\nidentifies novel object structures, enabling risk-aware grasp synthesis. To\nfurther enhance reliability, we integrate a discriminative grasp evaluator with\nthe flow likelihoods, formulating an uncertainty-aware ranking strategy that\nprioritizes grasps robust to shape ambiguity. Extensive experiments in\nsimulation and real-world setups demonstrate that FFHFlow outperforms\nstate-of-the-art baselines (including diffusion models) in grasp diversity and\nsuccess rate, while achieving run-time efficient sampling. We also showcase its\npractical value in cluttered and confined environments, where diversity-driven\nsampling excels by mitigating collisions (Project Page:\nhttps://sites.google.com/view/ffhflow/home/).\n","authors":["Qian Feng","Jianxiang Feng","Zhaopeng Chen","Rudolph Triebel","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2407.15161v4.pdf","comment":"First two authors contributed equally, whose ordering decided via\n  coin-tossing. Accepted for CoRL 2025"},{"id":"http://arxiv.org/abs/2509.04220v1","updated":"2025-09-04T13:52:11Z","published":"2025-09-04T13:52:11Z","title":"Compatibility of Multiple Control Barrier Functions for Constrained\n  Nonlinear Systems","summary":"  Control barrier functions (CBFs) are a powerful tool for the constrained\ncontrol of nonlinear systems; however, the majority of results in the\nliterature focus on systems subject to a single CBF constraint, making it\nchallenging to synthesize provably safe controllers that handle multiple state\nconstraints. This paper presents a framework for constrained control of\nnonlinear systems subject to box constraints on the systems' vector-valued\noutputs using multiple CBFs. Our results illustrate that when the output has a\nvector relative degree, the CBF constraints encoding these box constraints are\ncompatible, and the resulting optimization-based controller is locally\nLipschitz continuous and admits a closed-form expression. Additional results\nare presented to characterize the degradation of nominal tracking objectives in\nthe presence of safety constraints. Simulations of a planar quadrotor are\npresented to demonstrate the efficacy of the proposed framework.\n","authors":["Max H. Cohen","Eugene Lavretsky","Aaron D. Ames"],"pdf_url":"https://arxiv.org/pdf/2509.04220v1.pdf","comment":"To appear at IEEE CDC 2025"},{"id":"http://arxiv.org/abs/2508.21043v2","updated":"2025-09-04T13:32:15Z","published":"2025-08-28T17:49:12Z","title":"HITTER: A HumanoId Table TEnnis Robot via Hierarchical Planning and\n  Learning","summary":"  Humanoid robots have recently achieved impressive progress in locomotion and\nwhole-body control, yet they remain constrained in tasks that demand rapid\ninteraction with dynamic environments through manipulation. Table tennis\nexemplifies such a challenge: with ball speeds exceeding 5 m/s, players must\nperceive, predict, and act within sub-second reaction times, requiring both\nagility and precision. To address this, we present a hierarchical framework for\nhumanoid table tennis that integrates a model-based planner for ball trajectory\nprediction and racket target planning with a reinforcement learning-based\nwhole-body controller. The planner determines striking position, velocity and\ntiming, while the controller generates coordinated arm and leg motions that\nmimic human strikes and maintain stability and agility across consecutive\nrallies. Moreover, to encourage natural movements, human motion references are\nincorporated during training. We validate our system on a general-purpose\nhumanoid robot, achieving up to 106 consecutive shots with a human opponent and\nsustained exchanges against another humanoid. These results demonstrate\nreal-world humanoid table tennis with sub-second reactive control, marking a\nstep toward agile and interactive humanoid behaviors.\n","authors":["Zhi Su","Bike Zhang","Nima Rahmanian","Yuman Gao","Qiayuan Liao","Caitlin Regan","Koushil Sreenath","S. Shankar Sastry"],"pdf_url":"https://arxiv.org/pdf/2508.21043v2.pdf","comment":"add more references"},{"id":"http://arxiv.org/abs/2509.00215v2","updated":"2025-09-04T12:46:04Z","published":"2025-08-29T19:55:25Z","title":"First Order Model-Based RL through Decoupled Backpropagation","summary":"  There is growing interest in reinforcement learning (RL) methods that\nleverage the simulator's derivatives to improve learning efficiency. While\nearly gradient-based approaches have demonstrated superior performance compared\nto derivative-free methods, accessing simulator gradients is often impractical\ndue to their implementation cost or unavailability. Model-based RL (MBRL) can\napproximate these gradients via learned dynamics models, but the solver\nefficiency suffers from compounding prediction errors during training rollouts,\nwhich can degrade policy performance. We propose an approach that decouples\ntrajectory generation from gradient computation: trajectories are unrolled\nusing a simulator, while gradients are computed via backpropagation through a\nlearned differentiable model of the simulator. This hybrid design enables\nefficient and consistent first-order policy optimization, even when simulator\ngradients are unavailable, as well as learning a critic from simulation\nrollouts, which is more accurate. Our method achieves the sample efficiency and\nspeed of specialized optimizers such as SHAC, while maintaining the generality\nof standard approaches like PPO and avoiding ill behaviors observed in other\nfirst-order MBRL methods. We empirically validate our algorithm on benchmark\ncontrol tasks and demonstrate its effectiveness on a real Go2 quadruped robot,\nacross both quadrupedal and bipedal locomotion tasks.\n","authors":["Joseph Amigo","Rooholla Khorrambakht","Elliot Chane-Sane","Nicolas Mansard","Ludovic Righetti"],"pdf_url":"https://arxiv.org/pdf/2509.00215v2.pdf","comment":"CoRL 2025. Project website: https://machines-in-motion.github.io/DMO/"},{"id":"http://arxiv.org/abs/2509.04156v1","updated":"2025-09-04T12:32:04Z","published":"2025-09-04T12:32:04Z","title":"YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind\n  Turbine Components","summary":"  Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up\nnew opportunities for monitoring wind power plants, including blades, towers,\nand other critical components. However, reliable defect detection requires\nhigh-resolution data and efficient methods to process multispectral imagery. In\nthis research, we aim to enhance defect detection accuracy through the\ndevelopment of an ensemble of YOLO-based deep learning models that integrate\nboth visible and thermal channels. We propose an ensemble approach that\nintegrates a general-purpose YOLOv8 model with a specialized thermal model,\nusing a sophisticated bounding box fusion algorithm to combine their\npredictions. Our experiments show this approach achieves a mean Average\nPrecision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone\nYOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that\ncombining multiple YOLO architectures with fused multispectral data provides a\nmore reliable solution, improving the detection of both visual and thermal\ndefects.\n","authors":["Serhii Svystun","Pavlo Radiuk","Oleksandr Melnychenko","Oleg Savenko","Anatoliy Sachenko"],"pdf_url":"https://arxiv.org/pdf/2509.04156v1.pdf","comment":"The 13th IEEE International Conference on Intelligent Data\n  Acquisition and Advanced Computing Systems: Technology and Applications, 4-6\n  September, 2025, Gliwice, Poland"},{"id":"http://arxiv.org/abs/2509.02478v2","updated":"2025-09-04T12:09:21Z","published":"2025-09-02T16:29:06Z","title":"Classification of Vision-Based Tactile Sensors: A Review","summary":"  Vision-based tactile sensors (VBTS) have gained widespread application in\nrobotic hands, grippers and prosthetics due to their high spatial resolution,\nlow manufacturing costs, and ease of customization. While VBTSs have common\ndesign features, such as a camera module, they can differ in a rich diversity\nof sensing principles, material compositions, multimodal approaches, and data\ninterpretation methods. Here, we propose a novel classification of VBTS that\ncategorizes the technology into two primary sensing principles based on the\nunderlying transduction of contact into a tactile image: the Marker-Based\nTransduction Principle and the Intensity-Based Transduction Principle.\nMarker-Based Transduction interprets tactile information by detecting marker\ndisplacement and changes in marker density. In contrast, Intensity-Based\nTransduction maps external disturbances with variations in pixel values.\nDepending on the design of the contact module, Marker-Based Transduction can be\nfurther divided into two subtypes: Simple Marker-Based (SMB) and Morphological\nMarker-Based (MMB) mechanisms. Similarly, the Intensity-Based Transduction\nPrinciple encompasses the Reflective Layer-based (RLB) and Transparent\nLayer-Based (TLB) mechanisms. This paper provides a comparative study of the\nhardware characteristics of these four types of sensors including various\ncombination types, and discusses the commonly used methods for interpreting\ntactile information. This~comparison reveals some current challenges faced by\nVBTS technology and directions for future research.\n","authors":["Haoran Li","Yijiong Lin","Chenghua Lu","Max Yang","Efi Psomopoulou","Nathan F Lepora"],"pdf_url":"https://arxiv.org/pdf/2509.02478v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2509.04119v1","updated":"2025-09-04T11:33:53Z","published":"2025-09-04T11:33:53Z","title":"Lightweight Kinematic and Static Modeling of Cable-Driven Continuum\n  Robots via Actuation-Space Energy Formulation","summary":"  Continuum robots, inspired by octopus arms and elephant trunks, combine\ndexterity with intrinsic compliance, making them well suited for unstructured\nand confined environments. Yet their continuously deformable morphology poses\nchallenges for motion planning and control, calling for accurate but\nlightweight models. We propose the Lightweight Actuation Space Energy Modeling\n(LASEM) framework for cable driven continuum robots, which formulates actuation\npotential energy directly in actuation space. LASEM yields an analytical\nforward model derived from geometrically nonlinear beam and rod theories via\nHamilton's principle, while avoiding explicit modeling of cable backbone\ncontact. It accepts both force and displacement inputs, thereby unifying\nkinematic and static formulations. Assuming the friction is neglected, the\nframework generalizes to nonuniform geometries, arbitrary cable routings,\ndistributed loading and axial extensibility, while remaining computationally\nefficient for real-time use. Numerical simulations validate its accuracy, and a\nsemi-analytical iterative scheme is developed for inverse kinematics. To\naddress discretization in practical robots, LASEM further reformulates the\nfunctional minimization as a numerical optimization, which also naturally\nincorporates cable potential energy without explicit contact modeling.\n","authors":["Ke Wu","Yuhao Wang","Kevin Henry","Cesare Stefanini","Gang Zheng"],"pdf_url":"https://arxiv.org/pdf/2509.04119v1.pdf","comment":"Journal"},{"id":"http://arxiv.org/abs/2509.04095v1","updated":"2025-09-04T10:53:27Z","published":"2025-09-04T10:53:27Z","title":"Cloud-Assisted Remote Control for Aerial Robots: From Theory to\n  Proof-of-Concept Implementation","summary":"  Cloud robotics has emerged as a promising technology for robotics\napplications due to its advantages of offloading computationally intensive\ntasks, facilitating data sharing, and enhancing robot coordination. However,\nintegrating cloud computing with robotics remains a complex challenge due to\nnetwork latency, security concerns, and the need for efficient resource\nmanagement. In this work, we present a scalable and intuitive framework for\ntesting cloud and edge robotic systems. The framework consists of two main\ncomponents enabled by containerized technology: (a) a containerized cloud\ncluster and (b) the containerized robot simulation environment. The system\nincorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling\nbidirectional communication between the cloud cluster container and the robot\nsimulation environment, while simulating realistic network conditions. To\nachieve this, we consider the use case of cloud-assisted remote control for\naerial robots, while utilizing Linux-based traffic control to introduce\nartificial delay and jitter, replicating variable network conditions\nencountered in practical cloud-robot deployments.\n","authors":["Achilleas Santi Seisa","Viswa Narayanan Sankaranarayanan","Gerasimos Damigos","Sumeet Gajanan Satpute","George Nikolakopoulos"],"pdf_url":"https://arxiv.org/pdf/2509.04095v1.pdf","comment":"6 pages, 7 figures, CCGridW 2025"},{"id":"http://arxiv.org/abs/2509.04094v1","updated":"2025-09-04T10:52:27Z","published":"2025-09-04T10:52:27Z","title":"Object-Reconstruction-Aware Whole-body Control of Mobile Manipulators","summary":"  Object reconstruction and inspection tasks play a crucial role in various\nrobotics applications. Identifying paths that reveal the most unknown areas of\nthe object becomes paramount in this context, as it directly affects\nefficiency, and this problem is known as the view path planning problem.\nCurrent methods often use sampling-based path planning techniques, evaluating\npotential views along the path to enhance reconstruction performance. However,\nthese methods are computationally expensive as they require evaluating several\ncandidate views on the path. To this end, we propose a computationally\nefficient solution that relies on calculating a focus point in the most\ninformative (unknown) region and having the robot maintain this point in the\ncamera field of view along the path. We incorporated this strategy into the\nwhole-body control of a mobile manipulator employing a visibility constraint\nwithout the need for an additional path planner. We conducted comprehensive and\nrealistic simulations using a large dataset of 114 diverse objects of varying\nsizes from 57 categories to compare our method with a sampling-based planning\nstrategy using Bayesian data analysis. Furthermore, we performed real-world\nexperiments with an 8-DoF mobile manipulator to demonstrate the proposed\nmethod's performance in practice. Our results suggest that there is no\nsignificant difference in object coverage and entropy. In contrast, our method\nis approximately nine times faster than the baseline sampling-based method in\nterms of the average time the robot spends between views.\n","authors":["Fatih Dursun","Bruno Vilhena Adorno","Simon Watson","Wei Pan"],"pdf_url":"https://arxiv.org/pdf/2509.04094v1.pdf","comment":"14 pages, 13 figures, 3 tables. Under Review for the IEEE\n  Transactions on Robotics (T-RO)"},{"id":"http://arxiv.org/abs/2509.04076v1","updated":"2025-09-04T10:11:51Z","published":"2025-09-04T10:11:51Z","title":"Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot","summary":"  We propose a novel diffusion-based action model for robotic motion planning.\nCommonly, established numerical planning approaches are used to solve general\nmotion planning problems, but have significant runtime requirements. By\nleveraging the power of deep learning, we are able to achieve good results in a\nmuch smaller runtime by learning from a dataset generated by these planners.\nWhile our initial model uses point cloud embeddings in the input to predict\nkeypoint-based joint sequences in its output, we observed in our ablation study\nthat it remained challenging to condition the network on the point cloud\nembeddings. We identified some biases in our dataset and refined it, which\nimproved the model's performance. Our model, even without the use of the point\ncloud encodings, outperforms numerical models by an order of magnitude\nregarding the runtime, while reaching a success rate of up to 90% of collision\nfree solutions on the test set.\n","authors":["Lennart Clasmeier","Jan-Gerrit Habekost","Connor Gäde","Philipp Allgeuer","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2509.04076v1.pdf","comment":"Submitted to ICANN 20255 Special Session on Neural Robotics"},{"id":"http://arxiv.org/abs/2509.04069v1","updated":"2025-09-04T10:02:32Z","published":"2025-09-04T10:02:32Z","title":"Solving Robotics Tasks with Prior Demonstration via\n  Exploration-Efficient Deep Reinforcement Learning","summary":"  This paper proposes an exploration-efficient Deep Reinforcement Learning with\nReference policy (DRLR) framework for learning robotics tasks that incorporates\ndemonstrations. The DRLR framework is developed based on an algorithm called\nImitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve\nIBRL by modifying the action selection module. The proposed action selection\nmodule provides a calibrated Q-value, which mitigates the bootstrapping error\nthat otherwise leads to inefficient exploration. Furthermore, to prevent the RL\npolicy from converging to a sub-optimal policy, SAC is used as the RL policy\ninstead of TD3. The effectiveness of our method in mitigating bootstrapping\nerror and preventing overfitting is empirically validated by learning two\nrobotics tasks: bucket loading and open drawer, which require extensive\ninteractions with the environment. Simulation results also demonstrate the\nrobustness of the DRLR framework across tasks with both low and high\nstate-action dimensions, and varying demonstration qualities. To evaluate the\ndeveloped framework on a real-world industrial robotics task, the bucket\nloading task is deployed on a real wheel loader. The sim2real results validate\nthe successful deployment of the DRLR framework.\n","authors":["Chengyandan Shen","Christoffer Sloth"],"pdf_url":"https://arxiv.org/pdf/2509.04069v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.04421v4","updated":"2025-09-04T10:00:39Z","published":"2025-04-06T09:07:10Z","title":"Deliberate Planning of 3D Bin Packing on Packing Configuration Trees","summary":"  Online 3D Bin Packing Problem (3D-BPP) has widespread applications in\nindustrial automation. Existing methods usually solve the problem with limited\nresolution of spatial discretization, and/or cannot deal with complex practical\nconstraints well. We propose to enhance the practical applicability of online\n3D-BPP via learning on a novel hierarchical representation, packing\nconfiguration tree (PCT). PCT is a full-fledged description of the state and\naction space of bin packing which can support packing policy learning based on\ndeep reinforcement learning (DRL). The size of the packing action space is\nproportional to the number of leaf nodes, making the DRL model easy to train\nand well-performing even with continuous solution space. We further discover\nthe potential of PCT as tree-based planners in deliberately solving packing\nproblems of industrial significance, including large-scale packing and\ndifferent variations of BPP setting. A recursive packing method is proposed to\ndecompose large-scale packing into smaller sub-trees while a spatial ensemble\nmechanism integrates local solutions into global. For different BPP variations\nwith additional decision variables, such as lookahead, buffering, and offline\npacking, we propose a unified planning framework enabling out-of-the-box\nproblem solving. Extensive evaluations demonstrate that our method outperforms\nexisting online BPP baselines and is versatile in incorporating various\npractical constraints. The planning process excels across large-scale problems\nand diverse problem variations. We develop a real-world packing robot for\nindustrial warehousing, with careful designs accounting for constrained\nplacement and transportation stability. Our packing robot operates reliably and\nefficiently on unprotected pallets at 10 seconds per box. It achieves averagely\n19 boxes per pallet with 57.4% space utilization for relatively large-size\nboxes.\n","authors":["Hang Zhao","Juzhan Xu","Kexiong Yu","Ruizhen Hu","Chenyang Zhu","Bo Du","Kai Xu"],"pdf_url":"https://arxiv.org/pdf/2504.04421v4.pdf","comment":"International Journal of Robotics Research"},{"id":"http://arxiv.org/abs/2509.04063v1","updated":"2025-09-04T09:48:43Z","published":"2025-09-04T09:48:43Z","title":"Balancing Signal and Variance: Adaptive Offline RL Post-Training for VLA\n  Flow Models","summary":"  Vision-Language-Action (VLA) models based on flow matching have shown\nexcellent performance in general-purpose robotic manipulation tasks. However,\nthe action accuracy of these models on complex downstream tasks is\nunsatisfactory. One important reason is that these models rely solely on the\npost-training paradigm of imitation learning, which makes it difficult to have\na deeper understanding of the distribution properties of data quality, which is\nexactly what Reinforcement Learning (RL) excels at. In this paper, we\ntheoretically propose an offline RL post-training objective for VLA flow models\nand induce an efficient and feasible offline RL fine-tuning algorithm --\nAdaptive Reinforced Flow Matching (ARFM). By introducing an adaptively adjusted\nscaling factor in the VLA flow model loss, we construct a principled\nbias-variance trade-off objective function to optimally control the impact of\nRL signal on flow loss. ARFM adaptively balances RL advantage preservation and\nflow loss gradient variance control, resulting in a more stable and efficient\nfine-tuning process. Extensive simulation and real-world experimental results\nshow that ARFM exhibits excellent generalization, robustness, few-shot\nlearning, and continuous learning performance.\n","authors":["Hongyin Zhang","Shiyuan Zhang","Junxi Jin","Qixin Zeng","Yifan Qiao","Hongchao Lu","Donglin Wang"],"pdf_url":"https://arxiv.org/pdf/2509.04063v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05041v2","updated":"2025-09-04T09:47:45Z","published":"2025-04-07T13:07:17Z","title":"Segmented Trajectory Optimization for Autonomous Parking in Unstructured\n  Environments","summary":"  This paper presents a Segmented Trajectory Optimization (STO) method for\nautonomous parking, which refines an initial trajectory into a dynamically\nfeasible and collision-free one using an iterative SQP-based approach. STO\nmaintains the maneuver strategy of the high-level global planner while allowing\ncurvature discontinuities at switching points to improve maneuver efficiency.\nTo ensure safety, a convex corridor is constructed via GJK-accelerated ellipse\nshrinking and expansion, serving as safety constraints in each iteration.\nNumerical simulations in perpendicular and reverse-angled parking scenarios\ndemonstrate that STO enhances maneuver efficiency while ensuring safety.\nMoreover, computational performance confirms its practicality for real-world\napplications.\n","authors":["Hang Yu","Renjie Li"],"pdf_url":"https://arxiv.org/pdf/2504.05041v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2509.04061v1","updated":"2025-09-04T09:47:10Z","published":"2025-09-04T09:47:10Z","title":"Integrated Wheel Sensor Communication using ESP32 -- A Contribution\n  towards a Digital Twin of the Road System","summary":"  While current onboard state estimation methods are adequate for most driving\nand safety-related applications, they do not provide insights into the\ninteraction between tires and road surfaces. This paper explores a novel\ncommunication concept for efficiently transmitting integrated wheel sensor data\nfrom an ESP32 microcontroller. Our proposed approach utilizes a\npublish-subscribe system, surpassing comparable solutions in the literature\nregarding data transmission volume. We tested this approach on a drum tire test\nrig with our prototype sensors system utilizing a diverse selection of sample\nfrequencies between 1 Hz and 32 000 Hz to demonstrate the efficacy of our\ncommunication concept. The implemented prototype sensor showcases minimal data\nloss, approximately 0.1 % of the sampled data, validating the reliability of\nour developed communication system. This work contributes to advancing\nreal-time data acquisition, providing insights into optimizing integrated wheel\nsensor communication.\n","authors":["Ventseslav Yordanov","Simon Schäfer","Alexander Mann","Stefan Kowalewski","Bassam Alrifaee","Lutz Eckstein"],"pdf_url":"https://arxiv.org/pdf/2509.04061v1.pdf","comment":"6 pages, 2 figures, this work was submitted to and accepted by IEEE\n  International Conference on Intelligent Transportation Systems (ITSC) 2025"},{"id":"http://arxiv.org/abs/2509.04018v1","updated":"2025-09-04T08:47:26Z","published":"2025-09-04T08:47:26Z","title":"FPC-VLA: A Vision-Language-Action Framework with a Supervisor for\n  Failure Prediction and Correction","summary":"  Robotic manipulation is a fundamental component of automation. However,\ntraditional perception-planning pipelines often fall short in open-ended tasks\ndue to limited flexibility, while the architecture of a single end-to-end\nVision-Language-Action (VLA) offers promising capabilities but lacks crucial\nmechanisms for anticipating and recovering from failure. To address these\nchallenges, we propose FPC-VLA, a dual-model framework that integrates VLA with\na supervisor for failure prediction and correction. The supervisor evaluates\naction viability through vision-language queries and generates corrective\nstrategies when risks arise, trained efficiently without manual labeling. A\nsimilarity-guided fusion module further refines actions by leveraging past\npredictions. Evaluation results on multiple simulation platforms (SIMPLER and\nLIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA\noutperforms state-of-the-art models in both zero-shot and fine-tuned settings.\nBy activating the supervisor only at keyframes, our approach significantly\nincreases task success rates with minimal impact on execution time. Successful\nreal-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong\ngeneralization and practical utility for building more reliable autonomous\nsystems.\n","authors":["Yifan Yang","Zhixiang Duan","Tianshi Xie","Fuyu Cao","Pinxi Shen","Peili Song","Piaopiao Jin","Guokang Sun","Shaoqing Xu","Yangwei You","Jingtai Liu"],"pdf_url":"https://arxiv.org/pdf/2509.04018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04016v1","updated":"2025-09-04T08:44:36Z","published":"2025-09-04T08:44:36Z","title":"Odometry Calibration and Pose Estimation of a 4WIS4WID Mobile Wall\n  Climbing Robot","summary":"  This paper presents the design of a pose estimator for a four wheel\nindependent steer four wheel independent drive (4WIS4WID) wall climbing mobile\nrobot, based on the fusion of multimodal measurements, including wheel\nodometry, visual odometry, and an inertial measurement unit (IMU) data using\nExtended Kalman Filter (EKF) and Unscented Kalman Filter (UKF). The pose\nestimator is a critical component of wall climbing mobile robots, as their\noperational environment involves carrying precise measurement equipment and\nmaintenance tools in construction, requiring information about pose on the\nbuilding at the time of measurement. Due to the complex geometry and material\nproperties of building facades, the use of traditional localization sensors\nsuch as laser, ultrasonic, or radar is often infeasible for wall-climbing\nrobots. Moreover, GPS-based localization is generally unreliable in these\nenvironments because of signal degradation caused by reinforced concrete and\nelectromagnetic interference. Consequently, robot odometry remains the primary\nsource of velocity and position information, despite being susceptible to drift\ncaused by both systematic and non-systematic errors. The calibrations of the\nrobot's systematic parameters were conducted using nonlinear optimization and\nLevenberg-Marquardt methods as Newton-Gauss and gradient-based model fitting\nmethods, while Genetic algorithm and Particle swarm were used as\nstochastic-based methods for kinematic parameter calibration. Performance and\nresults of the calibration methods and pose estimators were validated in detail\nwith experiments on the experimental mobile wall climbing robot.\n","authors":["Branimir Ćaran","Vladimir Milić","Marko Švaco","Bojan Jerbić"],"pdf_url":"https://arxiv.org/pdf/2509.04016v1.pdf","comment":"ACCEPTED FOR IEEE EUROPEAN CONFERENCE ON MOBILE ROBOTS 2025. PREPRINT\n  VERSION. ACCEPTED JUNE, 2025 AND PRESENTED SEPTEMBER, 2025"},{"id":"http://arxiv.org/abs/2405.20321v2","updated":"2025-09-04T08:23:37Z","published":"2024-05-30T17:56:54Z","title":"Vision-based Manipulation from Single Human Video with Open-World Object\n  Graphs","summary":"  This work presents an object-centric approach to learning vision-based\nmanipulation skills from human videos. We investigate the problem of robot\nmanipulation via imitation in the open-world setting, where a robot learns to\nmanipulate novel objects from a single video demonstration. We introduce ORION,\nan algorithm that tackles the problem by extracting an object-centric\nmanipulation plan from a single RGB or RGB-D video and deriving a policy that\nconditions on the extracted plan. Our method enables the robot to learn from\nvideos captured by daily mobile devices and to generalize the policies to\ndeployment environments with varying visual backgrounds, camera angles, spatial\nlayouts, and novel object instances. We systematically evaluate our method on\nboth short-horizon and long-horizon tasks, using RGB-D and RGB-only\ndemonstration videos. Across varied tasks and demonstration types (RGB-D /\nRGB), we observe an average success rate of 74.4%, demonstrating the efficacy\nof ORION in learning from a single human video in the open world. Additional\nmaterials can be found on our project website:\nhttps://ut-austin-rpl.github.io/ORION-release.\n","authors":["Yifeng Zhu","Arisrei Lim","Peter Stone","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2405.20321v2.pdf","comment":"Extended version of paper adding results with RGB-only demonstration\n  videos uploaded on 09/04/2025"},{"id":"http://arxiv.org/abs/2509.02760v2","updated":"2025-09-04T07:45:49Z","published":"2025-09-02T19:06:25Z","title":"A Digital Twin for Robotic Post Mortem Tissue Sampling using Virtual\n  Reality","summary":"  Studying tissue samples obtained during autopsies is the gold standard when\ndiagnosing the cause of death and for understanding disease pathophysiology.\nRecently, the interest in post mortem minimally invasive biopsies has grown\nwhich is a less destructive approach in comparison to an open autopsy and\nreduces the risk of infection. While manual biopsies under ultrasound guidance\nare more widely performed, robotic post mortem biopsies have been recently\nproposed. This approach can further reduce the risk of infection for\nphysicians. However, planning of the procedure and control of the robot need to\nbe efficient and usable. We explore a virtual reality setup with a digital twin\nto realize fully remote planning and control of robotic post mortem biopsies.\nThe setup is evaluated with forensic pathologists in a usability study for\nthree interaction methods. Furthermore, we evaluate clinical feasibility and\nevaluate the system with three human cadavers. Overall, 132 needle insertions\nwere performed with an off-axis needle placement error of 5.30+-3.25 mm. Tissue\nsamples were successfully biopsied and histopathologically verified. Users\nreported a very intuitive needle placement approach, indicating that the system\nis a promising, precise, and low-risk alternative to conventional approaches.\n","authors":["Maximilian Neidhardt","Ludwig Bosse","Vidas Raudonis","Kristina Allgoewer","Axel Heinemann","Benjamin Ondruschka","Alexander Schlaefer"],"pdf_url":"https://arxiv.org/pdf/2509.02760v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03889v1","updated":"2025-09-04T05:16:56Z","published":"2025-09-04T05:16:56Z","title":"Reactive In-Air Clothing Manipulation with Confidence-Aware Dense\n  Correspondence and Visuotactile Affordance","summary":"  Manipulating clothing is challenging due to complex configurations, variable\nmaterial dynamics, and frequent self-occlusion. Prior systems often flatten\ngarments or assume visibility of key features. We present a dual-arm\nvisuotactile framework that combines confidence-aware dense visual\ncorrespondence and tactile-supervised grasp affordance to operate directly on\ncrumpled and suspended garments. The correspondence model is trained on a\ncustom, high-fidelity simulated dataset using a distributional loss that\ncaptures cloth symmetries and generates correspondence confidence estimates.\nThese estimates guide a reactive state machine that adapts folding strategies\nbased on perceptual uncertainty. In parallel, a visuotactile grasp affordance\nnetwork, self-supervised using high-resolution tactile feedback, determines\nwhich regions are physically graspable. The same tactile classifier is used\nduring execution for real-time grasp validation. By deferring action in\nlow-confidence states, the system handles highly occluded table-top and in-air\nconfigurations. We demonstrate our task-agnostic grasp selection module in\nfolding and hanging tasks. Moreover, our dense descriptors provide a reusable\nintermediate representation for other planning modalities, such as extracting\ngrasp targets from human video demonstrations, paving the way for more\ngeneralizable and scalable garment manipulation.\n","authors":["Neha Sunil","Megha Tippur","Arnau Saumell","Edward Adelson","Alberto Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2509.03889v1.pdf","comment":"Accepted at CoRL 2025. Project website:\n  https://mhtippur.github.io/inairclothmanipulation/"},{"id":"http://arxiv.org/abs/2506.14317v3","updated":"2025-09-04T05:12:41Z","published":"2025-06-17T08:50:49Z","title":"ClutterDexGrasp: A Sim-to-Real System for General Dexterous Grasping in\n  Cluttered Scenes","summary":"  Dexterous grasping in cluttered scenes presents significant challenges due to\ndiverse object geometries, occlusions, and potential collisions. Existing\nmethods primarily focus on single-object grasping or grasp-pose prediction\nwithout interaction, which are insufficient for complex, cluttered scenes.\nRecent vision-language-action models offer a potential solution but require\nextensive real-world demonstrations, making them costly and difficult to scale.\nTo address these limitations, we revisit the sim-to-real transfer pipeline and\ndevelop key techniques that enable zero-shot deployment in reality while\nmaintaining robust generalization. We propose ClutterDexGrasp, a two-stage\nteacher-student framework for closed-loop target-oriented dexterous grasping in\ncluttered scenes. The framework features a teacher policy trained in simulation\nusing clutter density curriculum learning, incorporating both a geometry and\nspatially-embedded scene representation and a novel comprehensive safety\ncurriculum, enabling general, dynamic, and safe grasping behaviors. Through\nimitation learning, we distill the teacher's knowledge into a student 3D\ndiffusion policy (DP3) that operates on partial point cloud observations. To\nthe best of our knowledge, this represents the first zero-shot sim-to-real\nclosed-loop system for target-oriented dexterous grasping in cluttered scenes,\ndemonstrating robust performance across diverse objects and layouts. More\ndetails and videos are available at https://clutterdexgrasp.github.io/.\n","authors":["Zeyuan Chen","Qiyang Yan","Yuanpei Chen","Tianhao Wu","Jiyao Zhang","Zihan Ding","Jinzhou Li","Yaodong Yang","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2506.14317v3.pdf","comment":"Accepted at CoRL 2025"},{"id":"http://arxiv.org/abs/2509.03859v1","updated":"2025-09-04T03:36:07Z","published":"2025-09-04T03:36:07Z","title":"Learning Multi-Stage Pick-and-Place with a Legged Mobile Manipulator","summary":"  Quadruped-based mobile manipulation presents significant challenges in\nrobotics due to the diversity of required skills, the extended task horizon,\nand partial observability. After presenting a multi-stage pick-and-place task\nas a succinct yet sufficiently rich setup that captures key desiderata for\nquadruped-based mobile manipulation, we propose an approach that can train a\nvisuo-motor policy entirely in simulation, and achieve nearly 80\\% success in\nthe real world. The policy efficiently performs search, approach, grasp,\ntransport, and drop into actions, with emerged behaviors such as re-grasping\nand task chaining. We conduct an extensive set of real-world experiments with\nablation studies highlighting key techniques for efficient training and\neffective sim-to-real transfer. Additional experiments demonstrate deployment\nacross a variety of indoor and outdoor environments. Demo videos and additional\nresources are available on the project page:\nhttps://horizonrobotics.github.io/gail/SLIM.\n","authors":["Haichao Zhang","Haonan Yu","Le Zhao","Andrew Choi","Qinxun Bai","Yiqing Yang","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2509.03859v1.pdf","comment":"Project: https://horizonrobotics.github.io/gail/SLIM"},{"id":"http://arxiv.org/abs/2509.03842v1","updated":"2025-09-04T03:08:01Z","published":"2025-09-04T03:08:01Z","title":"INGRID: Intelligent Generative Robotic Design Using Large Language\n  Models","summary":"  The integration of large language models (LLMs) into robotic systems has\naccelerated progress in embodied artificial intelligence, yet current\napproaches remain constrained by existing robotic architectures, particularly\nserial mechanisms. This hardware dependency fundamentally limits the scope of\nrobotic intelligence. Here, we present INGRID (Intelligent Generative Robotic\nDesign), a framework that enables the automated design of parallel robotic\nmechanisms through deep integration with reciprocal screw theory and kinematic\nsynthesis methods. We decompose the design challenge into four progressive\ntasks: constraint analysis, kinematic joint generation, chain construction, and\ncomplete mechanism design. INGRID demonstrates the ability to generate novel\nparallel mechanisms with both fixed and variable mobility, discovering\nkinematic configurations not previously documented in the literature. We\nvalidate our approach through three case studies demonstrating how INGRID\nassists users in designing task-specific parallel robots based on desired\nmobility requirements. By bridging the gap between mechanism theory and machine\nlearning, INGRID enables researchers without specialized robotics training to\ncreate custom parallel mechanisms, thereby decoupling advances in robotic\nintelligence from hardware constraints. This work establishes a foundation for\nmechanism intelligence, where AI systems actively design robotic hardware,\npotentially transforming the development of embodied AI systems.\n","authors":["Guanglu Jia","Ceng Zhang","Gregory S. Chirikjian"],"pdf_url":"https://arxiv.org/pdf/2509.03842v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2506.14770v2","updated":"2025-09-04T02:48:07Z","published":"2025-06-17T17:59:33Z","title":"GMT: General Motion Tracking for Humanoid Whole-Body Control","summary":"  The ability to track general whole-body motions in the real world is a useful\nway to build general-purpose humanoid robots. However, achieving this can be\nchallenging due to the temporal and kinematic diversity of the motions, the\npolicy's capability, and the difficulty of coordination of the upper and lower\nbodies. To address these issues, we propose GMT, a general and scalable\nmotion-tracking framework that trains a single unified policy to enable\nhumanoid robots to track diverse motions in the real world. GMT is built upon\ntwo core components: an Adaptive Sampling strategy and a Motion\nMixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically\nbalances easy and difficult motions during training. The MoE ensures better\nspecialization of different regions of the motion manifold. We show through\nextensive experiments in both simulation and the real world the effectiveness\nof GMT, achieving state-of-the-art performance across a broad spectrum of\nmotions using a unified general policy. Videos and additional information can\nbe found at https://gmt-humanoid.github.io.\n","authors":["Zixuan Chen","Mazeyu Ji","Xuxin Cheng","Xuanbin Peng","Xue Bin Peng","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2506.14770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22524v2","updated":"2025-09-04T02:32:37Z","published":"2025-03-28T15:28:36Z","title":"Robust Offline Imitation Learning Through State-level Trajectory\n  Stitching","summary":"  Imitation learning (IL) has proven effective for enabling robots to acquire\nvisuomotor skills through expert demonstrations. However, traditional IL\nmethods are limited by their reliance on high-quality, often scarce, expert\ndata, and suffer from covariate shift. To address these challenges, recent\nadvances in offline IL have incorporated suboptimal, unlabeled datasets into\nthe training. In this paper, we propose a novel approach to enhance policy\nlearning from mixed-quality offline datasets by leveraging task-relevant\ntrajectory fragments and rich environmental dynamics. Specifically, we\nintroduce a state-based search framework that stitches state-action pairs from\nimperfect demonstrations, generating more diverse and informative training\ntrajectories. Experimental results on standard IL benchmarks and real-world\nrobotic tasks showcase that our proposed method significantly improves both\ngeneralization and performance.\n","authors":["Shuze Wang","Yunpeng Mei","Hongjie Cao","Yetian Yuan","Gang Wang","Jian Sun","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2503.22524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03804v1","updated":"2025-09-04T01:42:11Z","published":"2025-09-04T01:42:11Z","title":"Real-Time Buoyancy Estimation for AUV Simulations Using Convex\n  Hull-Based Submerged Volume Calculation","summary":"  Accurate real-time buoyancy modeling is essential for high-fidelity\nAutonomous Underwater Vehicle (AUV) simulations, yet NVIDIA Isaac Sim lacks a\nnative buoyancy system, requiring external solutions for precise underwater\nphysics. This paper presents a novel convex hull-based approach to dynamically\ncompute the submerged volume of an AUV in real time. By extracting mesh\ngeometry from the simulation environment and calculating the hull portion\nintersecting the water level along the z-axis, our method enhances accuracy\nover traditional geometric approximations. A cross-sectional area extension\nreduces computational overhead, enabling efficient buoyant force updates that\nadapt to orientation, depth, and sinusoidal wave fluctuations (+-0.3 m). Tested\non a custom AUV design for SAUVC 2025, this approach delivers real-time\nperformance and scalability, improving simulation fidelity for underwater\nrobotics research without precomputed hydrodynamic models.\n","authors":["Ad-Deen Mahbub","Md Ragib Shaharear"],"pdf_url":"https://arxiv.org/pdf/2509.03804v1.pdf","comment":"7 pages, 10 figures"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2509.04450v1","updated":"2025-09-04T17:59:55Z","published":"2025-09-04T17:59:55Z","title":"Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual\n  Try-On from a Single Image -- Technical Preview","summary":"  We introduce the Virtual Fitting Room (VFR), a novel video generative model\nthat produces arbitrarily long virtual try-on videos. Our VFR models long video\ngeneration tasks as an auto-regressive, segment-by-segment generation process,\neliminating the need for resource-intensive generation and lengthy video data,\nwhile providing the flexibility to generate videos of arbitrary length. The key\nchallenges of this task are twofold: ensuring local smoothness between adjacent\nsegments and maintaining global temporal consistency across different segments.\nTo address these challenges, we propose our VFR framework, which ensures\nsmoothness through a prefix video condition and enforces consistency with the\nanchor video -- a 360-degree video that comprehensively captures the human's\nwholebody appearance. Our VFR generates minute-scale virtual try-on videos with\nboth local smoothness and global temporal consistency under various motions,\nmaking it a pioneering work in long virtual try-on video generation.\n","authors":["Jun-Kun Chen","Aayush Bansal","Minh Phuoc Vo","Yu-Xiong Wang"],"pdf_url":"https://arxiv.org/pdf/2509.04450v1.pdf","comment":"Project Page: https://immortalco.github.io/VirtualFittingRoom/"},{"id":"http://arxiv.org/abs/2509.04448v1","updated":"2025-09-04T17:59:43Z","published":"2025-09-04T17:59:43Z","title":"TRUST-VL: An Explainable News Assistant for General Multimodal\n  Misinformation Detection","summary":"  Multimodal misinformation, encompassing textual, visual, and cross-modal\ndistortions, poses an increasing societal threat that is amplified by\ngenerative AI. Existing methods typically focus on a single type of distortion\nand struggle to generalize to unseen scenarios. In this work, we observe that\ndifferent distortion types share common reasoning capabilities while also\nrequiring task-specific skills. We hypothesize that joint training across\ndistortion types facilitates knowledge sharing and enhances the model's ability\nto generalize. To this end, we introduce TRUST-VL, a unified and explainable\nvision-language model for general multimodal misinformation detection. TRUST-VL\nincorporates a novel Question-Aware Visual Amplifier module, designed to\nextract task-specific visual features. To support training, we also construct\nTRUST-Instruct, a large-scale instruction dataset containing 198K samples\nfeaturing structured reasoning chains aligned with human fact-checking\nworkflows. Extensive experiments on both in-domain and zero-shot benchmarks\ndemonstrate that TRUST-VL achieves state-of-the-art performance, while also\noffering strong generalization and interpretability.\n","authors":["Zehong Yan","Peng Qi","Wynne Hsu","Mong Li Lee"],"pdf_url":"https://arxiv.org/pdf/2509.04448v1.pdf","comment":"EMNLP 2025; Project Homepage: https://yanzehong.github.io/trust-vl/"},{"id":"http://arxiv.org/abs/2509.04446v1","updated":"2025-09-04T17:59:34Z","published":"2025-09-04T17:59:34Z","title":"Plot'n Polish: Zero-shot Story Visualization and Disentangled Editing\n  with Text-to-Image Diffusion Models","summary":"  Text-to-image diffusion models have demonstrated significant capabilities to\ngenerate diverse and detailed visuals in various domains, and story\nvisualization is emerging as a particularly promising application. However, as\ntheir use in real-world creative domains increases, the need for providing\nenhanced control, refinement, and the ability to modify images post-generation\nin a consistent manner becomes an important challenge. Existing methods often\nlack the flexibility to apply fine or coarse edits while maintaining visual and\nnarrative consistency across multiple frames, preventing creators from\nseamlessly crafting and refining their visual stories. To address these\nchallenges, we introduce Plot'n Polish, a zero-shot framework that enables\nconsistent story generation and provides fine-grained control over story\nvisualizations at various levels of detail.\n","authors":["Kiymet Akdemir","Jing Shi","Kushal Kafle","Brian Price","Pinar Yanardag"],"pdf_url":"https://arxiv.org/pdf/2509.04446v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04444v1","updated":"2025-09-04T17:59:10Z","published":"2025-09-04T17:59:10Z","title":"One Flight Over the Gap: A Survey from Perspective to Panoramic Vision","summary":"  Driven by the demand for spatial intelligence and holistic scene perception,\nomnidirectional images (ODIs), which provide a complete 360\\textdegree{} field\nof view, are receiving growing attention across diverse applications such as\nvirtual reality, autonomous driving, and embodied robotics. Despite their\nunique characteristics, ODIs exhibit remarkable differences from perspective\nimages in geometric projection, spatial distribution, and boundary continuity,\nmaking it challenging for direct domain adaption from perspective methods. This\nsurvey reviews recent panoramic vision techniques with a particular emphasis on\nthe perspective-to-panorama adaptation. We first revisit the panoramic imaging\npipeline and projection methods to build the prior knowledge required for\nanalyzing the structural disparities. Then, we summarize three challenges of\ndomain adaptation: severe geometric distortions near the poles, non-uniform\nsampling in Equirectangular Projection (ERP), and periodic boundary continuity.\nBuilding on this, we cover 20+ representative tasks drawn from more than 300\nresearch papers in two dimensions. On one hand, we present a cross-method\nanalysis of representative strategies for addressing panoramic specific\nchallenges across different tasks. On the other hand, we conduct a cross-task\ncomparison and classify panoramic vision into four major categories: visual\nquality enhancement and assessment, visual understanding, multimodal\nunderstanding, and visual generation. In addition, we discuss open challenges\nand future directions in data, models, and applications that will drive the\nadvancement of panoramic vision research. We hope that our work can provide new\ninsight and forward looking perspectives to advance the development of\npanoramic vision technologies. Our project page is\nhttps://insta360-research-team.github.io/Survey-of-Panorama\n","authors":["Xin Lin","Xian Ge","Dizhe Zhang","Zhaoliang Wan","Xianshun Wang","Xiangtai Li","Wenjie Jiang","Bo Du","Dacheng Tao","Ming-Hsuan Yang","Lu Qi"],"pdf_url":"https://arxiv.org/pdf/2509.04444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04441v1","updated":"2025-09-04T17:57:13Z","published":"2025-09-04T17:57:13Z","title":"DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation","summary":"  We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.\n","authors":["Hao-Shu Fang","Branden Romero","Yichen Xie","Arthur Hu","Bo-Ruei Huang","Juan Alvarez","Matthew Kim","Gabriel Margolis","Kavya Anbarasu","Masayoshi Tomizuka","Edward Adelson","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2509.04441v1.pdf","comment":"project page: https://dex-op.github.io"},{"id":"http://arxiv.org/abs/2509.04438v1","updated":"2025-09-04T17:53:52Z","published":"2025-09-04T17:53:52Z","title":"The Telephone Game: Evaluating Semantic Drift in Unified Models","summary":"  Employing a single, unified model (UM) for both visual understanding\n(image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened\na new direction in Visual Language Model (VLM) research. While UMs can also\nsupport broader unimodal tasks (e.g., text-to-text, image-to-image), we focus\non the core cross-modal pair T2I and I2T, as consistency between understanding\nand generation is critical for downstream use. Existing evaluations consider\nthese capabilities in isolation: FID and GenEval for T2I, and benchmarks such\nas MME, MMBench for I2T. These single-pass metrics do not reveal whether a\nmodel that understands a concept can also render it, nor whether meaning is\npreserved when cycling between image and text modalities. To address this, we\nintroduce the Unified Consistency Framework for Unified Models (UCF-UM), a\ncyclic evaluation protocol that alternates I2T and T2I over multiple\ngenerations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean\nCumulative Drift (MCD), an embedding-based measure of overall semantic loss;\n(ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii)\nMulti-Generation GenEval (MGG), an object-level compliance score extending\nGenEval. To assess generalization beyond COCO, which is widely used in\ntraining; we create a new benchmark ND400, sampled from NoCaps and DOCCI and\nevaluate on seven recent models. UCF-UM reveals substantial variation in\ncross-modal stability: some models like BAGEL maintain semantics over many\nalternations, whereas others like Vila-u drift quickly despite strong\nsingle-pass scores. Our results highlight cyclic consistency as a necessary\ncomplement to standard I2T and T2I evaluations, and provide practical metrics\nto consistently assess unified model's cross-modal stability and strength of\ntheir shared representations. Code:\nhttps://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models\n","authors":["Sabbir Mollah","Rohit Gupta","Sirnam Swetha","Qingyang Liu","Ahnaf Munir","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2509.04438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04437v1","updated":"2025-09-04T17:53:45Z","published":"2025-09-04T17:53:45Z","title":"From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray\n  Collimators via Hough Transform","summary":"  Collimation in X-ray imaging restricts exposure to the region-of-interest\n(ROI) and minimizes the radiation dose applied to the patient. The detection of\ncollimator shadows is an essential image-based preprocessing step in digital\nradiography posing a challenge when edges get obscured by scattered X-ray\nradiation. Regardless, the prior knowledge that collimation forms\npolygonal-shaped shadows is evident. For this reason, we introduce a deep\nlearning-based segmentation that is inherently constrained to its geometry. We\nachieve this by incorporating a differentiable Hough transform-based network to\ndetect the collimation borders and enhance its capability to extract the\ninformation about the ROI center. During inference, we combine the information\nof both tasks to enable the generation of refined, line-constrained\nsegmentation masks. We demonstrate robust reconstruction of collimated regions\nachieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real\nXray images. While this application involves at most four shadow borders, our\nmethod is not fundamentally limited by a specific number of edges.\n","authors":["Benjamin El-Zein","Dominik Eckert","Andreas Fieselmann","Christopher Syben","Ludwig Ritschl","Steffen Kappler","Sebastian Stober"],"pdf_url":"https://arxiv.org/pdf/2509.04437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04434v1","updated":"2025-09-04T17:53:03Z","published":"2025-09-04T17:53:03Z","title":"Durian: Dual Reference-guided Portrait Animation with Attribute Transfer","summary":"  We present Durian, the first method for generating portrait animation videos\nwith facial attribute transfer from a given reference image to a target\nportrait in a zero-shot manner. To enable high-fidelity and spatially\nconsistent attribute transfer across frames, we introduce dual reference\nnetworks that inject spatial features from both the portrait and attribute\nimages into the denoising process of a diffusion model. We train the model\nusing a self-reconstruction formulation, where two frames are sampled from the\nsame portrait video: one is treated as the attribute reference and the other as\nthe target portrait, and the remaining frames are reconstructed conditioned on\nthese inputs and their corresponding masks. To support the transfer of\nattributes with varying spatial extent, we propose a mask expansion strategy\nusing keypoint-conditioned image generation for training. In addition, we\nfurther augment the attribute and portrait images with spatial and\nappearance-level transformations to improve robustness to positional\nmisalignment between them. These strategies allow the model to effectively\ngeneralize across diverse attributes and in-the-wild reference combinations,\ndespite being trained without explicit triplet supervision. Durian achieves\nstate-of-the-art performance on portrait animation with attribute transfer, and\nnotably, its dual reference design enables multi-attribute composition in a\nsingle generation pass without additional training.\n","authors":["Hyunsoo Cha","Byungjun Kim","Hanbyul Joo"],"pdf_url":"https://arxiv.org/pdf/2509.04434v1.pdf","comment":"Project Page: https://hyunsoocha.github.io/durian"},{"id":"http://arxiv.org/abs/2505.03498v2","updated":"2025-09-04T17:50:32Z","published":"2025-05-06T13:02:40Z","title":"Res-MoCoDiff: Residual-guided diffusion models for motion artifact\n  correction in brain MRI","summary":"  Objective. Motion artifacts in brain MRI, mainly from rigid head motion,\ndegrade image quality and hinder downstream applications. Conventional methods\nto mitigate these artifacts, including repeated acquisitions or motion\ntracking, impose workflow burdens. This study introduces Res-MoCoDiff, an\nefficient denoising diffusion probabilistic model specifically designed for MRI\nmotion artifact correction.Approach.Res-MoCoDiff exploits a novel residual\nerror shifting mechanism during the forward diffusion process to incorporate\ninformation from motion-corrupted images. This mechanism allows the model to\nsimulate the evolution of noise with a probability distribution closely\nmatching that of the corrupted data, enabling a reverse diffusion process that\nrequires only four steps. The model employs a U-net backbone, with attention\nlayers replaced by Swin Transformer blocks, to enhance robustness across\nresolutions. Furthermore, the training process integrates a combined l1+l2 loss\nfunction, which promotes image sharpness and reduces pixel-level errors.\nRes-MoCoDiff was evaluated on both an in-silico dataset generated using a\nrealistic motion simulation framework and an in-vivo MR-ART dataset.\nComparative analyses were conducted against established methods, including\nCycleGAN, Pix2pix, and a diffusion model with a vision transformer backbone,\nusing quantitative metrics such as PSNR, SSIM, and NMSE.Main results. The\nproposed method demonstrated superior performance in removing motion artifacts\nacross minor, moderate, and heavy distortion levels. Res-MoCoDiff consistently\nachieved the highest SSIM and the lowest NMSE values, with a PSNR of up to\n41.91+-2.94 dB for minor distortions. Notably, the average sampling time was\nreduced to 0.37 seconds per batch of two image slices, compared with 101.74\nseconds for conventional approaches.\n","authors":["Mojtaba Safari","Shansong Wang","Qiang Li","Zach Eidex","Richard L. J. Qiu","Chih-Wei Chang","Hui Mao","Xiaofeng Yang"],"pdf_url":"https://arxiv.org/pdf/2505.03498v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04406v1","updated":"2025-09-04T17:24:31Z","published":"2025-09-04T17:24:31Z","title":"Few-step Flow for 3D Generation via Marginal-Data Transport Distillation","summary":"  Flow-based 3D generation models typically require dozens of sampling steps\nduring inference. Though few-step distillation methods, particularly\nConsistency Models (CMs), have achieved substantial advancements in\naccelerating 2D diffusion models, they remain under-explored for more complex\n3D generation tasks. In this study, we propose a novel framework, MDT-dist, for\nfew-step 3D flow distillation. Our approach is built upon a primary objective:\ndistilling the pretrained model to learn the Marginal-Data Transport. Directly\nlearning this objective needs to integrate the velocity fields, while this\nintegral is intractable to be implemented. Therefore, we propose two\noptimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),\nto equivalently convert the optimization target from the transport level to the\nvelocity and the distribution level respectively. Velocity Matching (VM) learns\nto stably match the velocity fields between the student and the teacher, but\ninevitably provides biased gradient estimates. Velocity Distillation (VD)\nfurther enhances the optimization process by leveraging the learned velocity\nfields to perform probability density distillation. When evaluated on the\npioneer 3D generation framework TRELLIS, our method reduces sampling steps of\neach flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s\n(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high\nvisual and geometric fidelity. Extensive experiments demonstrate that our\nmethod significantly outperforms existing CM distillation methods, and enables\nTRELLIS to achieve superior performance in few-step 3D generation.\n","authors":["Zanwei Zhou","Taoran Yi","Jiemin Fang","Chen Yang","Lingxi Xie","Xinggang Wang","Wei Shen","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2509.04406v1.pdf","comment":"Project page: https://github.com/Zanue/MDT-dist"},{"id":"http://arxiv.org/abs/2502.08352v2","updated":"2025-09-04T17:14:29Z","published":"2025-02-12T12:27:32Z","title":"Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images\n  with Depth and Normal Supervision","summary":"  With advancements in satellite imaging technology, acquiring high-resolution\nmulti-view satellite imagery has become increasingly accessible, enabling rapid\nand location-independent ground model reconstruction. However, traditional\nstereo matching methods struggle to capture fine details, and while neural\nradiance fields (NeRFs) achieve high-quality reconstructions, their training\ntime is prohibitively long. Moreover, challenges such as low visibility of\nbuilding facades, illumination and style differences between pixels, and weakly\ntextured regions in satellite imagery further make it hard to reconstruct\nreasonable terrain geometry and detailed building facades. To address these\nissues, we propose Sat-DN, a novel framework leveraging a progressively trained\nmulti-resolution hash grid reconstruction architecture with explicit depth\nguidance and surface normal consistency constraints to enhance reconstruction\nquality. The multi-resolution hash grid accelerates training, while the\nprogressive strategy incrementally increases the learning frequency, using\ncoarse low-frequency geometry to guide the reconstruction of fine\nhigh-frequency details. The depth and normal constraints ensure a clear\nbuilding outline and correct planar distribution. Extensive experiments on the\nDFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving\nstate-of-the-art results in both qualitative and quantitative evaluations. The\ncode is available at https://github.com/costune/SatDN.\n","authors":["Tianle Liu","Shuangming Zhao","Wanshou Jiang","Bingxuan Guo"],"pdf_url":"https://arxiv.org/pdf/2502.08352v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04403v1","updated":"2025-09-04T17:13:59Z","published":"2025-09-04T17:13:59Z","title":"Self-adaptive Dataset Construction for Real-World Multimodal Safety\n  Scenarios","summary":"  Multimodal large language models (MLLMs) are rapidly evolving, presenting\nincreasingly complex safety challenges. However, current dataset construction\nmethods, which are risk-oriented, fail to cover the growing complexity of\nreal-world multimodal safety scenarios (RMS). And due to the lack of a unified\nevaluation metric, their overall effectiveness remains unproven. This paper\nintroduces a novel image-oriented self-adaptive dataset construction method for\nRMS, which starts with images and end constructing paired text and guidance\nresponses. Using the image-oriented method, we automatically generate an RMS\ndataset comprising 35k image-text pairs with guidance responses. Additionally,\nwe introduce a standardized safety dataset evaluation metric: fine-tuning a\nsafety judge model and evaluating its capabilities on other safety\ndatasets.Extensive experiments on various tasks demonstrate the effectiveness\nof the proposed image-oriented pipeline. The results confirm the scalability\nand effectiveness of the image-oriented approach, offering a new perspective\nfor the construction of real-world multimodal safety datasets.\n","authors":["Jingen Qu","Lijun Li","Bo Zhang","Yichen Yan","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2509.04403v1.pdf","comment":"Accepted at EMNLP 2025 Findings"},{"id":"http://arxiv.org/abs/2509.04402v1","updated":"2025-09-04T17:13:19Z","published":"2025-09-04T17:13:19Z","title":"Learning neural representations for X-ray ptychography reconstruction\n  with unknown probes","summary":"  X-ray ptychography provides exceptional nanoscale resolution and is widely\napplied in materials science, biology, and nanotechnology. However, its full\npotential is constrained by the critical challenge of accurately reconstructing\nimages when the illuminating probe is unknown. Conventional iterative methods\nand deep learning approaches are often suboptimal, particularly under the\nlow-signal conditions inherent to low-dose and high-speed experiments. These\nlimitations compromise reconstruction fidelity and restrict the broader\nadoption of the technique. In this work, we introduce the Ptychographic\nImplicit Neural Representation (PtyINR), a self-supervised framework that\nsimultaneously addresses the object and probe recovery problem. By\nparameterizing both as continuous neural representations, PtyINR performs\nend-to-end reconstruction directly from raw diffraction patterns without\nrequiring any pre-characterization of the probe. Extensive evaluations\ndemonstrate that PtyINR achieves superior reconstruction quality on both\nsimulated and experimental data, with remarkable robustness under challenging\nlow-signal conditions. Furthermore, PtyINR offers a generalizable,\nphysics-informed framework for addressing probe-dependent inverse problems,\nmaking it applicable to a wide range of computational microscopy problems.\n","authors":["Tingyou Li","Zixin Xu","Zirui Gao","Hanfei Yan","Xiaojing Huang","Jizhou Li"],"pdf_url":"https://arxiv.org/pdf/2509.04402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04394v1","updated":"2025-09-04T17:05:59Z","published":"2025-09-04T17:05:59Z","title":"Transition Models: Rethinking the Generative Learning Objective","summary":"  A fundamental dilemma in generative modeling persists: iterative diffusion\nmodels achieve outstanding fidelity, but at a significant computational cost,\nwhile efficient few-step alternatives are constrained by a hard quality\nceiling. This conflict between generation steps and output quality arises from\nrestrictive training objectives that focus exclusively on either infinitesimal\ndynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by\nintroducing an exact, continuous-time dynamics equation that analytically\ndefines state transitions across any finite time interval. This leads to a\nnovel generative paradigm, Transition Models (TiM), which adapt to\narbitrary-step transitions, seamlessly traversing the generative trajectory\nfrom single leaps to fine-grained refinement with more steps. Despite having\nonly 865M parameters, TiM achieves state-of-the-art performance, surpassing\nleading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across\nall evaluated step counts. Importantly, unlike previous few-step generators,\nTiM demonstrates monotonic quality improvement as the sampling budget\nincreases. Additionally, when employing our native-resolution strategy, TiM\ndelivers exceptional fidelity at resolutions up to 4096x4096.\n","authors":["Zidong Wang","Yiyuan Zhang","Xiaoyu Yue","Xiangyu Yue","Yangguang Li","Wanli Ouyang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2509.04394v1.pdf","comment":"The code is released at https://github.com/WZDTHU/TiM"},{"id":"http://arxiv.org/abs/2507.12964v5","updated":"2025-09-04T16:55:10Z","published":"2025-07-17T10:03:57Z","title":"Demographic-aware fine-grained classification of pediatric wrist\n  fractures","summary":"  Wrist pathologies are frequently observed, particularly among children who\nconstitute the majority of fracture cases. Computer vision presents a promising\navenue, contingent upon the availability of extensive datasets, a notable\nchallenge in medical imaging. Therefore, reliance solely on one modality, such\nas images, proves inadequate, especially in an era of diverse and plentiful\ndata types. This study addresses the problem using a multifaceted approach:\nframing it as a fine-grained recognition task, fusing patient metadata with\nX-rays, and leveraging weights from a separate fine-grained dataset rather than\nfrom a coarse-grained dataset like ImageNet. Unlike prior work, this is the\nfirst application of metadata integration for wrist pathology recognition. Our\nresults show that combining fine-grained transformer approach, fine-grained\npre-training, and metadata integration improves diagnostic accuracy by 2% on\nsmall custom curated dataset and over 10% on a larger fracture dataset.\n","authors":["Ammar Ahmed","Ali Shariq Imran","Zenun Kastrati","Sher Muhammad Daudpota"],"pdf_url":"https://arxiv.org/pdf/2507.12964v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04379v1","updated":"2025-09-04T16:40:44Z","published":"2025-09-04T16:40:44Z","title":"SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer","summary":"  Recent advancements in neural representations, such as Neural Radiance Fields\nand 3D Gaussian Splatting, have increased interest in applying style transfer\nto 3D scenes. While existing methods can transfer style patterns onto\n3D-consistent neural representations, they struggle to effectively extract and\ntransfer high-level style semantics from the reference style image.\nAdditionally, the stylized results often lack structural clarity and\nseparation, making it difficult to distinguish between different instances or\nobjects within the 3D scene. To address these limitations, we propose a novel\n3D style transfer pipeline that effectively integrates prior knowledge from\npretrained 2D diffusion models. Our pipeline consists of two key stages: First,\nwe leverage diffusion priors to generate stylized renderings of key viewpoints.\nThen, we transfer the stylized key views onto the 3D representation. This\nprocess incorporates two innovative designs. The first is cross-view style\nalignment, which inserts cross-view attention into the last upsampling block of\nthe UNet, allowing feature interactions across multiple key views. This ensures\nthat the diffusion model generates stylized key views that maintain both style\nfidelity and instance-level consistency. The second is instance-level style\ntransfer, which effectively leverages instance-level consistency across\nstylized key views and transfers it onto the 3D representation. This results in\na more structured, visually coherent, and artistically enriched stylization.\nExtensive qualitative and quantitative experiments demonstrate that our 3D\nstyle transfer pipeline significantly outperforms state-of-the-art methods\nacross a wide range of scenes, from forward-facing to challenging 360-degree\nenvironments. Visit our project page https://jm-xu.github.io/SSGaussian for\nimmersive visualization.\n","authors":["Jimin Xu","Bosheng Qin","Tao Jin","Zhou Zhao","Zhenhui Ye","Jun Yu","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2509.04379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04378v1","updated":"2025-09-04T16:40:15Z","published":"2025-09-04T16:40:15Z","title":"Aesthetic Image Captioning with Saliency Enhanced MLLMs","summary":"  Aesthetic Image Captioning (AIC) aims to generate textual descriptions of\nimage aesthetics, becoming a key research direction in the field of\ncomputational aesthetics. In recent years, pretrained Multimodal Large Language\nModels (MLLMs) have advanced rapidly, leading to a significant increase in\nimage aesthetics research that integrates both visual and textual modalities.\nHowever, most existing studies on image aesthetics primarily focus on\npredicting aesthetic ratings and have shown limited application in AIC.\nExisting AIC works leveraging MLLMs predominantly rely on fine-tuning methods\nwithout specifically adapting MLLMs to focus on target aesthetic content. To\naddress this limitation, we propose the Aesthetic Saliency Enhanced Multimodal\nLarge Language Model (ASE-MLLM), an end-to-end framework that explicitly\nincorporates aesthetic saliency into MLLMs. Within this framework, we introduce\nthe Image Aesthetic Saliency Module (IASM), which efficiently and effectively\nextracts aesthetic saliency features from images. Additionally, we design\nIAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency\nfeatures with original image features via a cross-attention mechanism. To the\nbest of our knowledge, ASE-MLLM is the first framework to integrate image\naesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments\ndemonstrated that our approach significantly outperformed traditional methods\nand generic MLLMs on current mainstream AIC benchmarks, achieving\nstate-of-the-art (SOTA) performance.\n","authors":["Yilin Tao","Jiashui Huang","Huaze Xu","Ling Shao"],"pdf_url":"https://arxiv.org/pdf/2509.04378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.06949v2","updated":"2025-09-04T16:39:57Z","published":"2025-07-09T15:31:44Z","title":"Ecological Legacies of Pre-Columbian Settlements Evident in Palm\n  Clusters of Neotropical Mountain Forests","summary":"  Ancient populations markedly transformed Neotropical forests, yet the spatial\nextent of their ecological influence remains underexplored at high resolution.\nHere we present a deep learning and remote sensing based approach to estimate\nareas of pre-Columbian forest modification based on modern vegetation. We apply\nthis method to high-resolution satellite imagery from the Sierra Nevada de\nSanta Marta, Colombia, as a demonstration of a scalable approach, to evaluate\npalm tree distributions in relation to archaeological infrastructure. Palms\nwere significantly more abundant near archaeological sites with large\ninfrastructure investment. The extent of the largest palm cluster indicates\nthat ancient human-managed areas linked to major infrastructure sites may be up\nto two orders of magnitude bigger than indicated by current archaeological\nevidence alone. Our findings suggest that pre-Columbian populations influenced\nvegetation, fostering conditions conducive to palm proliferation, leaving a\nlasting ecological footprint. This may have lowered the logistical costs of\nestablishing infrastructure-heavy settlements in less accessible locations.\n","authors":["Sebastian Fajardo","Sina Mohammadi","Jonas Gregorio de Souza","César Ardila","Alan Tapscott Baltar","Shaddai Heidgen","Maria Isabel Mayorga Hernández","Sylvia Mota de Oliveira","Fernando Montejo","Marco Moderato","Vinicius Peripato","Katy Puche","Carlos Reina","Juan Carlos Vargas","Frank W. Takes","Marco Madella"],"pdf_url":"https://arxiv.org/pdf/2507.06949v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05750v2","updated":"2025-09-04T16:38:49Z","published":"2024-08-11T11:43:56Z","title":"FADE: A Dataset for Detecting Falling Objects around Buildings in Video","summary":"  Falling objects from buildings can cause severe injuries to pedestrians due\nto the great impact force they exert. Although surveillance cameras are\ninstalled around some buildings, it is challenging for humans to capture such\nevents in surveillance videos due to the small size and fast motion of falling\nobjects, as well as the complex background. Therefore, it is necessary to\ndevelop methods to automatically detect falling objects around buildings in\nsurveillance videos. To facilitate the investigation of falling object\ndetection, we propose a large, diverse video dataset called FADE (FAlling\nObject DEtection around Buildings) for the first time. FADE contains 1,881\nvideos from 18 scenes, featuring 8 falling object categories, 4 weather\nconditions, and 4 video resolutions. Additionally, we develop a new object\ndetection method called FADE-Net, which effectively leverages motion\ninformation and produces small-sized but high-quality proposals for detecting\nfalling objects around buildings. Importantly, our method is extensively\nevaluated and analyzed by comparing it with the previous approaches used for\ngeneric object detection, video object detection, and moving object detection\non the FADE dataset. Experimental results show that the proposed FADE-Net\nsignificantly outperforms other methods, providing an effective baseline for\nfuture research. The dataset and code are publicly available at\nhttps://fadedataset.github.io/FADE.github.io/.\n","authors":["Zhigang Tu","Zitao Gao","Zhengbo Zhang","Chunluan Zhou","Junsong Yuan","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2408.05750v2.pdf","comment":"Accepted by IEEE Transactions on Information Forensics and Security\n  (TIFS), 2025"},{"id":"http://arxiv.org/abs/2509.02175v2","updated":"2025-09-04T16:38:44Z","published":"2025-09-02T10:32:58Z","title":"Understanding Space Is Rocket Science -- Only Top Reasoning Models Can\n  Solve Spatial Understanding Tasks","summary":"  We propose RocketScience, an open-source contrastive VLM benchmark that tests\nfor spatial relation understanding. It is comprised of entirely new real-world\nimage-text pairs covering mostly relative spatial understanding and the order\nof objects. The benchmark is designed to be very easy for humans and hard for\nthe current generation of VLMs, and this is empirically verified. Our results\nshow a striking lack of spatial relation understanding in open source and\nfrontier commercial VLMs and a surprisingly high performance of reasoning\nmodels. Additionally, we perform a disentanglement analysis to separate the\ncontributions of object localization and spatial reasoning in\nchain-of-thought-based models and find that the performance on the benchmark is\nbottlenecked by spatial reasoning and not object localization capabilities. We\nrelease the dataset with a CC-BY-4.0 license and make the evaluation code\navailable at: https://github.com/nilshoehing/rocketscience\n","authors":["Nils Hoehing","Mayug Maniparambil","Ellen Rushe","Noel E. O'Connor","Anthony Ventresque"],"pdf_url":"https://arxiv.org/pdf/2509.02175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04376v1","updated":"2025-09-04T16:34:46Z","published":"2025-09-04T16:34:46Z","title":"AnomalyLMM: Bridging Generative Knowledge and Discriminative Retrieval\n  for Text-Based Person Anomaly Search","summary":"  With growing public safety demands, text-based person anomaly search has\nemerged as a critical task, aiming to retrieve individuals with abnormal\nbehaviors via natural language descriptions. Unlike conventional person search,\nthis task presents two unique challenges: (1) fine-grained cross-modal\nalignment between textual anomalies and visual behaviors, and (2) anomaly\nrecognition under sparse real-world samples. While Large Multi-modal Models\n(LMMs) excel in multi-modal understanding, their potential for fine-grained\nanomaly retrieval remains underexplored, hindered by: (1) a domain gap between\ngenerative knowledge and discriminative retrieval, and (2) the absence of\nefficient adaptation strategies for deployment. In this work, we propose\nAnomalyLMM, the first framework that harnesses LMMs for text-based person\nanomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline\nintegrating LMMs to bridge generative world knowledge with retrieval-centric\nanomaly detection; (2) A training-free adaptation cookbook featuring masked\ncross-modal prompting, behavioral saliency prediction, and knowledge-aware\nre-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study\nto explore LMMs for this task, we conduct a rigorous evaluation on the PAB\ndataset, the only publicly available benchmark for text-based person anomaly\nsearch, with its curated real-world anomalies covering diverse scenarios (e.g.,\nfalling, collision, and being hit). Experiments show the effectiveness of the\nproposed method, surpassing the competitive baseline by +0.96% Recall@1\naccuracy. Notably, our method reveals interpretable alignment between textual\nanomalies and visual behaviors, validated via qualitative analysis. Our code\nand models will be released for future research.\n","authors":["Hao Ju","Hu Zhang","Zhedong Zheng"],"pdf_url":"https://arxiv.org/pdf/2509.04376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04370v1","updated":"2025-09-04T16:27:53Z","published":"2025-09-04T16:27:53Z","title":"Stitching the Story: Creating Panoramic Incident Summaries from\n  Body-Worn Footage","summary":"  First responders widely adopt body-worn cameras to document incident scenes\nand support post-event analysis. However, reviewing lengthy video footage is\nimpractical in time-critical situations. Effective situational awareness\ndemands a concise visual summary that can be quickly interpreted. This work\npresents a computer vision pipeline that transforms body-camera footage into\ninformative panoramic images summarizing the incident scene. Our method\nleverages monocular Simultaneous Localization and Mapping (SLAM) to estimate\ncamera trajectories and reconstruct the spatial layout of the environment. Key\nviewpoints are identified by clustering camera poses along the trajectory, and\nrepresentative frames from each cluster are selected. These frames are fused\ninto spatially coherent panoramic images using multi-frame stitching\ntechniques. The resulting summaries enable rapid understanding of complex\nenvironments and facilitate efficient decision-making and incident review.\n","authors":["Dor Cohen","Inga Efrosman","Yehudit Aperstein","Alexander Apartsin"],"pdf_url":"https://arxiv.org/pdf/2509.04370v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2509.04351v1","updated":"2025-09-04T16:12:14Z","published":"2025-09-04T16:12:14Z","title":"Global-to-Local or Local-to-Global? Enhancing Image Retrieval with\n  Efficient Local Search and Effective Global Re-ranking","summary":"  The dominant paradigm in image retrieval systems today is to search large\ndatabases using global image features, and re-rank those initial results with\nlocal image feature matching techniques. This design, dubbed global-to-local,\nstems from the computational cost of local matching approaches, which can only\nbe afforded for a small number of retrieved images. However, emerging efficient\nlocal feature search approaches have opened up new possibilities, in particular\nenabling detailed retrieval at large scale, to find partial matches which are\noften missed by global feature search. In parallel, global feature-based\nre-ranking has shown promising results with high computational efficiency. In\nthis work, we leverage these building blocks to introduce a local-to-global\nretrieval paradigm, where efficient local feature search meets effective global\nfeature re-ranking. Critically, we propose a re-ranking method where global\nfeatures are computed on-the-fly, based on the local feature retrieval\nsimilarities. Such re-ranking-only global features leverage multidimensional\nscaling techniques to create embeddings which respect the local similarities\nobtained during search, enabling a significant re-ranking boost.\nExperimentally, we demonstrate solid retrieval performance, setting new\nstate-of-the-art results on the Revisited Oxford and Paris datasets.\n","authors":["Dror Aiger","Bingyi Cao","Kaifeng Chen","Andre Araujo"],"pdf_url":"https://arxiv.org/pdf/2509.04351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04344v1","updated":"2025-09-04T16:03:14Z","published":"2025-09-04T16:03:14Z","title":"MICACL: Multi-Instance Category-Aware Contrastive Learning for\n  Long-Tailed Dynamic Facial Expression Recognition","summary":"  Dynamic facial expression recognition (DFER) faces significant challenges due\nto long-tailed category distributions and complexity of spatio-temporal feature\nmodeling. While existing deep learning-based methods have improved DFER\nperformance, they often fail to address these issues, resulting in severe model\ninduction bias. To overcome these limitations, we propose a novel\nmulti-instance learning framework called MICACL, which integrates\nspatio-temporal dependency modeling and long-tailed contrastive learning\noptimization. Specifically, we design the Graph-Enhanced Instance Interaction\nModule (GEIIM) to capture intricate spatio-temporal between adjacent instances\nrelationships through adaptive adjacency matrices and multiscale convolutions.\nTo enhance instance-level feature aggregation, we develop the Weighted Instance\nAggregation Network (WIAN), which dynamically assigns weights based on instance\nimportance. Furthermore, we introduce a Multiscale Category-aware Contrastive\nLearning (MCCL) strategy to balance training between major and minor\ncategories. Extensive experiments on in-the-wild datasets (i.e., DFEW and\nFERV39k) demonstrate that MICACL achieves state-of-the-art performance with\nsuperior robustness and generalization.\n","authors":["Feng-Qi Cui","Zhen Lin","Xinlong Rao","Anyang Tong","Shiyao Li","Fei Wang","Changlin Chen","Bin Liu"],"pdf_url":"https://arxiv.org/pdf/2509.04344v1.pdf","comment":"Accepted by IEEE ISPA2025"},{"id":"http://arxiv.org/abs/2509.04338v1","updated":"2025-09-04T15:58:50Z","published":"2025-09-04T15:58:50Z","title":"From Editor to Dense Geometry Estimator","summary":"  Leveraging visual priors from pre-trained text-to-image (T2I) generative\nmodels has shown success in dense prediction. However, dense prediction is\ninherently an image-to-image task, suggesting that image editing models, rather\nthan T2I generative models, may be a more suitable foundation for fine-tuning.\n  Motivated by this, we conduct a systematic analysis of the fine-tuning\nbehaviors of both editors and generators for dense geometry estimation. Our\nfindings show that editing models possess inherent structural priors, which\nenable them to converge more stably by ``refining\" their innate features, and\nultimately achieve higher performance than their generative counterparts.\n  Based on these findings, we introduce \\textbf{FE2E}, a framework that\npioneeringly adapts an advanced editing model based on Diffusion Transformer\n(DiT) architecture for dense geometry prediction. Specifically, to tailor the\neditor for this deterministic task, we reformulate the editor's original flow\nmatching loss into the ``consistent velocity\" training objective. And we use\nlogarithmic quantization to resolve the precision conflict between the editor's\nnative BFloat16 format and the high precision demand of our tasks.\nAdditionally, we leverage the DiT's global attention for a cost-free joint\nestimation of depth and normals in a single forward pass, enabling their\nsupervisory signals to mutually enhance each other.\n  Without scaling up the training data, FE2E achieves impressive performance\nimprovements in zero-shot monocular depth and normal estimation across multiple\ndatasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset\nand outperforms the DepthAnything series, which is trained on 100$\\times$ data.\nThe project page can be accessed \\href{https://amap-ml.github.io/FE2E/}{here}.\n","authors":["JiYuan Wang","Chunyu Lin","Lei Sun","Rongying Liu","Lang Nie","Mingxing Li","Kang Liao","Xiangxiang Chu","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2509.04338v1.pdf","comment":"20pages"},{"id":"http://arxiv.org/abs/2509.04334v1","updated":"2025-09-04T15:52:04Z","published":"2025-09-04T15:52:04Z","title":"GeoArena: An Open Platform for Benchmarking Large Vision-language Models\n  on WorldWide Image Geolocalization","summary":"  Image geolocalization aims to predict the geographic location of images\ncaptured anywhere on Earth, but its global nature presents significant\nchallenges. Current evaluation methodologies suffer from two major limitations.\nFirst, data leakage: advanced approaches often rely on large vision-language\nmodels (LVLMs) to predict image locations, yet these models are frequently\npretrained on the test datasets, compromising the accuracy of evaluating a\nmodel's actual geolocalization capability. Second, existing metrics primarily\nrely on exact geographic coordinates to assess predictions, which not only\nneglects the reasoning process but also raises privacy concerns when user-level\nlocation data is required. To address these issues, we propose GeoArena, a\nfirst open platform for evaluating LVLMs on worldwide image geolocalization\ntasks, offering true in-the-wild and human-centered benchmarking. GeoArena\nenables users to upload in-the-wild images for a more diverse evaluation\ncorpus, and it leverages pairwise human judgments to determine which model\noutput better aligns with human expectations. Our platform has been deployed\nonline for two months, during which we collected over thousands voting records.\nBased on this data, we conduct a detailed analysis and establish a leaderboard\nof different LVLMs on the image geolocalization task.\n","authors":["Pengyue Jia","Yingyi Zhang","Xiangyu Zhao","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2509.04334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04326v1","updated":"2025-09-04T15:44:37Z","published":"2025-09-04T15:44:37Z","title":"Efficient Odd-One-Out Anomaly Detection","summary":"  The recently introduced odd-one-out anomaly detection task involves\nidentifying the odd-looking instances within a multi-object scene. This problem\npresents several challenges for modern deep learning models, demanding spatial\nreasoning across multiple views and relational reasoning to understand context\nand generalize across varying object categories and layouts. We argue that\nthese challenges must be addressed with efficiency in mind. To this end, we\npropose a DINO-based model that reduces the number of parameters by one third\nand shortens training time by a factor of three compared to the current\nstate-of-the-art, while maintaining competitive performance. Our experimental\nevaluation also introduces a Multimodal Large Language Model baseline,\nproviding insights into its current limitations in structured visual reasoning\ntasks. The project page can be found at\nhttps://silviochito.github.io/EfficientOddOneOut/\n","authors":["Silvio Chito","Paolo Rabino","Tatiana Tommasi"],"pdf_url":"https://arxiv.org/pdf/2509.04326v1.pdf","comment":"Accepted at ICIAP 2025"},{"id":"http://arxiv.org/abs/2509.04324v1","updated":"2025-09-04T15:42:36Z","published":"2025-09-04T15:42:36Z","title":"OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent\n  Detection","summary":"  Grasping assistance is essential for restoring autonomy in individuals with\nmotor impairments, particularly in unstructured environments where object\ncategories and user intentions are diverse and unpredictable. We present\nOVGrasp, a hierarchical control framework for soft exoskeleton-based grasp\nassistance that integrates RGB-D vision, open-vocabulary prompts, and voice\ncommands to enable robust multimodal interaction. To enhance generalization in\nopen environments, OVGrasp incorporates a vision-language foundation model with\nan open-vocabulary mechanism, allowing zero-shot detection of previously unseen\nobjects without retraining. A multimodal decision-maker further fuses spatial\nand linguistic cues to infer user intent, such as grasp or release, in\nmulti-object scenarios. We deploy the complete framework on a custom\negocentric-view wearable exoskeleton and conduct systematic evaluations on 15\nobjects across three grasp types. Experimental results with ten participants\ndemonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,\noutperforming state-of-the-art baselines and achieving improved kinematic\nalignment with natural hand motion.\n","authors":["Chen Hu","Shan Luo","Letizia Gionfrida"],"pdf_url":"https://arxiv.org/pdf/2509.04324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.15387v4","updated":"2025-09-04T15:15:09Z","published":"2025-08-21T09:23:51Z","title":"DIO: Refining Mutual Information and Causal Chain to Enhance Machine\n  Abstract Reasoning Ability","summary":"  Despite the outstanding performance of current deep learning models across\nvarious domains, their fundamental bottleneck in abstract reasoning remains\nunresolved. To address this challenge, the academic community has introduced\nRaven's Progressive Matrices (RPM) problems as an authoritative benchmark for\nevaluating the abstract reasoning capabilities of deep learning algorithms,\nwith a focus on core intelligence dimensions such as abstract reasoning,\npattern recognition, and complex problem-solving. Therefore, this paper centers\non solving RPM problems, aiming to contribute to enhancing the abstract\nreasoning abilities of machine intelligence. Firstly, this paper adopts a\n``causal chain modeling'' perspective to systematically analyze the complete\ncausal chain in RPM tasks: image $\\rightarrow$ abstract attributes\n$\\rightarrow$ progressive attribute patterns $\\rightarrow$ pattern consistency\n$\\rightarrow$ correct answer. Based on this analysis, the network architecture\nof the baseline model DIO is designed. However, experiments reveal that the\noptimization objective formulated for DIO, namely maximizing the variational\nlower bound of mutual information between the context and the correct option,\nfails to enable the model to genuinely acquire the predefined human reasoning\nlogic. This is attributed to two main reasons: the tightness of the lower bound\nsignificantly impacts the effectiveness of mutual information maximization, and\nmutual information, as a statistical measure, does not capture the causal\nrelationship between subjects and objects. To overcome these limitations, this\npaper progressively proposes three improvement methods:\n","authors":["Ruizhuo Song","Beiming Yuan"],"pdf_url":"https://arxiv.org/pdf/2508.15387v4.pdf","comment":"15 pages, 9 figures, 8 tables"},{"id":"http://arxiv.org/abs/2509.04298v1","updated":"2025-09-04T15:13:29Z","published":"2025-09-04T15:13:29Z","title":"Noisy Label Refinement with Semantically Reliable Synthetic Images","summary":"  Semantic noise in image classification datasets, where visually similar\ncategories are frequently mislabeled, poses a significant challenge to\nconventional supervised learning approaches. In this paper, we explore the\npotential of using synthetic images generated by advanced text-to-image models\nto address this issue. Although these high-quality synthetic images come with\nreliable labels, their direct application in training is limited by domain gaps\nand diversity constraints. Unlike conventional approaches, we propose a novel\nmethod that leverages synthetic images as reliable reference points to identify\nand correct mislabeled samples in noisy datasets. Extensive experiments across\nmultiple benchmark datasets show that our approach significantly improves\nclassification accuracy under various noise conditions, especially in\nchallenging scenarios with semantic label noise. Additionally, since our method\nis orthogonal to existing noise-robust learning techniques, when combined with\nstate-of-the-art noise-robust training methods, it achieves superior\nperformance, improving accuracy by 30% on CIFAR-10 and by 11% on CIFAR-100\nunder 70% semantic noise, and by 24% on ImageNet-100 under real-world noise\nconditions.\n","authors":["Yingxuan Li","Jiafeng Mao","Yusuke Matsui"],"pdf_url":"https://arxiv.org/pdf/2509.04298v1.pdf","comment":"Accepted to ICIP2025"},{"id":"http://arxiv.org/abs/2505.02980v2","updated":"2025-09-04T15:06:07Z","published":"2025-05-05T19:17:29Z","title":"Completing Spatial Transcriptomics Data for Gene Expression Prediction\n  Benchmarking","summary":"  Spatial Transcriptomics is a groundbreaking technology that integrates\nhistology images with spatially resolved gene expression profiles. Among the\nvarious Spatial Transcriptomics techniques available, Visium has emerged as the\nmost widely adopted. However, its accessibility is limited by high costs, the\nneed for specialized expertise, and slow clinical integration. Additionally,\ngene capture inefficiencies lead to significant dropout, corrupting acquired\ndata. To address these challenges, the deep learning community has explored the\ngene expression prediction task directly from histology images. Yet,\ninconsistencies in datasets, preprocessing, and training protocols hinder fair\ncomparisons between models. To bridge this gap, we introduce SpaRED, a\nsystematically curated database comprising 26 public datasets, providing a\nstandardized resource for model evaluation. We further propose SpaCKLE, a\nstate-of-the-art transformer-based gene expression completion model that\nreduces mean squared error by over 82.5% compared to existing approaches.\nFinally, we establish the SpaRED benchmark, evaluating eight state-of-the-art\nprediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE\nsubstantially improves the results across all the gene expression prediction\nmodels. Altogether, our contributions constitute the most comprehensive\nbenchmark of gene expression prediction from histology images to date and a\nstepping stone for future research on Spatial Transcriptomics.\n","authors":["Daniela Ruiz","Paula Cárdenas","Leonardo Manrique","Daniela Vega","Gabriel M. Mejia","Pablo Arbeláez"],"pdf_url":"https://arxiv.org/pdf/2505.02980v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2407.13027"},{"id":"http://arxiv.org/abs/2507.22832v3","updated":"2025-09-04T15:05:54Z","published":"2025-07-30T16:47:42Z","title":"Pulling Back the Curtain on ReLU Networks","summary":"  Since any ReLU network is piecewise affine, its hidden units can be\ncharacterized by their pullbacks through the active subnetwork, i.e., by their\ngradients (up to bias terms). However, gradients of deeper neurons are\nnotoriously misaligned, which obscures the network's internal representations.\nWe posit that models do align gradients with data, yet this is concealed by the\nintrinsic noise of the ReLU hard gating. We validate this intuition by applying\nsoft gating in the backward pass only, reducing the local impact of weakly\nexcited neurons. The resulting modified gradients, which we call \"excitation\npullbacks\", exhibit striking perceptual alignment on a number of\nImageNet-pretrained architectures, while the rudimentary pixel-space gradient\nascent quickly produces easily interpretable input- and target-specific\nfeatures. Inspired by these findings, we formulate the \"path stability\"\nhypothesis, claiming that the binary activation patterns largely stabilize\nduring training and get encoded in the pre-activation distribution of the final\nmodel. When true, excitation pullbacks become aligned with the gradients of a\nkernel machine that mainly determines the network's decision. This provides a\ntheoretical justification for the apparent faithfulness of the feature\nattributions based on these pullbacks, potentially even leading to mechanistic\ninterpretability of deeper models. Incidentally, we give a possible explanation\nfor the effectiveness of Batch Normalization and Deep Features, together with a\nnovel perspective on the network's internal memory and generalization\nproperties. We release the code and an interactive app for easier exploration\nof the excitation pullbacks.\n","authors":["Maciej Satkiewicz"],"pdf_url":"https://arxiv.org/pdf/2507.22832v3.pdf","comment":"12 pages, 3-page appendix, 4 figures, preprint; v3 changes: changed\n  title, improved abstract, expanded introduction, added section on\n  implications of the path stability"},{"id":"http://arxiv.org/abs/2502.10118v2","updated":"2025-09-04T15:00:25Z","published":"2025-02-14T12:33:19Z","title":"Image Embedding Sampling Method for Diverse Captioning","summary":"  Image Captioning for state-of-the-art VLMs has significantly improved over\ntime; however, this comes at the cost of increased computational complexity,\nmaking them less accessible for resource-constrained applications such as\nmobile devices and assistive technologies. Alternatively, comparably smaller\nVLMs prioritize high-level scene descriptions, overlooking finer details that\ncontribute to a richer understanding of an image. In this paper, we introduce a\ntraining-free framework that enhances caption diversity and informativeness by\nexplicitly attending to distinct image regions using a comparably small VLM,\nBLIP, as the backbone. Our approach leverages structured segmentation to\nproduce hierarchical representations that capture both global and localized\nsemantics. Without requiring additional model training, we demonstrate that our\nmethod allows smaller VLMs to achieve performance comparable to larger models\nin terms of image-caption alignment, semantic integrity, and diversity. We\nevaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets,\nachieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset,\nrespectively, while maintaining strong image-caption relevancy and semantic\nintegrity with the human-annotated captions.\n","authors":["Sania Waheed","Na Min An"],"pdf_url":"https://arxiv.org/pdf/2502.10118v2.pdf","comment":"17 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2509.04276v1","updated":"2025-09-04T14:51:03Z","published":"2025-09-04T14:51:03Z","title":"PAOLI: Pose-free Articulated Object Learning from Sparse-view Images","summary":"  We present a novel self-supervised framework for learning articulated object\nrepresentations from sparse-view, unposed images. Unlike prior methods that\nrequire dense multi-view observations and ground-truth camera poses, our\napproach operates with as few as four views per articulation and no camera\nsupervision. To address the inherent challenges, we first reconstruct each\narticulation independently using recent advances in sparse-view 3D\nreconstruction, then learn a deformation field that establishes dense\ncorrespondences across poses. A progressive disentanglement strategy further\nseparates static from moving parts, enabling robust separation of camera and\nobject motion. Finally, we jointly optimize geometry, appearance, and\nkinematics with a self-supervised loss that enforces cross-view and cross-pose\nconsistency. Experiments on the standard benchmark and real-world examples\ndemonstrate that our method produces accurate and detailed articulated object\nrepresentations under significantly weaker input assumptions than existing\napproaches.\n","authors":["Jianning Deng","Kartic Subr","Hakan Bilen"],"pdf_url":"https://arxiv.org/pdf/2509.04276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04273v1","updated":"2025-09-04T14:47:25Z","published":"2025-09-04T14:47:25Z","title":"Dual-Scale Volume Priors with Wasserstein-Based Consistency for\n  Semi-Supervised Medical Image Segmentation","summary":"  Despite signi cant progress in semi-supervised medical image segmentation,\nmost existing segmentation networks overlook e ective methodological guidance\nfor feature extraction and important prior information from\n  datasets. In this paper, we develop a semi-supervised medical image\nsegmentation framework that e ectively integrates spatial regularization\nmethods and volume priors. Speci cally, our approach integrates a strong\nexplicit volume prior at the image scale and Threshold Dynamics spatial\nregularization, both derived from variational models, into the backbone\nsegmentation network. The target region volumes for each unlabeled image are\nestimated by a regression network, which e ectively regularizes the backbone\nsegmentation network through an image-scale Wasserstein distance constraint,\nensuring that the class ratios in the segmentation results for each unlabeled\nimage match those predicted by the regression network. Additionally, we design\na dataset-scale Wasserstein distance loss function based on a weak implicit\nvolume prior, which enforces that the volume distribution predicted for the\nunlabeled dataset is similar to that of labeled dataset. Experimental results\non the 2017 ACDC dataset, PROMISE12 dataset, and thigh muscle MR image dataset\nshow the superiority of the proposed method.\n","authors":["Junying Meng","Gangxuan Zhou","Jun Liu","Weihong Guo"],"pdf_url":"https://arxiv.org/pdf/2509.04273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04269v1","updated":"2025-09-04T14:45:50Z","published":"2025-09-04T14:45:50Z","title":"TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D\n  Diffusion Models","summary":"  Accurate quantification of tau pathology via tau positron emission tomography\n(PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD).\nHowever, the high cost and limited availability of tau PET restrict its\nwidespread use. In contrast, structural magnetic resonance imaging (MRI) and\nplasma-based biomarkers provide non-invasive and widely available complementary\ninformation related to brain anatomy and disease progression. In this work, we\npropose a text-guided 3D diffusion model for 3D tau PET image synthesis,\nleveraging multimodal conditions from both structural MRI and plasma\nmeasurement. Specifically, the textual prompt is from the plasma p-tau217\nmeasurement, which is a key indicator of AD progression, while MRI provides\nanatomical structure constraints. The proposed framework is trained and\nevaluated using clinical AV1451 tau PET data from the Alzheimer's Disease\nNeuroimaging Initiative (ADNI) database. Experimental results demonstrate that\nour approach can generate realistic, clinically meaningful 3D tau PET across a\nrange of disease stages. The proposed framework can help perform tau PET data\naugmentation under different settings, provide a non-invasive, cost-effective\nalternative for visualizing tau pathology, and support the simulation of\ndisease progression under varying plasma biomarker levels and cognitive\nconditions.\n","authors":["Yuxin Gong","Se-in Jang","Wei Shao","Yi Su","Kuang Gong"],"pdf_url":"https://arxiv.org/pdf/2509.04269v1.pdf","comment":"9 pages, 4 figures, submitted to IEEE Transactions on Radiation and\n  Plasma Medical Sciences"},{"id":"http://arxiv.org/abs/2312.03993v2","updated":"2025-09-04T14:45:07Z","published":"2023-12-07T02:21:31Z","title":"Style Transfer to Calvin and Hobbes comics using Stable Diffusion","summary":"  This project report summarizes our journey to perform stable diffusion\nfine-tuning on a dataset containing Calvin and Hobbes comics. The purpose is to\nconvert any given input image into the comic style of Calvin and Hobbes,\nessentially performing style transfer. We train stable-diffusion-v1.5 using Low\nRank Adaptation (LoRA) to efficiently speed up the fine-tuning process. The\ndiffusion itself is handled by a Variational Autoencoder (VAE), which is a\nU-net. Our results were visually appealing for the amount of training time and\nthe quality of input data that went into training.\n","authors":["Asvin Kumar Venkataramanan","Sloke Shrestha","Sundar Sripada Venugopalaswamy Sriraman"],"pdf_url":"https://arxiv.org/pdf/2312.03993v2.pdf","comment":"Updated authorship"},{"id":"http://arxiv.org/abs/2509.04268v1","updated":"2025-09-04T14:44:18Z","published":"2025-09-04T14:44:18Z","title":"Differential Morphological Profile Neural Networks for Semantic\n  Segmentation","summary":"  Semantic segmentation of overhead remote sensing imagery enables applications\nin mapping, urban planning, and disaster response. State-of-the-art\nsegmentation networks are typically developed and tuned on ground-perspective\nphotographs and do not directly address remote sensing challenges such as\nextreme scale variation, foreground-background imbalance, and large image\nsizes. We explore the incorporation of the differential morphological profile\n(DMP), a multi-scale shape extraction method based on grayscale morphology,\ninto modern segmentation networks. Prior studies have shown that the DMP can\nprovide critical shape information to Deep Neural Networks to enable superior\ndetection and classification performance in overhead imagery. In this work, we\nextend prior DMPNet work beyond classification and object detection by\nintegrating DMP features into three state-of-the-art convolutional and\ntransformer semantic segmentation architectures. We utilize both direct input,\nwhich adapts the input stem of feature extraction architectures to accept DMP\nchannels, and hybrid architectures, a dual-stream design that fuses RGB and DMP\nencoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP\ndifferentials and structuring element shapes to more effectively provide shape\ninformation to the model. Our results show that while non-DMP models generally\noutperform the direct-input variants, hybrid DMP consistently outperforms\ndirect-input and is capable of surpassing a non-DMP model on mIoU, F1, and\nRecall.\n","authors":["David Huangal","J. Alex Hurt"],"pdf_url":"https://arxiv.org/pdf/2509.04268v1.pdf","comment":"14 pages, 7 figures"},{"id":"http://arxiv.org/abs/2311.16507v2","updated":"2025-09-04T14:24:04Z","published":"2023-11-28T06:19:30Z","title":"Straighter Flow Matching via a Diffusion-Based Coupling Prior","summary":"  Flow matching as a paradigm of generative model achieves notable success\nacross various domains. However, existing methods use either multi-round\ntraining or knowledge within minibatches, posing challenges in finding a\nfavorable coupling strategy for straightening trajectories to few-step\ngeneration. To address this issue, we propose a novel approach, Straighter\ntrajectories of Flow Matching (StraightFM). It straightens trajectories with\nthe coupling strategy from the entire distribution level. More specifically,\nduring training, StraightFM creates couplings of images and noise via one\ndiffusion model as a coupling prior to straighten trajectories for few-step\ngeneration. Our coupling strategy can also integrate with the existing coupling\ndirection from real data to noise, improving image quality in few-step\ngeneration. Experimental results on pixel space and latent space show that\nStraightFM yields attractive samples within 5 steps. Moreover, our\nunconditional StraightFM is seamlessly compatible with training-free multimodal\nconditional generation, maintaining high-quality image generation in few steps.\n","authors":["Siyu Xing","Jie Cao","Huaibo Huang","Haichao Shi","Xiao-Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2311.16507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.18733v3","updated":"2025-09-04T14:22:42Z","published":"2025-08-26T07:01:58Z","title":"Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from\n  Vector Drawings","summary":"  Computer-Aided Design (CAD) generative modeling is driving significant\ninnovations across industrial applications. Recent works have shown remarkable\nprogress in creating solid models from various inputs such as point clouds,\nmeshes, and text descriptions. However, these methods fundamentally diverge\nfrom traditional industrial workflows that begin with 2D engineering drawings.\nThe automatic generation of parametric CAD models from these 2D vector drawings\nremains underexplored despite being a critical step in engineering design. To\naddress this gap, our key insight is to reframe CAD generation as a\nsequence-to-sequence learning problem where vector drawing primitives directly\ninform the generation of parametric CAD operations, preserving geometric\nprecision and design intent throughout the transformation process. We propose\nDrawing2CAD, a framework with three key technical components: a\nnetwork-friendly vector primitive representation that preserves precise\ngeometric information, a dual-decoder transformer architecture that decouples\ncommand type and parameter generation while maintaining precise correspondence,\nand a soft target distribution loss function accommodating inherent flexibility\nin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,\na dataset of paired engineering drawings and parametric CAD models, and conduct\nthorough experiments to demonstrate the effectiveness of our method. Code and\ndataset are available at https://github.com/lllssc/Drawing2CAD.\n","authors":["Feiwei Qin","Shichao Lu","Junhao Hou","Changmiao Wang","Meie Fang","Ligang Liu"],"pdf_url":"https://arxiv.org/pdf/2508.18733v3.pdf","comment":"Accepted to ACM MM 2025"},{"id":"http://arxiv.org/abs/2509.04243v1","updated":"2025-09-04T14:17:01Z","published":"2025-09-04T14:17:01Z","title":"Learning Active Perception via Self-Evolving Preference Optimization for\n  GUI Grounding","summary":"  Vision Language Models (VLMs) have recently achieved significant progress in\nbridging visual perception and linguistic reasoning. Recently, OpenAI o3 model\nintroduced a zoom-in search strategy that effectively elicits active perception\ncapabilities in VLMs, improving downstream task performance. However, enabling\nVLMs to reason effectively over appropriate image regions remains a core\nchallenge in GUI grounding, particularly under high-resolution inputs and\ncomplex multi-element visual interactions. In this work, we propose LASER, a\nself-evolving framework that progressively endows VLMs with multi-step\nperception capabilities, enabling precise coordinate prediction. Specifically,\nour approach integrate Monte Carlo quality estimation with\nIntersection-over-Union (IoU)-based region quality evaluation to jointly\nencourage both accuracy and diversity in constructing high-quality preference\ndata. This combination explicitly guides the model to focus on\ninstruction-relevant key regions while adaptively allocating reasoning steps\nbased on task complexity. Comprehensive experiments on the ScreenSpot Pro and\nScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating\nthe effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER\nachieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new\nstate-of-the-art (SoTA) among 7B-scale models.\n","authors":["Wanfu Wang","Qipeng Huang","Guangquan Xue","Xiaobo Liang","Juntao Li"],"pdf_url":"https://arxiv.org/pdf/2509.04243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02807v2","updated":"2025-09-04T14:13:49Z","published":"2024-09-19T13:26:31Z","title":"AutoPETIII: The Tracer Frontier. What Frontier?","summary":"  For the last three years, the AutoPET competition gathered the medical\nimaging community around a hot topic: lesion segmentation on Positron Emitting\nTomography (PET) scans. Each year a different aspect of the problem is\npresented; in 2024 the multiplicity of existing and used tracers was at the\ncore of the challenge. Specifically, this year's edition aims to develop a\nfully automatic algorithm capable of performing lesion segmentation on a PET/CT\nscan, without knowing the tracer, which can either be a FDG or PSMA-based\ntracer. In this paper we describe how we used the nnUNetv2 framework to train\ntwo sets of 6 fold ensembles of models to perform fully automatic PET/CT lesion\nsegmentation as well as a MIP-CNN to choose which set of models to use for\nsegmentation.\n","authors":["Zacharia Mesbah","Léo Mottay","Romain Modzelewski","Pierre Decazes","Sébastien Hapdey","Su Ruan","Sébastien Thureau"],"pdf_url":"https://arxiv.org/pdf/2410.02807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14904v2","updated":"2025-09-04T13:42:44Z","published":"2025-07-20T10:28:06Z","title":"TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D\n  Visual Grounding based on CLIP","summary":"  3D visual grounding allows an embodied agent to understand visual information\nin real-world 3D environments based on human instructions, which is crucial for\nembodied intelligence. Existing 3D visual grounding methods typically rely on\nseparate encoders for different modalities (e.g., RGB images, text, and 3D\npoint clouds), resulting in large and complex models that are inefficient to\ntrain. While some approaches use pre-trained 2D multi-modal models like CLIP\nfor 3D tasks, they still struggle with aligning point cloud data to 2D\nencoders. As a result, these methods continue to depend on 3D encoders for\nfeature extraction, further increasing model complexity and training\ninefficiency. In this paper, we propose a unified 2D pre-trained multi-modal\nnetwork to process all three modalities (RGB images, text, and point clouds),\nsignificantly simplifying the architecture. By leveraging a 2D CLIP bi-modal\nmodel with adapter-based fine-tuning, this framework effectively adapts to the\ntri-modal setting, improving both adaptability and performance across\nmodalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module\nis designed to fuse geometric multi-scale features from point clouds and\nimages. We then integrate textual features for final modality fusion and\nintroduce a multi-modal decoder to facilitate deep cross-modal understanding.\nTogether, our method achieves unified feature extraction and fusion across the\nthree modalities, enabling an end-to-end 3D visual grounding model. Compared to\nthe baseline, our method reduces the number of trainable parameters by\napproximately 58\\%, while achieving a 6.52\\% improvement in the 3D detection\ntask and a 6.25\\% improvement in the 3D visual grounding task.\n","authors":["Fan Li","Zanyi Wang","Zeyi Huang","Guang Dai","Jingdong Wang","Mengmeng Wang"],"pdf_url":"https://arxiv.org/pdf/2507.14904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.22627v2","updated":"2025-09-04T13:26:56Z","published":"2025-07-30T12:48:29Z","title":"LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text\n  Pairing","summary":"  Fashion design is a complex creative process that blends visual and textual\nexpressions. Designers convey ideas through sketches, which define spatial\nstructure and design elements, and textual descriptions, capturing material,\ntexture, and stylistic details. In this paper, we present LOcalized Text and\nSketch for fashion image generation (LOTS), an approach for compositional\nsketch-text based generation of complete fashion outlooks. LOTS leverages a\nglobal description with paired localized sketch + text information for\nconditioning and introduces a novel step-based merging strategy for diffusion\nadaptation. First, a Modularized Pair-Centric representation encodes sketches\nand text into a shared latent space while preserving independent localized\nfeatures; then, a Diffusion Pair Guidance phase integrates both local and\nglobal conditioning via attention-based guidance within the diffusion model's\nmulti-step denoising process. To validate our method, we build on Fashionpedia\nto release Sketchy, the first fashion dataset where multiple text-sketch pairs\nare provided per image. Quantitative results show LOTS achieves\nstate-of-the-art image generation performance on both global and localized\nmetrics, while qualitative examples and a human evaluation study highlight its\nunprecedented level of design customization.\n","authors":["Federico Girella","Davide Talon","Ziyue Liu","Zanxi Ruan","Yiming Wang","Marco Cristani"],"pdf_url":"https://arxiv.org/pdf/2507.22627v2.pdf","comment":"Accepted at ICCV25 (Oral). Project page:\n  https://intelligolabs.github.io/lots/"},{"id":"http://arxiv.org/abs/2503.20652v5","updated":"2025-09-04T13:19:09Z","published":"2025-03-26T15:47:50Z","title":"Imitating Radiological Scrolling: A Global-Local Attention Model for 3D\n  Chest CT Volumes Multi-Label Anomaly Classification","summary":"  The rapid increase in the number of Computed Tomography (CT) scan\nexaminations has created an urgent need for automated tools, such as organ\nsegmentation, anomaly classification, and report generation, to assist\nradiologists with their growing workload. Multi-label classification of\nThree-Dimensional (3D) CT scans is a challenging task due to the volumetric\nnature of the data and the variety of anomalies to be detected. Existing deep\nlearning methods based on Convolutional Neural Networks (CNNs) struggle to\ncapture long-range dependencies effectively, while Vision Transformers require\nextensive pre-training, posing challenges for practical use. Additionally,\nthese existing methods do not explicitly model the radiologist's navigational\nbehavior while scrolling through CT scan slices, which requires both global\ncontext understanding and local detail awareness. In this study, we present\nCT-Scroll, a novel global-local attention model specifically designed to\nemulate the scrolling behavior of radiologists during the analysis of 3D CT\nscans. Our approach is evaluated on two public datasets, demonstrating its\nefficacy through comprehensive experiments and an ablation study that\nhighlights the contribution of each model component.\n","authors":["Theo Di Piazza","Carole Lazarus","Olivier Nempont","Loic Boussel"],"pdf_url":"https://arxiv.org/pdf/2503.20652v5.pdf","comment":"13 pages, 4 figures. Accepted for publication at MIDL 2025"},{"id":"http://arxiv.org/abs/2509.04193v1","updated":"2025-09-04T13:15:16Z","published":"2025-09-04T13:15:16Z","title":"DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval","summary":"  Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images of\nthe same category across diverse domains without relying on annotations.\nExisting UCIR methods, which align cross-domain features for the entire image,\noften struggle with the domain gap, as the object features critical for\nretrieval are frequently entangled with domain-specific styles. To address this\nchallenge, we propose DUDE, a novel UCIR method building upon feature\ndisentanglement. In brief, DUDE leverages a text-to-image generative model to\ndisentangle object features from domain-specific styles, thus facilitating\nsemantical image retrieval. To further achieve reliable alignment of the\ndisentangled object features, DUDE aligns mutual neighbors from within domains\nto across domains in a progressive manner. Extensive experiments demonstrate\nthat DUDE achieves state-of-the-art performance across three benchmark datasets\nover 13 domains. The code will be released.\n","authors":["Ruohong Yang","Peng Hu","Yunfan Li","Xi Peng"],"pdf_url":"https://arxiv.org/pdf/2509.04193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04180v1","updated":"2025-09-04T12:54:32Z","published":"2025-09-04T12:54:32Z","title":"VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision","summary":"  AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.\n","authors":["Safouane El Ghazouali","Umberto Michelucci"],"pdf_url":"https://arxiv.org/pdf/2509.04180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04156v1","updated":"2025-09-04T12:32:04Z","published":"2025-09-04T12:32:04Z","title":"YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind\n  Turbine Components","summary":"  Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up\nnew opportunities for monitoring wind power plants, including blades, towers,\nand other critical components. However, reliable defect detection requires\nhigh-resolution data and efficient methods to process multispectral imagery. In\nthis research, we aim to enhance defect detection accuracy through the\ndevelopment of an ensemble of YOLO-based deep learning models that integrate\nboth visible and thermal channels. We propose an ensemble approach that\nintegrates a general-purpose YOLOv8 model with a specialized thermal model,\nusing a sophisticated bounding box fusion algorithm to combine their\npredictions. Our experiments show this approach achieves a mean Average\nPrecision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone\nYOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that\ncombining multiple YOLO architectures with fused multispectral data provides a\nmore reliable solution, improving the detection of both visual and thermal\ndefects.\n","authors":["Serhii Svystun","Pavlo Radiuk","Oleksandr Melnychenko","Oleg Savenko","Anatoliy Sachenko"],"pdf_url":"https://arxiv.org/pdf/2509.04156v1.pdf","comment":"The 13th IEEE International Conference on Intelligent Data\n  Acquisition and Advanced Computing Systems: Technology and Applications, 4-6\n  September, 2025, Gliwice, Poland"},{"id":"http://arxiv.org/abs/2509.04150v1","updated":"2025-09-04T12:23:59Z","published":"2025-09-04T12:23:59Z","title":"Revisiting Simple Baselines for In-The-Wild Deepfake Detection","summary":"  The widespread adoption of synthetic media demands accessible deepfake\ndetectors and realistic benchmarks. While most existing research evaluates\ndeepfake detectors on highly controlled datasets, we focus on the recently\nreleased \"in-the-wild\" benchmark, Deepfake-Eval-2024. Initial reporting on\nDeepfake-Eval-2024 showed that three finetuned open-source models achieve\naccuracies between 61% and 69%, significantly lagging behind the leading\ncommercial deepfake detector with 82% accuracy. Our work revisits one of these\nbaseline approaches, originally introduced by Ojha et al., which adapts\nstandard pretrained vision backbones to produce generalizable deepfake\ndetectors. We demonstrate that with better-tuned hyperparameters, this simple\napproach actually yields much higher performance -- 81% accuracy on\nDeepfake-Eval-2024 -- surpassing the previously reported accuracy of this\nbaseline approach by 18% and competing with commercial deepfake detectors. We\ndiscuss tradeoffs in accuracy, computational costs, and interpretability,\nfocusing on how practical these deepfake detectors might be when deployed in\nreal-world settings. Our code can be found at\nhttps://github.com/Deepfake-Detection-KKO/deepfake-detection.\n","authors":["Orlando Castaneda","Kevin So-Tang","Kshitij Gurung"],"pdf_url":"https://arxiv.org/pdf/2509.04150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06911v2","updated":"2025-09-04T12:21:13Z","published":"2024-05-11T04:47:50Z","title":"Replication Study and Benchmarking of Real-Time Object Detection Models","summary":"  This work examines the reproducibility and benchmarking of state-of-the-art\nreal-time object detection models. As object detection models are often used in\nreal-world contexts, such as robotics, where inference time is paramount,\nsimply measuring models' accuracy is not enough to compare them. We thus\ncompare a large variety of object detection models' accuracy and inference\nspeed on multiple graphics cards. In addition to this large benchmarking\nattempt, we also reproduce the following models from scratch using PyTorch on\nthe MS COCO 2017 dataset: DETR, RTMDet, ViTDet and YOLOv7. More importantly, we\npropose a unified training and evaluation pipeline, based on MMDetection's\nfeatures, to better compare models. Our implementation of DETR and ViTDet could\nnot achieve accuracy or speed performances comparable to what is declared in\nthe original papers. On the other hand, reproduced RTMDet and YOLOv7 could\nmatch such performances. Studied papers are also found to be generally lacking\nfor reproducibility purposes. As for MMDetection pretrained models, speed\nperformances are severely reduced with limited computing resources (larger,\nmore accurate models even more so). Moreover, results exhibit a strong\ntrade-off between accuracy and speed, prevailed by anchor-free models - notably\nRTMDet or YOLOx models. The code used is this paper and all the experiments is\navailable in the repository at https://github.com/willGuimont/segdet_mlcr2024.\n","authors":["Pierre-Luc Asselin","Vincent Coulombe","William Guimont-Martin","William Larrivée-Hardy"],"pdf_url":"https://arxiv.org/pdf/2405.06911v2.pdf","comment":"Authors are presented in alphabetical order, each having equal\n  contribution to the work"},{"id":"http://arxiv.org/abs/2509.04145v1","updated":"2025-09-04T12:15:55Z","published":"2025-09-04T12:15:55Z","title":"Hyper Diffusion Avatars: Dynamic Human Avatar Generation using Network\n  Weight Space Diffusion","summary":"  Creating human avatars is a highly desirable yet challenging task. Recent\nadvancements in radiance field rendering have achieved unprecedented\nphotorealism and real-time performance for personalized dynamic human avatars.\nHowever, these approaches are typically limited to person-specific rendering\nmodels trained on multi-view video data for a single individual, limiting their\nability to generalize across different identities. On the other hand,\ngenerative approaches leveraging prior knowledge from pre-trained 2D diffusion\nmodels can produce cartoonish, static human avatars, which are animated through\nsimple skeleton-based articulation. Therefore, the avatars generated by these\nmethods suffer from lower rendering quality compared to person-specific\nrendering methods and fail to capture pose-dependent deformations such as cloth\nwrinkles. In this paper, we propose a novel approach that unites the strengths\nof person-specific rendering and diffusion-based generative modeling to enable\ndynamic human avatar generation with both high photorealism and realistic\npose-dependent deformations. Our method follows a two-stage pipeline: first, we\noptimize a set of person-specific UNets, with each network representing a\ndynamic human avatar that captures intricate pose-dependent deformations. In\nthe second stage, we train a hyper diffusion model over the optimized network\nweights. During inference, our method generates network weights for real-time,\ncontrollable rendering of dynamic human avatars. Using a large-scale,\ncross-identity, multi-view video dataset, we demonstrate that our approach\noutperforms state-of-the-art human avatar generation methods.\n","authors":["Dongliang Cao","Guoxing Sun","Marc Habermann","Florian Bernard"],"pdf_url":"https://arxiv.org/pdf/2509.04145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04126v1","updated":"2025-09-04T11:44:28Z","published":"2025-09-04T11:44:28Z","title":"MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image\n  Generation","summary":"  Text-to-image diffusion models have achieved remarkable image quality, but\nthey still struggle with complex, multiele ment prompts, and limited stylistic\ndiversity. To address these limitations, we propose a Multi-Expert Planning and\nGen eration Framework (MEPG) that synergistically integrates position- and\nstyle-aware large language models (LLMs) with spatial-semantic expert modules.\nThe framework comprises two core components: (1) a Position-Style-Aware (PSA)\nmodule that utilizes a supervised fine-tuned LLM to decom pose input prompts\ninto precise spatial coordinates and style encoded semantic instructions; and\n(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera\ntion through dynamic expert routing across both local regions and global areas.\nDuring the generation process for each lo cal region, specialized models (e.g.,\nrealism experts, styliza tion specialists) are selectively activated for each\nspatial par tition via attention-based gating mechanisms. The architec ture\nsupports lightweight integration and replacement of ex pert models, providing\nstrong extensibility. Additionally, an interactive interface enables real-time\nspatial layout editing and per-region style selection from a portfolio of\nexperts. Ex periments show that MEPG significantly outperforms base line models\nwith the same backbone in both image quality\n  and style diversity.\n","authors":["Yuan Zhao","Liu Lin"],"pdf_url":"https://arxiv.org/pdf/2509.04126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04123v1","updated":"2025-09-04T11:37:06Z","published":"2025-09-04T11:37:06Z","title":"TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering","summary":"  Text-to-story visualization is challenging due to the need for consistent\ninteraction among multiple characters across frames. Existing methods struggle\nwith character consistency, leading to artifact generation and inaccurate\ndialogue rendering, which results in disjointed storytelling. In response, we\nintroduce TaleDiffusion, a novel framework for generating multi-character\nstories with an iterative process, maintaining character consistency, and\naccurate dialogue assignment via postprocessing. Given a story, we use a\npre-trained LLM to generate per-frame descriptions, character details, and\ndialogues via in-context learning, followed by a bounded attention-based\nper-box mask technique to control character interactions and minimize\nartifacts. We then apply an identity-consistent self-attention mechanism to\nensure character consistency across frames and region-aware cross-attention for\nprecise object placement. Dialogues are also rendered as bubbles and assigned\nto characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion\noutperforms existing methods in consistency, noise reduction, and dialogue\nrendering.\n","authors":["Ayan Banerjee","Josep Lladós","Umapada Pal","Anjan Dutta"],"pdf_url":"https://arxiv.org/pdf/2509.04123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04117v1","updated":"2025-09-04T11:30:29Z","published":"2025-09-04T11:30:29Z","title":"DVS-PedX: Synthetic-and-Real Event-Based Pedestrian Dataset","summary":"  Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness\nchanges instead of full frames, offering low latency, high dynamic range, and\nmotion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a\nneuromorphic dataset designed for pedestrian detection and crossing-intention\nanalysis in normal and adverse weather conditions across two complementary\nsources: (1) synthetic event streams generated in the CARLA simulator for\ncontrolled \"approach-cross\" scenes under varied weather and lighting; and (2)\nreal-world JAAD dash-cam videos converted to event streams using the v2e tool,\npreserving natural behaviors and backgrounds. Each sequence includes paired RGB\nframes, per-frame DVS \"event frames\" (33 ms accumulations), and frame-level\nlabels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0\nevent files and AVI DVS video files and metadata for flexible re-processing.\nBaseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset\nusability and reveal a sim-to-real gap, motivating domain adaptation and\nmultimodal fusion. DVS-PedX aims to accelerate research in event-based\npedestrian safety, intention prediction, and neuromorphic perception.\n","authors":["Mustafa Sakhai","Kaung Sithu","Min Khant Soe Oke","Maciej Wielgosz"],"pdf_url":"https://arxiv.org/pdf/2509.04117v1.pdf","comment":"12 pages, 8 figures, 3 tables; dataset descriptor paper introducing\n  DVS-PedX (synthetic-and-real event-based pedestrian dataset with baselines)\n  External URL: https://doi.org/10.5281/zenodo.17030898"},{"id":"http://arxiv.org/abs/2509.04107v1","updated":"2025-09-04T11:11:10Z","published":"2025-09-04T11:11:10Z","title":"FedQuad: Federated Stochastic Quadruplet Learning to Mitigate Data\n  Heterogeneity","summary":"  Federated Learning (FL) provides decentralised model training, which\neffectively tackles problems such as distributed data and privacy preservation.\nHowever, the generalisation of global models frequently faces challenges from\ndata heterogeneity among clients. This challenge becomes even more pronounced\nwhen datasets are limited in size and class imbalance. To address data\nheterogeneity, we propose a novel method, \\textit{FedQuad}, that explicitly\noptimises smaller intra-class variance and larger inter-class variance across\nclients, thereby decreasing the negative impact of model aggregation on the\nglobal model over client representations. Our approach minimises the distance\nbetween similar pairs while maximising the distance between negative pairs,\neffectively disentangling client data in the shared feature space. We evaluate\nour method on the CIFAR-10 and CIFAR-100 datasets under various data\ndistributions and with many clients, demonstrating superior performance\ncompared to existing approaches. Furthermore, we provide a detailed analysis of\nmetric learning-based strategies within both supervised and federated learning\nparadigms, highlighting their efficacy in addressing representational learning\nchallenges in federated settings.\n","authors":["Ozgu Goksu","Nicolas Pugeault"],"pdf_url":"https://arxiv.org/pdf/2509.04107v1.pdf","comment":"The 3rd IEEE International Conference on Federated Learning\n  Technologies and Applications (FLTA25)"},{"id":"http://arxiv.org/abs/2411.15537v4","updated":"2025-09-04T11:00:46Z","published":"2024-11-23T12:18:28Z","title":"MUNBa: Machine Unlearning via Nash Bargaining","summary":"  Machine Unlearning (MU) aims to selectively erase harmful behaviors from\nmodels while retaining the overall utility of the model. As a multi-task\nlearning problem, MU involves balancing objectives related to forgetting\nspecific concepts/data and preserving general performance. A naive integration\nof these forgetting and preserving objectives can lead to gradient conflicts\nand dominance, impeding MU algorithms from reaching optimal solutions. To\naddress the gradient conflict and dominance issue, we reformulate MU as a\ntwo-player cooperative game, where the two players, namely, the forgetting\nplayer and the preservation player, contribute via their gradient proposals to\nmaximize their overall gain and balance their contributions. To this end,\ninspired by the Nash bargaining theory, we derive a closed-form solution to\nguide the model toward the Pareto stationary point. Our formulation of MU\nguarantees an equilibrium solution, where any deviation from the final state\nwould lead to a reduction in the overall objectives for both players, ensuring\noptimality in each objective. We evaluate our algorithm's effectiveness on a\ndiverse set of tasks across image classification and image generation.\nExtensive experiments with ResNet, vision-language model CLIP, and\ntext-to-image diffusion models demonstrate that our method outperforms\nstate-of-the-art MU algorithms, achieving a better trade-off between forgetting\nand preserving. Our results also highlight improvements in forgetting\nprecision, preservation of generalization, and robustness against adversarial\nattacks.\n","authors":["Jing Wu","Mehrtash Harandi"],"pdf_url":"https://arxiv.org/pdf/2411.15537v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05774v2","updated":"2025-09-04T10:49:27Z","published":"2025-04-08T07:53:51Z","title":"Transferable Mask Transformer: Cross-domain Semantic Segmentation with\n  Region-adaptive Transferability Estimation","summary":"  Recent advances in Vision Transformers (ViTs) have set new benchmarks in\nsemantic segmentation. However, when adapting pretrained ViTs to new target\ndomains, significant performance degradation often occurs due to distribution\nshifts, resulting in suboptimal global attention. Since self-attention\nmechanisms are inherently data-driven, they may fail to effectively attend to\nkey objects when source and target domains exhibit differences in texture,\nscale, or object co-occurrence patterns. While global and patch-level domain\nadaptation methods provide partial solutions, region-level adaptation with\ndynamically shaped regions is crucial due to spatial heterogeneity in\ntransferability across different image areas. We present Transferable Mask\nTransformer (TMT), a novel region-level adaptation framework for semantic\nsegmentation that aligns cross-domain representations through spatial\ntransferability analysis. TMT consists of two key components: (1) An Adaptive\nCluster-based Transferability Estimator (ACTE) that dynamically segments images\ninto structurally and semantically coherent regions for localized\ntransferability assessment, and (2) A Transferable Masked Attention (TMA)\nmodule that integrates region-specific transferability maps into ViTs'\nattention mechanisms, prioritizing adaptation in regions with low\ntransferability and high semantic uncertainty. Comprehensive evaluations across\n20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2%\nMIoU improvement over vanilla fine-tuning and a 1.28% increase compared to\nstate-of-the-art baselines. The source code will be publicly available.\n","authors":["Jianhua Liu","Zhengyu Li","Yanru Wu","Jingge Wang","Yang Tan","Ruizhe Zhao","Guan Wang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2504.05774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04092v1","updated":"2025-09-04T10:48:25Z","published":"2025-09-04T10:48:25Z","title":"TriLiteNet: Lightweight Model for Multi-Task Visual Perception","summary":"  Efficient perception models are essential for Advanced Driver Assistance\nSystems (ADAS), as these applications require rapid processing and response to\nensure safety and effectiveness in real-world environments. To address the\nreal-time execution needs of such perception models, this study introduces the\nTriLiteNet model. This model can simultaneously manage multiple tasks related\nto panoramic driving perception. TriLiteNet is designed to optimize performance\nwhile maintaining low computational costs. Experimental results on the BDD100k\ndataset demonstrate that the model achieves competitive performance across\nthree key tasks: vehicle detection, drivable area segmentation, and lane line\nsegmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of\n85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for\ndrivable area segmentation, and an Acc of 82.3% for lane line segmentation with\nonly 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed\nmodel includes a tiny configuration with just 0.14M parameters, which provides\na multi-task solution with minimal computational demand. Evaluated for latency\nand power consumption on embedded devices, TriLiteNet in both configurations\nshows low latency and reasonable power during inference. By balancing\nperformance, computational efficiency, and scalability, TriLiteNet offers a\npractical and deployable solution for real-world autonomous driving\napplications. Code is available at https://github.com/chequanghuy/TriLiteNet.\n","authors":["Quang-Huy Che","Duc-Khai Lam"],"pdf_url":"https://arxiv.org/pdf/2509.04092v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04086v1","updated":"2025-09-04T10:32:40Z","published":"2025-09-04T10:32:40Z","title":"TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale\n  Category-Aware Temporal Graph","summary":"  Audio-Visual Video Parsing (AVVP) task aims to identify event categories and\ntheir occurrence times in a given video with weakly supervised labels. Existing\nmethods typically fall into two categories: (i) designing enhanced\narchitectures based on attention mechanism for better temporal modeling, and\n(ii) generating richer pseudo-labels to compensate for the absence of\nframe-level annotations. However, the first type methods treat noisy\nsegment-level pseudo labels as reliable supervision and the second type methods\nlet indiscriminate attention spread them across all frames, the initial errors\nare repeatedly amplified during training. To address this issue, we propose a\nmethod that combines the Bi-Directional Text Fusion (BiT) module and\nCategory-Aware Temporal Graph (CATS) module. Specifically, we integrate the\nstrengths and complementarity of the two previous research directions. We first\nperform semantic injection and dynamic calibration on audio and visual modality\nfeatures through the BiT module, to locate and purify cleaner and richer\nsemantic cues. Then, we leverage the CATS module for semantic propagation and\nconnection to enable precise semantic information dissemination across time.\nExperimental results demonstrate that our proposed method achieves\nstate-of-the-art (SOTA) performance in multiple key indicators on two benchmark\ndatasets, LLP and UnAV-100.\n","authors":["Yaru Chen","Faegheh Sardari","Peiliang Zhang","Ruohao Guo","Yang Xiang","Zhenbo Li","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2509.04086v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13923v2","updated":"2025-09-04T10:10:23Z","published":"2024-06-20T01:43:08Z","title":"PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal\n  Documents","summary":"  Recent advancements in large multimodal models (LMMs) have leveraged\nextensive multimodal datasets to enhance capabilities in complex\nknowledge-driven tasks. However, persistent challenges in perceptual and\nreasoning errors limit their efficacy, particularly in interpreting intricate\nvisual data and deducing multimodal relationships. To address these issues, we\nintroduce PIN (Paired and INterleaved multimodal documents), a novel data\nformat designed to foster a deeper integration of visual and textual knowledge.\nThe PIN format uniquely combines semantically rich Markdown files, which\npreserve fine-grained textual structures, with holistic overall images that\ncapture the complete document layout. Following this format, we construct and\nrelease two large-scale, open-source datasets: PIN-200M (~200 million\ndocuments) and PIN-14M (~14 million), compiled from diverse web and scientific\nsources in both English and Chinese. To maximize usability, we provide detailed\nstatistical analyses and equip the datasets with quality signals, enabling\nresearchers to easily filter and select data for specific tasks. Our work\nprovides the community with a versatile data format and substantial resources,\noffering a foundation for new research in pre-training strategies and the\ndevelopment of more powerful knowledge-intensive LMMs.\n","authors":["Junjie Wang","Yuxiang Zhang","Minghao Liu","Yin Zhang","Yatai Ji","Weihao Xuan","Nie Lin","Kang Zhu","Zhiqiang Lin","Yiming Ren","Chunyang Jiang","Yiyao Yu","Zekun Wang","Tiezhen Wang","Wenhao Huang","Jie Fu","Qunshu Liu","Yujiu Yang","Ge Zhang","Ruibin Yuan","Bei Chen","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.13923v2.pdf","comment":"Technical report v1.0"},{"id":"http://arxiv.org/abs/2405.20188v2","updated":"2025-09-04T10:02:22Z","published":"2024-05-30T15:55:04Z","title":"SPARE: Symmetrized Point-to-Plane Distance for Robust Non-Rigid\n  Registration","summary":"  Existing optimization-based methods for non-rigid registration typically\nminimize an alignment error metric based on the point-to-point or\npoint-to-plane distance between corresponding point pairs on the source surface\nand target surface. However, these metrics can result in slow convergence or a\nloss of detail. In this paper, we propose SPARE, a novel formulation that\nutilizes a symmetrized point-to-plane distance for robust non-rigid\nregistration. The symmetrized point-to-plane distance relies on both the\npositions and normals of the corresponding points, resulting in a more accurate\napproximation of the underlying geometry and can achieve higher accuracy than\nexisting methods. To solve this optimization problem efficiently, we introduce\nan as-rigid-as-possible regulation term to estimate the deformed normals and\npropose an alternating minimization solver using a majorization-minimization\nstrategy. Moreover, for effective initialization of the solver, we incorporate\na deformation graph-based coarse alignment that improves registration quality\nand efficiency. Extensive experiments show that the proposed method greatly\nimproves the accuracy of non-rigid registration problems and maintains\nrelatively high solution efficiency. The code is publicly available at\nhttps://github.com/yaoyx689/spare.\n","authors":["Yuxin Yao","Bailin Deng","Junhui Hou","Juyong Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.20188v2.pdf","comment":"Accepted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence"},{"id":"http://arxiv.org/abs/2509.04058v1","updated":"2025-09-04T09:41:18Z","published":"2025-09-04T09:41:18Z","title":"SMooGPT: Stylized Motion Generation using Large Language Models","summary":"  Stylized motion generation is actively studied in computer graphics,\nespecially benefiting from the rapid advances in diffusion models. The goal of\nthis task is to produce a novel motion respecting both the motion content and\nthe desired motion style, e.g., ``walking in a loop like a Monkey''. Existing\nresearch attempts to address this problem via motion style transfer or\nconditional motion generation. They typically embed the motion style into a\nlatent space and guide the motion implicitly in a latent space as well. Despite\nthe progress, their methods suffer from low interpretability and control,\nlimited generalization to new styles, and fail to produce motions other than\n``walking'' due to the strong bias in the public stylization dataset. In this\npaper, we propose to solve the stylized motion generation problem from a new\nperspective of reasoning-composition-generation, based on our observations: i)\nhuman motion can often be effectively described using natural language in a\nbody-part centric manner, ii) LLMs exhibit a strong ability to understand and\nreason about human motion, and iii) human motion has an inherently\ncompositional nature, facilitating the new motion content or style generation\nvia effective recomposing. We thus propose utilizing body-part text space as an\nintermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a\nreasoner, composer, and generator when generating the desired stylized motion.\nOur method executes in the body-part text space with much higher\ninterpretability, enabling fine-grained motion control, effectively resolving\npotential conflicts between motion content and style, and generalizes well to\nnew styles thanks to the open-vocabulary ability of LLMs. Comprehensive\nexperiments and evaluations, and a user perceptual study, demonstrate the\neffectiveness of our approach, especially under the pure text-driven stylized\nmotion generation.\n","authors":["Lei Zhong","Yi Yang","Changjian Li"],"pdf_url":"https://arxiv.org/pdf/2509.04058v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.15269v2","updated":"2025-09-04T09:36:19Z","published":"2025-07-21T06:16:27Z","title":"Conditional Video Generation for High-Efficiency Video Compression","summary":"  Perceptual studies demonstrate that conditional diffusion models excel at\nreconstructing video content aligned with human visual perception. Building on\nthis insight, we propose a video compression framework that leverages\nconditional diffusion models for perceptually optimized reconstruction.\nSpecifically, we reframe video compression as a conditional generation task,\nwhere a generative model synthesizes video from sparse, yet informative\nsignals. Our approach introduces three key modules: (1) Multi-granular\nconditioning that captures both static scene structure and dynamic\nspatio-temporal cues; (2) Compact representations designed for efficient\ntransmission without sacrificing semantic richness; (3) Multi-condition\ntraining with modality dropout and role-aware embeddings, which prevent\nover-reliance on any single modality and enhance robustness. Extensive\nexperiments show that our method significantly outperforms both traditional and\nneural codecs on perceptual quality metrics such as Fr\\'echet Video Distance\n(FVD) and LPIPS, especially under high compression ratios.\n","authors":["Fangqiu Yi","Jingyu Xu","Jiawei Shao","Chi Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2507.15269v2.pdf","comment":"Critical methodology flaws invalidate key results"},{"id":"http://arxiv.org/abs/2409.06002v5","updated":"2025-09-04T09:32:46Z","published":"2024-09-09T19:01:14Z","title":"Enhanced Generative Data Augmentation for Semantic Segmentation via\n  Stronger Guidance","summary":"  Data augmentation is crucial for pixel-wise annotation tasks like semantic\nsegmentation, where labeling requires significant effort and intensive labor.\nTraditional methods, involving simple transformations such as rotations and\nflips, create new images but often lack diversity along key semantic dimensions\nand fail to alter high-level semantic properties. To address this issue,\ngenerative models have emerged as an effective solution for augmenting data by\ngenerating synthetic images. Controllable Generative models offer data\naugmentation methods for semantic segmentation tasks by using prompts and\nvisual references from the original image. However, these models face\nchallenges in generating synthetic images that accurately reflect the content\nand structure of the original image due to difficulties in creating effective\nprompts and visual references. In this work, we introduce an effective data\naugmentation pipeline for semantic segmentation using Controllable Diffusion\nmodel. Our proposed method includes efficient prompt generation using\nClass-Prompt Appending and Visual Prior Blending to enhance attention to\nlabeled classes in real images, allowing the pipeline to generate a precise\nnumber of augmented images while preserving the structure of\nsegmentation-labeled classes. In addition, we implement a class balancing\nalgorithm to ensure a balanced training dataset when merging the synthetic and\noriginal images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates\nits effectiveness in generating high-quality synthetic images for semantic\nsegmentation. Our code is available at\nhttps://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance.\n","authors":["Quang-Huy Che","Duc-Tri Le","Bich-Nga Pham","Duc-Khai Lam","Vinh-Tiep Nguyen"],"pdf_url":"https://arxiv.org/pdf/2409.06002v5.pdf","comment":"Published in ICPRAM 2025, ISBN 978-989-758-730-6, ISSN 2184-4313"},{"id":"http://arxiv.org/abs/2509.04050v1","updated":"2025-09-04T09:29:25Z","published":"2025-09-04T09:29:25Z","title":"A Re-ranking Method using K-nearest Weighted Fusion for Person\n  Re-identification","summary":"  In person re-identification, re-ranking is a crucial step to enhance the\noverall accuracy by refining the initial ranking of retrieved results. Previous\nstudies have mainly focused on features from single-view images, which can\ncause view bias and issues like pose variation, viewpoint changes, and\nocclusions. Using multi-view features to present a person can help reduce view\nbias. In this work, we present an efficient re-ranking method that generates\nmulti-view features by aggregating neighbors' features using K-nearest Weighted\nFusion (KWF) method. Specifically, we hypothesize that features extracted from\nre-identification models are highly similar when representing the same\nidentity. Thus, we select K neighboring features in an unsupervised manner to\ngenerate multi-view features. Additionally, this study explores the weight\nselection strategies during feature aggregation, allowing us to identify an\neffective strategy. Our re-ranking approach does not require model fine-tuning\nor extra annotations, making it applicable to large-scale datasets. We evaluate\nour method on the person re-identification datasets Market1501, MSMT17, and\nOccluded-DukeMTMC. The results show that our method significantly improves\nRank@1 and mAP when re-ranking the top M candidates from the initial ranking\nresults. Specifically, compared to the initial results, our re-ranking method\nachieves improvements of 9.8%/22.0% in Rank@1 on the challenging datasets:\nMSMT17 and Occluded-DukeMTMC, respectively. Furthermore, our approach\ndemonstrates substantial enhancements in computational efficiency compared to\nother re-ranking methods.\n","authors":["Quang-Huy Che","Le-Chuong Nguyen","Gia-Nghia Tran","Dinh-Duy Phan","Vinh-Tiep Nguyen"],"pdf_url":"https://arxiv.org/pdf/2509.04050v1.pdf","comment":"Published in ICPRAM 2025, ISBN 978-989-758-730-6, ISSN 2184-4313"},{"id":"http://arxiv.org/abs/2509.04047v1","updated":"2025-09-04T09:28:20Z","published":"2025-09-04T09:28:20Z","title":"TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface\n  Scattering for Perlin Distributed Heterogeneous Media","summary":"  Estimating scattering parameters of heterogeneous media from images is a\nseverely under-constrained and challenging problem. Most of the existing\napproaches model BSSRDF either through an analysis-by-synthesis approach,\napproximating complex path integrals, or using differentiable volume rendering\ntechniques to account for heterogeneity. However, only a few studies have\napplied learning-based methods to estimate subsurface scattering parameters,\nbut they assume homogeneous media. Interestingly, no specific distribution is\nknown to us that can explicitly model the heterogeneous scattering parameters\nin the real world. Notably, procedural noise models such as Perlin and Fractal\nPerlin noise have been effective in representing intricate heterogeneities of\nnatural, organic, and inorganic surfaces. Leveraging this, we first create\nHeteroSynth, a synthetic dataset comprising photorealistic images of\nheterogeneous media whose scattering parameters are modeled using Fractal\nPerlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a\nlearning-based feed-forward framework to estimate these Perlin-distributed\nheterogeneous scattering parameters from sparse multi-view image observations.\nInstead of directly predicting the 3D scattering parameter volume, TensoIS uses\nlearnable low-rank tensor components to represent the scattering volume. We\nevaluate TensoIS on unseen heterogeneous variations over shapes from the\nHeteroSynth test set, smoke and cloud geometries obtained from open-source\nrealistic volumetric simulations, and some real-world samples to establish its\neffectiveness for inverse scattering. Overall, this study is an attempt to\nexplore Perlin noise distribution, given the lack of any such well-defined\ndistribution in literature, to potentially model real-world heterogeneous\nscattering in a feed-forward manner.\n","authors":["Ashish Tiwari","Satyam Bhardwaj","Yash Bachwana","Parag Sarvoday Sahu","T. M. Feroz Ali","Bhargava Chintalapati","Shanmuganathan Raman"],"pdf_url":"https://arxiv.org/pdf/2509.04047v1.pdf","comment":"To appear in Pacific Graphics 2025 (CGF Journal Track), Project page:\n  https://yashbachwana.github.io/TensoIS/"},{"id":"http://arxiv.org/abs/2505.07611v2","updated":"2025-09-04T09:28:05Z","published":"2025-05-12T14:34:22Z","title":"Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A\n  Comprehensive Review of Methods, Datasets, and Future Directions","summary":"  Traffic accident prediction and detection are critical for enhancing road\nsafety, and vision-based traffic accident anticipation (Vision-TAA) has emerged\nas a promising approach in the era of deep learning. This paper reviews 147\nrecent studies, focusing on the application of supervised, unsupervised, and\nhybrid deep learning models for accident prediction, alongside the use of\nreal-world and synthetic datasets. Current methodologies are categorized into\nfour key approaches: image and video feature-based prediction, spatio-temporal\nfeature-based prediction, scene understanding, and multi modal data fusion.\nWhile these methods demonstrate significant potential, challenges such as data\nscarcity, limited generalization to complex scenarios, and real-time\nperformance constraints remain prevalent. This review highlights opportunities\nfor future research, including the integration of multi modal data fusion,\nself-supervised learning, and Transformer-based architectures to enhance\nprediction accuracy and scalability. By synthesizing existing advancements and\nidentifying critical gaps, this paper provides a foundational reference for\ndeveloping robust and adaptive Vision-TAA systems, contributing to road safety\nand traffic management.\n","authors":["Ruonan Lin","Tao Tang","Yongtai Liu","Wenye Zhou","Xin Yang","Hao Zheng","Jianpu Lin","Yi Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.07611v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04043v1","updated":"2025-09-04T09:26:00Z","published":"2025-09-04T09:26:00Z","title":"Millisecond-Response Tracking and Gazing System for UAVs: A Domestic\n  Solution Based on \"Phytium + Cambricon\"","summary":"  In the frontier research and application of current video surveillance\ntechnology, traditional camera systems exhibit significant limitations of\nresponse delay exceeding 200 ms in dynamic scenarios due to the insufficient\ndeep feature extraction capability of automatic recognition algorithms and the\nefficiency bottleneck of computing architectures, failing to meet the real-time\nrequirements in complex scenes. To address this issue, this study proposes a\nheterogeneous computing architecture based on Phytium processors and Cambricon\naccelerator cards, constructing a UAV tracking and gazing system with\nmillisecond-level response capability. At the hardware level, the system adopts\na collaborative computing architecture of Phytium FT-2000/4 processors and\nMLU220 accelerator cards, enhancing computing power through multi-card\nparallelism. At the software level, it innovatively integrates a lightweight\nYOLOv5s detection network with a DeepSORT cascaded tracking algorithm, forming\na closed-loop control chain of \"detection-tracking-feedback\". Experimental\nresults demonstrate that the system achieves a stable single-frame\ncomprehensive processing delay of 50-100 ms in 1920*1080 resolution video\nstream processing, with a multi-scale target recognition accuracy of over\n98.5%, featuring both low latency and high precision. This study provides an\ninnovative solution for UAV monitoring and the application of domestic chips.\n","authors":["Yuchen Zhu","Longxiang Yin","Kai Zhao"],"pdf_url":"https://arxiv.org/pdf/2509.04043v1.pdf","comment":"16 pages,17 figures"},{"id":"http://arxiv.org/abs/2403.05702v2","updated":"2025-09-04T09:22:57Z","published":"2024-03-08T22:25:15Z","title":"Spatial-aware Transformer-GRU Framework for Enhanced Glaucoma Diagnosis\n  from 3D OCT Imaging","summary":"  Glaucoma, a leading cause of irreversible blindness, necessitates early\ndetection for accurate and timely intervention to prevent irreversible vision\nloss. In this study, we present a novel deep learning framework that leverages\nthe diagnostic value of 3D Optical Coherence Tomography (OCT) imaging for\nautomated glaucoma detection. In this framework, we integrate a pre-trained\nVision Transformer on retinal data for rich slice-wise feature extraction and a\nbidirectional Gated Recurrent Unit for capturing inter-slice spatial\ndependencies. This dual-component approach enables comprehensive analysis of\nlocal nuances and global structural integrity, crucial for accurate glaucoma\ndiagnosis. Experimental results on a large dataset demonstrate the superior\nperformance of the proposed method over state-of-the-art ones, achieving an\nF1-score of 93.01%, Matthews Correlation Coefficient (MCC) of 69.33%, and AUC\nof 94.20%. The framework's ability to leverage the valuable information in 3D\nOCT data holds significant potential for enhancing clinical decision support\nsystems and improving patient outcomes in glaucoma management.\n","authors":["Mona Ashtari-Majlan","David Masip"],"pdf_url":"https://arxiv.org/pdf/2403.05702v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.17832v2","updated":"2025-09-04T09:06:17Z","published":"2025-08-25T09:32:57Z","title":"HLG: Comprehensive 3D Room Construction via Hierarchical Layout\n  Generation","summary":"  Realistic 3D indoor scene generation is crucial for virtual reality, interior\ndesign, embodied intelligence, and scene understanding. While existing methods\nhave made progress in coarse-scale furniture arrangement, they struggle to\ncapture fine-grained object placements, limiting the realism and utility of\ngenerated environments. This gap hinders immersive virtual experiences and\ndetailed scene comprehension for embodied AI applications. To address these\nissues, we propose Hierarchical Layout Generation (HLG), a novel method for\nfine-grained 3D scene generation. HLG is the first to adopt a coarse-to-fine\nhierarchical approach, refining scene layouts from large-scale furniture\nplacement to intricate object arrangements. Specifically, our fine-grained\nlayout alignment module constructs a hierarchical layout through vertical and\nhorizontal decoupling, effectively decomposing complex 3D indoor scenes into\nmultiple levels of granularity. Additionally, our trainable layout optimization\nnetwork addresses placement issues, such as incorrect positioning, orientation\nerrors, and object intersections, ensuring structurally coherent and physically\nplausible scene generation. We demonstrate the effectiveness of our approach\nthrough extensive experiments, showing superior performance in generating\nrealistic indoor scenes compared to existing methods. This work advances the\nfield of scene generation and opens new possibilities for applications\nrequiring detailed 3D environments. We will release our code upon publication\nto encourage future research.\n","authors":["Xiping Wang","Yuxi Wang","Mengqi Zhou","Junsong Fan","Zhaoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.17832v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.16494v3","updated":"2025-09-04T09:03:13Z","published":"2023-09-28T14:59:16Z","title":"Accurate and lightweight dehazing via multi-receptive-field non-local\n  network and novel contrastive regularization","summary":"  Recently, deep learning-based methods have dominated image dehazing domain. A\nmulti-receptive-field non-local network (MRFNLN) consisting of the multi-stream\nfeature attention block (MSFAB) and the cross non-local block (CNLB) is\npresented in this paper to further enhance the performance. We start with\nextracting richer features for dehazing. Specifically, a multi-stream feature\nextraction (MSFE) sub-block, which contains three parallel convolutions with\ndifferent receptive fields (i.e., $1\\times 1$, $3\\times 3$, $5\\times 5$), is\ndesigned for extracting multi-scale features. Following MSFE, an attention\nsub-block is employed to make the model adaptively focus on important\nchannels/regions. These two sub-blocks constitute our MSFAB. Then, we design a\ncross non-local block (CNLB), which can capture long-range dependencies beyond\nthe query. Instead of the same input source of query branch, the key and value\nbranches are enhanced by fusing more preceding features. CNLB is\ncomputation-friendly by leveraging a spatial pyramid down-sampling (SPDS)\nstrategy to reduce the computation and memory consumption without sacrificing\nthe performance. Last but not least, a novel detail-focused contrastive\nregularization (DFCR) is presented by emphasizing the low-level details and\nignoring the high-level semantic information in a representation space\nspecially designed for dehazing. Comprehensive experimental results demonstrate\nthat the proposed MRFNLN model outperforms recent state-of-the-art dehazing\nmethods with less than 1.5 Million parameters.\n","authors":["Zewei He","Zixuan Chen","Jinlei Li","Ziqian Lu","Xuecheng Sun","Hao Luo","Zhe-Ming Lu","Evangelos K. Markakis"],"pdf_url":"https://arxiv.org/pdf/2309.16494v3.pdf","comment":"submitted to the IEEE Journal for possible publication"},{"id":"http://arxiv.org/abs/2508.18826v2","updated":"2025-09-04T08:59:58Z","published":"2025-08-26T09:03:18Z","title":"SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation","summary":"  Recent studies have shown that Machine Learning (ML) models can exhibit bias\nin real-world scenarios, posing significant challenges in ethically sensitive\ndomains such as healthcare. Such bias can negatively affect model fairness,\nmodel generalization abilities and further risks amplifying social\ndiscrimination. There is a need to remove biases from trained models. Existing\ndebiasing approaches often necessitate access to original training data and\nneed extensive model retraining; they also typically exhibit trade-offs between\nmodel fairness and discriminative performance. To address these challenges, we\npropose Soft-Mask Weight Fine-Tuning (SWiFT), a debiasing framework that\nefficiently improves fairness while preserving discriminative performance with\nmuch less debiasing costs. Notably, SWiFT requires only a small external\ndataset and only a few epochs of model fine-tuning. The idea behind SWiFT is to\nfirst find the relative, and yet distinct, contributions of model parameters to\nboth bias and predictive performance. Then, a two-step fine-tuning process\nupdates each parameter with different gradient flows defined by its\ncontribution. Extensive experiments with three bias sensitive attributes\n(gender, skin tone, and age) across four dermatological and two chest X-ray\ndatasets demonstrate that SWiFT can consistently reduce model bias while\nachieving competitive or even superior diagnostic accuracy under common\nfairness and accuracy metrics, compared to the state-of-the-art. Specifically,\nwe demonstrate improved model generalization ability as evidenced by superior\nperformance on several out-of-distribution (OOD) datasets.\n","authors":["Junyu Yan","Feng Chen","Yuyang Xue","Yuning Du","Konstantinos Vilouras","Sotirios A. Tsaftaris","Steven McDonagh"],"pdf_url":"https://arxiv.org/pdf/2508.18826v2.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2025:015"},{"id":"http://arxiv.org/abs/2508.19762v3","updated":"2025-09-04T08:58:39Z","published":"2025-08-27T10:40:15Z","title":"BuzzSet v1.0: A Dataset for Pollinator Detection in Field Conditions","summary":"  Pollinator insects such as honeybees and bumblebees are vital to global food\nproduction and ecosystem stability, yet their populations are declining due to\nanthropogenic and environmental stressors. Scalable, automated monitoring in\nagricultural environments remains an open challenge due to the difficulty of\ndetecting small, fast-moving, and often camouflaged insects. To address this,\nwe present BuzzSet v1.0, a large-scale dataset of high-resolution pollinator\nimages collected under real field conditions. BuzzSet contains 7,856 manually\nverified images with more than 8,000 annotated instances across three classes:\nhoneybees, bumblebees, and unidentified insects. Initial annotations were\nproduced using a YOLOv12 model trained on external data and refined through\nhuman verification with open-source tools. All images were preprocessed into\n256 x 256 tiles to improve the detection of small insects. We provide baselines\nusing the RF-DETR transformer-based object detector. The model achieves strong\nclassification accuracy with F1 scores of 0.94 and 0.92 for honeybees and\nbumblebees, with minimal confusion between these categories. The unidentified\nclass remains more difficult due to label ambiguity and fewer samples, yet\nstill contributes insights for robustness evaluation. Overall detection\nperformance (mAP at 0.50 of 0.559) illustrates the challenging nature of the\ndataset and its potential to drive advances in small object detection under\nrealistic ecological conditions. Future work focuses on expanding the dataset\nto version 2.0 with additional annotations and evaluating further detection\nstrategies. BuzzSet establishes a benchmark for ecological computer vision,\nwith the primary challenge being reliable detection of insects frequently\ncamouflaged within natural vegetation, highlighting an open problem for future\nresearch.\n","authors":["Ahmed Emam","Mohamed Elbassiouny","Julius Miller","Patrick Donworth","Sabine Seidel","Ribana Roscher"],"pdf_url":"https://arxiv.org/pdf/2508.19762v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04023v1","updated":"2025-09-04T08:50:03Z","published":"2025-09-04T08:50:03Z","title":"Learning from Majority Label: A Novel Problem in Multi-class\n  Multiple-Instance Learning","summary":"  The paper proposes a novel multi-class Multiple-Instance Learning (MIL)\nproblem called Learning from Majority Label (LML). In LML, the majority class\nof instances in a bag is assigned as the bag-level label. The goal of LML is to\ntrain a classification model that estimates the class of each instance using\nthe majority label. This problem is valuable in a variety of applications,\nincluding pathology image segmentation, political voting prediction, customer\nsentiment analysis, and environmental monitoring. To solve LML, we propose a\nCounting Network trained to produce bag-level majority labels, estimated by\ncounting the number of instances in each class. Furthermore, analysis\nexperiments on the characteristics of LML revealed that bags with a high\nproportion of the majority class facilitate learning. Based on this result, we\ndeveloped a Majority Proportion Enhancement Module (MPEM) that increases the\nproportion of the majority class by removing minority class instances within\nthe bags. Experiments demonstrate the superiority of the proposed method on\nfour datasets compared to conventional MIL methods. Moreover, ablation studies\nconfirmed the effectiveness of each module. The code is available at\n\\href{https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}.\n","authors":["Shiku Kaito","Shinnosuke Matsuo","Daiki Suehiro","Ryoma Bise"],"pdf_url":"https://arxiv.org/pdf/2509.04023v1.pdf","comment":"35 pages, 9 figures, Accepted in Pattern recognition"},{"id":"http://arxiv.org/abs/2509.04009v1","updated":"2025-09-04T08:40:40Z","published":"2025-09-04T08:40:40Z","title":"Detecting Regional Spurious Correlations in Vision Transformers via\n  Token Discarding","summary":"  Due to their powerful feature association capabilities, neural network-based\ncomputer vision models have the ability to detect and exploit unintended\npatterns within the data, potentially leading to correct predictions based on\nincorrect or unintended but statistically relevant signals. These clues may\nvary from simple color aberrations to small texts within the image. In\nsituations where these unintended signals align with the predictive task,\nmodels can mistakenly link these features with the task and rely on them for\nmaking predictions. This phenomenon is referred to as spurious correlations,\nwhere patterns appear to be associated with the task but are actually\ncoincidental. As a result, detection and mitigation of spurious correlations\nhave become crucial tasks for building trustworthy, reliable, and generalizable\nmachine learning models. In this work, we present a novel method to detect\nspurious correlations in vision transformers, a type of neural network\narchitecture that gained significant popularity in recent years. Using both\nsupervised and self-supervised trained models, we present large-scale\nexperiments on the ImageNet dataset demonstrating the ability of the proposed\nmethod to identify spurious correlations. We also find that, even if the same\narchitecture is used, the training methodology has a significant impact on the\nmodel's reliance on spurious correlations. Furthermore, we show that certain\nclasses in the ImageNet dataset contain spurious signals that are easily\ndetected by the models and discuss the underlying reasons for those spurious\nsignals. In light of our findings, we provide an exhaustive list of the\naforementioned images and call for caution in their use in future research\nefforts. Lastly, we present a case study investigating spurious signals in\ninvasive breast mass classification, grounding our work in real-world\nscenarios.\n","authors":["Solha Kang","Esla Timothy Anzaku","Wesley De Neve","Arnout Van Messem","Joris Vankerschaver","Francois Rameau","Utku Ozbulak"],"pdf_url":"https://arxiv.org/pdf/2509.04009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03999v1","updated":"2025-09-04T08:27:54Z","published":"2025-09-04T08:27:54Z","title":"SliceSemOcc: Vertical Slice Based Multimodal 3D Semantic Occupancy\n  Representation","summary":"  Driven by autonomous driving's demands for precise 3D perception, 3D semantic\noccupancy prediction has become a pivotal research topic. Unlike\nbird's-eye-view (BEV) methods, which restrict scene representation to a 2D\nplane, occupancy prediction leverages a complete 3D voxel grid to model spatial\nstructures in all dimensions, thereby capturing semantic variations along the\nvertical axis. However, most existing approaches overlook height-axis\ninformation when processing voxel features. And conventional SENet-style\nchannel attention assigns uniform weight across all height layers, limiting\ntheir ability to emphasize features at different heights. To address these\nlimitations, we propose SliceSemOcc, a novel vertical slice based multimodal\nframework for 3D semantic occupancy representation. Specifically, we extract\nvoxel features along the height-axis using both global and local vertical\nslices. Then, a global local fusion module adaptively reconciles fine-grained\nspatial details with holistic contextual information. Furthermore, we propose\nthe SEAttention3D module, which preserves height-wise resolution through\naverage pooling and assigns dynamic channel attention weights to each height\nlayer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy\ndatasets verify that our method significantly enhances mean IoU, achieving\nespecially pronounced gains on most small-object categories. Detailed ablation\nstudies further validate the effectiveness of the proposed SliceSemOcc\nframework.\n","authors":["Han Huang","Han Sun","Ningzhong Liu","Huiyu Zhou","Jiaquan Shen"],"pdf_url":"https://arxiv.org/pdf/2509.03999v1.pdf","comment":"14 pages, accepted by PRCV2025"},{"id":"http://arxiv.org/abs/2504.09885v2","updated":"2025-09-04T08:25:53Z","published":"2025-04-14T05:17:41Z","title":"Separate to Collaborate: Dual-Stream Diffusion Model for Coordinated\n  Piano Hand Motion Synthesis","summary":"  Automating the synthesis of coordinated bimanual piano performances poses\nsignificant challenges, particularly in capturing the intricate choreography\nbetween the hands while preserving their distinct kinematic signatures. In this\npaper, we propose a dual-stream neural framework designed to generate\nsynchronized hand gestures for piano playing from audio input, addressing the\ncritical challenge of modeling both hand independence and coordination. Our\nframework introduces two key innovations: (i) a decoupled diffusion-based\ngeneration framework that independently models each hand's motion via\ndual-noise initialization, sampling distinct latent noise for each while\nleveraging a shared positional condition, and (ii) a Hand-Coordinated\nAsymmetric Attention (HCAA) mechanism suppresses symmetric (common-mode) noise\nto highlight asymmetric hand-specific features, while adaptively enhancing\ninter-hand coordination during denoising. Comprehensive evaluations demonstrate\nthat our framework outperforms existing state-of-the-art methods across\nmultiple metrics. Our project is available at\nhttps://monkek123king.github.io/S2C_page/.\n","authors":["Zihao Liu","Mingwen Ou","Zunnan Xu","Jiaqi Huang","Haonan Han","Ronghui Li","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2504.09885v2.pdf","comment":"15 pages, 7 figures, Accepted to ACMMM 2025"},{"id":"http://arxiv.org/abs/2405.20321v2","updated":"2025-09-04T08:23:37Z","published":"2024-05-30T17:56:54Z","title":"Vision-based Manipulation from Single Human Video with Open-World Object\n  Graphs","summary":"  This work presents an object-centric approach to learning vision-based\nmanipulation skills from human videos. We investigate the problem of robot\nmanipulation via imitation in the open-world setting, where a robot learns to\nmanipulate novel objects from a single video demonstration. We introduce ORION,\nan algorithm that tackles the problem by extracting an object-centric\nmanipulation plan from a single RGB or RGB-D video and deriving a policy that\nconditions on the extracted plan. Our method enables the robot to learn from\nvideos captured by daily mobile devices and to generalize the policies to\ndeployment environments with varying visual backgrounds, camera angles, spatial\nlayouts, and novel object instances. We systematically evaluate our method on\nboth short-horizon and long-horizon tasks, using RGB-D and RGB-only\ndemonstration videos. Across varied tasks and demonstration types (RGB-D /\nRGB), we observe an average success rate of 74.4%, demonstrating the efficacy\nof ORION in learning from a single human video in the open world. Additional\nmaterials can be found on our project website:\nhttps://ut-austin-rpl.github.io/ORION-release.\n","authors":["Yifeng Zhu","Arisrei Lim","Peter Stone","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2405.20321v2.pdf","comment":"Extended version of paper adding results with RGB-only demonstration\n  videos uploaded on 09/04/2025"},{"id":"http://arxiv.org/abs/2509.00451v2","updated":"2025-09-04T08:19:24Z","published":"2025-08-30T10:45:39Z","title":"Encoder-Only Image Registration","summary":"  Learning-based techniques have significantly improved the accuracy and speed\nof deformable image registration. However, challenges such as reducing\ncomputational complexity and handling large deformations persist. To address\nthese challenges, we analyze how convolutional neural networks (ConvNets)\ninfluence registration performance using the Horn-Schunck optical flow\nequation. Supported by prior studies and our empirical experiments, we observe\nthat ConvNets play two key roles in registration: linearizing local intensities\nand harmonizing global contrast variations. Based on these insights, we propose\nthe Encoder-Only Image Registration (EOIR) framework, designed to achieve a\nbetter accuracy-efficiency trade-off. EOIR separates feature learning from flow\nestimation, employing only a 3-layer ConvNet for feature extraction and a set\nof 3-layer flow estimators to construct a Laplacian feature pyramid,\nprogressively composing diffeomorphic deformations under a large-deformation\nmodel. Results on five datasets across different modalities and anatomical\nregions demonstrate EOIR's effectiveness, achieving superior\naccuracy-efficiency and accuracy-smoothness trade-offs. With comparable\naccuracy, EOIR provides better efficiency and smoothness, and vice versa. The\nsource code of EOIR is publicly available on\nhttps://github.com/XiangChen1994/EOIR.\n","authors":["Xiang Chen","Renjiu Hu","Jinwei Zhang","Yuxi Zhang","Xinyao Yue","Min Liu","Yaonan Wang","Hang Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.00451v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03986v1","updated":"2025-09-04T08:13:06Z","published":"2025-09-04T08:13:06Z","title":"Promptception: How Sensitive Are Large Multimodal Models to Prompts?","summary":"  Despite the success of Large Multimodal Models (LMMs) in recent years, prompt\ndesign for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly\nunderstood. We show that even minor variations in prompt phrasing and structure\ncan lead to accuracy deviations of up to 15% for certain prompts and models.\nThis variability poses a challenge for transparent and fair LMM evaluation, as\nmodels often report their best-case performance using carefully selected\nprompts. To address this, we introduce Promptception, a systematic framework\nfor evaluating prompt sensitivity in LMMs. It consists of 61 prompt types,\nspanning 15 categories and 6 supercategories, each targeting specific aspects\nof prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight\nopen-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks:\nMMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit\ngreater sensitivity to prompt phrasing, reflecting tighter alignment with\ninstruction semantics, while open-source models are steadier but struggle with\nnuanced and complex phrasing. Based on this analysis, we propose Prompting\nPrinciples tailored to proprietary and open-source LMMs, enabling more robust\nand fair model evaluation.\n","authors":["Mohamed Insaf Ismithdeen","Muhammad Uzair Khattak","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2509.03986v1.pdf","comment":"Accepted to EMNLP 2025"},{"id":"http://arxiv.org/abs/2508.13238v2","updated":"2025-09-04T08:05:29Z","published":"2025-08-18T03:28:57Z","title":"DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool\n  Interleaved Vision-Language Model","summary":"  Recent advances in large vision-language models (LVLMs) have enabled a new\nparadigm of end-to-end document image parsing, excelling in Optical Character\nRecognition (OCR) tasks such as text, table, and formula recognition. However,\ngenerative LVLMs, similarly to large language models (LLMs), are prone to\nhallucinations--generating words that do not exist in input images.\nFurthermore, LVLMs are designed for general purposes and tend to be less\neffective on OCR tasks compared to expert models that are trained on\ndomain-specific datasets. In this paper, we propose DianJin-OCR-R1, a\nreasoning-enhanced framework designed to address these limitations through\ntraining reasoning-and-tool interleaved VLMs. Given a recognition instruction,\nour DianJin-OCR-R1 model first recognizes the content in the input image by its\nown OCR capabilities, and then calls other tools (i.e., other expert models) to\nobtain their results as references, finally \"looks again\" the image and\nrethinks about the reasoning process to provide the final recognized content.\nSince architectures of expert models are tailored for specific OCR tasks, which\nmakes them less prone to hallucinations, their results can help VLMs mitigate\nhallucinations. We evaluate our model on ReST and OmniDocBench, and\nexperimental results show that our DianJin-OCR-R1 models consistently\noutperform their non-reasoning counterparts and expert OCR models, which proves\nthe effectiveness of our method. Additionally, the results indicate that\nenhancing expert models, which are typically small and easy to iterate, enable\nperformance improvements for VLMs.\n","authors":["Qian Chen","Xianyin Zhang","Lifan Guo","Feng Chen","Chi Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.13238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03975v1","updated":"2025-09-04T08:01:27Z","published":"2025-09-04T08:01:27Z","title":"Improving Vessel Segmentation with Multi-Task Learning and Auxiliary\n  Data Available Only During Model Training","summary":"  Liver vessel segmentation in magnetic resonance imaging data is important for\nthe computational analysis of vascular remodelling, associated with a wide\nspectrum of diffuse liver diseases. Existing approaches rely on contrast\nenhanced imaging data, but the necessary dedicated imaging sequences are not\nuniformly acquired. Images without contrast enhancement are acquired more\nfrequently, but vessel segmentation is challenging, and requires large-scale\nannotated data. We propose a multi-task learning framework to segment vessels\nin liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data\navailable only during training to reduce the need for annotated training\nexamples. Our approach draws on paired native and contrast enhanced data with\nand without vessel annotations for model training. Results show that auxiliary\ndata improves the accuracy of vessel segmentation, even if they are not\navailable during inference. The advantage is most pronounced if only few\nannotations are available for training, since the feature representation\nbenefits from the shared task structure. A validation of this approach to\naugment a model for brain tumor segmentation confirms its benefits across\ndifferent domains. An auxiliary informative imaging modality can augment expert\nannotations even if it is only available during training.\n","authors":["Daniel Sobotka","Alexander Herold","Matthias Perkonigg","Lucian Beer","Nina Bastati","Alina Sablatnig","Ahmed Ba-Ssalamah","Georg Langs"],"pdf_url":"https://arxiv.org/pdf/2509.03975v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03973v1","updated":"2025-09-04T07:58:52Z","published":"2025-09-04T07:58:52Z","title":"SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for\n  Histopathology Whole Slide Image Classification","summary":"  We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for\nperforming WSI classification. SAC-MIL consists of a positional encoding module\nto encode position information and a SAC block to perform full instance\ncorrelations. The positional encoding module utilizes the instance coordinates\nwithin the slide to encode the spatial relationships instead of the instance\nindex in the input WSI sequence. The positional encoding module can also handle\nthe length extrapolation issue where the training and testing sequences have\ndifferent lengths. The SAC block is an MLP-based method that performs full\ninstance correlation in linear time complexity with respect to the sequence\nlength. Due to the simple structure of MLP, it is easy to deploy since it does\nnot require custom CUDA kernels, compared to Transformer-based methods for WSI\nclassification. SAC-MIL has achieved state-of-the-art performance on the\nCAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon\nacceptance.\n","authors":["Yu Bai","Zitong Yu","Haowen Tian","Xijing Wang","Shuo Yan","Lin Wang","Honglin Li","Xitong Ling","Bo Zhang","Zheng Zhang","Wufan Wang","Hui Gao","Xiangyang Gong","Wendong Wang"],"pdf_url":"https://arxiv.org/pdf/2509.03973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.23357v2","updated":"2025-09-04T07:47:30Z","published":"2025-07-31T09:08:11Z","title":"Foundations and Models in Modern Computer Vision: Key Building Blocks in\n  Landmark Architectures","summary":"  This report analyzes the evolution of key design patterns in computer vision\nby examining six influential papers. The analysis begins with foundational\narchitectures for image recognition. We review ResNet, which introduced\nresidual connections to overcome the vanishing gradient problem and enable\neffective training of significantly deeper convolutional networks.\nSubsequently, we examine the Vision Transformer (ViT), which established a new\nparadigm by applying the Transformer architecture to sequences of image\npatches, demonstrating the efficacy of attention-based models for large-scale\nimage recognition. Building on these visual representation backbones, we\ninvestigate generative models. Generative Adversarial Networks (GANs) are\nanalyzed for their novel adversarial training process, which challenges a\ngenerator against a discriminator to learn complex data distributions. Then,\nLatent Diffusion Models (LDMs) are covered, which improve upon prior generative\nmethods by performing a sequential denoising process in a perceptually\ncompressed latent space. LDMs achieve high-fidelity synthesis with greater\ncomputational efficiency, representing the current state-of-the-art for image\ngeneration. Finally, we explore self-supervised learning techniques that reduce\ndependency on labeled data. DINO is a self-distillation framework in which a\nstudent network learns to match the output of a momentum-updated teacher,\nyielding features with strong k-NN classification performance. We conclude with\nMasked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design\nto reconstruct heavily masked inputs, providing a highly scalable and effective\nmethod for pre-training large-scale vision models.\n","authors":["Radu-Andrei Bourceanu","Neil De La Fuente","Jan Grimm","Andrei Jardan","Andriy Manucharyan","Cornelius Weiss","Daniel Cremers","Roman Pflugfelder"],"pdf_url":"https://arxiv.org/pdf/2507.23357v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03961v1","updated":"2025-09-04T07:39:18Z","published":"2025-09-04T07:39:18Z","title":"Multimodal Feature Fusion Network with Text Difference Enhancement for\n  Remote Sensing Change Detection","summary":"  Although deep learning has advanced remote sensing change detection (RSCD),\nmost methods rely solely on image modality, limiting feature representation,\nchange pattern modeling, and generalization especially under illumination and\nnoise disturbances. To address this, we propose MMChange, a multimodal RSCD\nmethod that combines image and text modalities to enhance accuracy and\nrobustness. An Image Feature Refinement (IFR) module is introduced to highlight\nkey regions and suppress environmental noise. To overcome the semantic\nlimitations of image features, we employ a vision language model (VLM) to\ngenerate semantic descriptions of bitemporal images. A Textual Difference\nEnhancement (TDE) module then captures fine grained semantic shifts, guiding\nthe model toward meaningful changes. To bridge the heterogeneity between\nmodalities, we design an Image Text Feature Fusion (ITFF) module that enables\ndeep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and\nSYSUCD demonstrate that MMChange consistently surpasses state of the art\nmethods across multiple metrics, validating its effectiveness for multimodal\nRSCD. Code is available at: https://github.com/yikuizhai/MMChange.\n","authors":["Yijun Zhou","Yikui Zhai","Zilu Ying","Tingfeng Xian","Wenlve Zhou","Zhiheng Zhou","Xiaolin Tian","Xudong Jia","Hongsheng Zhang","C. L. Philip Chen"],"pdf_url":"https://arxiv.org/pdf/2509.03961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20789v3","updated":"2025-09-04T07:37:08Z","published":"2025-05-27T06:49:02Z","title":"Integrating Intermediate Layer Optimization and Projected Gradient\n  Descent for Solving Inverse Problems with Diffusion Models","summary":"  Inverse problems (IPs) involve reconstructing signals from noisy\nobservations. Recently, diffusion models (DMs) have emerged as a powerful\nframework for solving IPs, achieving remarkable reconstruction performance.\nHowever, existing DM-based methods frequently encounter issues such as heavy\ncomputational demands and suboptimal convergence. In this work, building upon\nthe idea of the recent work DMPlug, we propose two novel methods, DMILO and\nDMILO-PGD, to address these challenges. Our first method, DMILO, employs\nintermediate layer optimization (ILO) to alleviate the memory burden inherent\nin DMPlug. Additionally, by introducing sparse deviations, we expand the range\nof DMs, enabling the exploration of underlying signals that may lie outside the\nrange of the diffusion model. We further propose DMILO-PGD, which integrates\nILO with projected gradient descent (PGD), thereby reducing the risk of\nsuboptimal convergence. We provide an intuitive theoretical analysis of our\napproaches under appropriate conditions and validate their superiority through\nextensive experiments on diverse image datasets, encompassing both linear and\nnonlinear IPs. Our results demonstrate significant performance gains over\nstate-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD\nin addressing common challenges in DM-based IP solvers.\n","authors":["Yang Zheng","Wen Li","Zhaoqiang Liu"],"pdf_url":"https://arxiv.org/pdf/2505.20789v3.pdf","comment":"ICML 2025"},{"id":"http://arxiv.org/abs/2502.06289v2","updated":"2025-09-04T07:35:48Z","published":"2025-02-10T09:31:39Z","title":"Is an Ultra Large Natural Image-Based Foundation Model Superior to a\n  Retina-Specific Model for Detecting Ocular and Systemic Diseases?","summary":"  The advent of foundation models (FMs) is transforming medical domain. In\nophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4\nmillion natural images and 1.6 million retinal images, has demonstrated high\nadaptability across clinical applications. Conversely, DINOv2, a\ngeneral-purpose vision FM pre-trained on 142 million natural images, has shown\npromise in non-medical domains. However, its applicability to clinical tasks\nremains underexplored. To address this, we conducted head-to-head evaluations\nby fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular\ndisease detection and systemic disease prediction tasks, across eight\nstandardized open-source ocular datasets, as well as the Moorfields AlzEye and\nthe UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting\ndiabetic retinopathy (AUROC=0.850-0.952 vs 0.823-0.944, across three datasets,\nall P<=0.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In\nglaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940,\nP<0.001). Conversely, RETFound achieved superior performance over all DINOv2\nmodels in predicting heart failure, myocardial infarction, and ischaemic stroke\n(AUROC=0.732-0.796 vs 0.663-0.771, all P<0.001). These trends persisted even\nwith 10% of the fine-tuning data. These findings showcase the distinct\nscenarios where general-purpose and domain-specific FMs excel, highlighting the\nimportance of aligning FM selection with task-specific requirements to optimise\nclinical performance.\n","authors":["Qingshan Hou","Yukun Zhou","Jocelyn Hui Lin Goh","Ke Zou","Samantha Min Er Yew","Sahana Srinivasan","Meng Wang","Thaddaeus Lo","Xiaofeng Lei","Siegfried K. Wagner","Mark A. Chia","Dawei Yang","Hongyang Jiang","An Ran Ran","Rui Santos","Gabor Mark Somfai","Juan Helen Zhou","Haoyu Chen","Qingyu Chen","Carol Y. Cheung","Pearse A. Keane","Yih Chung Tham"],"pdf_url":"https://arxiv.org/pdf/2502.06289v2.pdf","comment":"Accepted by Ophthalmology Science and is currently in press"},{"id":"http://arxiv.org/abs/2509.03951v1","updated":"2025-09-04T07:26:20Z","published":"2025-09-04T07:26:20Z","title":"ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD\n  Detection","summary":"  The introduction of negative labels (NLs) has proven effective in enhancing\nOut-of-Distribution (OOD) detection. However, existing methods often lack an\nunderstanding of OOD images, making it difficult to construct an accurate\nnegative space. In addition, the presence of false negative labels\nsignificantly degrades their near-OOD performance. To address these issues, we\npropose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the\nunderstanding and reasoning capabilities of multimodal large language models\n(MLLMs). Specifically, we identify images likely to be OOD samples as negative\nimages and prompt the MLLM to describe these images, generating expressive\nnegative sentences that precisely characterize the OOD distribution and enhance\nfar-OOD detection. For the near-OOD setting, where OOD samples resemble the\nin-distribution (ID) subset, we first identify the subset of ID classes that\nare visually similar to negative images and then leverage the reasoning\ncapability of MLLMs to generate visually similar negative labels tailored to\nthis subset, effectively reducing false negatives and improving near-OOD\ndetection. To balance these two types of negative textual spaces, we design an\nadaptive weighted score that enables the method to handle different OOD task\nsettings (near-OOD and far-OOD) without relying on task-specific prior\nknowledge, making it highly adaptable in open environments. On the ImageNet\nbenchmark, our ANTS significantly reduces the FPR95 by 4.2\\%, establishing a\nnew state-of-the-art. Furthermore, our method is training-free and zero-shot,\nenabling high scalability.\n","authors":["Zhu Wenjie","Zhang Yabin","Xin Jin","Wenjun Zeng","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.03951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03950v1","updated":"2025-09-04T07:21:37Z","published":"2025-09-04T07:21:37Z","title":"Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer\n  Learning in a U-Net Architecture","summary":"  Pneumothorax, the abnormal accumulation of air in the pleural space, can be\nlife-threatening if undetected. Chest X-rays are the first-line diagnostic\ntool, but small cases may be subtle. We propose an automated deep-learning\npipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax\nregions. Trained on the SIIM-ACR dataset with data augmentation and a combined\nbinary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and\nDice score of 0.8241 on the independent PTX-498 dataset. These results\ndemonstrate that the model can accurately localize pneumothoraces and support\nradiologists.\n","authors":["Alvaro Aranibar Roque","Helga Sebastian"],"pdf_url":"https://arxiv.org/pdf/2509.03950v1.pdf","comment":"10 page, 5 figures"},{"id":"http://arxiv.org/abs/2505.23525v2","updated":"2025-09-04T07:13:25Z","published":"2025-05-29T15:04:00Z","title":"Hallo4: High-Fidelity Dynamic Portrait Animation via Direct Preference\n  Optimization and Temporal Motion Modulation","summary":"  Generating highly dynamic and photorealistic portrait animations driven by\naudio and skeletal motion remains challenging due to the need for precise lip\nsynchronization, natural facial expressions, and high-fidelity body motion\ndynamics. We propose a human-preference-aligned diffusion framework that\naddresses these challenges through two key innovations. First, we introduce\ndirect preference optimization tailored for human-centric animation, leveraging\na curated dataset of human preferences to align generated outputs with\nperceptual metrics for portrait motion-video alignment and naturalness of\nexpression. Second, the proposed temporal motion modulation resolves\nspatiotemporal resolution mismatches by reshaping motion conditions into\ndimensionally aligned latent features through temporal channel redistribution\nand proportional feature expansion, preserving the fidelity of high-frequency\nmotion details in diffusion-based synthesis. The proposed mechanism is\ncomplementary to existing UNet and DiT-based portrait diffusion approaches, and\nexperiments demonstrate obvious improvements in lip-audio synchronization,\nexpression vividness, body motion coherence over baseline methods, alongside\nnotable gains in human preference metrics. Our model and source code can be\nfound at: https://github.com/xyz123xyz456/hallo4.\n","authors":["Jiahao Cui","Yan Chen","Mingwang Xu","Hanlin Shang","Yuxuan Chen","Yun Zhan","Zilong Dong","Yao Yao","Jingdong Wang","Siyu Zhu"],"pdf_url":"https://arxiv.org/pdf/2505.23525v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06119v2","updated":"2025-09-04T07:02:34Z","published":"2024-11-09T08:58:57Z","title":"Hardware-Friendly Diffusion Models with Fixed-Size Reusable Structures\n  for On-Device Image Generation","summary":"  Vision Transformers and U-Net architectures have been widely adopted in the\nimplementation of Diffusion Models. However, each architecture presents\nspecific challenges while realizing them on-device. Vision Transformers require\npositional embedding to maintain correspondence between the tokens processed by\nthe transformer, although they offer the advantage of using fixed-size,\nreusable repetitive blocks following tokenization. The U-Net architecture lacks\nthese attributes, as it utilizes variable-sized intermediate blocks for\ndown-convolution and up-convolution in the noise estimation backbone for the\ndiffusion process. To address these issues, we propose an architecture that\nutilizes a fixed-size, reusable transformer block as a core structure, making\nit more suitable for hardware implementation. Our architecture is characterized\nby low complexity, token-free design, absence of positional embeddings,\nuniformity, and scalability, making it highly suitable for deployment on mobile\nand resource-constrained devices. The proposed model exhibit competitive and\nconsistent performance across both unconditional and conditional image\ngeneration tasks. The model achieved a state-of-the-art FID score of 1.6 on\nunconditional image generation with the CelebA.\n","authors":["Sanchar Palit","Sathya Veera Reddy Dendi","Mallikarjuna Talluri","Raj Narayana Gadde"],"pdf_url":"https://arxiv.org/pdf/2411.06119v2.pdf","comment":"presented at IJCNN 2025 poster track"},{"id":"http://arxiv.org/abs/2509.03938v1","updated":"2025-09-04T06:56:06Z","published":"2025-09-04T06:56:06Z","title":"TopoSculpt: Betti-Steered Topological Sculpting of 3D Fine-grained\n  Tubular Shapes","summary":"  Medical tubular anatomical structures are inherently three-dimensional\nconduits with lumens, enclosing walls, and complex branching topologies.\nAccurate reconstruction of their geometry and topology is crucial for\napplications such as bronchoscopic navigation and cerebral arterial\nconnectivity assessment. Existing methods often rely on voxel-wise overlap\nmeasures, which fail to capture topological correctness and completeness.\nAlthough topology-aware losses and persistent homology constraints have shown\npromise, they are usually applied patch-wise and cannot guarantee global\npreservation or correct geometric errors at inference. To address these\nlimitations, we propose a novel TopoSculpt, a framework for topological\nrefinement of 3D fine-grained tubular structures. TopoSculpt (i) adopts a\nholistic whole-region modeling strategy to capture full spatial context, (ii)\nfirst introduces a Topological Integrity Betti (TIB) constraint that jointly\nenforces Betti number priors and global integrity, and (iii) employs a\ncurriculum refinement scheme with persistent homology to progressively correct\nerrors from coarse to fine scales. Extensive experiments on challenging\npulmonary airway and Circle of Willis datasets demonstrate substantial\nimprovements in both geometry and topology. For instance, $\\beta_{0}$ errors\nare reduced from 69.00 to 3.40 on the airway dataset and from 1.65 to 0.30 on\nthe CoW dataset, with Tree length detected and branch detected rates improving\nby nearly 10\\%. These results highlight the effectiveness of TopoSculpt in\ncorrecting critical topological errors and advancing the high-fidelity modeling\nof complex 3D tubular anatomy. The project homepage is available at:\nhttps://github.com/Puzzled-Hui/TopoSculpt.\n","authors":["Minghui Zhang","Yaoyu Liu","Junyang Wu","Xin You","Hanxiao Zhang","Junjun He","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2509.03938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12722v2","updated":"2025-09-04T06:43:22Z","published":"2024-12-17T09:38:58Z","title":"Defending LVLMs Against Vision Attacks through Partial-Perception\n  Supervision","summary":"  Recent studies have raised significant concerns regarding the vulnerability\nof Large Vision Language Models (LVLMs) to maliciously injected or perturbed\ninput images, which can mislead their responses. Existing defense methods show\nthat such vision attacks are sensitive to image modifications especially\ncropping, using majority voting across responses of modified images as\ncorrected responses. However, these modifications often result in partial\nimages and distort the semantics, which reduces response quality on clean\nimages after voting. Instead of directly using responses from partial images\nfor voting, we investigate using them to supervise the LVLM's responses to the\noriginal images. We propose a black-box, training-free method called DPS\n(Defense through Partial-Perception Supervision). In this approach, the model\nis prompted using the responses generated by a model that perceives only a\npartial image. With DPS, the model can adjust its response based on partial\nimage understanding when under attack, while confidently maintaining its\noriginal response for clean input. Our findings show that the weak model can\nsupervise the strong model: when faced with an attacked input, the strong model\nbecomes less confident and adjusts its response based on the weak model's\npartial understanding, effectively defending against the attack. With clean\ninput, it confidently maintains its original response. Empirical experiments\nshow our method outperforms the baseline, cutting the average attack success\nrate by 76.3% across six datasets on three popular models.\n","authors":["Qi Zhou","Tianlin Li","Qing Guo","Dongxia Wang","Yun Lin","Yang Liu","Jin Song Dong"],"pdf_url":"https://arxiv.org/pdf/2412.12722v2.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2502.07107v2","updated":"2025-09-04T06:28:55Z","published":"2025-02-10T23:05:35Z","title":"A Framework for Supervised and Unsupervised Segmentation and\n  Classification of Materials Microstructure Images","summary":"  Microstructure of materials is often characterized through image analysis to\nunderstand processing-structure-properties linkages. We propose a largely\nautomated framework that integrates unsupervised and supervised learning\nmethods to classify micrographs according to microstructure phase/class and,\nfor multiphase microstructures, segments them into different homogeneous\nregions. With the advance of manufacturing and imaging techniques, the\nultra-high resolution of imaging that reveals the complexity of microstructures\nand the rapidly increasing quantity of images (i.e., micrographs) enables and\nnecessitates a more powerful and automated framework to extract materials\ncharacteristics and knowledge. The framework we propose can be used to\ngradually build a database of microstructure classes relevant to a particular\nprocess or group of materials, which can help in analyzing and\ndiscovering/identifying new materials. The framework has three steps: (1)\nsegmentation of multiphase micrographs through a recently developed score-based\nmethod so that different microstructure homogeneous regions can be identified\nin an unsupervised manner; (2) {identification and classification of}\nhomogeneous regions of micrographs through an uncertainty-aware supervised\nclassification network trained using the segmented micrographs from Step $1$\nwith their identified labels verified via the built-in uncertainty\nquantification and minimal human inspection; (3) supervised segmentation (more\npowerful than the segmentation in Step $1$) of multiphase microstructures\nthrough a segmentation network trained with micrographs and the results from\nSteps $1$-$2$ using a form of data augmentation. This framework can iteratively\ncharacterize/segment new homogeneous or multiphase materials while expanding\nthe database to enhance performance. The framework is demonstrated on various\nsets of materials and texture images.\n","authors":["Kungang Zhang","Wei Chen","Wing K. Liu","L. Catherine Brinson","Daniel W. Apley"],"pdf_url":"https://arxiv.org/pdf/2502.07107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03922v1","updated":"2025-09-04T06:15:41Z","published":"2025-09-04T06:15:41Z","title":"LMVC: An End-to-End Learned Multiview Video Coding Framework","summary":"  Multiview video is a key data source for volumetric video, enabling immersive\n3D scene reconstruction but posing significant challenges in storage and\ntransmission due to its massive data volume. Recently, deep learning-based\nend-to-end video coding has achieved great success, yet most focus on\nsingle-view or stereo videos, leaving general multiview scenarios\nunderexplored. This paper proposes an end-to-end learned multiview video coding\n(LMVC) framework that ensures random access and backward compatibility while\nenhancing compression efficiency. Our key innovation lies in effectively\nleveraging independent-view motion and content information to enhance\ndependent-view compression. Specifically, to exploit the inter-view motion\ncorrelation, we propose a feature-based inter-view motion vector prediction\nmethod that conditions dependent-view motion encoding on decoded\nindependent-view motion features, along with an inter-view motion entropy model\nthat learns inter-view motion priors. To exploit the inter-view content\ncorrelation, we propose a disparity-free inter-view context prediction module\nthat predicts inter-view contexts from decoded independent-view content\nfeatures, combined with an inter-view contextual entropy model that captures\ninter-view context priors. Experimental results show that our proposed LMVC\nframework outperforms the reference software of the traditional MV-HEVC\nstandard by a large margin, establishing a strong baseline for future research\nin this field.\n","authors":["Xihua Sheng","Yingwen Zhang","Long Xu","Shiqi Wang"],"pdf_url":"https://arxiv.org/pdf/2509.03922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03903v1","updated":"2025-09-04T05:53:58Z","published":"2025-09-04T05:53:58Z","title":"A Generative Foundation Model for Chest Radiography","summary":"  The scarcity of well-annotated diverse medical images is a major hurdle for\ndeveloping reliable AI models in healthcare. Substantial technical advances\nhave been made in generative foundation models for natural images. Here we\ndevelop `ChexGen', a generative vision-language foundation model that\nintroduces a unified framework for text-, mask-, and bounding box-guided\nsynthesis of chest radiographs. Built upon the latent diffusion transformer\narchitecture, ChexGen was pretrained on the largest curated chest X-ray dataset\nto date, consisting of 960,000 radiograph-report pairs. ChexGen achieves\naccurate synthesis of radiographs through expert evaluations and quantitative\nmetrics. We demonstrate the utility of ChexGen for training data augmentation\nand supervised pretraining, which led to performance improvements across\ndisease classification, detection, and segmentation tasks using a small\nfraction of training data. Further, our model enables the creation of diverse\npatient cohorts that enhance model fairness by detecting and mitigating\ndemographic biases. Our study supports the transformative role of generative\nfoundation models in building more accurate, data-efficient, and equitable\nmedical AI systems.\n","authors":["Yuanfeng Ji","Dan Lin","Xiyue Wang","Lu Zhang","Wenhui Zhou","Chongjian Ge","Ruihang Chu","Xiaoli Yang","Junhan Zhao","Junsong Chen","Xiangde Luo","Sen Yang","Jin Fang","Ping Luo","Ruijiang Li"],"pdf_url":"https://arxiv.org/pdf/2509.03903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03897v1","updated":"2025-09-04T05:43:50Z","published":"2025-09-04T05:43:50Z","title":"SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation","summary":"  As interest grows in generating long, detailed image captions, standard\nevaluation metrics become increasingly unreliable. N-gram-based metrics though\nefficient, fail to capture semantic correctness. Representational Similarity\n(RS) metrics, designed to address this, initially saw limited use due to high\ncomputational costs, while today, despite advances in hardware, they remain\nunpopular due to low correlation to human judgments. Meanwhile, metrics based\non large language models (LLMs) show strong correlation with human judgments,\nbut remain too expensive for iterative use during model development.\n  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS\nmetric tailored to long image captioning. SPECS modifies CLIP with a new\nobjective that emphasizes specificity: rewarding correct details and penalizing\nincorrect ones. We show that SPECS matches the performance of open-source\nLLM-based metrics in correlation to human judgments, while being far more\nefficient. This makes it a practical alternative for iterative checkpoint\nevaluation during image captioning model development.Our code can be found at\nhttps://github.com/mbzuai-nlp/SPECS.\n","authors":["Xiaofu Chen","Israfel Salazar","Yova Kementchedjhieva"],"pdf_url":"https://arxiv.org/pdf/2509.03897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03895v1","updated":"2025-09-04T05:42:02Z","published":"2025-09-04T05:42:02Z","title":"Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of\n  Vision-Language Model","summary":"  Contrastive vision-language models excel in zero-shot image recognition but\nface challenges in few-shot scenarios due to computationally intensive offline\nfine-tuning using prompt learning, which risks overfitting. To overcome these\nlimitations, we propose Attn-Adapter, a novel online few-shot learning\nframework that enhances CLIP's adaptability via a dual attention mechanism. Our\ndesign incorporates dataset-specific information through two components: the\nMemory Attn-Adapter, which refines category embeddings using support examples,\nand the Local-Global Attn-Adapter, which enriches image embeddings by\nintegrating local and global features. This architecture enables dynamic\nadaptation from a few labeled samples without retraining the base model.\nAttn-Adapter outperforms state-of-the-art methods in cross-category and\ncross-dataset generalization, maintaining efficient inference and scaling\nacross CLIP backbones.\n","authors":["Phuoc-Nguyen Bui","Khanh-Binh Nguyen","Hyunseung Choo"],"pdf_url":"https://arxiv.org/pdf/2509.03895v1.pdf","comment":"ICCV 2025 - LIMIT Workshop"},{"id":"http://arxiv.org/abs/2509.03893v1","updated":"2025-09-04T05:39:16Z","published":"2025-09-04T05:39:16Z","title":"Weakly-Supervised Learning of Dense Functional Correspondences","summary":"  Establishing dense correspondences across image pairs is essential for tasks\nsuch as shape reconstruction and robot manipulation. In the challenging setting\nof matching across different categories, the function of an object, i.e., the\neffect that an object can cause on other objects, can guide how correspondences\nshould be established. This is because object parts that enable specific\nfunctions often share similarities in shape and appearance. We derive the\ndefinition of dense functional correspondence based on this observation and\npropose a weakly-supervised learning paradigm to tackle the prediction task.\nThe main insight behind our approach is that we can leverage vision-language\nmodels to pseudo-label multi-view images to obtain functional parts. We then\nintegrate this with dense contrastive learning from pixel correspondences to\ndistill both functional and spatial knowledge into a new model that can\nestablish dense functional correspondence. Further, we curate synthetic and\nreal evaluation datasets as task benchmarks. Our results demonstrate the\nadvantages of our approach over baseline solutions consisting of off-the-shelf\nself-supervised image representations and grounded vision language models.\n","authors":["Stefan Stojanov","Linan Zhao","Yunzhi Zhang","Daniel L. K. Yamins","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2509.03893v1.pdf","comment":"Accepted at ICCV 2025. Project website:\n  https://dense-functional-correspondence.github.io/"},{"id":"http://arxiv.org/abs/2509.03891v1","updated":"2025-09-04T05:22:42Z","published":"2025-09-04T05:22:42Z","title":"MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation","summary":"  Smartphones have become indispensable in people's daily lives, permeating\nnearly every aspect of modern society. With the continuous advancement of large\nlanguage models (LLMs), numerous LLM-based mobile agents have emerged. These\nagents are capable of accurately parsing diverse user queries and automatically\nassisting users in completing complex or repetitive operations. However,\ncurrent agents 1) heavily rely on the comprehension ability of LLMs, which can\nlead to errors caused by misoperations or omitted steps during tasks, 2) lack\ninteraction with the external environment, often terminating tasks when an app\ncannot fulfill user queries, and 3) lack memory capabilities, requiring each\ninstruction to reconstruct the interface and being unable to learn from and\ncorrect previous mistakes. To alleviate the above issues, we propose MobileRAG,\na mobile agents framework enhanced by Retrieval-Augmented Generation (RAG),\nwhich includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly\nand accurately identify user queries and accomplish complex and long-sequence\nmobile tasks. Additionally, to more comprehensively assess the performance of\nMobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark\ncharacterized by numerous complex, real-world mobile tasks that require\nexternal knowledge assistance. Extensive experimental results on MobileRAG-Eval\ndemonstrate that MobileRAG can easily handle real-world mobile tasks, achieving\n10.3\\% improvement over state-of-the-art methods with fewer operational steps.\nOur code is publicly available at:\nhttps://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv\n","authors":["Gowen Loo","Chang Liu","Qinghong Yin","Xiang Chen","Jiawei Chen","Jingyuan Zhang","Yu Tian"],"pdf_url":"https://arxiv.org/pdf/2509.03891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20323v5","updated":"2025-09-04T05:17:20Z","published":"2025-02-27T17:49:01Z","title":"ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model","summary":"  Speech-driven 3D facial animation aims to generate realistic lip movements\nand facial expressions for 3D head models from arbitrary audio clips. Although\nexisting diffusion-based methods are capable of producing natural motions,\ntheir slow generation speed limits their application potential. In this paper,\nwe introduce a novel autoregressive model that achieves real-time generation of\nhighly synchronized lip movements and realistic head poses and eye blinks by\nlearning a mapping from speech to a multi-scale motion codebook. Furthermore,\nour model can adapt to unseen speaking styles, enabling the creation of 3D\ntalking avatars with unique personal styles beyond the identities seen during\ntraining. Extensive evaluations and user studies demonstrate that our method\noutperforms existing approaches in lip synchronization accuracy and perceived\nquality.\n","authors":["Xuangeng Chu","Nabarun Goswami","Ziteng Cui","Hanqin Wang","Tatsuya Harada"],"pdf_url":"https://arxiv.org/pdf/2502.20323v5.pdf","comment":"SIGGRAPH Asia 2025, More video demonstrations, code, models and data\n  can be found on our project website: http://xg-chu.site/project_artalk/"},{"id":"http://arxiv.org/abs/2503.23746v2","updated":"2025-09-04T05:10:37Z","published":"2025-03-31T05:53:15Z","title":"Short-video Propagation Influence Rating: A New Real-world Dataset and A\n  New Large Graph Model","summary":"  Short-video platforms have gained immense popularity, captivating the\ninterest of millions, if not billions, of users globally. Recently, researchers\nhave highlighted the significance of analyzing the propagation of short-videos,\nwhich typically involves discovering commercial values, public opinions, user\nbehaviors, etc. This paper proposes a new Short-video Propagation Influence\nRating (SPIR) task and aims to promote SPIR from both the dataset and method\nperspectives. First, we propose a new Cross-platform Short-Video (XS-Video)\ndataset, which aims to provide a large-scale and real-world short-video\npropagation network across various platforms to facilitate the research on\nshort-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926\nsamples, and 535 topics across 5 biggest Chinese platforms, annotated with the\npropagation influence from level 0 to 9. To the best of our knowledge, this is\nthe first large-scale short-video dataset that contains cross-platform data or\nprovides all of the views, likes, shares, collects, fans, comments, and comment\ncontent. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a\nnovel three-stage training mechanism, to bridge heterogeneous graph-structured\ndata with the powerful reasoning ability and knowledge of Large Language Models\n(LLMs). Our NetGPT can comprehend and analyze the short-video propagation\ngraph, enabling it to predict the long-term propagation influence of\nshort-videos. Comprehensive experimental results evaluated by both\nclassification and regression metrics on our XS-Video dataset indicate the\nsuperiority of our method for SPIR.\n","authors":["Dizhan Xue","Shengsheng Qian","Chuanrui Hu","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2503.23746v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08557v6","updated":"2025-09-04T05:08:04Z","published":"2024-03-13T14:08:45Z","title":"OC4-ReID: Occluded Cloth-Changing Person Re-Identification","summary":"  The study of Cloth-Changing Person Re-identification (CC-ReID) focuses on\nretrieving specific pedestrians when their clothing has changed, typically\nunder the assumption that the entire pedestrian images are visible. Pedestrian\nimages in real-world scenarios, however, are often partially obscured by\nobstacles, presenting a significant challenge to existing CC-ReID systems. In\nthis paper, we introduce a more challenging task termed Occluded Cloth-Changing\nPerson Re-Identification (OC4-ReID), which simultaneously addresses two\nchallenges of clothing changes and occlusion. Concretely, we construct two new\ndatasets, Occ-LTCC and Occ-PRCC, based on original CC-ReID datasets to include\nrandom occlusions of key pedestrians components (e.g., head, torso). Moreover,\na novel benchmark is proposed for OC4-ReID incorporating a Train-Test Micro\nGranularity Screening (T2MGS) module to mitigate the influence of occlusion and\nproposing a Part-Robust Triplet (PRT) loss for partial features learning.\nComprehensive experiments on the proposed datasets, as well as on two CC-ReID\nbenchmark datasets demonstrate the superior performance of proposed method\nagainst other state-of-the-art methods. The codes and datasets are available\nat: https://github.com/1024AILab/OC4-ReID.\n","authors":["Zhihao Chen","Yiyuan Ge","Yanyan Lv","Ziyang Wang","Mingya Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08557v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03887v1","updated":"2025-09-04T05:06:47Z","published":"2025-09-04T05:06:47Z","title":"OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction","summary":"  In this paper, we propose OccTENS, a generative occupancy world model that\nenables controllable, high-fidelity long-term occupancy generation while\nmaintaining computational efficiency. Different from visual generation, the\noccupancy world model must capture the fine-grained 3D geometry and dynamic\nevolution of the 3D scenes, posing great challenges for the generative models.\nRecent approaches based on autoregression (AR) have demonstrated the potential\nto predict vehicle movement and future occupancy scenes simultaneously from\nhistorical observations, but they typically suffer from \\textbf{inefficiency},\n\\textbf{temporal degradation} in long-term generation and \\textbf{lack of\ncontrollability}. To holistically address these issues, we reformulate the\noccupancy world model as a temporal next-scale prediction (TENS) task, which\ndecomposes the temporal sequence modeling problem into the modeling of spatial\nscale-by-scale generation and temporal scene-by-scene prediction. With a\n\\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and\nspatial relationships of occupancy sequences in a flexible and scalable way. To\nenhance the pose controllability, we further propose a holistic pose\naggregation strategy, which features a unified sequence modeling for occupancy\nand ego-motion. Experiments show that OccTENS outperforms the state-of-the-art\nmethod with both higher occupancy quality and faster inference time.\n","authors":["Bu Jin","Songen Gu","Xiaotao Hu","Yupeng Zheng","Xiaoyang Guo","Qian Zhang","Xiaoxiao Long","Wei Yin"],"pdf_url":"https://arxiv.org/pdf/2509.03887v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11491v2","updated":"2025-09-04T04:49:14Z","published":"2024-05-19T09:17:43Z","title":"BOSC: A Backdoor-based Framework for Open Set Synthetic Image\n  Attribution","summary":"  Synthetic image attribution addresses the problem of tracing back the origin\nof images produced by generative models. Extensive efforts have been made to\nexplore unique representations of generative models and use them to attribute a\nsynthetic image to the model that produced it. Most of the methods classify the\nmodels or the architectures among those in a closed set without considering the\npossibility that the system is fed with samples produced by unknown\narchitectures. With the continuous progress of AI technology, new generative\narchitectures continuously appear, thus driving the attention of researchers\ntowards the development of tools capable of working in open-set scenarios. In\nthis paper, we propose a framework for open set attribution of synthetic\nimages, named BOSC (Backdoor-based Open Set Classification), that relies on the\nconcept of backdoor attacks to design a classifier with rejection option. BOSC\nworks by purposely injecting class-specific triggers inside a portion of the\nimages in the training set to induce the network to establish a matching\nbetween class features and trigger features. The behavior of the trained model\nwith respect to triggered samples is then exploited at test time to perform\nsample rejection using an ad-hoc score. Experiments show that the proposed\nmethod has good performance, always surpassing the state-of-the-art. Robustness\nagainst image processing is also very good. Although we designed our method for\nthe task of synthetic image attribution, the proposed framework is a general\none and can be used for other image forensic applications.\n","authors":["Jun Wang","Benedetta Tondi","Mauro Barni"],"pdf_url":"https://arxiv.org/pdf/2405.11491v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03883v1","updated":"2025-09-04T04:39:21Z","published":"2025-09-04T04:39:21Z","title":"Human Motion Video Generation: A Survey","summary":"  Human motion video generation has garnered significant research interest due\nto its broad applications, enabling innovations such as photorealistic singing\nheads or dynamic avatars that seamlessly dance to music. However, existing\nsurveys in this field focus on individual methods, lacking a comprehensive\noverview of the entire generative process. This paper addresses this gap by\nproviding an in-depth survey of human motion video generation, encompassing\nover ten sub-tasks, and detailing the five key phases of the generation\nprocess: input, motion planning, motion video generation, refinement, and\noutput. Notably, this is the first survey that discusses the potential of large\nlanguage models in enhancing human motion video generation. Our survey reviews\nthe latest developments and technological trends in human motion video\ngeneration across three primary modalities: vision, text, and audio. By\ncovering over two hundred papers, we offer a thorough overview of the field and\nhighlight milestone works that have driven significant technological\nbreakthroughs. Our goal for this survey is to unveil the prospects of human\nmotion video generation and serve as a valuable resource for advancing the\ncomprehensive applications of digital humans. A complete list of the models\nexamined in this survey is available in Our Repository\nhttps://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.\n","authors":["Haiwei Xue","Xiangyang Luo","Zhanghao Hu","Xin Zhang","Xunzhi Xiang","Yuqin Dai","Jianzhuang Liu","Zhensong Zhang","Minglei Li","Jian Yang","Fei Ma","Zhiyong Wu","Changpeng Yang","Zonghong Dai","Fei Richard Yu"],"pdf_url":"https://arxiv.org/pdf/2509.03883v1.pdf","comment":"Accepted by TPAMI. Github Repo:\n  https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation IEEE Access:\n  https://ieeexplore.ieee.org/document/11106267"},{"id":"http://arxiv.org/abs/2509.03873v1","updated":"2025-09-04T04:22:36Z","published":"2025-09-04T04:22:36Z","title":"SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition","summary":"  Food recognition has gained significant attention, but the rapid emergence of\nnew dishes requires methods for recognizing unseen food categories, motivating\nZero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot\nFood Recognition (CZSFR), where cuisines and ingredients naturally align with\nattributes and objects in Compositional Zero-Shot learning (CZSL). However,\nCZSFR faces three challenges: (1) Redundant background information distracts\nmodels from learning meaningful food features, (2) Role confusion between\nstaple and side dishes leads to misclassification, and (3) Semantic bias in a\nsingle attribute can lead to confusion of understanding. Therefore, we propose\nSalientFusion, a context-aware CZSFR method with two components: SalientFormer,\nwhich removes background redundancy and uses depth features to resolve role\nconfusion; DebiasAT, which reduces the semantic bias by aligning prompts with\nvisual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we\nshow that SalientFusion achieves state-of-the-art results on these benchmarks\nand the most popular general datasets for the general CZSL. The code is\navaliable at https://github.com/Jiajun-RUC/SalientFusion.\n","authors":["Jiajun Song","Xiaoou Liu"],"pdf_url":"https://arxiv.org/pdf/2509.03873v1.pdf","comment":"34th International Conference on Artificial Neural Networks - ICANN\n  2025"},{"id":"http://arxiv.org/abs/2509.03872v1","updated":"2025-09-04T04:18:46Z","published":"2025-09-04T04:18:46Z","title":"Focus Through Motion: RGB-Event Collaborative Token Sparsification for\n  Efficient Object Detection","summary":"  Existing RGB-Event detection methods process the low-information regions of\nboth modalities (background in images and non-event regions in event data)\nuniformly during feature extraction and fusion, resulting in high computational\ncosts and suboptimal performance. To mitigate the computational redundancy\nduring feature extraction, researchers have respectively proposed token\nsparsification methods for the image and event modalities. However, these\nmethods employ a fixed number or threshold for token selection, hindering the\nretention of informative tokens for samples with varying complexity. To achieve\na better balance between accuracy and efficiency, we propose FocusMamba, which\nperforms adaptive collaborative sparsification of multimodal features and\nefficiently integrates complementary information. Specifically, an Event-Guided\nMultimodal Sparsification (EGMS) strategy is designed to identify and\nadaptively discard low-information regions within each modality by leveraging\nscene content changes perceived by the event camera. Based on the\nsparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed\nto effectively capture and integrate complementary features from both\nmodalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate\nthat the proposed method achieves superior performance in both accuracy and\nefficiency compared to existing methods. The code will be available at\nhttps://github.com/Zizzzzzzz/FocusMamba.\n","authors":["Nan Yang","Yang Wang","Zhanwen Liu","Yuchao Dai","Yang Liu","Xiangmo Zhao"],"pdf_url":"https://arxiv.org/pdf/2509.03872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.03522v2","updated":"2025-09-04T04:08:12Z","published":"2025-05-06T13:35:59Z","title":"Optimization of Module Transferability in Single Image Super-Resolution:\n  Universality Assessment and Cycle Residual Blocks","summary":"  Deep learning has substantially advanced the field of Single Image\nSuper-Resolution (SISR). However, existing research has predominantly focused\non raw performance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions, which extend the\ntraditional notion of \"Generalization\" to encompass the ease of transferability\nof modules. We then propose the Universality Assessment Equation (UAE), a\nmetric that quantifies how readily a given module can be transplanted across\nmodels and reveals the combined influence of multiple existing metrics on\ntransferability. Guided by the UAE results of standard residual blocks and\nother plug-and-play modules, we further design two optimized modules: the Cycle\nResidual Block (CRB) and the Depth-Wise Cycle Residual Block (DCRB). Through\ncomprehensive experiments on natural-scene benchmarks, remote-sensing datasets,\nand other low-level tasks, we demonstrate that networks embedded with the\nproposed plug-and-play modules outperform several state-of-the-art methods,\nachieving a PSNR improvement of up to 0.83 dB or enabling a 71.3% reduction in\nparameters with negligible loss in reconstruction fidelity. Similar\noptimization approaches could be applied to a broader range of basic modules,\noffering a new paradigm for the design of plug-and-play modules.\n","authors":["Haotong Cheng","Zhiqi Zhang","Hao Li","Xinshang Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.03522v2.pdf","comment":"The paper has been accepted to IET Image Processing"},{"id":"http://arxiv.org/abs/2504.13392v2","updated":"2025-09-04T03:45:36Z","published":"2025-04-18T00:54:36Z","title":"POET: Supporting Prompting Creativity and Personalization with Automated\n  Expansion of Text-to-Image Generation","summary":"  State-of-the-art visual generative AI tools hold immense potential to assist\nusers in the early ideation stages of creative tasks -- offering the ability to\ngenerate (rather than search for) novel and unprecedented (instead of existing)\nimages of considerable quality that also adhere to boundless combinations of\nuser specifications. However, many large-scale text-to-image systems are\ndesigned for broad applicability, yielding conventional output that may limit\ncreative exploration. They also employ interaction methods that may be\ndifficult for beginners. Given that creative end users often operate in\ndiverse, context-specific ways that are often unpredictable, more variation and\npersonalization are necessary. We introduce POET, a real-time interactive tool\nthat (1) automatically discovers dimensions of homogeneity in text-to-image\ngenerative models, (2) expands these dimensions to diversify the output space\nof generated images, and (3) learns from user feedback to personalize\nexpansions. An evaluation with 28 users spanning four creative task domains\ndemonstrated POET's ability to generate results with higher perceived diversity\nand help users reach satisfaction in fewer prompts during creative tasks,\nthereby prompting them to deliberate and reflect more on a wider range of\npossible produced results during the co-creative process. Focusing on visual\ncreativity, POET offers a first glimpse of how interaction techniques of future\ntext-to-image generation tools may support and align with more pluralistic\nvalues and the needs of end users during the ideation stages of their work.\n","authors":["Evans Xu Han","Alice Qian Zhang","Haiyi Zhu","Hong Shen","Paul Pu Liang","Jane Hsieh"],"pdf_url":"https://arxiv.org/pdf/2504.13392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03850v1","updated":"2025-09-04T03:24:35Z","published":"2025-09-04T03:24:35Z","title":"Data-Augmented Quantization-Aware Knowledge Distillation","summary":"  Quantization-aware training (QAT) and Knowledge Distillation (KD) are\ncombined to achieve competitive performance in creating low-bit deep learning\nmodels. Existing KD and QAT works focus on improving the accuracy of quantized\nmodels from the network output perspective by designing better KD loss\nfunctions or optimizing QAT's forward and backward propagation. However,\nlimited attention has been given to understanding the impact of input\ntransformations, such as data augmentation (DA). The relationship between\nquantization-aware KD and DA remains unexplored. In this paper, we address the\nquestion: how to select a good DA in quantization-aware KD, especially for the\nmodels with low precisions? We propose a novel metric which evaluates DAs\naccording to their capacity to maximize the Contextual Mutual Information--the\ninformation not directly related to an image's label--while also ensuring the\npredictions for each class are close to the ground truth labels on average. The\nproposed method automatically ranks and selects DAs, requiring minimal training\noverhead, and it is compatible with any KD or QAT algorithm. Extensive\nevaluations demonstrate that selecting DA strategies using our metric\nsignificantly improves state-of-the-art QAT and KD works across various model\narchitectures and datasets.\n","authors":["Justin Kur","Kaiqi Zhao"],"pdf_url":"https://arxiv.org/pdf/2509.03850v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2509.01563v2","updated":"2025-09-04T03:24:09Z","published":"2025-09-01T15:46:58Z","title":"Kwai Keye-VL 1.5 Technical Report","summary":"  In recent years, the development of Large Language Models (LLMs) has\nsignificantly advanced, extending their capabilities to multimodal tasks\nthrough Multimodal Large Language Models (MLLMs). However, video understanding\nremains a challenging area due to the dynamic and information-dense nature of\nvideos. Existing models struggle with the trade-off between spatial resolution\nand temporal coverage when processing video content. We present Keye-VL-1.5,\nwhich addresses fundamental challenges in video comprehension through three key\ninnovations. First, we introduce a novel Slow-Fast video encoding strategy that\ndynamically allocates computational resources based on inter-frame similarity,\nprocessing key frames with significant visual changes at higher resolution\n(Slow pathway) while handling relatively static frames with increased temporal\ncoverage at lower resolution (Fast pathway). Second, we implement a progressive\nfour-stage pre-training methodology that systematically extends the model's\ncontext length from 8K to 128K tokens, enabling processing of longer videos and\nmore complex visual content. Third, we develop a comprehensive post-training\npipeline focusing on reasoning enhancement and human preference alignment,\nincorporating a 5-step chain-of-thought data construction process, iterative\nGSPO-based reinforcement learning with progressive prompt hinting for difficult\ncases, and alignment training. Through extensive evaluation on public\nbenchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates\nsignificant improvements over existing models, particularly excelling in video\nunderstanding tasks while maintaining competitive performance on general\nmultimodal benchmarks.\n","authors":["Biao Yang","Bin Wen","Boyang Ding","Changyi Liu","Chenglong Chu","Chengru Song","Chongling Rao","Chuan Yi","Da Li","Dunju Zang","Fan Yang","Guorui Zhou","Guowang Zhang","Han Shen","Hao Peng","Haojie Ding","Hao Wang","Haonan Fang","Hengrui Ju","Jiaming Huang","Jiangxia Cao","Jiankang Chen","Jingyun Hua","Kaibing Chen","Kaiyu Jiang","Kaiyu Tang","Kun Gai","Muhao Wei","Qiang Wang","Ruitao Wang","Sen Na","Shengnan Zhang","Siyang Mao","Sui Huang","Tianke Zhang","Tingting Gao","Wei Chen","Wei Yuan","Xiangyu Wu","Xiao Hu","Xingyu Lu","Yi-Fan Zhang","Yiping Yang","Yulong Chen","Zeyi Lu","Zhenhua Wu","Zhixin Ling","Zhuoran Yang","Ziming Li","Di Xu","Haixuan Gao","Hang Li","Jing Wang","Lejian Ren","Qigen Hu","Qianqian Wang","Shiyao Wang","Xinchen Luo","Yan Li","Yuhang Hu","Zixing Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.01563v2.pdf","comment":"Github page: https://github.com/Kwai-Keye/Keye"},{"id":"http://arxiv.org/abs/2502.14891v3","updated":"2025-09-04T02:53:35Z","published":"2025-02-17T03:20:52Z","title":"CoDiff: Conditional Diffusion Model for Collaborative 3D Object\n  Detection","summary":"  Collaborative 3D object detection holds significant importance in the field\nof autonomous driving, as it greatly enhances the perception capabilities of\neach individual agent by facilitating information exchange among multiple\nagents. However, in practice, due to pose estimation errors and time delays,\nthe fusion of information across agents often results in feature\nrepresentations with spatial and temporal noise, leading to detection errors.\nDiffusion models naturally have the ability to denoise noisy samples to the\nideal data, which motivates us to explore the use of diffusion models to\naddress the noise problem between multi-agent systems. In this work, we propose\nCoDiff, a novel robust collaborative perception framework that leverages the\npotential of diffusion models to generate more comprehensive and clearer\nfeature representations. To the best of our knowledge, this is the first work\nto apply diffusion models to multi-agent collaborative perception.\nSpecifically, we project high-dimensional feature map into the latent space of\na powerful pre-trained autoencoder. Within this space, individual agent\ninformation serves as a condition to guide the diffusion model's sampling. This\nprocess denoises coarse feature maps and progressively refines the fused\nfeatures. Experimental study on both simulated and real-world datasets\ndemonstrates that the proposed framework CoDiff consistently outperforms\nexisting relevant methods in terms of the collaborative object detection\nperformance, and exhibits highly desired robustness when the pose and delay\ninformation of agents is with high-level noise. The code is released at\nhttps://github.com/HuangZhe885/CoDiff\n","authors":["Zhe Huang","Shuo Wang","Yongcai Wang","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.14891v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03830v1","updated":"2025-09-04T02:35:14Z","published":"2025-09-04T02:35:14Z","title":"A Multidimensional AI-powered Framework for Analyzing Tourist Perception\n  in Historic Urban Quarters: A Case Study in Shanghai","summary":"  Historic urban quarters play a vital role in preserving cultural heritage\nwhile serving as vibrant spaces for tourism and everyday life. Understanding\nhow tourists perceive these environments is essential for sustainable,\nhuman-centered urban planning. This study proposes a multidimensional\nAI-powered framework for analyzing tourist perception in historic urban\nquarters using multimodal data from social media. Applied to twelve historic\nquarters in central Shanghai, the framework integrates focal point extraction,\ncolor theme analysis, and sentiment mining. Visual focus areas are identified\nfrom tourist-shared photos using a fine-tuned semantic segmentation model. To\nassess aesthetic preferences, dominant colors are extracted using a clustering\nmethod, and their spatial distribution across quarters is analyzed. Color\nthemes are further compared between social media photos and real-world street\nviews, revealing notable shifts. This divergence highlights potential gaps\nbetween visual expectations and the built environment, reflecting both\nstylistic preferences and perceptual bias. Tourist reviews are evaluated\nthrough a hybrid sentiment analysis approach combining a rule-based method and\na multi-task BERT model. Satisfaction is assessed across four dimensions:\ntourist activities, built environment, service facilities, and business\nformats. The results reveal spatial variations in aesthetic appeal and\nemotional response. Rather than focusing on a single technical innovation, this\nframework offers an integrated, data-driven approach to decoding tourist\nperception and contributes to informed decision-making in tourism, heritage\nconservation, and the design of aesthetically engaging public spaces.\n","authors":["Kaizhen Tan","Yufan Wu","Yuxuan Liu","Haoran Zeng"],"pdf_url":"https://arxiv.org/pdf/2509.03830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.12348v3","updated":"2025-09-04T01:53:54Z","published":"2025-06-14T04:57:21Z","title":"Real-Time Per-Garment Virtual Try-On with Temporal Consistency for\n  Loose-Fitting Garments","summary":"  Per-garment virtual try-on methods collect garment-specific datasets and\ntrain networks tailored to each garment to achieve superior results. However,\nthese approaches often struggle with loose-fitting garments due to two key\nlimitations: (1) They rely on human body semantic maps to align garments with\nthe body, but these maps become unreliable when body contours are obscured by\nloose-fitting garments, resulting in degraded outcomes; (2) They train garment\nsynthesis networks on a per-frame basis without utilizing temporal information,\nleading to noticeable jittering artifacts. To address the first limitation, we\npropose a two-stage approach for robust semantic map estimation. First, we\nextract a garment-invariant representation from the raw input image. This\nrepresentation is then passed through an auxiliary network to estimate the\nsemantic map. This enhances the robustness of semantic map estimation under\nloose-fitting garments during garment-specific dataset generation. To address\nthe second limitation, we introduce a recurrent garment synthesis framework\nthat incorporates temporal dependencies to improve frame-to-frame coherence\nwhile maintaining real-time performance. We conducted qualitative and\nquantitative evaluations to demonstrate that our method outperforms existing\napproaches in both image quality and temporal coherence. Ablation studies\nfurther validate the effectiveness of the garment-invariant representation and\nthe recurrent synthesis framework.\n","authors":["Zaiqiang Wu","I-Chao Shen","Takeo Igarashi"],"pdf_url":"https://arxiv.org/pdf/2506.12348v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03808v1","updated":"2025-09-04T01:49:13Z","published":"2025-09-04T01:49:13Z","title":"EGTM: Event-guided Efficient Turbulence Mitigation","summary":"  Turbulence mitigation (TM) aims to remove the stochastic distortions and\nblurs introduced by atmospheric turbulence into frame cameras. Existing\nstate-of-the-art deep-learning TM methods extract turbulence cues from multiple\ndegraded frames to find the so-called \"lucky'', not distorted patch, for \"lucky\nfusion''. However, it requires high-capacity network to learn from\ncoarse-grained turbulence dynamics between synchronous frames with limited\nframe-rate, thus fall short in computational and storage efficiency. Event\ncameras, with microsecond-level temporal resolution, have the potential to\nfundamentally address this bottleneck with efficient sparse and asynchronous\nimaging mechanism. In light of this, we (i) present the fundamental\n\\textbf{``event-lucky insight''} to reveal the correlation between turbulence\ndistortions and inverse spatiotemporal distribution of event streams. Then,\nbuild upon this insight, we (ii) propose a novel EGTM framework that extracts\npixel-level reliable turbulence-free guidance from the explicit but noisy\nturbulent events for temporal lucky fusion. Moreover, we (iii) build the first\nturbulence data acquisition system to contribute the first real-world\nevent-driven TM dataset. Extensive experimental results demonstrate that our\napproach significantly surpass the existing SOTA TM method by 710 times, 214\ntimes and 224 times in model size, inference latency and model complexity\nrespectively, while achieving the state-of-the-art in restoration quality\n(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating\nthe great efficiency merit of introducing event modality into TM task. Demo\ncode and data have been uploaded in supplementary material and will be released\nonce accepted.\n","authors":["Huanan Li","Rui Fan","Juntao Guan","Weidong Hao","Lai Rui","Tong Wu","Yikai Wang","Lin Gu"],"pdf_url":"https://arxiv.org/pdf/2509.03808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03803v1","updated":"2025-09-04T01:40:41Z","published":"2025-09-04T01:40:41Z","title":"Causality-guided Prompt Learning for Vision-language Models via Visual\n  Granulation","summary":"  Prompt learning has recently attracted much attention for adapting\npre-trained vision-language models (e.g., CLIP) to downstream recognition\ntasks. However, most of the existing CLIP-based prompt learning methods only\nshow a limited ability for handling fine-grained datasets. To address this\nissue, we propose a causality-guided text prompt learning method via visual\ngranulation for CLIP, called CaPL, where the explored visual granulation\ntechnique could construct sets of visual granules for the text prompt to\ncapture subtle discrepancies among different fine-grained classes through\ncasual inference. The CaPL method contains the following two modules: (1) An\nattribute disentanglement module is proposed to decompose visual features into\nnon-individualized attributes (shared by some classes) and individualized\nattributes (specific to single classes) using a Brownian Bridge Diffusion\nModel; (2) A granule learning module is proposed to construct visual granules\nby integrating the aforementioned attributes for recognition under two causal\ninference strategies. Thanks to the learned visual granules, more\ndiscriminative text prompt is expected to be learned. Extensive experimental\nresults on 15 datasets demonstrate that our CaPL method significantly\noutperforms the state-of-the-art prompt learning methods, especially on\nfine-grained datasets.\n","authors":["Mengyu Gao","Qiulei Dong"],"pdf_url":"https://arxiv.org/pdf/2509.03803v1.pdf","comment":"ICCV 2025 Accepted"},{"id":"http://arxiv.org/abs/2509.03800v1","updated":"2025-09-04T01:28:44Z","published":"2025-09-04T01:28:44Z","title":"MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in\n  3D CT Disease Detection, Understanding and Reporting","summary":"  Radiologic diagnostic errors-under-reading errors, inattentional blindness,\nand communication failures-remain prevalent in clinical practice. These issues\noften stem from missed localized abnormalities, limited global context, and\nvariability in report language. These challenges are amplified in 3D imaging,\nwhere clinicians must examine hundreds of slices per scan. Addressing them\nrequires systems with precise localized detection, global volume-level\nreasoning, and semantically consistent natural language reporting. However,\nexisting 3D vision-language models are unable to meet all three needs jointly,\nlacking local-global understanding for spatial reasoning and struggling with\nthe variability and noise of uncurated radiology reports. We present\nMedVista3D, a multi-scale semantic-enriched vision-language pretraining\nframework for 3D CT analysis. To enable joint disease detection and holistic\ninterpretation, MedVista3D performs local and global image-text alignment for\nfine-grained representation learning within full-volume context. To address\nreport variability, we apply language model rewrites and introduce a Radiology\nSemantic Matching Bank for semantics-aware alignment. MedVista3D achieves\nstate-of-the-art performance on zero-shot disease classification, report\nretrieval, and medical visual question answering, while transferring well to\norgan segmentation and prognosis prediction. Code and datasets will be\nreleased.\n","authors":["Yuheng Li","Yenho Chen","Yuxiang Lai","Jike Zhong","Vanessa Wildman","Xiaofeng Yang"],"pdf_url":"https://arxiv.org/pdf/2509.03800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03794v1","updated":"2025-09-04T01:04:54Z","published":"2025-09-04T01:04:54Z","title":"Fitting Image Diffusion Models on Video Datasets","summary":"  Image diffusion models are trained on independently sampled static images.\nWhile this is the bedrock task protocol in generative modeling, capturing the\ntemporal world through the lens of static snapshots is information-deficient by\ndesign. This limitation leads to slower convergence, limited distributional\ncoverage, and reduced generalization. In this work, we propose a simple and\neffective training strategy that leverages the temporal inductive bias present\nin continuous video frames to improve diffusion training. Notably, the proposed\nmethod requires no architectural modification and can be seamlessly integrated\ninto standard diffusion training pipelines. We evaluate our method on the\nHandCo dataset, where hand-object interactions exhibit dense temporal coherence\nand subtle variations in finger articulation often result in semantically\ndistinct motions. Empirically, our method accelerates convergence by over\n2$\\text{x}$ faster and achieves lower FID on both training and validation\ndistributions. It also improves generative diversity by encouraging the model\nto capture meaningful temporal variations. We further provide an optimization\nanalysis showing that our regularization reduces the gradient variance, which\ncontributes to faster convergence.\n","authors":["Juhun Lee","Simon S. Woo"],"pdf_url":"https://arxiv.org/pdf/2509.03794v1.pdf","comment":"ICCV25 Workshop"},{"id":"http://arxiv.org/abs/2509.02591v2","updated":"2025-09-04T00:59:33Z","published":"2025-08-29T03:24:57Z","title":"Ensemble of Pathology Foundation Models for MIDOG 2025 Track 2: Atypical\n  Mitosis Classification","summary":"  Mitotic figures are classified into typical and atypical variants, with\natypical counts correlating strongly with tumor aggressiveness. Accurate\ndifferentiation is therefore essential for patient prognostication and resource\nallocation, yet remains challenging even for expert pathologists. Here, we\nleveraged Pathology Foundation Models (PFMs) pre-trained on large\nhistopathology datasets and applied parameter-efficient fine-tuning via\nlow-rank adaptation. In addition, we incorporated ConvNeXt V2, a\nstate-of-the-art convolutional neural network architecture, to complement PFMs.\nDuring training, we employed a fisheye transform to emphasize mitoses and\nFourier Domain Adaptation using ImageNet target images. Finally, we ensembled\nmultiple PFMs to integrate complementary morphological insights, achieving\ncompetitive balanced accuracy on the Preliminary Evaluation Phase dataset.\n","authors":["Mieko Ochi","Bae Yuan"],"pdf_url":"https://arxiv.org/pdf/2509.02591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03786v1","updated":"2025-09-04T00:44:32Z","published":"2025-09-04T00:44:32Z","title":"SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object\n  Detection","summary":"  Underwater Camouflaged Object Detection (UCOD) aims to identify objects that\nblend seamlessly into underwater environments. This task is critically\nimportant to marine ecology. However, it remains largely underexplored and\naccurate identification is severely hindered by optical distortions, water\nturbidity, and the complex traits of marine organisms. To address these\nchallenges, we introduce the UCOD task and present DeepCamo, a benchmark\ndataset designed for this domain. We also propose Semantic Localization and\nEnhancement Network (SLENet), a novel framework for UCOD. We first benchmark\nstate-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet\nis built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)\nmodule and a Localization Guidance Branch (LGB) to enhance multi-scale feature\nrepresentation while generating a location map enriched with global semantic\ninformation. This map guides the Multi-Scale Supervised Decoder (MSSD) to\nproduce more accurate predictions. Experiments on our DeepCamo dataset and\nthree benchmark COD datasets confirm SLENet's superior performance over SOTA\nmethods, and underscore its high generality for the broader COD task.\n","authors":["Xinxin Wang","Han Sun","Ningzhong Liu","Huiyu Zhou","Yinan Yao"],"pdf_url":"https://arxiv.org/pdf/2509.03786v1.pdf","comment":"14pages, accepted by PRCV2025"},{"id":"http://arxiv.org/abs/2506.05195v2","updated":"2025-09-04T00:40:57Z","published":"2025-06-05T16:07:22Z","title":"Vision-Based Autonomous MM-Wave Reflector Using ArUco-Driven\n  Angle-of-Arrival Estimation","summary":"  Reliable millimeter-wave (mmWave) communication in non-line-of-sight (NLoS)\nconditions remains a major challenge for both military and civilian operations,\nespecially in urban or infrastructure-limited environments. This paper presents\na vision-aided autonomous reflector system designed to enhance mmWave link\nperformance by dynamically steering signal reflections using a motorized\nmetallic plate. The proposed system leverages a monocular camera to detect\nArUco markers on allied transmitter and receiver nodes, estimate their angles\nof arrival, and align the reflector in real time for optimal signal\nredirection. This approach enables selective beam coverage by serving only\nauthenticated targets with visible markers and reduces the risk of unintended\nsignal exposure. The designed prototype, built on a Raspberry Pi 4 and\nlow-power hardware, operates autonomously without reliance on external\ninfrastructure or GPS. Experimental results at 60\\,GHz demonstrate a 23\\,dB\naverage gain in received signal strength and an 0.89 probability of maintaining\nsignal reception above a target threshold of -65 dB in an indoor environment,\nfar exceeding the static and no-reflector baselines. These results demonstrate\nthe system's potential for resilient and adaptive mmWave connectivity in\ncomplex and dynamic environments.\n","authors":["Josue Marroquin","Nan Inzali","Miles Dillon Lantz","Campbell Freeman","Amod Ashtekar","\\\\Ajinkya Umesh Mulik","Mohammed E Eltayeb"],"pdf_url":"https://arxiv.org/pdf/2506.05195v2.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2509.04449v1","updated":"2025-09-04T17:59:52Z","published":"2025-09-04T17:59:52Z","title":"ChronoGraph: A Real-World Graph-Based Multivariate Time Series Dataset","summary":"  We present ChronoGraph, a graph-structured multivariate time series\nforecasting dataset built from real-world production microservices. Each node\nis a service that emits a multivariate stream of system-level performance\nmetrics, capturing CPU, memory, and network usage patterns, while directed\nedges encode dependencies between services. The primary task is forecasting\nfuture values of these signals at the service level. In addition, ChronoGraph\nprovides expert-annotated incident windows as anomaly labels, enabling\nevaluation of anomaly detection methods and assessment of forecast robustness\nduring operational disruptions. Compared to existing benchmarks from industrial\ncontrol systems or traffic and air-quality domains, ChronoGraph uniquely\ncombines (i) multivariate time series, (ii) an explicit, machine-readable\ndependency graph, and (iii) anomaly labels aligned with real incidents. We\nreport baseline results spanning forecasting models, pretrained time-series\nfoundation models, and standard anomaly detectors. ChronoGraph offers a\nrealistic benchmark for studying structure-aware forecasting and incident-aware\nevaluation in microservice systems.\n","authors":["Adrian Catalin Lutu","Ioana Pintilie","Elena Burceanu","Andrei Manolache"],"pdf_url":"https://arxiv.org/pdf/2509.04449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04442v1","updated":"2025-09-04T17:59:06Z","published":"2025-09-04T17:59:06Z","title":"Delta Activations: A Representation for Finetuned Large Language Models","summary":"  The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations.\n","authors":["Zhiqiu Xu","Amish Sethi","Mayur Naik","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2509.04442v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04441v1","updated":"2025-09-04T17:57:13Z","published":"2025-09-04T17:57:13Z","title":"DEXOP: A Device for Robotic Transfer of Dexterous Human Manipulation","summary":"  We introduce perioperation, a paradigm for robotic data collection that\nsensorizes and records human manipulation while maximizing the transferability\nof the data to real robots. We implement this paradigm in DEXOP, a passive hand\nexoskeleton designed to maximize human ability to collect rich sensory (vision\n+ tactile) data for diverse dexterous manipulation tasks in natural\nenvironments. DEXOP mechanically connects human fingers to robot fingers,\nproviding users with direct contact feedback (via proprioception) and mirrors\nthe human hand pose to the passive robot hand to maximize the transfer of\ndemonstrated skills to the robot. The force feedback and pose mirroring make\ntask demonstrations more natural for humans compared to teleoperation,\nincreasing both speed and accuracy. We evaluate DEXOP across a range of\ndexterous, contact-rich tasks, demonstrating its ability to collect\nhigh-quality demonstration data at scale. Policies learned with DEXOP data\nsignificantly improve task performance per unit time of data collection\ncompared to teleoperation, making DEXOP a powerful tool for advancing robot\ndexterity. Our project page is at https://dex-op.github.io.\n","authors":["Hao-Shu Fang","Branden Romero","Yichen Xie","Arthur Hu","Bo-Ruei Huang","Juan Alvarez","Matthew Kim","Gabriel Margolis","Kavya Anbarasu","Masayoshi Tomizuka","Edward Adelson","Pulkit Agrawal"],"pdf_url":"https://arxiv.org/pdf/2509.04441v1.pdf","comment":"project page: https://dex-op.github.io"},{"id":"http://arxiv.org/abs/2411.12736v2","updated":"2025-09-04T17:56:24Z","published":"2024-11-19T18:58:03Z","title":"ACING: Actor-Critic for Instruction Learning in Black-Box LLMs","summary":"  The effectiveness of Large Language Models (LLMs) in solving tasks depends\nsignificantly on the quality of their instructions, which often require\nsubstantial human effort to craft. This underscores the need for automated\ninstruction optimization. However, optimizing instructions is particularly\nchallenging when working with black-box LLMs, where model parameters and\ngradients are inaccessible. We introduce ACING, an actor-critic reinforcement\nlearning framework that formulates instruction optimization as a stateless,\ncontinuous-action problem, enabling exploration of infinite instruction spaces\nusing only black-box feedback. ACING automatically discovers prompts that\noutperform human-written prompts in 76% of instruction-induction tasks, with\ngains of up to 33 points and a 10-point median improvement over the best\nautomatic baseline in 33 tasks spanning instruction-induction, summarization,\nand chain-of-thought reasoning. Extensive ablations highlight its robustness\nand efficiency. An implementation of ACING is available at\nhttps://github.com/salmakh1/ACING.\n","authors":["Salma Kharrat","Fares Fourati","Marco Canini"],"pdf_url":"https://arxiv.org/pdf/2411.12736v2.pdf","comment":"Accepted at EMNLP 2025"},{"id":"http://arxiv.org/abs/2509.04439v1","updated":"2025-09-04T17:54:19Z","published":"2025-09-04T17:54:19Z","title":"ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory","summary":"  While inference-time scaling enables LLMs to carry out increasingly long and\ncapable reasoning traces, the patterns and insights uncovered during these\ntraces are immediately discarded once the context window is reset for a new\nquery. External memory is a natural way to persist these discoveries, and\nrecent work has shown clear benefits for reasoning-intensive tasks. We see an\nopportunity to make such memories more broadly reusable and scalable by moving\nbeyond instance-based memory entries (e.g. exact query/response pairs, or\nsummaries tightly coupled with the original problem context) toward\nconcept-level memory: reusable, modular abstractions distilled from solution\ntraces and stored in natural language. For future queries, relevant concepts\nare selectively retrieved and integrated into the prompt, enabling test-time\ncontinual learning without weight updates. Our design introduces new strategies\nfor abstracting takeaways from rollouts and retrieving entries for new queries,\npromoting reuse and allowing memory to expand with additional experiences. On\nthe challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over\na strong no-memory baseline with performance continuing to scale with inference\ncompute. We find abstract concepts to be the most consistent memory design,\noutscoring the baseline at all tested inference compute scales. Moreover, we\nconfirm that dynamically updating memory during test-time outperforms an\notherwise identical fixed memory setting with additional attempts, supporting\nthe hypothesis that solving more problems and abstracting more patterns to\nmemory enables further solutions in a form of self-improvement. Code available\nat https://github.com/matt-seb-ho/arc_memo.\n","authors":["Matthew Ho","Chen Si","Zhaoxiang Feng","Fangxu Yu","Zhijian Liu","Zhiting Hu","Lianhui Qin"],"pdf_url":"https://arxiv.org/pdf/2509.04439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04419v1","updated":"2025-09-04T17:40:33Z","published":"2025-09-04T17:40:33Z","title":"Towards a Unified View of Large Language Model Post-Training","summary":"  Two major sources of training data exist for post-training modern language\nmodels: online (model-generated rollouts) data, and offline (human or\nother-model demonstrations) data. These two types of data are typically used by\napproaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT),\nrespectively. In this paper, we show that these approaches are not in\ncontradiction, but are instances of a single optimization process. We derive a\nUnified Policy Gradient Estimator, and present the calculations of a wide\nspectrum of post-training approaches as the gradient of a common objective\nunder different data distribution assumptions and various bias-variance\ntradeoffs. The gradient estimator is constructed with four interchangeable\nparts: stabilization mask, reference policy denominator, advantage estimate,\nand likelihood gradient. Motivated by our theoretical findings, we propose\nHybrid Post-Training (HPT), an algorithm that dynamically selects different\ntraining signals. HPT is designed to yield both effective exploitation of\ndemonstration and stable exploration without sacrificing learned reasoning\npatterns. We provide extensive experiments and ablation studies to verify the\neffectiveness of our unified theoretical framework and HPT. Across six\nmathematical reasoning benchmarks and two out-of-distribution suites, HPT\nconsistently surpasses strong baselines across models of varying scales and\nfamilies.\n","authors":["Xingtai Lv","Yuxin Zuo","Youbang Sun","Hongyi Liu","Yuntian Wei","Zhekai Chen","Lixuan He","Xuekai Zhu","Kaiyan Zhang","Bingning Wang","Ning Ding","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2509.04419v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01185v2","updated":"2025-09-04T17:22:16Z","published":"2025-09-01T07:08:45Z","title":"Modular Techniques for Synthetic Long-Context Data Generation in\n  Language Model Training and Evaluation","summary":"  The ability of large language models (LLMs) to process and reason over long\ntextual inputs is critical for a wide range of real-world applications.\nHowever, progress in this area is significantly constrained by the absence of\nhigh-quality, diverse, and verifiable long-context datasets suitable for both\ntraining and evaluation. This work introduces a modular, extensible framework\nfor synthetic long-context data generation via prompt-based interaction with\nLLMs. The framework supports multiple training and alignment objectives,\nincluding Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),\nand Group Relative Policy Optimization (GRPO). It encompasses four core\ngeneration paradigms: multi-turn conversational dialogues, document-grounded\ninput-output pairs, verifiable instruction-response tasks, and long-context\nreasoning examples. Through templated prompting, a model-agnostic architecture,\nand metadata-enriched outputs, the proposed approach facilitates scalable,\ncontrollable, and purpose-aligned dataset creation for advancing long-context\ncapabilities in LLMs.\n","authors":["Seganrasan Subramanian","Abhigya Verma"],"pdf_url":"https://arxiv.org/pdf/2509.01185v2.pdf","comment":"26 pages, 4 figures"},{"id":"http://arxiv.org/abs/2509.04404v1","updated":"2025-09-04T17:16:26Z","published":"2025-09-04T17:16:26Z","title":"No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in\n  Resume Screening","summary":"  In this study, we conduct a resume-screening experiment (N=528) where people\ncollaborate with simulated AI models exhibiting race-based preferences (bias)\nto evaluate candidates for 16 high and low status occupations. Simulated AI\nbias approximates factual and counterfactual estimates of racial bias in\nreal-world AI systems. We investigate people's preferences for White, Black,\nHispanic, and Asian candidates (represented through names and affinity groups\non quality-controlled resumes) across 1,526 scenarios and measure their\nunconscious associations between race and status using implicit association\ntests (IATs), which predict discriminatory hiring decisions but have not been\ninvestigated in human-AI collaboration. When making decisions without AI or\nwith AI that exhibits no race-based preferences, people select all candidates\nat equal rates. However, when interacting with AI favoring a particular group,\npeople also favor those candidates up to 90% of the time, indicating a\nsignificant behavioral shift. The likelihood of selecting candidates whose\nidentities do not align with common race-status stereotypes can increase by 13%\nif people complete an IAT before conducting resume screening. Finally, even if\npeople think AI recommendations are low quality or not important, their\ndecisions are still vulnerable to AI bias under certain circumstances. This\nwork has implications for people's autonomy in AI-HITL scenarios, AI and work,\ndesign and evaluation of AI hiring systems, and strategies for mitigating bias\nin collaborative decision-making tasks. In particular, organizational and\nregulatory policy should acknowledge the complex nature of AI-HITL decision\nmaking when implementing these systems, educating people who use them, and\ndetermining which are subject to oversight.\n","authors":["Kyra Wilson","Mattea Sim","Anna-Maria Gueorguieva","Aylin Caliskan"],"pdf_url":"https://arxiv.org/pdf/2509.04404v1.pdf","comment":"Published in Proceedings of the 2025 AAAI/ACM Conference on AI,\n  Ethics, and Society; code available at\n  https://github.com/kyrawilson/No-Thoughts-Just-AI"},{"id":"http://arxiv.org/abs/2509.04398v1","updated":"2025-09-04T17:10:01Z","published":"2025-09-04T17:10:01Z","title":"IPA: An Information-Preserving Input Projection Framework for Efficient\n  Foundation Model Adaptation","summary":"  Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce\nadaptation cost by injecting low-rank updates into pretrained weights. However,\nLoRA's down-projection is randomly initialized and data-agnostic, discarding\npotentially useful information. Prior analyses show that this projection\nchanges little during training, while the up-projection carries most of the\nadaptation, making the random input compression a performance bottleneck. We\npropose IPA, a feature-aware projection framework that explicitly preserves\ninformation in the reduced hidden space. In the linear case, we instantiate IPA\nwith algorithms approximating top principal components, enabling efficient\nprojector pretraining with negligible inference overhead. Across language and\nvision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on\naverage 1.5 points higher accuracy on commonsense reasoning and 2.3 points on\nVTAB-1k, while matching full LoRA performance with roughly half the trainable\nparameters when the projection is frozen.\n","authors":["Yuan Yin","Shashanka Venkataramanan","Tuan-Hung Vu","Andrei Bursuc","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2509.04398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.12964v5","updated":"2025-09-04T16:55:10Z","published":"2025-07-17T10:03:57Z","title":"Demographic-aware fine-grained classification of pediatric wrist\n  fractures","summary":"  Wrist pathologies are frequently observed, particularly among children who\nconstitute the majority of fracture cases. Computer vision presents a promising\navenue, contingent upon the availability of extensive datasets, a notable\nchallenge in medical imaging. Therefore, reliance solely on one modality, such\nas images, proves inadequate, especially in an era of diverse and plentiful\ndata types. This study addresses the problem using a multifaceted approach:\nframing it as a fine-grained recognition task, fusing patient metadata with\nX-rays, and leveraging weights from a separate fine-grained dataset rather than\nfrom a coarse-grained dataset like ImageNet. Unlike prior work, this is the\nfirst application of metadata integration for wrist pathology recognition. Our\nresults show that combining fine-grained transformer approach, fine-grained\npre-training, and metadata integration improves diagnostic accuracy by 2% on\nsmall custom curated dataset and over 10% on a larger fracture dataset.\n","authors":["Ammar Ahmed","Ali Shariq Imran","Zenun Kastrati","Sher Muhammad Daudpota"],"pdf_url":"https://arxiv.org/pdf/2507.12964v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04379v1","updated":"2025-09-04T16:40:44Z","published":"2025-09-04T16:40:44Z","title":"SSGaussian: Semantic-Aware and Structure-Preserving 3D Style Transfer","summary":"  Recent advancements in neural representations, such as Neural Radiance Fields\nand 3D Gaussian Splatting, have increased interest in applying style transfer\nto 3D scenes. While existing methods can transfer style patterns onto\n3D-consistent neural representations, they struggle to effectively extract and\ntransfer high-level style semantics from the reference style image.\nAdditionally, the stylized results often lack structural clarity and\nseparation, making it difficult to distinguish between different instances or\nobjects within the 3D scene. To address these limitations, we propose a novel\n3D style transfer pipeline that effectively integrates prior knowledge from\npretrained 2D diffusion models. Our pipeline consists of two key stages: First,\nwe leverage diffusion priors to generate stylized renderings of key viewpoints.\nThen, we transfer the stylized key views onto the 3D representation. This\nprocess incorporates two innovative designs. The first is cross-view style\nalignment, which inserts cross-view attention into the last upsampling block of\nthe UNet, allowing feature interactions across multiple key views. This ensures\nthat the diffusion model generates stylized key views that maintain both style\nfidelity and instance-level consistency. The second is instance-level style\ntransfer, which effectively leverages instance-level consistency across\nstylized key views and transfers it onto the 3D representation. This results in\na more structured, visually coherent, and artistically enriched stylization.\nExtensive qualitative and quantitative experiments demonstrate that our 3D\nstyle transfer pipeline significantly outperforms state-of-the-art methods\nacross a wide range of scenes, from forward-facing to challenging 360-degree\nenvironments. Visit our project page https://jm-xu.github.io/SSGaussian for\nimmersive visualization.\n","authors":["Jimin Xu","Bosheng Qin","Tao Jin","Zhou Zhao","Zhenhui Ye","Jun Yu","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2509.04379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02175v2","updated":"2025-09-04T16:38:44Z","published":"2025-09-02T10:32:58Z","title":"Understanding Space Is Rocket Science -- Only Top Reasoning Models Can\n  Solve Spatial Understanding Tasks","summary":"  We propose RocketScience, an open-source contrastive VLM benchmark that tests\nfor spatial relation understanding. It is comprised of entirely new real-world\nimage-text pairs covering mostly relative spatial understanding and the order\nof objects. The benchmark is designed to be very easy for humans and hard for\nthe current generation of VLMs, and this is empirically verified. Our results\nshow a striking lack of spatial relation understanding in open source and\nfrontier commercial VLMs and a surprisingly high performance of reasoning\nmodels. Additionally, we perform a disentanglement analysis to separate the\ncontributions of object localization and spatial reasoning in\nchain-of-thought-based models and find that the performance on the benchmark is\nbottlenecked by spatial reasoning and not object localization capabilities. We\nrelease the dataset with a CC-BY-4.0 license and make the evaluation code\navailable at: https://github.com/nilshoehing/rocketscience\n","authors":["Nils Hoehing","Mayug Maniparambil","Ellen Rushe","Noel E. O'Connor","Anthony Ventresque"],"pdf_url":"https://arxiv.org/pdf/2509.02175v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.07900v2","updated":"2025-09-04T16:23:02Z","published":"2025-06-09T16:16:50Z","title":"MiniCPM4: Ultra-Efficient LLMs on End Devices","summary":"  This paper introduces MiniCPM4, a highly efficient large language model (LLM)\ndesigned explicitly for end-side devices. We achieve this efficiency through\nsystematic innovation in four key dimensions: model architecture, training\ndata, training algorithms, and inference systems. Specifically, in terms of\nmodel architecture, we propose InfLLM v2, a trainable sparse attention\nmechanism that accelerates both prefilling and decoding phases for long-context\nprocessing. Regarding training data, we propose UltraClean, an efficient and\naccurate pre-training data filtering and generation strategy, and UltraChat v2,\na comprehensive supervised fine-tuning dataset. These datasets enable\nsatisfactory model performance to be achieved using just 8 trillion training\ntokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient\npre-training strategy search, and improve existing post-training methods by\nintroducing chunk-wise rollout for load-balanced reinforcement learning and\ndata-efficient tenary LLM, BitCPM. Regarding inference systems, we propose\nCPM.cu that integrates sparse attention, model quantization, and speculative\nsampling to achieve efficient prefilling and decoding. To meet diverse\non-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B\nparameters, respectively. Furthermore, we construct a hybrid reasoning model,\nMiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning\nmode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform\nsimilar-sized open-source models across benchmarks, with the 8B variants\nshowing significant speed improvements on long sequence understanding and\ngeneration.\n","authors":[" MiniCPM Team","Chaojun Xiao","Yuxuan Li","Xu Han","Yuzhuo Bai","Jie Cai","Haotian Chen","Wentong Chen","Xin Cong","Ganqu Cui","Ning Ding","Shengda Fan","Yewei Fang","Zixuan Fu","Wenyu Guan","Yitong Guan","Junshao Guo","Yufeng Han","Bingxiang He","Yuxiang Huang","Baoxi Ji","Cunliang Kong","Qiuzuo Li","Siyuan Li","Wenhao Li","Xin Li","Yanghao Li","Yishan Li","Zhen Li","Dan Liu","Biyuan Lin","Yankai Lin","Xiang Long","Quanyu Lu","Yaxi Lu","Peiyan Luo","Hongya Lyu","Litu Ou","Yinxu Pan","Lushi Pu","Zekai Qu","Qundong Shi","Zijun Song","Jiayuan Su","Zhou Su","Ao Sun","Xianghui Sun","Peijun Tang","Fangzheng Wang","Feng Wang","Shuo Wang","Yudong Wang","Zheng Wang","Yesai Wu","Zhenyu Xiao","Jie Xie","Zihao Xie","Xiaoyue Xu","Yukun Yan","Jiarui Yuan","Jinqian Zhang","Kaihuo Zhang","Lei Zhang","Linyue Zhang","Xueren Zhang","Yudi Zhang","Hengyu Zhao","Weilin Zhao","Weilun Zhao","Yuanqian Zhao","Zhi Zheng","Chuyue Zhou","Ge Zhou","Jie Zhou","Wei Zhou","Yanghao Zhou","Zihan Zhou","Zixuan Zhou","Zhiyuan Liu","Guoyang Zeng","Chao Jia","Dahai Li","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2506.07900v2.pdf","comment":"MiniCPM4 Technical Report"},{"id":"http://arxiv.org/abs/2509.04362v1","updated":"2025-09-04T16:22:29Z","published":"2025-09-04T16:22:29Z","title":"Parking Availability Prediction via Fusing Multi-Source Data with A\n  Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer","summary":"  The rapid growth of private car ownership has worsened the urban parking\npredicament, underscoring the need for accurate and effective parking\navailability prediction to support urban planning and management. To address\nkey limitations in modeling spatio-temporal dependencies and exploiting\nmulti-source data for parking availability prediction, this study proposes a\nnovel approach with SST-iTransformer. The methodology leverages K-means\nclustering to establish parking cluster zones (PCZs), extracting and\nintegrating traffic demand characteristics from various transportation modes\n(i.e., metro, bus, online ride-hailing, and taxi) associated with the targeted\nparking lots. Upgraded on vanilla iTransformer, SST-iTransformer integrates\nmasking-reconstruction-based pretext tasks for self-supervised spatio-temporal\nrepresentation learning, and features an innovative dual-branch attention\nmechanism: Series Attention captures long-term temporal dependencies via\npatching operations, while Channel Attention models cross-variate interactions\nthrough inverted dimensions. Extensive experiments using real-world data from\nChengdu, China, demonstrate that SST-iTransformer outperforms baseline deep\nlearning models (including Informer, Autoformer, Crossformer, and\niTransformer), achieving state-of-the-art performance with the lowest mean\nsquared error (MSE) and competitive mean absolute error (MAE). Comprehensive\nablation studies quantitatively reveal the relative importance of different\ndata sources: incorporating ride-hailing data provides the largest performance\ngains, followed by taxi, whereas fixed-route transit features (bus/metro)\ncontribute marginally. Spatial correlation analysis further confirms that\nexcluding historical data from correlated parking lots within PCZs leads to\nsubstantial performance degradation, underscoring the importance of modeling\nspatial dependencies.\n","authors":["Yin Huang","Yongqi Dong","Youhua Tang","Li Li"],"pdf_url":"https://arxiv.org/pdf/2509.04362v1.pdf","comment":"25 pages, 5 figures, under review for journal publication"},{"id":"http://arxiv.org/abs/2509.04357v1","updated":"2025-09-04T16:18:34Z","published":"2025-09-04T16:18:34Z","title":"PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity\n  Disambiguation","summary":"  Automatic speech recognition (ASR) systems struggle with domain-specific\nnamed entities, especially homophones. Contextual ASR improves recognition but\noften fails to capture fine-grained phoneme variations due to limited entity\ndiversity. Moreover, prior methods treat entities as independent tokens,\nleading to incomplete multi-token biasing. To address these issues, we propose\nPhoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation\n(PARCO), which integrates phoneme-aware encoding, contrastive entity\ndisambiguation, entity-level supervision, and hierarchical entity filtering.\nThese components enhance phonetic discrimination, ensure complete entity\nretrieval, and reduce false positives under uncertainty. Experiments show that\nPARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English\nDATA2 under 1,000 distractors, significantly outperforming baselines. PARCO\nalso demonstrates robust gains on out-of-domain datasets like THCHS-30 and\nLibriSpeech.\n","authors":["Jiajun He","Naoki Sawada","Koichi Miyazaki","Tomoki Toda"],"pdf_url":"https://arxiv.org/pdf/2509.04357v1.pdf","comment":"Accepted by ASRU 2025"},{"id":"http://arxiv.org/abs/2509.04345v1","updated":"2025-09-04T16:03:44Z","published":"2025-09-04T16:03:44Z","title":"AUDETER: A Large-scale Dataset for Deepfake Audio Detection in Open\n  Worlds","summary":"  Speech generation systems can produce remarkably realistic vocalisations that\nare often indistinguishable from human speech, posing significant authenticity\nchallenges. Although numerous deepfake detection methods have been developed,\ntheir effectiveness in real-world environments remains unrealiable due to the\ndomain shift between training and test samples arising from diverse human\nspeech and fast evolving speech synthesis systems. This is not adequately\naddressed by current datasets, which lack real-world application challenges\nwith diverse and up-to-date audios in both real and deep-fake categories. To\nfill this gap, we introduce AUDETER (AUdio DEepfake TEst Range), a large-scale,\nhighly diverse deepfake audio dataset for comprehensive evaluation and robust\ndevelopment of generalised models for deepfake audio detection. It consists of\nover 4,500 hours of synthetic audio generated by 11 recent TTS models and 10\nvocoders with a broad range of TTS/vocoder patterns, totalling 3 million audio\nclips, making it the largest deepfake audio dataset by scale. Through extensive\nexperiments with AUDETER, we reveal that i) state-of-the-art (SOTA) methods\ntrained on existing datasets struggle to generalise to novel deepfake audio\nsamples and suffer from high false positive rates on unseen human voice,\nunderscoring the need for a comprehensive dataset; and ii) these methods\ntrained on AUDETER achieve highly generalised detection performance and\nsignificantly reduce detection error rate by 44.1% to 51.6%, achieving an error\nrate of only 4.17% on diverse cross-domain samples in the popular In-the-Wild\ndataset, paving the way for training generalist deepfake audio detectors.\nAUDETER is available on GitHub.\n","authors":["Qizhou Wang","Hanxun Huang","Guansong Pang","Sarah Erfani","Christopher Leckie"],"pdf_url":"https://arxiv.org/pdf/2509.04345v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04343v1","updated":"2025-09-04T16:03:03Z","published":"2025-09-04T16:03:03Z","title":"Psychologically Enhanced AI Agents","summary":"  We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of\nLarge Language Model (LLM) agents through psychologically grounded personality\nconditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method\nprimes agents with distinct personality archetypes via prompt engineering,\nenabling control over behavior along two foundational axes of human psychology,\ncognition and affect. We show that such personality priming yields consistent,\ninterpretable behavioral biases across diverse tasks: emotionally expressive\nagents excel in narrative generation, while analytically primed agents adopt\nmore stable strategies in game-theoretic settings. Our framework supports\nexperimenting with structured multi-agent communication protocols and reveals\nthat self-reflection prior to interaction improves cooperation and reasoning\nquality. To ensure trait persistence, we integrate the official 16Personalities\ntest for automated verification. While our focus is on MBTI, we show that our\napproach generalizes seamlessly to other psychological frameworks such as Big\nFive, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior\ndesign, we establish a foundation for psychologically enhanced AI agents\nwithout any fine-tuning.\n","authors":["Maciej Besta","Shriram Chandran","Robert Gerstenberger","Mathis Lindner","Marcin Chrapek","Sebastian Hermann Martschat","Taraneh Ghandi","Patrick Iff","Hubert Niewiadomski","Piotr Nyczyk","Jürgen Müller","Torsten Hoefler"],"pdf_url":"https://arxiv.org/pdf/2509.04343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00167v2","updated":"2025-09-04T15:59:28Z","published":"2025-08-29T18:07:11Z","title":"Pilot Study on Generative AI and Critical Thinking in Higher Education\n  Classrooms","summary":"  Generative AI (GAI) tools have seen rapid adoption in educational settings,\nyet their role in fostering critical thinking remains underexplored. While\nprevious studies have examined GAI as a tutor for specific lessons or as a tool\nfor completing assignments, few have addressed how students critically evaluate\nthe accuracy and appropriateness of GAI-generated responses. This pilot study\ninvestigates students' ability to apply structured critical thinking when\nassessing Generative AI outputs in introductory Computational and Data Science\ncourses. Given that GAI tools often produce contextually flawed or factually\nincorrect answers, we designed learning activities that require students to\nanalyze, critique, and revise AI-generated solutions. Our findings offer\ninitial insights into students' ability to engage critically with GAI content\nand lay the groundwork for more comprehensive studies in future semesters.\n","authors":["W. F. Lamberti","S. R. Lawrence","D. White","S. Kim","S. Abdullah"],"pdf_url":"https://arxiv.org/pdf/2509.00167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04338v1","updated":"2025-09-04T15:58:50Z","published":"2025-09-04T15:58:50Z","title":"From Editor to Dense Geometry Estimator","summary":"  Leveraging visual priors from pre-trained text-to-image (T2I) generative\nmodels has shown success in dense prediction. However, dense prediction is\ninherently an image-to-image task, suggesting that image editing models, rather\nthan T2I generative models, may be a more suitable foundation for fine-tuning.\n  Motivated by this, we conduct a systematic analysis of the fine-tuning\nbehaviors of both editors and generators for dense geometry estimation. Our\nfindings show that editing models possess inherent structural priors, which\nenable them to converge more stably by ``refining\" their innate features, and\nultimately achieve higher performance than their generative counterparts.\n  Based on these findings, we introduce \\textbf{FE2E}, a framework that\npioneeringly adapts an advanced editing model based on Diffusion Transformer\n(DiT) architecture for dense geometry prediction. Specifically, to tailor the\neditor for this deterministic task, we reformulate the editor's original flow\nmatching loss into the ``consistent velocity\" training objective. And we use\nlogarithmic quantization to resolve the precision conflict between the editor's\nnative BFloat16 format and the high precision demand of our tasks.\nAdditionally, we leverage the DiT's global attention for a cost-free joint\nestimation of depth and normals in a single forward pass, enabling their\nsupervisory signals to mutually enhance each other.\n  Without scaling up the training data, FE2E achieves impressive performance\nimprovements in zero-shot monocular depth and normal estimation across multiple\ndatasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset\nand outperforms the DepthAnything series, which is trained on 100$\\times$ data.\nThe project page can be accessed \\href{https://amap-ml.github.io/FE2E/}{here}.\n","authors":["JiYuan Wang","Chunyu Lin","Lei Sun","Rongying Liu","Lang Nie","Mingxing Li","Kang Liao","Xiangxiang Chu","Yao Zhao"],"pdf_url":"https://arxiv.org/pdf/2509.04338v1.pdf","comment":"20pages"},{"id":"http://arxiv.org/abs/2508.14723v2","updated":"2025-09-04T15:58:17Z","published":"2025-08-20T14:05:18Z","title":"Transplant Then Regenerate: A New Paradigm for Text Data Augmentation","summary":"  Data augmentation is a critical technique in deep learning. Traditional\nmethods like Back-translation typically focus on lexical-level rephrasing,\nwhich primarily produces variations with the same semantics. While large\nlanguage models (LLMs) have enhanced text augmentation by their \"knowledge\nemergence\" capability, controlling the style and structure of these outputs\nremains challenging and requires meticulous prompt engineering. In this paper,\nwe propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.\nThe core idea of LMTransplant is transplant-then-regenerate: incorporating seed\ntext into a context expanded by LLM, and asking the LLM to regenerate a variant\nbased on the expanded context. This strategy allows the model to create more\ndiverse and creative content-level variants by fully leveraging the knowledge\nembedded in LLMs, while preserving the core attributes of the original text. We\nevaluate LMTransplant across various text-related tasks, demonstrating its\nsuperior performance over existing text augmentation methods. Moreover,\nLMTransplant demonstrates exceptional scalability as the size of augmented data\ngrows.\n","authors":["Guangzhan Wang","Hongyu Zhang","Beijun Shen","Xiaodong Gu"],"pdf_url":"https://arxiv.org/pdf/2508.14723v2.pdf","comment":"Accepted by EMNLP 2025"},{"id":"http://arxiv.org/abs/2509.04337v1","updated":"2025-09-04T15:56:40Z","published":"2025-09-04T15:56:40Z","title":"Decoupled Entity Representation Learning for Pinterest Ads Ranking","summary":"  In this paper, we introduce a novel framework following an\nupstream-downstream paradigm to construct user and item (Pin) embeddings from\ndiverse data sources, which are essential for Pinterest to deliver personalized\nPins and ads effectively. Our upstream models are trained on extensive data\nsources featuring varied signals, utilizing complex architectures to capture\nintricate relationships between users and Pins on Pinterest. To ensure\nscalability of the upstream models, entity embeddings are learned, and\nregularly refreshed, rather than real-time computation, allowing for\nasynchronous interaction between the upstream and downstream models. These\nembeddings are then integrated as input features in numerous downstream tasks,\nincluding ad retrieval and ranking models for CTR and CVR predictions. We\ndemonstrate that our framework achieves notable performance improvements in\nboth offline and online settings across various downstream tasks. This\nframework has been deployed in Pinterest's production ad ranking systems,\nresulting in significant gains in online metrics.\n","authors":["Jie Liu","Yinrui Li","Jiankai Sun","Kungang Li","Han Sun","Sihan Wang","Huasen Wu","Siyuan Gao","Paulo Soares","Nan Li","Zhifang Liu","Haoyang Li","Siping Ji","Ling Leng","Prathibha Deshikachar"],"pdf_url":"https://arxiv.org/pdf/2509.04337v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.12143v3","updated":"2025-09-04T15:52:13Z","published":"2022-11-22T10:18:09Z","title":"Autonomation, Not Automation: Activities and Needs of European\n  Fact-checkers as a Basis for Designing Human-Centered AI Systems","summary":"  To mitigate the negative effects of false information more effectively, the\ndevelopment of Artificial Intelligence (AI) systems to assist fact-checkers is\nneeded. Nevertheless, the lack of focus on the needs of these stakeholders\nresults in their limited acceptance and skepticism toward automating the whole\nfact-checking process. In this study, we conducted semi-structured in-depth\ninterviews with Central European fact-checkers. Their activities and problems\nwere analyzed using iterative content analysis. The most significant problems\nwere validated with a survey of European fact-checkers, in which we collected\n24 responses from 20 countries, i.e., 62% of active European signatories of the\nInternational Fact-Checking Network (IFCN). Our contributions include an\nin-depth examination of the variability of fact-checking work in\nnon-English-speaking regions, which still remained largely uncovered. By\naligning them with the knowledge from prior studies, we created conceptual\nmodels that help to understand the fact-checking processes. In addition, we\nmapped our findings on the fact-checkers' activities and needs to the relevant\ntasks for AI research, while providing a discussion on three AI tasks that were\nnot covered by previous similar studies. The new opportunities identified for\nAI researchers and developers have implications for the focus of AI research in\nthis domain.\n","authors":["Andrea Hrckova","Robert Moro","Ivan Srba","Jakub Simko","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2211.12143v3.pdf","comment":"44 pages, 13 figures, 2 annexes. Accepted to ACM Journal on\n  Responsible Computing"},{"id":"http://arxiv.org/abs/2509.04317v1","updated":"2025-09-04T15:38:37Z","published":"2025-09-04T15:38:37Z","title":"Improving Robustness of AlphaZero Algorithms to Test-Time Environment\n  Changes","summary":"  The AlphaZero framework provides a standard way of combining Monte Carlo\nplanning with prior knowledge provided by a previously trained policy-value\nneural network. AlphaZero usually assumes that the environment on which the\nneural network was trained will not change at test time, which constrains its\napplicability. In this paper, we analyze the problem of deploying AlphaZero\nagents in potentially changed test environments and demonstrate how the\ncombination of simple modifications to the standard framework can significantly\nboost performance, even in settings with a low planning budget available. The\ncode is publicly available on GitHub.\n","authors":["Isidoro Tamassia","Wendelin Böhmer"],"pdf_url":"https://arxiv.org/pdf/2509.04317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02761v2","updated":"2025-09-04T15:30:53Z","published":"2025-09-02T19:06:56Z","title":"Plan Verification for LLM-Based Embodied Task Completion Agents","summary":"  Large language model (LLM) based task plans and corresponding human\ndemonstrations for embodied AI may be noisy, with unnecessary actions,\nredundant navigation, and logical errors that reduce policy quality. We propose\nan iterative verification framework in which a Judge LLM critiques action\nsequences and a Planner LLM applies the revisions, yielding progressively\ncleaner and more spatially coherent trajectories. Unlike rule-based approaches,\nour method relies on natural language prompting, enabling broad generalization\nacross error types including irrelevant actions, contradictions, and missing\nsteps. On a set of manually annotated actions from the TEACh embodied AI\ndataset, our framework achieves up to 90% recall and 100% precision across four\nstate-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).\nThe refinement loop converges quickly, with 96.5% of sequences requiring at\nmost three iterations, while improving both temporal efficiency and spatial\naction organization. Crucially, the method preserves human error-recovery\npatterns rather than collapsing them, supporting future work on robust\ncorrective behavior. By establishing plan verification as a reliable LLM\ncapability for spatial planning and action refinement, we provide a scalable\npath to higher-quality training data for imitation learning in embodied AI.\n","authors":["Ananth Hariharan","Vardhan Dongre","Dilek Hakkani-Tür","Gokhan Tur"],"pdf_url":"https://arxiv.org/pdf/2509.02761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04310v1","updated":"2025-09-04T15:23:58Z","published":"2025-09-04T15:23:58Z","title":"EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn\n  Negotiation","summary":"  Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models\n(LLMs) has demonstrated that agents can engage in \\textit{complex},\n\\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,\nexisting LLM agents largely overlook the functional role of emotions in such\nnegotiations, instead generating passive, preference-driven emotional responses\nthat make them vulnerable to manipulation and strategic exploitation by\nadversarial counterparts. To address this gap, we present EvoEmo, an\nevolutionary reinforcement learning framework that optimizes dynamic emotional\nexpression in negotiations. EvoEmo models emotional state transitions as a\nMarkov Decision Process and employs population-based genetic optimization to\nevolve high-reward emotion policies across diverse negotiation scenarios. We\nfurther propose an evaluation framework with two baselines -- vanilla\nstrategies and fixed-emotion strategies -- for benchmarking emotion-aware\nnegotiation. Extensive experiments and ablation studies show that EvoEmo\nconsistently outperforms both baselines, achieving higher success rates, higher\nefficiency, and increased buyer savings. This findings highlight the importance\nof adaptive emotional expression in enabling more effective LLM agents for\nmulti-turn negotiation.\n","authors":["Yunbo Long","Liming Xu","Lukas Beckenbauer","Yuhan Liu","Alexandra Brintrup"],"pdf_url":"https://arxiv.org/pdf/2509.04310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.13755v2","updated":"2025-09-04T15:21:27Z","published":"2025-08-19T11:51:40Z","title":"Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with\n  Adaptive Exploration","summary":"  Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a\npowerful paradigm for unlocking reasoning capabilities in large language\nmodels, yet its full potential is hindered by two under-explored dimensions:\nDepth-the hardest problem a model can sample; Breadth-the number of instances\nconsumed in a single iteration. We dissect the popular GRPO algorithm and\nreveal a systematic bias: the cumulative-advantage disproportionately weights\nsamples with medium accuracy, while down-weighting the low-accuracy instances\nthat are crucial for pushing reasoning boundaries. To rectify the depth\nneglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which\nre-weights hard problems through targeted multi-stage rollouts, thereby\nincreasing the number of positive rollouts for hard problems. Empirically,\nnaively enlarging rollout size only accelerates convergence and even hurts\nPass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra\ninference cost at convergence. Just as we adaptively expanded the depth of\nexploration, we now ask whether aggressively scaling the breadth of training\ndata can further amplify reasoning gains. To this end, we intensely scale batch\nsize and replace PPO's mini-batch iterations with full-batch updates over\nmultiple epochs. Increasing breadth significantly enhances Pass@1 performance.\nLarge-breadth training sustains high token-level entropy, indicating continued\nexploration and reduced gradient noise. We further present DARS-B, which\naugments DARS with large breadth, and demonstrate simultaneous gains in Pass@K\nand Pass@1. The results confirm that breadth and adaptive exploration across\ndepth operate as orthogonal dimensions in RLVR, which are key to unleashing the\nreasoning power of RLVR.\n","authors":["Zhicheng Yang","Zhijiang Guo","Yinya Huang","Yongxin Wang","Dongchun Xie","Yiwei Wang","Xiaodan Liang","Jing Tang"],"pdf_url":"https://arxiv.org/pdf/2508.13755v2.pdf","comment":"16 pages, 14 figures"},{"id":"http://arxiv.org/abs/2509.04304v1","updated":"2025-09-04T15:17:50Z","published":"2025-09-04T15:17:50Z","title":"Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge\n  in Large Language Models","summary":"  The growing capabilities of Large Language Models (LLMs) show significant\npotential to enhance healthcare by assisting medical researchers and\nphysicians. However, their reliance on static training data is a major risk\nwhen medical recommendations evolve with new research and developments. When\nLLMs memorize outdated medical knowledge, they can provide harmful advice or\nfail at clinical reasoning tasks. To investigate this problem, we introduce two\nnovel question-answering (QA) datasets derived from systematic reviews:\nMedRevQA (16,501 QA pairs covering general biomedical knowledge) and\nMedChangeQA (a subset of 512 QA pairs where medical consensus has changed over\ntime). Our evaluation of eight prominent LLMs on the datasets reveals\nconsistent reliance on outdated knowledge across all models. We additionally\nanalyze the influence of obsolete pre-training data and training strategies to\nexplain this phenomenon and propose future directions for mitigation, laying\nthe groundwork for developing more current and reliable medical AI systems.\n","authors":["Juraj Vladika","Mahdi Dhaini","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2509.04304v1.pdf","comment":"Accepted to Findings of EMNLP 2025"},{"id":"http://arxiv.org/abs/2509.04303v1","updated":"2025-09-04T15:16:38Z","published":"2025-09-04T15:16:38Z","title":"HumAIne-Chatbot: Real-Time Personalized Conversational AI via\n  Reinforcement Learning","summary":"  Current conversational AI systems often provide generic, one-size-fits-all\ninteractions that overlook individual user characteristics and lack adaptive\ndialogue management. To address this gap, we introduce\n\\textbf{HumAIne-chatbot}, an AI-driven conversational agent that personalizes\nresponses through a novel user profiling framework. The system is pre-trained\non a diverse set of GPT-generated virtual personas to establish a broad prior\nover user types. During live interactions, an online reinforcement learning\nagent refines per-user models by combining implicit signals (e.g. typing speed,\nsentiment, engagement duration) with explicit feedback (e.g., likes and\ndislikes). This profile dynamically informs the chatbot dialogue policy,\nenabling real-time adaptation of both content and style. To evaluate the\nsystem, we performed controlled experiments with 50 synthetic personas in\nmultiple conversation domains. The results showed consistent improvements in\nuser satisfaction, personalization accuracy, and task achievement when\npersonalization features were enabled. Statistical analysis confirmed\nsignificant differences between personalized and nonpersonalized conditions,\nwith large effect sizes across key metrics. These findings highlight the\neffectiveness of AI-driven user profiling and provide a strong foundation for\nfuture real-world validation.\n","authors":["Georgios Makridis","Georgios Fragiadakis","Jorge Oliveira","Tomaz Saraiva","Philip Mavrepis","Georgios Fatouros","Dimosthenis Kyriazis"],"pdf_url":"https://arxiv.org/pdf/2509.04303v1.pdf","comment":"11 pages, 4 figures, IEEE conference format"},{"id":"http://arxiv.org/abs/2508.02548v2","updated":"2025-09-04T15:06:45Z","published":"2025-08-04T16:01:28Z","title":"The KG-ER Conceptual Schema Language","summary":"  We propose KG-ER, a conceptual schema language for knowledge graphs that\ndescribes the structure of knowledge graphs independently of their\nrepresentation (relational databases, property graphs, RDF) while helping to\ncapture the semantics of the information stored in a knowledge graph.\n","authors":["Enrico Franconi","Benoît Groz","Jan Hidders","Nina Pardal","Sławek Staworko","Jan Van den Bussche","Piotr Wieczorek"],"pdf_url":"https://arxiv.org/pdf/2508.02548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04288v1","updated":"2025-09-04T15:01:03Z","published":"2025-09-04T15:01:03Z","title":"Reinforcement Learning for Robust Ageing-Aware Control of Li-ion Battery\n  Systems with Data-Driven Formal Verification","summary":"  Rechargeable lithium-ion (Li-ion) batteries are a ubiquitous element of\nmodern technology. In the last decades, the production and design of such\nbatteries and their adjacent embedded charging and safety protocols, denoted by\nBattery Management Systems (BMS), has taken central stage. A fundamental\nchallenge to be addressed is the trade-off between the speed of charging and\nthe ageing behavior, resulting in the loss of capacity in the battery cell. We\nrely on a high-fidelity physics-based battery model and propose an approach to\ndata-driven charging and safety protocol design. Following a\nCounterexample-Guided Inductive Synthesis scheme, we combine Reinforcement\nLearning (RL) with recent developments in data-driven formal methods to obtain\na hybrid control strategy: RL is used to synthesise the individual controllers,\nand a data-driven abstraction guides their partitioning into a switched\nstructure, depending on the initial output measurements of the battery. The\nresulting discrete selection among RL-based controllers, coupled with the\ncontinuous battery dynamics, realises a hybrid system. When a design meets the\ndesired criteria, the abstraction provides probabilistic guarantees on the\nclosed-loop performance of the cell.\n","authors":["Rudi Coppola","Hovsep Touloujian","Pierfrancesco Ombrini","Manuel Mazo Jr"],"pdf_url":"https://arxiv.org/pdf/2509.04288v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10118v2","updated":"2025-09-04T15:00:25Z","published":"2025-02-14T12:33:19Z","title":"Image Embedding Sampling Method for Diverse Captioning","summary":"  Image Captioning for state-of-the-art VLMs has significantly improved over\ntime; however, this comes at the cost of increased computational complexity,\nmaking them less accessible for resource-constrained applications such as\nmobile devices and assistive technologies. Alternatively, comparably smaller\nVLMs prioritize high-level scene descriptions, overlooking finer details that\ncontribute to a richer understanding of an image. In this paper, we introduce a\ntraining-free framework that enhances caption diversity and informativeness by\nexplicitly attending to distinct image regions using a comparably small VLM,\nBLIP, as the backbone. Our approach leverages structured segmentation to\nproduce hierarchical representations that capture both global and localized\nsemantics. Without requiring additional model training, we demonstrate that our\nmethod allows smaller VLMs to achieve performance comparable to larger models\nin terms of image-caption alignment, semantic integrity, and diversity. We\nevaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets,\nachieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset,\nrespectively, while maintaining strong image-caption relevancy and semantic\nintegrity with the human-annotated captions.\n","authors":["Sania Waheed","Na Min An"],"pdf_url":"https://arxiv.org/pdf/2502.10118v2.pdf","comment":"17 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2502.14791v4","updated":"2025-09-04T14:58:09Z","published":"2025-02-20T18:11:38Z","title":"Rapid Word Learning Through Meta In-Context Learning","summary":"  Humans can quickly learn a new word from a few illustrative examples, and\nthen systematically and flexibly use it in novel contexts. Yet the abilities of\ncurrent language models for few-shot word learning, and methods for improving\nthese abilities, are underexplored. In this study, we introduce a novel method,\nMeta-training for IN-context learNing Of Words (Minnow). This method trains\nlanguage models to generate new examples of a word's usage given a few\nin-context examples, using a special placeholder token to represent the new\nword. This training is repeated on many new words to develop a general\nword-learning ability. We find that training models from scratch with Minnow on\nhuman-scale child-directed language enables strong few-shot word learning,\ncomparable to a large language model (LLM) pre-trained on orders of magnitude\nmore data. Furthermore, through discriminative and generative evaluations, we\ndemonstrate that finetuning pre-trained LLMs with Minnow improves their ability\nto discriminate between new words, identify syntactic categories of new words,\nand generate reasonable new usages and definitions for new words, based on one\nor a few in-context examples. These findings highlight the data efficiency of\nMinnow and its potential to improve language model performance in word learning\ntasks.\n","authors":["Wentao Wang","Guangyuan Jiang","Tal Linzen","Brenden M. Lake"],"pdf_url":"https://arxiv.org/pdf/2502.14791v4.pdf","comment":"EMNLP 2025"},{"id":"http://arxiv.org/abs/2312.03993v2","updated":"2025-09-04T14:45:07Z","published":"2023-12-07T02:21:31Z","title":"Style Transfer to Calvin and Hobbes comics using Stable Diffusion","summary":"  This project report summarizes our journey to perform stable diffusion\nfine-tuning on a dataset containing Calvin and Hobbes comics. The purpose is to\nconvert any given input image into the comic style of Calvin and Hobbes,\nessentially performing style transfer. We train stable-diffusion-v1.5 using Low\nRank Adaptation (LoRA) to efficiently speed up the fine-tuning process. The\ndiffusion itself is handled by a Variational Autoencoder (VAE), which is a\nU-net. Our results were visually appealing for the amount of training time and\nthe quality of input data that went into training.\n","authors":["Asvin Kumar Venkataramanan","Sloke Shrestha","Sundar Sripada Venugopalaswamy Sriraman"],"pdf_url":"https://arxiv.org/pdf/2312.03993v2.pdf","comment":"Updated authorship"},{"id":"http://arxiv.org/abs/2508.08193v2","updated":"2025-09-04T14:42:06Z","published":"2025-08-11T17:12:55Z","title":"Street-Level AI: Are Large Language Models Ready for Real-World\n  Judgments?","summary":"  A surge of recent work explores the ethical and societal implications of\nlarge-scale AI models that make \"moral\" judgments. Much of this literature\nfocuses either on alignment with human judgments through various thought\nexperiments or on the group fairness implications of AI judgments. However, the\nmost immediate and likely use of AI is to help or fully replace the so-called\nstreet-level bureaucrats, the individuals deciding to allocate scarce social\nresources or approve benefits. There is a rich history underlying how\nprinciples of local justice determine how society decides on prioritization\nmechanisms in such domains. In this paper, we examine how well LLM judgments\nalign with human judgments, as well as with socially and politically determined\nvulnerability scoring systems currently used in the domain of homelessness\nresource allocation. Crucially, we use real data on those needing services\n(maintaining strict confidentiality by only using local large models) to\nperform our analyses. We find that LLM prioritizations are extremely\ninconsistent in several ways: internally on different runs, between different\nLLMs, and between LLMs and the vulnerability scoring systems. At the same time,\nLLMs demonstrate qualitative consistency with lay human judgments in pairwise\ntesting. Findings call into question the readiness of current generation AI\nsystems for naive integration in high-stakes societal decision-making.\n","authors":["Gaurab Pokharel","Shafkat Farabi","Patrick J. Fowler","Sanmay Das"],"pdf_url":"https://arxiv.org/pdf/2508.08193v2.pdf","comment":"This work has been accepted for publication as a full paper at the\n  AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)"},{"id":"http://arxiv.org/abs/2405.17527v5","updated":"2025-09-04T14:38:34Z","published":"2024-05-27T15:34:35Z","title":"Unisolver: PDE-Conditional Transformers Towards Universal Neural PDE\n  Solvers","summary":"  Deep models have recently emerged as promising tools to solve partial\ndifferential equations (PDEs), known as neural PDE solvers. While neural\nsolvers trained from either simulation data or physics-informed loss can solve\nPDEs reasonably well, they are mainly restricted to a few instances of PDEs,\ne.g. a certain equation with a limited set of coefficients. This limits their\ngeneralization to diverse PDEs, preventing them from being practical surrogate\nmodels of numerical solvers. In this paper, we present Unisolver, a novel\nTransformer model trained on diverse data and conditioned on diverse PDEs,\naiming towards a universal neural PDE solver capable of solving a wide scope of\nPDEs. Instead of purely scaling up data and parameters, Unisolver stems from\nthe theoretical analysis of the PDE-solving process. Inspired by the\nmathematical structure of PDEs that a PDE solution is fundamentally governed by\na series of PDE components such as equation symbols and boundary conditions, we\ndefine a complete set of PDE components and flexibly embed them as domain-wise\nand point-wise deep conditions for Transformer PDE solvers. Integrating\nphysical insights with recent Transformer advances, Unisolver achieves\nconsistent state-of-the-art on three challenging large-scale benchmarks,\nshowing impressive performance and generalizability. Code is available at\nhttps://github.com/thuml/Unisolver.\n","authors":["Hang Zhou","Yuezhou Ma","Haixu Wu","Haowen Wang","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2405.17527v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04260v1","updated":"2025-09-04T14:38:28Z","published":"2025-09-04T14:38:28Z","title":"An Empirical Study of Vulnerabilities in Python Packages and Their\n  Detection","summary":"  In the rapidly evolving software development landscape, Python stands out for\nits simplicity, versatility, and extensive ecosystem. Python packages, as units\nof organization, reusability, and distribution, have become a pressing concern,\nhighlighted by the considerable number of vulnerability reports. As a scripting\nlanguage, Python often cooperates with other languages for performance or\ninteroperability. This adds complexity to the vulnerabilities inherent to\nPython packages, and the effectiveness of current vulnerability detection tools\nremains underexplored. This paper addresses these gaps by introducing PyVul,\nthe first comprehensive benchmark suite of Python-package vulnerabilities.\nPyVul includes 1,157 publicly reported, developer-verified vulnerabilities,\neach linked to its affected packages. To accommodate diverse detection\ntechniques, it provides annotations at both commit and function levels. An\nLLM-assisted data cleansing method is incorporated to improve label accuracy,\nachieving 100% commit-level and 94% function-level accuracy, establishing PyVul\nas the most precise large-scale Python vulnerability benchmark. We further\ncarry out a distribution analysis of PyVul, which demonstrates that\nvulnerabilities in Python packages involve multiple programming languages and\nexhibit a wide variety of types. Moreover, our analysis reveals that\nmulti-lingual Python packages are potentially more susceptible to\nvulnerabilities. Evaluation of state-of-the-art detectors using this benchmark\nreveals a significant discrepancy between the capabilities of existing tools\nand the demands of effectively identifying real-world security issues in Python\npackages. Additionally, we conduct an empirical review of the top-ranked CWEs\nobserved in Python packages, to diagnose the fine-grained limitations of\ncurrent detection tools and highlight the necessity for future advancements in\nthe field.\n","authors":["Haowei Quan","Junjie Wang","Xinzhe Li","Terry Yue Zhuo","Xiao Chen","Xiaoning Du"],"pdf_url":"https://arxiv.org/pdf/2509.04260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02349v2","updated":"2025-09-04T14:25:57Z","published":"2025-09-02T14:15:22Z","title":"AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation","summary":"  Multimodal Large Language Models (MLLMs) have been widely applied in speech\nand music. This tendency has led to a focus on audio tokenization for Large\nModels (LMs). Unlike semantic-only text tokens, audio tokens must both capture\nglobal semantic content and preserve fine-grained acoustic details. Moreover,\nthey provide a discrete method for speech and music that can be effectively\nintegrated into MLLMs. However, existing research is unsuitable in the\ndefinitions of semantic tokens and acoustic tokens. In addition, the evaluation\nof different codecs typically concentrates on specific domains or tasks, such\nas reconstruction or Automatic Speech Recognition (ASR) task, which prevents\nfair and comprehensive comparisons. To address these problems, this paper\nprovides suitable definitions for semantic and acoustic tokens and introduces a\nsystematic evaluation framework. This framework allows for a comprehensive\nassessment of codecs' capabilities which evaluate across four dimensions: audio\nreconstruction metric, codebook index (ID) stability, decoder-only transformer\nperplexity, and performance on downstream probe tasks. Our results show the\ncorrectness of the provided suitable definitions and the correlation among\nreconstruction metrics, codebook ID stability, downstream probe tasks and\nperplexity.\n","authors":["Lu Wang","Hao Chen","Siyu Wu","Zhiyue Wu","Hao Zhou","Chengfeng Zhang","Ting Wang","Haodi Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.02349v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13958v2","updated":"2025-09-04T14:24:19Z","published":"2025-01-21T06:25:21Z","title":"A Survey of Graph Retrieval-Augmented Generation for Customized Large\n  Language Models","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities in a\nwide range of tasks, yet their application to specialized domains remains\nchallenging due to the need for deep expertise. Retrieval-Augmented generation\n(RAG) has emerged as a promising solution to customize LLMs for professional\nfields by seamlessly integrating external knowledge bases, enabling real-time\naccess to domain-specific expertise during inference. Despite its potential,\ntraditional RAG systems, based on flat text retrieval, face three critical\nchallenges: (i) complex query understanding in professional contexts, (ii)\ndifficulties in knowledge integration across distributed sources, and (iii)\nsystem efficiency bottlenecks at scale. This survey presents a systematic\nanalysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new\nparadigm that revolutionizes domain-specific LLM applications. GraphRAG\naddresses traditional RAG limitations through three key innovations: (i)\ngraph-structured knowledge representation that explicitly captures entity\nrelationships and domain hierarchies, (ii) efficient graph-based retrieval\ntechniques that enable context-preserving knowledge retrieval with multihop\nreasoning ability, and (iii) structure-aware knowledge integration algorithms\nthat leverage retrieved knowledge for accurate and logical coherent generation\nof LLMs. In this survey, we systematically analyze the technical foundations of\nGraphRAG and examine current implementations across various professional\ndomains, identifying key technical challenges and promising research\ndirections. All the related resources of GraphRAG, including research papers,\nopen-source data, and projects, are collected for the community in\nhttps://github.com/DEEP-PolyU/Awesome-GraphRAG.\n","authors":["Qinggang Zhang","Shengyuan Chen","Yuanchen Bei","Zheng Yuan","Huachi Zhou","Zijin Hong","Hao Chen","Yilin Xiao","Chuang Zhou","Yi Chang","Xiao Huang"],"pdf_url":"https://arxiv.org/pdf/2501.13958v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04250v1","updated":"2025-09-04T14:23:35Z","published":"2025-09-04T14:23:35Z","title":"How many patients could we save with LLM priors?","summary":"  Imagine a world where clinical trials need far fewer patients to achieve the\nsame statistical power, thanks to the knowledge encoded in large language\nmodels (LLMs). We present a novel framework for hierarchical Bayesian modeling\nof adverse events in multi-center clinical trials, leveraging LLM-informed\nprior distributions. Unlike data augmentation approaches that generate\nsynthetic data points, our methodology directly obtains parametric priors from\nthe model. Our approach systematically elicits informative priors for\nhyperparameters in hierarchical Bayesian models using a pre-trained LLM,\nenabling the incorporation of external clinical expertise directly into\nBayesian safety modeling. Through comprehensive temperature sensitivity\nanalysis and rigorous cross-validation on real-world clinical trial data, we\ndemonstrate that LLM-derived priors consistently improve predictive performance\ncompared to traditional meta-analytical approaches. This methodology paves the\nway for more efficient and expert-informed clinical trial design, enabling\nsubstantial reductions in the number of patients required to achieve robust\nsafety assessment and with the potential to transform drug safety monitoring\nand regulatory decision making.\n","authors":["Shota Arai","David Selby","Andrew Vargo","Sebastian Vollmer"],"pdf_url":"https://arxiv.org/pdf/2509.04250v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2509.04243v1","updated":"2025-09-04T14:17:01Z","published":"2025-09-04T14:17:01Z","title":"Learning Active Perception via Self-Evolving Preference Optimization for\n  GUI Grounding","summary":"  Vision Language Models (VLMs) have recently achieved significant progress in\nbridging visual perception and linguistic reasoning. Recently, OpenAI o3 model\nintroduced a zoom-in search strategy that effectively elicits active perception\ncapabilities in VLMs, improving downstream task performance. However, enabling\nVLMs to reason effectively over appropriate image regions remains a core\nchallenge in GUI grounding, particularly under high-resolution inputs and\ncomplex multi-element visual interactions. In this work, we propose LASER, a\nself-evolving framework that progressively endows VLMs with multi-step\nperception capabilities, enabling precise coordinate prediction. Specifically,\nour approach integrate Monte Carlo quality estimation with\nIntersection-over-Union (IoU)-based region quality evaluation to jointly\nencourage both accuracy and diversity in constructing high-quality preference\ndata. This combination explicitly guides the model to focus on\ninstruction-relevant key regions while adaptively allocating reasoning steps\nbased on task complexity. Comprehensive experiments on the ScreenSpot Pro and\nScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating\nthe effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER\nachieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new\nstate-of-the-art (SoTA) among 7B-scale models.\n","authors":["Wanfu Wang","Qipeng Huang","Guangquan Xue","Xiaobo Liang","Juntao Li"],"pdf_url":"https://arxiv.org/pdf/2509.04243v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02807v2","updated":"2025-09-04T14:13:49Z","published":"2024-09-19T13:26:31Z","title":"AutoPETIII: The Tracer Frontier. What Frontier?","summary":"  For the last three years, the AutoPET competition gathered the medical\nimaging community around a hot topic: lesion segmentation on Positron Emitting\nTomography (PET) scans. Each year a different aspect of the problem is\npresented; in 2024 the multiplicity of existing and used tracers was at the\ncore of the challenge. Specifically, this year's edition aims to develop a\nfully automatic algorithm capable of performing lesion segmentation on a PET/CT\nscan, without knowing the tracer, which can either be a FDG or PSMA-based\ntracer. In this paper we describe how we used the nnUNetv2 framework to train\ntwo sets of 6 fold ensembles of models to perform fully automatic PET/CT lesion\nsegmentation as well as a MIP-CNN to choose which set of models to use for\nsegmentation.\n","authors":["Zacharia Mesbah","Léo Mottay","Romain Modzelewski","Pierre Decazes","Sébastien Hapdey","Su Ruan","Sébastien Thureau"],"pdf_url":"https://arxiv.org/pdf/2410.02807v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04239v1","updated":"2025-09-04T14:13:42Z","published":"2025-09-04T14:13:42Z","title":"Evaluating Quality of Gaming Narratives Co-created with AI","summary":"  This paper proposes a structured methodology to evaluate AI-generated game\nnarratives, leveraging the Delphi study structure with a panel of narrative\ndesign experts. Our approach synthesizes story quality dimensions from\nliterature and expert insights, mapping them into the Kano model framework to\nunderstand their impact on player satisfaction. The results can inform game\ndevelopers on prioritizing quality aspects when co-creating game narratives\nwith generative AI.\n","authors":["Arturo Valdivia","Paolo Burelli"],"pdf_url":"https://arxiv.org/pdf/2509.04239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.11701v2","updated":"2025-09-04T14:12:36Z","published":"2025-05-16T21:09:36Z","title":"DMN-Guided Prompting: A Framework for Controlling LLM Behavior","summary":"  Large Language Models (LLMs) have shown considerable potential in automating\ndecision logic within knowledge-intensive processes. However, their\neffectiveness largely depends on the strategy and quality of prompting. Since\ndecision logic is typically embedded in prompts, it becomes challenging for end\nusers to modify or refine it. Decision Model and Notation (DMN) offers a\nstandardized graphical approach for defining decision logic in a structured,\nuser-friendly manner. This paper introduces a DMN-guided prompting framework\nthat breaks down complex decision logic into smaller, manageable components,\nguiding LLMs through structured decision pathways. We implemented the framework\nin a graduate-level course where students submitted assignments. The\nassignments and DMN models representing feedback instructions served as inputs\nto our framework. The instructor evaluated the generated feedback and labeled\nit for performance assessment. Our approach demonstrated promising results,\noutperforming chain-of-thought (CoT) prompting in our case study. Students also\nresponded positively to the generated feedback, reporting high levels of\nperceived usefulness in a survey based on the Technology Acceptance Model.\n","authors":["Shaghayegh Abedi","Amin Jalali"],"pdf_url":"https://arxiv.org/pdf/2505.11701v2.pdf","comment":"Large Language Models, Decision Model and Notation, Automated\n  Feedback, Prompt Engineering"},{"id":"http://arxiv.org/abs/2407.15161v4","updated":"2025-09-04T14:07:56Z","published":"2024-07-21T13:33:08Z","title":"FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via\n  Flow Variational Inference","summary":"  Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from\npartial observations remains a critical challenge in robot learning. Prior\ngenerative methods struggle to model the intricate grasp distribution of\ndexterous hands and often fail to reason about shape uncertainty inherent in\npartial point clouds, leading to unreliable or overly conservative grasps. We\npropose FFHFlow, a flow-based variational framework that generates diverse,\nrobust multi-finger grasps while explicitly quantifying perceptual uncertainty\nin the partial point clouds. Our approach leverages a normalizing flow-based\ndeep latent variable model to learn a hierarchical grasp manifold, overcoming\nthe mode collapse and rigid prior limitations of conditional Variational\nAutoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of\nflows, FFHFlow introspects shape uncertainty in partial observations and\nidentifies novel object structures, enabling risk-aware grasp synthesis. To\nfurther enhance reliability, we integrate a discriminative grasp evaluator with\nthe flow likelihoods, formulating an uncertainty-aware ranking strategy that\nprioritizes grasps robust to shape ambiguity. Extensive experiments in\nsimulation and real-world setups demonstrate that FFHFlow outperforms\nstate-of-the-art baselines (including diffusion models) in grasp diversity and\nsuccess rate, while achieving run-time efficient sampling. We also showcase its\npractical value in cluttered and confined environments, where diversity-driven\nsampling excels by mitigating collisions (Project Page:\nhttps://sites.google.com/view/ffhflow/home/).\n","authors":["Qian Feng","Jianxiang Feng","Zhaopeng Chen","Rudolph Triebel","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2407.15161v4.pdf","comment":"First two authors contributed equally, whose ordering decided via\n  coin-tossing. Accepted for CoRL 2025"},{"id":"http://arxiv.org/abs/2411.03562v2","updated":"2025-09-04T14:07:26Z","published":"2024-11-05T23:55:23Z","title":"Kolb-Based Experiential Learning for Generalist Agents with Human-Level\n  Kaggle Data Science Performance","summary":"  Human expertise emerges through iterative cycles of interaction, reflection,\nand internal model updating, which are central to cognitive theories such as\nKolb's experiential learning and Vygotsky's zone of proximal development. In\ncontrast, current AI systems, particularly LLM agents, rely on static\npre-training or rigid workflows, lacking mechanisms for continual adaptation.\nRecent studies identified early cognitive traits in LLM agents (reflection,\nrevision, and self-correction) suggesting foundational elements of human-like\nexperiential learning. Thus the key question: Can we design LLM agents capable\nof structured, cognitively grounded learning similar to human processes? In\nresponse, we propose a computational framework of Kolb's learning cycle with\nVygotsky's ZPD for autonomous agents. Our architecture separates extrinsic\n(environment interaction) and intrinsic (internal reflection/abstraction)\nfunctions, enabling cognitively grounded scaffolded learning, where the agent\ninitially learns within structured environments, followed by open-ended\ngeneralisation. This approach empowers agents to master complex tasks ; domains\nthat traditional fine-tuning or simple reflective methods could not tackle\neffectively. Its potential is powerfully demonstrated via direct comparison\nwith humans in real-world Kaggle data science competitions. Learning fully\nautomated data science code generation across 81 tasks, our system, Agent K,\ndemonstrated the ability to perform the entire workflow autonomously, achieving\nan Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2%\namong 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals\nlevel performance - including 4 gold and 4 silver on prize-awarding\ncompetitions - Agent K is the 1st AI system to successfully integrate Kolb- and\nVygotsky-inspired human cognitive learning, marking a major step toward\ngeneralist AI.\n","authors":["Antoine Grosnit","Alexandre Maraval","Refinath S N","Zichao Zhao","James Dora","Giuseppe Paolo","Albert Thomas","Jonas Gonzalez","Abhineet Kumar","Khyati Khandelwal","Abdelhakim Benechehab","Hamza Cherkaoui","Youssef Attia El-Hili","Kun Shao","Jianye Hao","Jun Yao","Balázs Kégl","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03562v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.08524v3","updated":"2025-09-04T13:56:50Z","published":"2025-08-11T23:30:39Z","title":"StreetViewAI: Making Street View Accessible Using Context-Aware\n  Multimodal AI","summary":"  Interactive streetscape mapping tools such as Google Street View (GSV) and\nMeta Mapillary enable users to virtually navigate and experience real-world\nenvironments via immersive 360{\\deg} imagery but remain fundamentally\ninaccessible to blind users. We introduce StreetViewAI, the first-ever\naccessible street view tool, which combines context-aware, multimodal AI,\naccessible navigation controls, and conversational speech. With StreetViewAI,\nblind users can virtually examine destinations, engage in open-world\nexploration, or virtually tour any of the over 220 billion images and 100+\ncountries where GSV is deployed. We iteratively designed StreetViewAI with a\nmixed-visual ability team and performed an evaluation with eleven blind users.\nOur findings demonstrate the value of an accessible street view in supporting\nPOI investigations and remote route planning. We close by enumerating key\nguidelines for future work.\n","authors":["Jon E. Froehlich","Alexander Fiannaca","Nimer Jaber","Victor Tsaran","Shaun Kane"],"pdf_url":"https://arxiv.org/pdf/2508.08524v3.pdf","comment":"Accepted to UIST'25 v2. Fixed a missing word in the PDF v3. Fixed a\n  typo in an author's name"},{"id":"http://arxiv.org/abs/2508.00401v2","updated":"2025-09-04T13:51:30Z","published":"2025-08-01T08:02:35Z","title":"Theory of Mind Using Active Inference: A Framework for Multi-Agent\n  Cooperation","summary":"  Theory of Mind (ToM) -- the ability to understand that others can have\ndiffering knowledge and goals -- enables agents to reason about others' beliefs\nwhile planning their own actions. We present a novel approach to multi-agent\ncooperation by implementing ToM within active inference. Unlike previous active\ninference approaches to multi-agent cooperation, our method neither relies on\ntask-specific shared generative models nor requires explicit communication. In\nour framework, ToM-equipped agents maintain distinct representations of their\nown and others' beliefs and goals. ToM agents then use an extended and adapted\nversion of the sophisticated inference tree-based planning algorithm to\nsystematically explore joint policy spaces through recursive reasoning. We\nevaluate our approach through collision avoidance and foraging simulations.\nResults suggest that ToM agents cooperate better compared to non-ToM\ncounterparts by being able to avoid collisions and reduce redundant efforts.\nCrucially, ToM agents accomplish this by inferring others' beliefs solely from\nobservable behaviour and considering them when planning their own actions. Our\napproach shows potential for generalisable and scalable multi-agent systems\nwhile providing computational insights into ToM mechanisms.\n","authors":["Riddhi J. Pitliya","Ozan Çatal","Toon Van de Maele","Corrado Pezzato","Tim Verbelen"],"pdf_url":"https://arxiv.org/pdf/2508.00401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.14904v2","updated":"2025-09-04T13:42:44Z","published":"2025-07-20T10:28:06Z","title":"TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D\n  Visual Grounding based on CLIP","summary":"  3D visual grounding allows an embodied agent to understand visual information\nin real-world 3D environments based on human instructions, which is crucial for\nembodied intelligence. Existing 3D visual grounding methods typically rely on\nseparate encoders for different modalities (e.g., RGB images, text, and 3D\npoint clouds), resulting in large and complex models that are inefficient to\ntrain. While some approaches use pre-trained 2D multi-modal models like CLIP\nfor 3D tasks, they still struggle with aligning point cloud data to 2D\nencoders. As a result, these methods continue to depend on 3D encoders for\nfeature extraction, further increasing model complexity and training\ninefficiency. In this paper, we propose a unified 2D pre-trained multi-modal\nnetwork to process all three modalities (RGB images, text, and point clouds),\nsignificantly simplifying the architecture. By leveraging a 2D CLIP bi-modal\nmodel with adapter-based fine-tuning, this framework effectively adapts to the\ntri-modal setting, improving both adaptability and performance across\nmodalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module\nis designed to fuse geometric multi-scale features from point clouds and\nimages. We then integrate textual features for final modality fusion and\nintroduce a multi-modal decoder to facilitate deep cross-modal understanding.\nTogether, our method achieves unified feature extraction and fusion across the\nthree modalities, enabling an end-to-end 3D visual grounding model. Compared to\nthe baseline, our method reduces the number of trainable parameters by\napproximately 58\\%, while achieving a 6.52\\% improvement in the 3D detection\ntask and a 6.25\\% improvement in the 3D visual grounding task.\n","authors":["Fan Li","Zanyi Wang","Zeyi Huang","Guang Dai","Jingdong Wang","Mengmeng Wang"],"pdf_url":"https://arxiv.org/pdf/2507.14904v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.22627v2","updated":"2025-09-04T13:26:56Z","published":"2025-07-30T12:48:29Z","title":"LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text\n  Pairing","summary":"  Fashion design is a complex creative process that blends visual and textual\nexpressions. Designers convey ideas through sketches, which define spatial\nstructure and design elements, and textual descriptions, capturing material,\ntexture, and stylistic details. In this paper, we present LOcalized Text and\nSketch for fashion image generation (LOTS), an approach for compositional\nsketch-text based generation of complete fashion outlooks. LOTS leverages a\nglobal description with paired localized sketch + text information for\nconditioning and introduces a novel step-based merging strategy for diffusion\nadaptation. First, a Modularized Pair-Centric representation encodes sketches\nand text into a shared latent space while preserving independent localized\nfeatures; then, a Diffusion Pair Guidance phase integrates both local and\nglobal conditioning via attention-based guidance within the diffusion model's\nmulti-step denoising process. To validate our method, we build on Fashionpedia\nto release Sketchy, the first fashion dataset where multiple text-sketch pairs\nare provided per image. Quantitative results show LOTS achieves\nstate-of-the-art image generation performance on both global and localized\nmetrics, while qualitative examples and a human evaluation study highlight its\nunprecedented level of design customization.\n","authors":["Federico Girella","Davide Talon","Ziyue Liu","Zanxi Ruan","Yiming Wang","Marco Cristani"],"pdf_url":"https://arxiv.org/pdf/2507.22627v2.pdf","comment":"Accepted at ICCV25 (Oral). Project page:\n  https://intelligolabs.github.io/lots/"},{"id":"http://arxiv.org/abs/2509.04192v1","updated":"2025-09-04T13:15:02Z","published":"2025-09-04T13:15:02Z","title":"Domain size asymptotics for Markov logic networks","summary":"  A Markov logic network (MLN) determines a probability distribution on the set\nof structures, or ``possible worlds'', with an arbitrary finite domain. We\nstudy the properties of such distributions as the domain size tends to\ninfinity. Three types of concrete examples of MLNs will be considered, and the\nproperties of random structures with domain sizes tending to infinity will be\nstudied: (1) Arbitrary quantifier-free MLNs over a language with only one\nrelation symbol which has arity 1. In this case we give a pretty complete\ncharacterization of the possible limit behaviours of random structures. (2) An\nMLN that favours graphs with fewer triangles (or more generally, fewer\nk-cliques). As a corollary of the analysis a ``$\\delta$-approximate 0-1 law''\nfor first-order logic is obtained. (3) An MLN that favours graphs with fewer\nvertices with degree higher than a fixed (but arbitrary) number. The analysis\nshows that depending on which ``soft constraints'' an MLN uses the limit\nbehaviour of random structures can be quite different, and the weights of the\nsoft constraints may, or may not, have influence on the limit behaviour. It\nwill also be demonstrated, using (1), that quantifier-free MLNs and lifted\nBayesian networks (in a broad sense) are asymptotically incomparable, roughly\nmeaning that there is a sequence of distributions on possible worlds with\nincreasing domain sizes that can be defined by one of the formalisms but not\neven approximated by the other. In a rather general context it is also shown\nthat on large domains the distribution determined by an MLN concentrates almost\nall its probability mass on a totally different part of the space of possible\nworlds than the uniform distribution does.\n","authors":["Vera Koponen"],"pdf_url":"https://arxiv.org/pdf/2509.04192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04183v1","updated":"2025-09-04T12:59:24Z","published":"2025-09-04T12:59:24Z","title":"MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn\n  Mental Health Counseling Sessions","summary":"  The growing demand for scalable psychological counseling highlights the need\nfor fine-tuning open-source Large Language Models (LLMs) with high-quality,\nprivacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,\na novel multi-agent framework for synthetic psychological counseling session\ngeneration that decomposes counselor response generation into coordinated\nsub-tasks handled by specialized LLM agents, each modeling a key psychological\ntechnique. Unlike prior single-agent approaches, MAGneT better captures the\nstructure and nuance of real counseling. In addition, we address\ninconsistencies in prior evaluation protocols by proposing a unified evaluation\nframework integrating diverse automatic and expert metrics. Furthermore, we\nexpand the expert evaluations from four aspects of counseling in previous works\nto nine aspects, enabling a more thorough and robust assessment of data\nquality. Empirical results show that MAGneT significantly outperforms existing\nmethods in quality, diversity, and therapeutic alignment of the generated\ncounseling sessions, improving general counseling skills by 3.2% and\nCBT-specific skills by 4.3% on average on cognitive therapy rating scale\n(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases\non average across all aspects. Moreover, fine-tuning an open-source model on\nMAGneT-generated sessions shows better performance, with improvements of 6.3%\non general counseling skills and 7.3% on CBT-specific skills on average on CTRS\nover those fine-tuned with sessions generated by baseline methods. We also make\nour code and data public.\n","authors":["Aishik Mandal","Tanmoy Chakraborty","Iryna Gurevych"],"pdf_url":"https://arxiv.org/pdf/2509.04183v1.pdf","comment":"25 pages, 29 figures"},{"id":"http://arxiv.org/abs/2509.04180v1","updated":"2025-09-04T12:54:32Z","published":"2025-09-04T12:54:32Z","title":"VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer\n  Vision","summary":"  AI models rely on annotated data to learn pattern and perform prediction.\nAnnotation is usually a labor-intensive step that require associating labels\nranging from a simple classification label to more complex tasks such as object\ndetection, oriented bounding box estimation, and instance segmentation.\nTraditional tools often require extensive manual input, limiting scalability\nfor large datasets. To address this, we introduce VisioFirm, an open-source web\napplication designed to streamline image labeling through AI-assisted\nautomation. VisioFirm integrates state-of-the-art foundation models into an\ninterface with a filtering pipeline to reduce human-in-the-loop efforts. This\nhybrid approach employs CLIP combined with pre-trained detectors like\nUltralytics models for common classes and zero-shot models such as Grounding\nDINO for custom labels, generating initial annotations with low-confidence\nthresholding to maximize recall. Through this framework, when tested on\nCOCO-type of classes, initial prediction have been proven to be mostly correct\nthough the users can refine these via interactive tools supporting bounding\nboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has\non-the-fly segmentation powered by Segment Anything accelerated through WebGPU\nfor browser-side efficiency. The tool supports multiple export formats (YOLO,\nCOCO, Pascal VOC, CSV) and operates offline after model caching, enhancing\naccessibility. VisioFirm demonstrates up to 90\\% reduction in manual effort\nthrough benchmarks on diverse datasets, while maintaining high annotation\naccuracy via clustering of connected CLIP-based disambiguate components and\nIoU-graph for redundant detection suppression. VisioFirm can be accessed from\n\\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.\n","authors":["Safouane El Ghazouali","Umberto Michelucci"],"pdf_url":"https://arxiv.org/pdf/2509.04180v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01700v2","updated":"2025-09-04T12:50:20Z","published":"2025-08-03T10:04:17Z","title":"DeepVIS: Bridging Natural Language and Data Visualization Through\n  Step-wise Reasoning","summary":"  Although data visualization is powerful for revealing patterns and\ncommunicating insights, creating effective visualizations requires familiarity\nwith authoring tools and often disrupts the analysis flow. While large language\nmodels show promise for automatically converting analysis intent into\nvisualizations, existing methods function as black boxes without transparent\nreasoning processes, which prevents users from understanding design rationales\nand refining suboptimal outputs. To bridge this gap, we propose integrating\nChain-of-Thought (CoT) reasoning into the Natural Language to Visualization\n(NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for\nNL2VIS and develop an automatic pipeline to equip existing datasets with\nstructured reasoning steps. Second, we introduce nvBench-CoT, a specialized\ndataset capturing detailed step-by-step reasoning from ambiguous natural\nlanguage descriptions to finalized visualizations, which enables\nstate-of-the-art performance when used for model fine-tuning. Third, we develop\nDeepVIS, an interactive visual interface that tightly integrates with the CoT\nreasoning process, allowing users to inspect reasoning steps, identify errors,\nand make targeted adjustments to improve visualization outcomes. Quantitative\nbenchmark evaluations, two use cases, and a user study collectively demonstrate\nthat our CoT framework effectively enhances NL2VIS quality while providing\ninsightful reasoning steps to users.\n","authors":["Zhihao Shuai","Boyan Li","Siyu Yan","Yuyu Luo","Weikai Yang"],"pdf_url":"https://arxiv.org/pdf/2508.01700v2.pdf","comment":"IEEE VIS 2025 full paper"},{"id":"http://arxiv.org/abs/2509.00215v2","updated":"2025-09-04T12:46:04Z","published":"2025-08-29T19:55:25Z","title":"First Order Model-Based RL through Decoupled Backpropagation","summary":"  There is growing interest in reinforcement learning (RL) methods that\nleverage the simulator's derivatives to improve learning efficiency. While\nearly gradient-based approaches have demonstrated superior performance compared\nto derivative-free methods, accessing simulator gradients is often impractical\ndue to their implementation cost or unavailability. Model-based RL (MBRL) can\napproximate these gradients via learned dynamics models, but the solver\nefficiency suffers from compounding prediction errors during training rollouts,\nwhich can degrade policy performance. We propose an approach that decouples\ntrajectory generation from gradient computation: trajectories are unrolled\nusing a simulator, while gradients are computed via backpropagation through a\nlearned differentiable model of the simulator. This hybrid design enables\nefficient and consistent first-order policy optimization, even when simulator\ngradients are unavailable, as well as learning a critic from simulation\nrollouts, which is more accurate. Our method achieves the sample efficiency and\nspeed of specialized optimizers such as SHAC, while maintaining the generality\nof standard approaches like PPO and avoiding ill behaviors observed in other\nfirst-order MBRL methods. We empirically validate our algorithm on benchmark\ncontrol tasks and demonstrate its effectiveness on a real Go2 quadruped robot,\nacross both quadrupedal and bipedal locomotion tasks.\n","authors":["Joseph Amigo","Rooholla Khorrambakht","Elliot Chane-Sane","Nicolas Mansard","Ludovic Righetti"],"pdf_url":"https://arxiv.org/pdf/2509.00215v2.pdf","comment":"CoRL 2025. Project website: https://machines-in-motion.github.io/DMO/"},{"id":"http://arxiv.org/abs/2410.22381v2","updated":"2025-09-04T12:44:44Z","published":"2024-10-29T10:27:50Z","title":"Robust training of implicit generative models for multivariate and\n  heavy-tailed distributions with an invariant statistical loss","summary":"  Traditional implicit generative models are capable of learning highly complex\ndata distributions. However, their training involves distinguishing real data\nfrom synthetically generated data using adversarial discriminators, which can\nlead to unstable training dynamics and mode dropping issues. In this work, we\nbuild on the \\textit{invariant statistical loss} (ISL) method introduced in\n\\cite{de2024training}, and extend it to handle heavy-tailed and multivariate\ndata distributions.\n  The data generated by many real-world phenomena can only be properly\ncharacterised using heavy-tailed probability distributions, and traditional\nimplicit methods struggle to effectively capture their asymptotic behavior. To\naddress this problem, we introduce a generator trained with ISL, that uses\ninput noise from a generalised Pareto distribution (GPD). We refer to this\ngenerative scheme as Pareto-ISL for conciseness. Our experiments demonstrate\nthat Pareto-ISL accurately models the tails of the distributions while still\neffectively capturing their central characteristics.\n  The original ISL function was conceived for 1D data sets. When the actual\ndata is $n$-dimensional, a straightforward extension of the method was obtained\nby targeting the $n$ marginal distributions of the data. This approach is\ncomputationally infeasible and ineffective in high-dimensional spaces. To\novercome this, we extend the 1D approach using random projections and define a\nnew loss function suited for multivariate data, keeping problems tractable by\nadjusting the number of projections. We assess its performance in\nmultidimensional generative modeling and explore its potential as a pretraining\ntechnique for generative adversarial networks (GANs) to prevent mode collapse,\nreporting promising results and highlighting its robustness across various\nhyperparameter settings.\n","authors":["José Manuel de Frutos","Manuel A. Vázquez","Pablo Olmos","Joaquín Míguez"],"pdf_url":"https://arxiv.org/pdf/2410.22381v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04166v1","updated":"2025-09-04T12:39:05Z","published":"2025-09-04T12:39:05Z","title":"Crossing the Species Divide: Transfer Learning from Speech to Animal\n  Sounds","summary":"  Self-supervised speech models have demonstrated impressive performance in\nspeech processing, but their effectiveness on non-speech data remains\nunderexplored. We study the transfer learning capabilities of such models on\nbioacoustic detection and classification tasks. We show that models such as\nHuBERT, WavLM, and XEUS can generate rich latent representations of animal\nsounds across taxa. We analyze the models properties with linear probing on\ntime-averaged representations. We then extend the approach to account for the\neffect of time-wise information with other downstream architectures. Finally,\nwe study the implication of frequency range and noise on performance. Notably,\nour results are competitive with fine-tuned bioacoustic pre-trained models and\nshow the impact of noise-robust pre-training setups. These findings highlight\nthe potential of speech-based self-supervised learning as an efficient\nframework for advancing bioacoustic research.\n","authors":["Jules Cauzinille","Marius Miron","Olivier Pietquin","Masato Hagiwara","Ricard Marxer","Arnaud Rey","Benoit Favre"],"pdf_url":"https://arxiv.org/pdf/2509.04166v1.pdf","comment":"5 pages, 3 figures, uses dcase2025.sty, submitted to DCASE 2025"},{"id":"http://arxiv.org/abs/2509.04159v1","updated":"2025-09-04T12:34:56Z","published":"2025-09-04T12:34:56Z","title":"Towards an Action-Centric Ontology for Cooking Procedures Using Temporal\n  Graphs","summary":"  Formalizing cooking procedures remains a challenging task due to their\ninherent complexity and ambiguity. We introduce an extensible domain-specific\nlanguage for representing recipes as directed action graphs, capturing\nprocesses, transfers, environments, concurrency, and compositional structure.\nOur approach enables precise, modular modeling of complex culinary workflows.\nInitial manual evaluation on a full English breakfast recipe demonstrates the\nDSL's expressiveness and suitability for future automated recipe analysis and\nexecution. This work represents initial steps towards an action-centric\nontology for cooking, using temporal graphs to enable structured machine\nunderstanding, precise interpretation, and scalable automation of culinary\nprocesses - both in home kitchens and professional culinary settings.\n","authors":["Aarush Kumbhakern","Saransh Kumar Gupta","Lipika Dey","Partha Pratim Das"],"pdf_url":"https://arxiv.org/pdf/2509.04159v1.pdf","comment":"6 pages, 3 figures, 1 table, 11 references, ACM International\n  Conference on Multimedia 2025 - Multi-modal Food Computing Workshop"},{"id":"http://arxiv.org/abs/2509.04156v1","updated":"2025-09-04T12:32:04Z","published":"2025-09-04T12:32:04Z","title":"YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind\n  Turbine Components","summary":"  Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up\nnew opportunities for monitoring wind power plants, including blades, towers,\nand other critical components. However, reliable defect detection requires\nhigh-resolution data and efficient methods to process multispectral imagery. In\nthis research, we aim to enhance defect detection accuracy through the\ndevelopment of an ensemble of YOLO-based deep learning models that integrate\nboth visible and thermal channels. We propose an ensemble approach that\nintegrates a general-purpose YOLOv8 model with a specialized thermal model,\nusing a sophisticated bounding box fusion algorithm to combine their\npredictions. Our experiments show this approach achieves a mean Average\nPrecision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone\nYOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that\ncombining multiple YOLO architectures with fused multispectral data provides a\nmore reliable solution, improving the detection of both visual and thermal\ndefects.\n","authors":["Serhii Svystun","Pavlo Radiuk","Oleksandr Melnychenko","Oleg Savenko","Anatoliy Sachenko"],"pdf_url":"https://arxiv.org/pdf/2509.04156v1.pdf","comment":"The 13th IEEE International Conference on Intelligent Data\n  Acquisition and Advanced Computing Systems: Technology and Applications, 4-6\n  September, 2025, Gliwice, Poland"},{"id":"http://arxiv.org/abs/2509.04154v1","updated":"2025-09-04T12:29:14Z","published":"2025-09-04T12:29:14Z","title":"Attention as an Adaptive Filter","summary":"  We introduce Adaptive Filter Attention (AFA), a novel attention mechanism\nthat incorporates a learnable dynamics model directly into the computation of\nattention weights. Rather than comparing queries and keys directly, we model\nthe input sequence as discrete observations of a linear stochastic differential\nequation (SDE). By imposing a linear dynamics model with simultaneously\ndiagonalizable state matrices and noise covariances, we can make use of a\nclosed-form solution to the differential Lyapunov equation to efficiently\npropagate pairwise uncertainties through the dynamics. Attention naturally\narises as the maximum likelihood solution for this linear SDE, with attention\nweights corresponding to robust residual-based reweightings of the propagated\npairwise precisions. Imposing an additional constraint on the state matrix's\neigenvalues leads to a simplified variant with the same computational and\nmemory complexity as standard attention. In the limit of vanishing dynamics and\nprocess noise, and using a small-angle approximation, we recover ordinary\ndot-product attention.\n","authors":["Peter Racioppo"],"pdf_url":"https://arxiv.org/pdf/2509.04154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04152v1","updated":"2025-09-04T12:25:14Z","published":"2025-09-04T12:25:14Z","title":"TAGAL: Tabular Data Generation using Agentic LLM Methods","summary":"  The generation of data is a common approach to improve the performance of\nmachine learning tasks, among which is the training of models for\nclassification. In this paper, we present TAGAL, a collection of methods able\nto generate synthetic tabular data using an agentic workflow. The methods\nleverage Large Language Models (LLMs) for an automatic and iterative process\nthat uses feedback to improve the generated data without any further LLM\ntraining. The use of LLMs also allows for the addition of external knowledge in\nthe generation process. We evaluate TAGAL across diverse datasets and different\naspects of quality for the generated data. We look at the utility of downstream\nML models, both by training classifiers on synthetic data only and by combining\nreal and synthetic data. Moreover, we compare the similarities between the real\nand the generated data. We show that TAGAL is able to perform on par with\nstate-of-the-art approaches that require LLM training and generally outperforms\nother training-free approaches. These findings highlight the potential of\nagentic workflow and open new directions for LLM-based data generation methods.\n","authors":["Benoît Ronval","Pierre Dupont","Siegfried Nijssen"],"pdf_url":"https://arxiv.org/pdf/2509.04152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00265v3","updated":"2025-09-04T12:15:53Z","published":"2024-10-31T23:54:21Z","title":"Quantifying Calibration Error in Neural Networks Through Evidence-Based\n  Theory","summary":"  Trustworthiness in neural networks is crucial for their deployment in\ncritical applications, where reliability, confidence, and uncertainty play\npivotal roles in decision-making. Traditional performance metrics such as\naccuracy and precision fail to capture these aspects, particularly in cases\nwhere models exhibit overconfidence. To address these limitations, this paper\nintroduces a novel framework for quantifying the trustworthiness of neural\nnetworks by incorporating subjective logic into the evaluation of Expected\nCalibration Error (ECE). This method provides a comprehensive measure of trust,\ndisbelief, and uncertainty by clustering predicted probabilities and fusing\nopinions using appropriate fusion operators. We demonstrate the effectiveness\nof this approach through experiments on MNIST and CIFAR-10 datasets, where\npost-calibration results indicate improved trustworthiness. The proposed\nframework offers a more interpretable and nuanced assessment of AI models, with\npotential applications in sensitive domains such as healthcare and autonomous\nsystems.\n","authors":["Koffi Ismael Ouattara","Ioannis Krontiris","Theo Dimitrakos","Frank Kargl"],"pdf_url":"https://arxiv.org/pdf/2411.00265v3.pdf","comment":"This is the preprint of the paper accepted to Fusion 2025 (28th\n  International Conference on Information Fusion, Rio de Janeiro, Brazil, July\n  7-10, 2025). The published version is available at\n  https://doi.org/10.23919/FUSION65864.2025.11124121"},{"id":"http://arxiv.org/abs/2509.04139v1","updated":"2025-09-04T12:11:03Z","published":"2025-09-04T12:11:03Z","title":"Enhancing Technical Documents Retrieval for RAG","summary":"  In this paper, we introduce Technical-Embeddings, a novel framework designed\nto optimize semantic retrieval in technical documentation, with applications in\nboth hardware and software development. Our approach addresses the challenges\nof understanding and retrieving complex technical content by leveraging the\ncapabilities of Large Language Models (LLMs). First, we enhance user queries by\ngenerating expanded representations that better capture user intent and improve\ndataset diversity, thereby enriching the fine-tuning process for embedding\nmodels. Second, we apply summary extraction techniques to encode essential\ncontextual information, refining the representation of technical documents. To\nfurther enhance retrieval performance, we fine-tune a bi-encoder BERT model\nusing soft prompting, incorporating separate learning parameters for queries\nand document context to capture fine-grained semantic nuances. We evaluate our\napproach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that\nTechnical-Embeddings significantly outperforms baseline models in both\nprecision and recall. Our findings highlight the effectiveness of integrating\nquery expansion and contextual summarization to enhance information access and\ncomprehension in technical domains. This work advances the state of\nRetrieval-Augmented Generation (RAG) systems, offering new avenues for\nefficient and accurate technical document retrieval in engineering and product\ndevelopment workflows.\n","authors":["Songjiang Lai","Tsun-Hin Cheung","Ka-Chun Fung","Kaiwen Xue","Kwan-Ho Lin","Yan-Ming Choi","Vincent Ng","Kin-Man Lam"],"pdf_url":"https://arxiv.org/pdf/2509.04139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.16117v2","updated":"2025-09-04T11:54:35Z","published":"2025-08-22T06:18:51Z","title":"Extending FKG.in: Towards a Food Claim Traceability Network","summary":"  The global food landscape is rife with scientific, cultural, and commercial\nclaims about what foods are, what they do, what they should not do, or should\nnot do. These range from rigorously studied health benefits (probiotics improve\ngut health) and misrepresentations (soaked almonds make one smarter) to vague\npromises (superfoods boost immunity) and culturally rooted beliefs (cold foods\ncause coughs). Despite their widespread influence, the infrastructure for\ntracing, verifying, and contextualizing these claims remains fragmented and\nunderdeveloped. In this paper, we propose a Food Claim-Traceability Network\n(FCN) as an extension of FKG[.]in, a knowledge graph of Indian food that we\nhave been incrementally building. We also present the ontology design and the\nsemi-automated knowledge curation workflow that we used to develop a proof of\nconcept of FKG[.]in-FCN using Reddit data and Large Language Models. FCN\nintegrates curated data inputs, structured schemas, and provenance-aware\npipelines for food-related claim extraction and validation. While directly\nlinked to the Indian food knowledge graph as an application, our methodology\nremains application-agnostic and adaptable to other geographic, culinary, or\nregulatory settings. By modeling food claims and their traceability in a\nstructured, verifiable, and explainable way, we aim to contribute to more\ntransparent and accountable food knowledge ecosystems, supporting researchers,\npolicymakers, and most importantly, everyday consumers in navigating a world\nsaturated with dietary assertions.\n","authors":["Saransh Kumar Gupta","Rizwan Gulzar Mir","Lipika Dey","Partha Pratim Das","Anirban Sen","Ramesh Jain"],"pdf_url":"https://arxiv.org/pdf/2508.16117v2.pdf","comment":"10 pages, 3 figures, 1 table, 45 references, ACM International\n  Conference on Multimedia 2025 - Multi-modal Food Computing Workshop"},{"id":"http://arxiv.org/abs/2509.04130v1","updated":"2025-09-04T11:54:27Z","published":"2025-09-04T11:54:27Z","title":"The human biological advantage over AI","summary":"  Recent advances in AI raise the possibility that AI systems will one day be\nable to do anything humans can do, only better. If artificial general\nintelligence (AGI) is achieved, AI systems may be able to understand, reason,\nproblem solve, create, and evolve at a level and speed that humans will\nincreasingly be unable to match, or even understand. These possibilities raise\na natural question as to whether AI will eventually become superior to humans,\na successor \"digital species\", with a rightful claim to assume leadership of\nthe universe. However, a deeper consideration suggests the overlooked\ndifferentiator between human beings and AI is not the brain, but the central\nnervous system (CNS), providing us with an immersive integration with physical\nreality. It is our CNS that enables us to experience emotion including pain,\njoy, suffering, and love, and therefore to fully appreciate the consequences of\nour actions on the world around us. And that emotional understanding of the\nconsequences of our actions is what is required to be able to develop\nsustainable ethical systems, and so be fully qualified to be the leaders of the\nuniverse. A CNS cannot be manufactured or simulated; it must be grown as a\nbiological construct. And so, even the development of consciousness will not be\nsufficient to make AI systems superior to humans. AI systems may become more\ncapable than humans on almost every measure and transform our society. However,\nthe best foundation for leadership of our universe will always be DNA, not\nsilicon.\n","authors":["William Stewart"],"pdf_url":"https://arxiv.org/pdf/2509.04130v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2509.04129v1","updated":"2025-09-04T11:54:19Z","published":"2025-09-04T11:54:19Z","title":"Simplicity Lies in the Eye of the Beholder: A Strategic Perspective on\n  Controllers in Reactive Synthesis","summary":"  In the game-theoretic approach to controller synthesis, we model the\ninteraction between a system to be controlled and its environment as a game\nbetween these entities, and we seek an appropriate (e.g., winning or optimal)\nstrategy for the system. This strategy then serves as a formal blueprint for a\nreal-world controller. A common belief is that simple (e.g., using limited\nmemory) strategies are better: corresponding controllers are easier to conceive\nand understand, and cheaper to produce and maintain.\n  This invited contribution focuses on the complexity of strategies in a\nvariety of synthesis contexts. We discuss recent results concerning memory and\nrandomness, and take a brief look at what lies beyond our traditional notions\nof complexity for strategies.\n","authors":["Mickael Randour"],"pdf_url":"https://arxiv.org/pdf/2509.04129v1.pdf","comment":"Invited paper at RP 2025"},{"id":"http://arxiv.org/abs/2506.08570v3","updated":"2025-09-04T11:54:13Z","published":"2025-06-10T08:37:45Z","title":"Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling\n  Paradigms for Text-to-Music Generation","summary":"  Recent progress in text-to-music generation has enabled models to synthesize\nhigh-quality musical segments, full compositions, and even respond to\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\nsystems differ significantly in many dimensions, such as training datasets,\nmodeling paradigms, and architectural choices. This diversity complicates\nefforts to evaluate models fairly and identify which design choices influence\nperformance the most. While factors like data and architecture are important,\nin this study we focus exclusively on the modeling paradigm. We conduct a\nsystematic empirical analysis to isolate its effects, offering insights into\nassociated trade-offs and emergent behaviors that can guide future\ntext-to-music generation systems. Specifically, we compare the two arguably\nmost common modeling paradigms: auto-regressive decoding and conditional\nflow-matching. We conduct a controlled comparison by training all models from\nscratch using identical datasets, training configurations, and similar backbone\narchitectures. Performance is evaluated across multiple axes, including\ngeneration quality, robustness to inference configurations, scalability,\nadherence to both textual and temporally aligned conditioning, and editing\ncapabilities in the form of audio inpainting. This comparative study sheds\nlight on distinct strengths and limitations of each paradigm, providing\nactionable insights that can inform future architectural and training decisions\nin the evolving landscape of text-to-music generation. Audio sampled examples\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM\n","authors":["Or Tal","Felix Kreuk","Yossi Adi"],"pdf_url":"https://arxiv.org/pdf/2506.08570v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01909v2","updated":"2025-09-04T11:54:06Z","published":"2025-09-02T03:04:27Z","title":"Oyster-I: Beyond Refusal -- Constructive Safety Alignment for\n  Responsible Language Models","summary":"  Large language models (LLMs) typically deploy safety mechanisms to prevent\nharmful content generation. Most current approaches focus narrowly on risks\nposed by malicious actors, often framing risks as adversarial events and\nrelying on defensive refusals. However, in real-world settings, risks also come\nfrom non-malicious users seeking help while under psychological distress (e.g.,\nself-harm intentions). In such cases, the model's response can strongly\ninfluence the user's next actions. Simple refusals may lead them to repeat,\nescalate, or move to unsafe platforms, creating worse outcomes. We introduce\nConstructive Safety Alignment (CSA), a human-centric paradigm that protects\nagainst malicious misuse while actively guiding vulnerable users toward safe\nand helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic\nanticipation of user reactions, fine-grained risk boundary discovery, and\ninterpretable reasoning control, turning safety into a trust-building process.\nOy1 achieves state-of-the-art safety among open models while retaining high\ngeneral capabilities. On our Constructive Benchmark, it shows strong\nconstructive engagement, close to GPT-5, and unmatched robustness on the\nStrata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from\nrefusal-first to guidance-first safety, CSA redefines the model-user\nrelationship, aiming for systems that are not just safe, but meaningfully\nhelpful. We release Oy1, code, and the benchmark to support responsible,\nuser-centered AI.\n","authors":["Ranjie Duan","Jiexi Liu","Xiaojun Jia","Shiji Zhao","Ruoxi Cheng","Fengxiang Wang","Cheng Wei","Yong Xie","Chang Liu","Defeng Li","Yinpeng Dong","Yichi Zhang","Yuefeng Chen","Chongwen Wang","Xingjun Ma","Xingxing Wei","Yang Liu","Hang Su","Jun Zhu","Xinfeng Li","Yitong Sun","Jie Zhang","Jinzhao Hu","Sha Xu","Yitong Yang","Jialing Tao","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2509.01909v2.pdf","comment":"Technical Report Code & Model weights available:\n  https://github.com/Alibaba-AAIG/Oyster"},{"id":"http://arxiv.org/abs/2509.04126v1","updated":"2025-09-04T11:44:28Z","published":"2025-09-04T11:44:28Z","title":"MEPG:Multi-Expert Planning and Generation for Compositionally-Rich Image\n  Generation","summary":"  Text-to-image diffusion models have achieved remarkable image quality, but\nthey still struggle with complex, multiele ment prompts, and limited stylistic\ndiversity. To address these limitations, we propose a Multi-Expert Planning and\nGen eration Framework (MEPG) that synergistically integrates position- and\nstyle-aware large language models (LLMs) with spatial-semantic expert modules.\nThe framework comprises two core components: (1) a Position-Style-Aware (PSA)\nmodule that utilizes a supervised fine-tuned LLM to decom pose input prompts\ninto precise spatial coordinates and style encoded semantic instructions; and\n(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera\ntion through dynamic expert routing across both local regions and global areas.\nDuring the generation process for each lo cal region, specialized models (e.g.,\nrealism experts, styliza tion specialists) are selectively activated for each\nspatial par tition via attention-based gating mechanisms. The architec ture\nsupports lightweight integration and replacement of ex pert models, providing\nstrong extensibility. Additionally, an interactive interface enables real-time\nspatial layout editing and per-region style selection from a portfolio of\nexperts. Ex periments show that MEPG significantly outperforms base line models\nwith the same backbone in both image quality\n  and style diversity.\n","authors":["Yuan Zhao","Liu Lin"],"pdf_url":"https://arxiv.org/pdf/2509.04126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05248v3","updated":"2025-09-04T11:42:04Z","published":"2024-12-06T18:27:15Z","title":"Enhancing FKG.in: automating Indian food composition analysis","summary":"  This paper presents a novel approach to compute food composition data for\nIndian recipes using a knowledge graph for Indian food (FKG[.]in) and LLMs. The\nprimary focus is to provide a broad overview of an automated food composition\nanalysis workflow and describe its core functionalities: nutrition data\naggregation, food composition analysis, and LLM-augmented information\nresolution. This workflow aims to complement FKG[.]in and iteratively\nsupplement food composition data from verified knowledge bases. Additionally,\nthis paper highlights the challenges of representing Indian food and accessing\nfood composition data digitally. It also reviews three key sources of food\ncomposition data: the Indian Food Composition Tables, the Indian Nutrient\nDatabank, and the Nutritionix API. Furthermore, it briefly outlines how users\ncan interact with the workflow to obtain diet-based health recommendations and\ndetailed food composition information for numerous recipes. We then explore the\ncomplex challenges of analyzing Indian recipe information across dimensions\nsuch as structure, multilingualism, and uncertainty as well as present our\nongoing work on LLM-based solutions to address these issues. The methods\nproposed in this workshop paper for AI-driven knowledge curation and\ninformation resolution are application-agnostic, generalizable, and replicable\nfor any domain.\n","authors":["Saransh Kumar Gupta","Lipika Dey","Partha Pratim Das","Geeta Trilok-Kumar","Ramesh Jain"],"pdf_url":"https://arxiv.org/pdf/2412.05248v3.pdf","comment":"15 pages, 5 figures, 30 references, International Conference on\n  Pattern Recognition 2024 - Multimedia Assisted Dietary Management Workshop"},{"id":"http://arxiv.org/abs/2506.20790v2","updated":"2025-09-04T11:41:34Z","published":"2025-06-25T19:26:31Z","title":"Stochastic Parameter Decomposition","summary":"  A key step in reverse engineering neural networks is to decompose them into\nsimpler parts that can be studied in relative isolation. Linear parameter\ndecomposition -- a framework that has been proposed to resolve several issues\nwith current decomposition methods -- decomposes neural network parameters into\na sum of sparsely used vectors in parameter space. However, the current main\nmethod in this framework, Attribution-based Parameter Decomposition (APD), is\nimpractical on account of its computational cost and sensitivity to\nhyperparameters. In this work, we introduce \\textit{Stochastic Parameter\nDecomposition} (SPD), a method that is more scalable and robust to\nhyperparameters than APD, which we demonstrate by decomposing models that are\nslightly larger and more complex than was possible to decompose with APD. We\nalso show that SPD avoids other issues, such as shrinkage of the learned\nparameters, and better identifies ground truth mechanisms in toy models. By\nbridging causal mediation analysis and network decomposition methods, this\ndemonstration opens up new research possibilities in mechanistic\ninterpretability by removing barriers to scaling linear parameter decomposition\nmethods to larger models. We release a library for running SPD and reproducing\nour experiments at https://github.com/goodfire-ai/spd/tree/spd-paper.\n","authors":["Lucius Bushnaq","Dan Braun","Lee Sharkey"],"pdf_url":"https://arxiv.org/pdf/2506.20790v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04125v1","updated":"2025-09-04T11:40:24Z","published":"2025-09-04T11:40:24Z","title":"Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker","summary":"  In the game of poker, being unpredictable, or bluffing, is an essential\nskill. When humans play poker, they bluff. However, most works on\ncomputer-poker focus on performance metrics such as win rates, while bluffing\nis overlooked. In this paper we study whether two popular algorithms, DQN\n(based on reinforcement learning) and CFR (based on game theory), exhibit\nbluffing behavior in Leduc Hold'em, a simplified version of poker. We designed\nan experiment where we let the DQN and CFR agent play against each other while\nwe log their actions. We find that both DQN and CFR exhibit bluffing behavior,\nbut they do so in different ways. Although both attempt to perform bluffs at\ndifferent rates, the percentage of successful bluffs (where the opponent folds)\nis roughly the same. This suggests that bluffing is an essential aspect of the\ngame, not of the algorithm. Future work should look at different bluffing\nstyles and at the full game of poker. Code at\nhttps://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.\n","authors":["Tarik Zaciragic","Aske Plaat","K. Joost Batenburg"],"pdf_url":"https://arxiv.org/pdf/2509.04125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04118v1","updated":"2025-09-04T11:31:12Z","published":"2025-09-04T11:31:12Z","title":"EHVC: Efficient Hierarchical Reference and Quality Structure for Neural\n  Video Coding","summary":"  Neural video codecs (NVCs), leveraging the power of end-to-end learning, have\ndemonstrated remarkable coding efficiency improvements over traditional video\ncodecs. Recent research has begun to pay attention to the quality structures in\nNVCs, optimizing them by introducing explicit hierarchical designs. However,\nless attention has been paid to the reference structure design, which\nfundamentally should be aligned with the hierarchical quality structure. In\naddition, there is still significant room for further optimization of the\nhierarchical quality structure. To address these challenges in NVCs, we propose\nEHVC, an efficient hierarchical neural video codec featuring three key\ninnovations: (1) a hierarchical multi-reference scheme that draws on\ntraditional video codec design to align reference and quality structures,\nthereby addressing the reference-quality mismatch; (2) a lookahead strategy to\nutilize an encoder-side context from future frames to enhance the quality\nstructure; (3) a layer-wise quality scale with random quality training strategy\nto stabilize quality structures during inference. With these improvements, EHVC\nachieves significantly superior performance to the state-of-the-art NVCs. Code\nwill be released in: https://github.com/bytedance/NEVC.\n","authors":["Junqi Liao","Yaojun Wu","Chaoyi Lin","Zhipin Deng","Li Li","Dong Liu","Xiaoyan Sun"],"pdf_url":"https://arxiv.org/pdf/2509.04118v1.pdf","comment":"9 pages, 8 figures, Accepted to ACMMM 2025"},{"id":"http://arxiv.org/abs/2403.03726v3","updated":"2025-09-04T11:13:05Z","published":"2024-03-06T14:15:20Z","title":"Diffusion on language model encodings for protein sequence generation","summary":"  Protein sequence design has seen significant advances through discrete\ndiffusion and autoregressive approaches, yet the potential of continuous\ndiffusion remains underexplored. Here, we present DiMA, a latent diffusion\nframework that operates on protein language model representations. Through\nsystematic exploration of architectural choices and diffusion components, we\ndevelop a robust methodology that generalizes across multiple protein encoders\nranging from 8M to 3B parameters. We demonstrate that our framework achieves\nconsistently high performance across sequence-only (ESM-2, ESMc),\ndual-decodable (CHEAP), and multimodal (SaProt) representations using the same\narchitecture and training approach. We extensively evaluate existing methods\nalongside DiMA using multiple metrics across two protein modalities, covering\nquality, diversity, novelty, and distribution matching of generated proteins.\nDiMA consistently produces novel, high-quality and diverse protein sequences\nand achieves strong results compared to baselines such as autoregressive,\ndiscrete diffusion and flow matching language models. The model demonstrates\nversatile functionality, supporting conditional generation tasks including\nprotein family-generation, motif scaffolding and infilling, and fold-specific\nsequence design. This work provides a universal continuous diffusion framework\nfor protein sequence generation, offering both architectural insights and\npractical applicability across various protein design scenarios.\n","authors":["Viacheslav Meshchaninov","Pavel Strashnov","Andrey Shevtsov","Fedor Nikolaev","Nikita Ivanisenko","Olga Kardymon","Dmitry Vetrov"],"pdf_url":"https://arxiv.org/pdf/2403.03726v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18970v4","updated":"2025-09-04T11:06:55Z","published":"2024-10-24T17:59:16Z","title":"WASP: A Weight-Space Approach to Detecting Learned Spuriousness","summary":"  It is of crucial importance to train machine learning models such that they\nclearly understand what defines each class in a given task. Though there is a\nsum of works dedicated to identifying the spurious correlations featured by a\ndataset that may impact the model's understanding of the classes, all current\napproaches rely solely on data or error analysis. That is, they cannot point\nout spurious correlations learned by the model that are not already pointed out\nby the counterexamples featured in the validation or training sets. We propose\na method that transcends this limitation, switching the focus from analyzing a\nmodel's predictions to analyzing the model's weights, the mechanism behind the\nmaking of the decisions, which proves to be more insightful. Our proposed\nWeight-space Approach to detecting Spuriousness (WASP) relies on analyzing the\nweights of foundation models as they drift towards capturing various (spurious)\ncorrelations while being fine-tuned on a given dataset. We demonstrate that\ndifferent from previous works, our method (i) can expose spurious correlations\nfeatured by a dataset even when they are not exposed by training or validation\ncounterexamples, (ii) it works for multiple modalities such as image and text,\nand (iii) it can uncover previously untapped spurious correlations learned by\nImageNet-1k classifiers.\n","authors":["Cristian Daniel Păduraru","Antonio Bărbălau","Radu Filipescu","Andrei Liviu Nicolicioiu","Elena Burceanu"],"pdf_url":"https://arxiv.org/pdf/2410.18970v4.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2509.04100v1","updated":"2025-09-04T11:01:43Z","published":"2025-09-04T11:01:43Z","title":"Hybrid Reinforcement Learning and Search for Flight Trajectory Planning","summary":"  This paper explores the combination of Reinforcement Learning (RL) and\nsearch-based path planners to speed up the optimization of flight paths for\nairliners, where in case of emergency a fast route re-calculation can be\ncrucial. The fundamental idea is to train an RL Agent to pre-compute\nnear-optimal paths based on location and atmospheric data and use those at\nruntime to constrain the underlying path planning solver and find a solution\nwithin a certain distance from the initial guess. The approach effectively\nreduces the size of the solver's search space, significantly speeding up route\noptimization. Although global optimality is not guaranteed, empirical results\nconducted with Airbus aircraft's performance models show that fuel consumption\nremains nearly identical to that of an unconstrained solver, with deviations\ntypically within 1%. At the same time, computation speed can be improved by up\nto 50% as compared to using a conventional solver alone.\n","authors":["Alberto Luise","Michele Lombardi","Florent Teichteil Koenigsbuch"],"pdf_url":"https://arxiv.org/pdf/2509.04100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.05774v2","updated":"2025-09-04T10:49:27Z","published":"2025-04-08T07:53:51Z","title":"Transferable Mask Transformer: Cross-domain Semantic Segmentation with\n  Region-adaptive Transferability Estimation","summary":"  Recent advances in Vision Transformers (ViTs) have set new benchmarks in\nsemantic segmentation. However, when adapting pretrained ViTs to new target\ndomains, significant performance degradation often occurs due to distribution\nshifts, resulting in suboptimal global attention. Since self-attention\nmechanisms are inherently data-driven, they may fail to effectively attend to\nkey objects when source and target domains exhibit differences in texture,\nscale, or object co-occurrence patterns. While global and patch-level domain\nadaptation methods provide partial solutions, region-level adaptation with\ndynamically shaped regions is crucial due to spatial heterogeneity in\ntransferability across different image areas. We present Transferable Mask\nTransformer (TMT), a novel region-level adaptation framework for semantic\nsegmentation that aligns cross-domain representations through spatial\ntransferability analysis. TMT consists of two key components: (1) An Adaptive\nCluster-based Transferability Estimator (ACTE) that dynamically segments images\ninto structurally and semantically coherent regions for localized\ntransferability assessment, and (2) A Transferable Masked Attention (TMA)\nmodule that integrates region-specific transferability maps into ViTs'\nattention mechanisms, prioritizing adaptation in regions with low\ntransferability and high semantic uncertainty. Comprehensive evaluations across\n20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2%\nMIoU improvement over vanilla fine-tuning and a 1.28% increase compared to\nstate-of-the-art baselines. The source code will be publicly available.\n","authors":["Jianhua Liu","Zhengyu Li","Yanru Wu","Jingge Wang","Yang Tan","Ruizhe Zhao","Guan Wang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2504.05774v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04083v1","updated":"2025-09-04T10:25:50Z","published":"2025-09-04T10:25:50Z","title":"Intermediate Languages Matter: Formal Languages and LLMs affect\n  Neurosymbolic Reasoning","summary":"  Large language models (LLMs) achieve astonishing results on a wide range of\ntasks. However, their formal reasoning ability still lags behind. A promising\napproach is Neurosymbolic LLM reasoning. It works by using LLMs as translators\nfrom natural to formal languages and symbolic solvers for deriving correct\nresults. Still, the contributing factors to the success of Neurosymbolic LLM\nreasoning remain unclear. This paper demonstrates that one previously\noverlooked factor is the choice of the formal language. We introduce the\nintermediate language challenge: selecting a suitable formal language for\nneurosymbolic reasoning. By comparing four formal languages across three\ndatasets and seven LLMs, we show that the choice of formal language affects\nboth syntactic and semantic reasoning capabilities. We also discuss the varying\neffects across different LLMs.\n","authors":["Alexander Beiser","David Penz","Nysret Musliu"],"pdf_url":"https://arxiv.org/pdf/2509.04083v1.pdf","comment":"To appear in the proceedings of The Second Workshop on Knowledge\n  Graphs and Neurosymbolic AI (KG-NeSy) Co-located with SEMANTiCS 2025\n  Conference, Vienna, Austria - September 3rd, 2025"},{"id":"http://arxiv.org/abs/2509.04078v1","updated":"2025-09-04T10:13:21Z","published":"2025-09-04T10:13:21Z","title":"RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging\n  Evaluation of Large Language Models","summary":"  Large Language Models (LLMs) have exhibited significant proficiency in code\ndebugging, especially in automatic program repair, which may substantially\nreduce the time consumption of developers and enhance their efficiency.\nSignificant advancements in debugging datasets have been made to promote the\ndevelopment of code debugging. However, these datasets primarily focus on\nassessing the LLM's function-level code repair capabilities, neglecting the\nmore complex and realistic repository-level scenarios, which leads to an\nincomplete understanding of the LLM's challenges in repository-level debugging.\nWhile several repository-level datasets have been proposed, they often suffer\nfrom limitations such as limited diversity of tasks, languages, and error\ntypes. To mitigate this challenge, this paper introduces RepoDebug, a\nmulti-task and multi-language repository-level code debugging dataset with 22\nsubtypes of errors that supports 8 commonly used programming languages and 3\ndebugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,\nwhere Claude 3.5 Sonnect, the best-performing model, still cannot perform well\nin repository-level debugging.\n","authors":["Jingjing Liu","Zeming Liu","Zihao Cheng","Mengliang He","Xiaoming Shi","Yuhang Guo","Xiangrong Zhu","Yuanfang Guo","Yunhong Wang","Haifeng Wang"],"pdf_url":"https://arxiv.org/pdf/2509.04078v1.pdf","comment":"30 pages, 12 figures, EMNLP 2025 Findings"},{"id":"http://arxiv.org/abs/2509.04076v1","updated":"2025-09-04T10:11:51Z","published":"2025-09-04T10:11:51Z","title":"Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot","summary":"  We propose a novel diffusion-based action model for robotic motion planning.\nCommonly, established numerical planning approaches are used to solve general\nmotion planning problems, but have significant runtime requirements. By\nleveraging the power of deep learning, we are able to achieve good results in a\nmuch smaller runtime by learning from a dataset generated by these planners.\nWhile our initial model uses point cloud embeddings in the input to predict\nkeypoint-based joint sequences in its output, we observed in our ablation study\nthat it remained challenging to condition the network on the point cloud\nembeddings. We identified some biases in our dataset and refined it, which\nimproved the model's performance. Our model, even without the use of the point\ncloud encodings, outperforms numerical models by an order of magnitude\nregarding the runtime, while reaching a success rate of up to 90% of collision\nfree solutions on the test set.\n","authors":["Lennart Clasmeier","Jan-Gerrit Habekost","Connor Gäde","Philipp Allgeuer","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2509.04076v1.pdf","comment":"Submitted to ICANN 20255 Special Session on Neural Robotics"},{"id":"http://arxiv.org/abs/2406.13923v2","updated":"2025-09-04T10:10:23Z","published":"2024-06-20T01:43:08Z","title":"PIN: A Knowledge-Intensive Dataset for Paired and Interleaved Multimodal\n  Documents","summary":"  Recent advancements in large multimodal models (LMMs) have leveraged\nextensive multimodal datasets to enhance capabilities in complex\nknowledge-driven tasks. However, persistent challenges in perceptual and\nreasoning errors limit their efficacy, particularly in interpreting intricate\nvisual data and deducing multimodal relationships. To address these issues, we\nintroduce PIN (Paired and INterleaved multimodal documents), a novel data\nformat designed to foster a deeper integration of visual and textual knowledge.\nThe PIN format uniquely combines semantically rich Markdown files, which\npreserve fine-grained textual structures, with holistic overall images that\ncapture the complete document layout. Following this format, we construct and\nrelease two large-scale, open-source datasets: PIN-200M (~200 million\ndocuments) and PIN-14M (~14 million), compiled from diverse web and scientific\nsources in both English and Chinese. To maximize usability, we provide detailed\nstatistical analyses and equip the datasets with quality signals, enabling\nresearchers to easily filter and select data for specific tasks. Our work\nprovides the community with a versatile data format and substantial resources,\noffering a foundation for new research in pre-training strategies and the\ndevelopment of more powerful knowledge-intensive LMMs.\n","authors":["Junjie Wang","Yuxiang Zhang","Minghao Liu","Yin Zhang","Yatai Ji","Weihao Xuan","Nie Lin","Kang Zhu","Zhiqiang Lin","Yiming Ren","Chunyang Jiang","Yiyao Yu","Zekun Wang","Tiezhen Wang","Wenhao Huang","Jie Fu","Qunshu Liu","Yujiu Yang","Ge Zhang","Ruibin Yuan","Bei Chen","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.13923v2.pdf","comment":"Technical report v1.0"},{"id":"http://arxiv.org/abs/2507.15269v2","updated":"2025-09-04T09:36:19Z","published":"2025-07-21T06:16:27Z","title":"Conditional Video Generation for High-Efficiency Video Compression","summary":"  Perceptual studies demonstrate that conditional diffusion models excel at\nreconstructing video content aligned with human visual perception. Building on\nthis insight, we propose a video compression framework that leverages\nconditional diffusion models for perceptually optimized reconstruction.\nSpecifically, we reframe video compression as a conditional generation task,\nwhere a generative model synthesizes video from sparse, yet informative\nsignals. Our approach introduces three key modules: (1) Multi-granular\nconditioning that captures both static scene structure and dynamic\nspatio-temporal cues; (2) Compact representations designed for efficient\ntransmission without sacrificing semantic richness; (3) Multi-condition\ntraining with modality dropout and role-aware embeddings, which prevent\nover-reliance on any single modality and enhance robustness. Extensive\nexperiments show that our method significantly outperforms both traditional and\nneural codecs on perceptual quality metrics such as Fr\\'echet Video Distance\n(FVD) and LPIPS, especially under high compression ratios.\n","authors":["Fangqiu Yi","Jingyu Xu","Jiawei Shao","Chi Zhang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2507.15269v2.pdf","comment":"Critical methodology flaws invalidate key results"},{"id":"http://arxiv.org/abs/2509.01221v2","updated":"2025-09-04T09:30:16Z","published":"2025-09-01T08:06:49Z","title":"DaMoC: Efficiently Selecting the Optimal Large Language Model for\n  Fine-tuning Domain Tasks Based on Data and Model Compression","summary":"  Large language models (LLMs) excel in general tasks but struggle with\ndomain-specific ones, requiring fine-tuning with specific data. With many\nopen-source LLMs available, selecting the best model for fine-tuning downstream\ntasks is challenging, primarily focusing on how to quickly identify the optimal\nLLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses\nthis challenge by: 1) Data Level: A systematic categorization of data filtering\nmethodologies for LLMs is first established, classifying them into three\ndistinct paradigms: (1) distribution-aware methods, (2) quality-aware methods,\nand (3) hybrid approaches considering both dimensions. Further, we enhance the\ndensity of key tokens in the text achieving token compression. Subsequently, we\nuse an LLM to iterative rewrite the text to optimize its expression. 2) Model\nLevel: We use layer similarity scores to assess each layer's importance and\nremove those with lower importance. Then, we introduce a sparse merging\nparadigm to preserve as much of the original model's capability as possible.\nExtensive experiments on four datasets, medical Q&A, financial Q&A, general\nQ&A, and reading comprehension, show that we can select the optimal LLM while\nsaving approximately 20-fold in training time.\n","authors":["Wei Huang","Huang Wei","Yinggui Wang"],"pdf_url":"https://arxiv.org/pdf/2509.01221v2.pdf","comment":"Accepted by EMNLP 2025"},{"id":"http://arxiv.org/abs/2509.04051v1","updated":"2025-09-04T09:29:30Z","published":"2025-09-04T09:29:30Z","title":"Neural Video Compression with In-Loop Contextual Filtering and\n  Out-of-Loop Reconstruction Enhancement","summary":"  This paper explores the application of enhancement filtering techniques in\nneural video compression. Specifically, we categorize these techniques into\nin-loop contextual filtering and out-of-loop reconstruction enhancement based\non whether the enhanced representation affects the subsequent coding loop.\nIn-loop contextual filtering refines the temporal context by mitigating error\npropagation during frame-by-frame encoding. However, its influence on both the\ncurrent and subsequent frames poses challenges in adaptively applying filtering\nthroughout the sequence. To address this, we introduce an adaptive coding\ndecision strategy that dynamically determines filtering application during\nencoding. Additionally, out-of-loop reconstruction enhancement is employed to\nrefine the quality of reconstructed frames, providing a simple yet effective\nimprovement in coding efficiency. To the best of our knowledge, this work\npresents the first systematic study of enhancement filtering in the context of\nconditional-based neural video compression. Extensive experiments demonstrate a\n7.71% reduction in bit rate compared to state-of-the-art neural video codecs,\nvalidating the effectiveness of the proposed approach.\n","authors":["Yaojun Wu","Chaoyi Lin","Yiming Wang","Semih Esenlik","Zhaobin Zhang","Kai Zhang","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.04051v1.pdf","comment":"9 pages, 8 figures, Accepted to ACMMM 2025"},{"id":"http://arxiv.org/abs/2509.04041v1","updated":"2025-09-04T09:21:57Z","published":"2025-09-04T09:21:57Z","title":"Oruga: An Avatar of Representational Systems Theory","summary":"  Humans use representations flexibly. We draw diagrams, change representations\nand exploit creative analogies across different domains. We want to harness\nthis kind of power and endow machines with it to make them more compatible with\nhuman use. Previously we developed Representational Systems Theory (RST) to\nstudy the structure and transformations of representations. In this paper we\npresent Oruga (caterpillar in Spanish; a symbol of transformation), an\nimplementation of various aspects of RST. Oruga consists of a core of data\nstructures corresponding to concepts in RST, a language for communicating with\nthe core, and an engine for producing transformations using a method we call\nstructure transfer. In this paper we present an overview of the core and\nlanguage of Oruga, with a brief example of the kind of transformation that\nstructure transfer can execute.\n","authors":["Daniel Raggi","Gem Stapleton","Mateja Jamnik","Aaron Stockdill","Grecia Garcia Garcia","Peter C-H. Cheng"],"pdf_url":"https://arxiv.org/pdf/2509.04041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.06052v2","updated":"2025-09-04T09:10:05Z","published":"2025-06-06T12:56:02Z","title":"CP-Bench: Evaluating Large Language Models for Constraint Modelling","summary":"  Constraint Programming (CP) is widely used to solve combinatorial problems,\nbut its core process, namely constraint modelling, requires significant\nexpertise and is considered to be a bottleneck for wider adoption. Aiming to\nalleviate this bottleneck, recent studies have explored using Large Language\nModels (LLMs) to transform combinatorial problem descriptions into executable\nconstraint models. However, the existing evaluation datasets for constraint\nmodelling are often limited to small, homogeneous, or domain-specific\ninstances, which do not capture the diversity of real-world scenarios. This\nwork addresses this gap by introducing CP-Bench, a novel benchmark that\nincludes a diverse set of well-known combinatorial problems sourced from the CP\ncommunity, structured explicitly for evaluating LLM-driven CP modelling. With\nthis dataset, and given the variety of constraint modelling frameworks, we\ncompare and evaluate the modelling capabilities of LLMs for three distinct\nconstraint modelling systems, which vary in abstraction level and underlying\nsyntax. Notably, the results show higher performance when modelling with a\nhigh-level Python-based framework. Additionally, we systematically evaluate the\nuse of prompt-based and inference-time compute methods across different LLMs,\nwhich further increase accuracy, reaching up to 70% on this highly challenging\nbenchmark.\n","authors":["Kostis Michailidis","Dimos Tsouros","Tias Guns"],"pdf_url":"https://arxiv.org/pdf/2506.06052v2.pdf","comment":"ECAI 25"},{"id":"http://arxiv.org/abs/2506.03315v2","updated":"2025-09-04T09:04:10Z","published":"2025-06-03T19:03:12Z","title":"Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum\n  as Fallback","summary":"  We study how linear orders can be employed to realise choice functions for\nwhich the set of potential choices is restricted, i.e., the possible choice is\nnot possible among the full powerset of all alternatives. In such restricted\nsettings, constructing a choice function via a relation on the alternatives is\nnot always possible. However, we show that one can always construct a choice\nfunction via a linear order on sets of alternatives, even when a fallback value\nis encoded as the minimal element in the linear order. The axiomatics of such\nchoice functions are presented for the general case and the case of\nunion-closed input restrictions. Restricted choice structures have applications\nin knowledge representation and reasoning, and here we discuss their\napplications for theory change and abstract argumentation.\n","authors":["Kai Sauerwald","Kenneth Skiba","Eduardo Fermé","Thomas Meyer"],"pdf_url":"https://arxiv.org/pdf/2506.03315v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04027v1","updated":"2025-09-04T09:02:16Z","published":"2025-09-04T09:02:16Z","title":"CoT-Space: A Theoretical Framework for Internal Slow-Thinking via\n  Reinforcement Learning","summary":"  Reinforcement Learning (RL) has become a pivotal approach for enhancing the\nreasoning capabilities of Large Language Models (LLMs). However, a significant\ntheoretical gap persists, as traditional token-level RL frameworks fail to\nalign with the reasoning-level nature of complex, multi-step thought processes\nlike Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,\na novel theoretical framework that recasts LLM reasoning from a discrete\ntoken-prediction task to an optimization process within a continuous,\nreasoning-level semantic space. By analyzing this process from both a noise\nperspective and a risk perspective, we demonstrate that the convergence to an\noptimal CoT length is a natural consequence of the fundamental trade-off\nbetween underfitting and overfitting. Furthermore, extensive experiments\nprovide strong empirical validation for our theoretical findings. Our framework\nnot only provides a coherent explanation for empirical phenomena such as\noverthinking but also offers a solid theoretical foundation to guide the future\ndevelopment of more effective and generalizable reasoning agents.\n","authors":["Zeyu Gan","Hao Yi","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2509.04027v1.pdf","comment":"Preprint Edition"},{"id":"http://arxiv.org/abs/2509.03249v2","updated":"2025-09-04T08:55:32Z","published":"2025-09-03T12:07:23Z","title":"Structure Transfer: an Inference-Based Calculus for the Transformation\n  of Representations","summary":"  Representation choice is of fundamental importance to our ability to\ncommunicate and reason effectively. A major unsolved problem, addressed in this\npaper, is how to devise representational-system (RS) agnostic techniques that\ndrive representation transformation and choice. We present a novel calculus,\ncalled structure transfer, that enables representation transformation across\ndiverse RSs. Specifically, given a source representation drawn from a source\nRS, the rules of structure transfer allow us to generate a target\nrepresentation for a target RS. The generality of structure transfer comes in\npart from its ability to ensure that the source representation and the\ngenerated target representation satisfy any specified relation (such as\nsemantic equivalence). This is done by exploiting schemas, which encode\nknowledge about RSs. Specifically, schemas can express preservation of\ninformation across relations between any pair of RSs, and this knowledge is\nused by structure transfer to derive a structure for the target representation\nwhich ensures that the desired relation holds. We formalise this using\nRepresentational Systems Theory, building on the key concept of a construction\nspace. The abstract nature of construction spaces grants them the generality to\nmodel RSs of diverse kinds, including formal languages, geometric figures and\ndiagrams, as well as informal notations. Consequently, structure transfer is a\nsystem-agnostic calculus that can be used to identify alternative\nrepresentations in a wide range of practical settings.\n","authors":["Daniel Raggi","Gem Stapleton","Mateja Jamnik","Aaron Stockdill","Grecia Garcia Garcia","Peter C-H. Cheng"],"pdf_url":"https://arxiv.org/pdf/2509.03249v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04013v1","updated":"2025-09-04T08:43:27Z","published":"2025-09-04T08:43:27Z","title":"On Robustness and Reliability of Benchmark-Based Evaluation of LLMs","summary":"  Large Language Models (LLMs) effectiveness is usually evaluated by means of\nbenchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in\ntheir original wording, thus in a fixed, standardized format. However,\nreal-world applications involve linguistic variability, requiring models to\nmaintain their effectiveness across diverse rewordings of the same question or\nquery. In this study, we systematically assess the robustness of LLMs to\nparaphrased benchmark questions and investigate whether benchmark-based\nevaluations provide a reliable measure of model capabilities. We systematically\ngenerate various paraphrases of all the questions across six different common\nbenchmarks, and measure the resulting variations in effectiveness of 34\nstate-of-the-art LLMs, of different size and effectiveness. Our findings reveal\nthat while LLM rankings remain relatively stable across paraphrased inputs,\nabsolute effectiveness scores change, and decline significantly. This suggests\nthat LLMs struggle with linguistic variability, raising concerns about their\ngeneralization abilities and evaluation methodologies. Furthermore, the\nobserved performance drop challenges the reliability of benchmark-based\nevaluations, indicating that high benchmark scores may not fully capture a\nmodel's robustness to real-world input variations. We discuss the implications\nof these findings for LLM evaluation methodologies, emphasizing the need for\nrobustness-aware benchmarks that better reflect practical deployment scenarios.\n","authors":["Riccardo Lunardi","Vincenzo Della Mea","Stefano Mizzaro","Kevin Roitero"],"pdf_url":"https://arxiv.org/pdf/2509.04013v1.pdf","comment":"Accepted at ECAI 2025"},{"id":"http://arxiv.org/abs/2509.04011v1","updated":"2025-09-04T08:42:23Z","published":"2025-09-04T08:42:23Z","title":"NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware\n  Embeddings","summary":"  We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named\nEntity Retrieval, a variant of Named Entity Recognition (NER), where the types\nof interest are not provided in advance, and a user-defined type description is\nused to retrieve documents mentioning entities of that type. Instead of relying\non fixed schemas or fine-tuned models, our method builds on internal\nrepresentations of large language models (LLMs) to embed both entity mentions\nand user-provided open-ended type descriptions into a shared semantic space. We\nshow that internal representations, specifically the value vectors from\nmid-layer transformer blocks, encode fine-grained type information more\neffectively than commonly used top-layer embeddings. To refine these\nrepresentations, we train a lightweight contrastive projection network that\naligns type-compatible entities while separating unrelated types. The resulting\nentity embeddings are compact, type-aware, and well-suited for nearest-neighbor\nsearch. Evaluated on three benchmarks, NER Retriever significantly outperforms\nboth lexical and dense sentence-level retrieval baselines. Our findings provide\nempirical support for representation selection within LLMs and demonstrate a\npractical solution for scalable, schema-free entity retrieval. The NER\nRetriever Codebase is publicly available at\nhttps://github.com/ShacharOr100/ner_retriever\n","authors":["Or Shachar","Uri Katz","Yoav Goldberg","Oren Glickman"],"pdf_url":"https://arxiv.org/pdf/2509.04011v1.pdf","comment":"Findings of EMNLP 2025"},{"id":"http://arxiv.org/abs/2509.04009v1","updated":"2025-09-04T08:40:40Z","published":"2025-09-04T08:40:40Z","title":"Detecting Regional Spurious Correlations in Vision Transformers via\n  Token Discarding","summary":"  Due to their powerful feature association capabilities, neural network-based\ncomputer vision models have the ability to detect and exploit unintended\npatterns within the data, potentially leading to correct predictions based on\nincorrect or unintended but statistically relevant signals. These clues may\nvary from simple color aberrations to small texts within the image. In\nsituations where these unintended signals align with the predictive task,\nmodels can mistakenly link these features with the task and rely on them for\nmaking predictions. This phenomenon is referred to as spurious correlations,\nwhere patterns appear to be associated with the task but are actually\ncoincidental. As a result, detection and mitigation of spurious correlations\nhave become crucial tasks for building trustworthy, reliable, and generalizable\nmachine learning models. In this work, we present a novel method to detect\nspurious correlations in vision transformers, a type of neural network\narchitecture that gained significant popularity in recent years. Using both\nsupervised and self-supervised trained models, we present large-scale\nexperiments on the ImageNet dataset demonstrating the ability of the proposed\nmethod to identify spurious correlations. We also find that, even if the same\narchitecture is used, the training methodology has a significant impact on the\nmodel's reliance on spurious correlations. Furthermore, we show that certain\nclasses in the ImageNet dataset contain spurious signals that are easily\ndetected by the models and discuss the underlying reasons for those spurious\nsignals. In light of our findings, we provide an exhaustive list of the\naforementioned images and call for caution in their use in future research\nefforts. Lastly, we present a case study investigating spurious signals in\ninvasive breast mass classification, grounding our work in real-world\nscenarios.\n","authors":["Solha Kang","Esla Timothy Anzaku","Wesley De Neve","Arnout Van Messem","Joris Vankerschaver","Francois Rameau","Utku Ozbulak"],"pdf_url":"https://arxiv.org/pdf/2509.04009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.04007v1","updated":"2025-09-04T08:38:42Z","published":"2025-09-04T08:38:42Z","title":"AutoPBO: LLM-powered Optimization for Local Search PBO Solvers","summary":"  Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling\ncombinatorial problems through pseudo-Boolean (PB) constraints. Local search\nsolvers have shown excellent performance in PBO solving, and their efficiency\nis highly dependent on their internal heuristics to guide the search. Still,\ntheir design often requires significant expert effort and manual tuning in\npractice. While Large Language Models (LLMs) have demonstrated potential in\nautomating algorithm design, their application to optimizing PBO solvers\nremains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered\nframework to automatically enhance PBO local search solvers. We conduct\nexperiments on a broad range of four public benchmarks, including one\nreal-world benchmark, a benchmark from PB competition, an integer linear\nprogramming optimization benchmark, and a crafted combinatorial benchmark, to\nevaluate the performance improvement achieved by AutoPBO and compare it with\nsix state-of-the-art competitors, including two local search PBO solvers NuPBO\nand OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed\ninteger programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates\nsignificant improvements over previous local search approaches, while\nmaintaining competitive performance compared to state-of-the-art competitors.\nThe results suggest that AutoPBO offers a promising approach to automating\nlocal search solver design.\n","authors":["Jinyuan Li","Yi Chu","Yiwen Sun","Mengchuan Zou","Shaowei Cai"],"pdf_url":"https://arxiv.org/pdf/2509.04007v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.14048v2","updated":"2025-09-04T08:38:38Z","published":"2025-03-18T09:06:33Z","title":"Beyond holography: the entropic quantum gravity foundations of image\n  processing","summary":"  Recently, thanks to the development of artificial intelligence (AI) there is\nincreasing scientific attention in establishing the connections between\ntheoretical physics and AI. Traditionally, these connections have been focusing\nmostly on the relation between string theory and image processing and involve\nimportant theoretical paradigms such as holography. Recently G. Bianconi has\nformulated the Gravity from Entropy (GfE) approach to quantum gravity in which\ngravity is derived from the geometric quantum relative entropy (GQRE) between\ntwo metrics associated with the Lorentzian spacetime. Here it is demonstrated\nthat the famous Perona-Malik algorithm for image processing is the gradient\nflow of the GfE action in its simple warm-up scenario. Specifically, this\nalgorithm is the outcome of the minimization of the GQRE between two Euclidean\nmetrics: the one of the support of the image and the one induced by the image.\nAs the Perona-Malik algorithm is known to preserve sharp contours, this implies\nthat the GfE action, does not in general lead to uniform images upon iteration\nof the gradient flow dynamics as it would be intuitively expected from entropic\nactions maximising classical entropies. Rather, the outcome of the minimization\nof the GQRE is compatible with the preservation of complex structures. These\nresults provide the geometrical and information theory foundations for the\nPerona-Malik algorithm and might contribute to establish deeper connections\nbetween GfE, machine learning and brain research.\n","authors":["Ginestra Bianconi"],"pdf_url":"https://arxiv.org/pdf/2503.14048v2.pdf","comment":"(7 pages, 1 figure)"},{"id":"http://arxiv.org/abs/2509.03995v1","updated":"2025-09-04T08:25:01Z","published":"2025-09-04T08:25:01Z","title":"RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question\n  Answering with Large Language Models","summary":"  Current temporal knowledge graph question answering (TKGQA) methods primarily\nfocus on implicit temporal constraints, lacking the capability of handling more\ncomplex temporal queries, and struggle with limited reasoning abilities and\nerror propagation in decomposition frameworks. We propose RTQA, a novel\nframework to address these challenges by enhancing reasoning over TKGs without\nrequiring training. Following recursive thinking, RTQA recursively decomposes\nquestions into sub-problems, solves them bottom-up using LLMs and TKG\nknowledge, and employs multi-path answer aggregation to improve fault\ntolerance. RTQA consists of three core components: the Temporal Question\nDecomposer, the Recursive Solver, and the Answer Aggregator. Experiments on\nMultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements\nin \"Multiple\" and \"Complex\" categories, outperforming state-of-the-art methods.\nOur code and data are available at https://github.com/zjukg/RTQA.\n","authors":["Zhaoyan Gong","Juan Li","Zhiqiang Liu","Lei Liang","Huajun Chen","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.03995v1.pdf","comment":"EMNLP 2025"},{"id":"http://arxiv.org/abs/2509.03990v1","updated":"2025-09-04T08:18:39Z","published":"2025-09-04T08:18:39Z","title":"Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility\n  for Resource-Efficient LLM Agent","summary":"  Large language model (LLM) agents achieve impressive single-task performance\nbut commonly exhibit repeated failures, inefficient exploration, and limited\ncross-task adaptability. Existing reflective strategies (e.g., Reflexion,\nReAct) improve per-episode behavior but typically produce ephemeral,\ntask-specific traces that are not reused across tasks. Reinforcement-learning\nbased alternatives can produce transferable policies but require substantial\nparameter updates and compute. In this work we introduce Meta-Policy Reflexion\n(MPR): a hybrid framework that consolidates LLM-generated reflections into a\nstructured, predicate-like Meta-Policy Memory (MPM) and applies that memory at\ninference time through two complementary mechanisms soft memory-guided decoding\nand hard rule admissibility checks(HAC). MPR (i) externalizes reusable\ncorrective knowledge without model weight updates, (ii) enforces domain\nconstraints to reduce unsafe or invalid actions, and (iii) retains the\nadaptability of language-based reflection. We formalize the MPM representation,\npresent algorithms for update and decoding, and validate the approach in a\ntext-based agent environment following the experimental protocol described in\nthe provided implementation (AlfWorld-based). Empirical results reported in the\nsupplied material indicate consistent gains in execution accuracy and\nrobustness when compared to Reflexion baselines; rule admissibility further\nimproves stability. We analyze mechanisms that explain these gains, discuss\nscalability and failure modes, and outline future directions for multimodal and\nmulti?agent extensions.\n","authors":["Chunlong Wu","Zhibo Qu"],"pdf_url":"https://arxiv.org/pdf/2509.03990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03986v1","updated":"2025-09-04T08:13:06Z","published":"2025-09-04T08:13:06Z","title":"Promptception: How Sensitive Are Large Multimodal Models to Prompts?","summary":"  Despite the success of Large Multimodal Models (LMMs) in recent years, prompt\ndesign for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly\nunderstood. We show that even minor variations in prompt phrasing and structure\ncan lead to accuracy deviations of up to 15% for certain prompts and models.\nThis variability poses a challenge for transparent and fair LMM evaluation, as\nmodels often report their best-case performance using carefully selected\nprompts. To address this, we introduce Promptception, a systematic framework\nfor evaluating prompt sensitivity in LMMs. It consists of 61 prompt types,\nspanning 15 categories and 6 supercategories, each targeting specific aspects\nof prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight\nopen-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks:\nMMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit\ngreater sensitivity to prompt phrasing, reflecting tighter alignment with\ninstruction semantics, while open-source models are steadier but struggle with\nnuanced and complex phrasing. Based on this analysis, we propose Prompting\nPrinciples tailored to proprietary and open-source LMMs, enabling more robust\nand fair model evaluation.\n","authors":["Mohamed Insaf Ismithdeen","Muhammad Uzair Khattak","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2509.03986v1.pdf","comment":"Accepted to EMNLP 2025"},{"id":"http://arxiv.org/abs/2509.03985v1","updated":"2025-09-04T08:12:06Z","published":"2025-09-04T08:12:06Z","title":"NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language\n  Models","summary":"  In deployment and application, large language models (LLMs) typically undergo\nsafety alignment to prevent illegal and unethical outputs. However, the\ncontinuous advancement of jailbreak attack techniques, designed to bypass\nsafety mechanisms with adversarial prompts, has placed increasing pressure on\nthe security defenses of LLMs. Strengthening resistance to jailbreak attacks\nrequires an in-depth understanding of the security mechanisms and\nvulnerabilities of LLMs. However, the vast number of parameters and complex\nstructure of LLMs make analyzing security weaknesses from an internal\nperspective a challenging task. This paper presents NeuroBreak, a top-down\njailbreak analysis system designed to analyze neuron-level safety mechanisms\nand mitigate vulnerabilities. We carefully design system requirements through\ncollaboration with three experts in the field of AI security. The system\nprovides a comprehensive analysis of various jailbreak attack methods. By\nincorporating layer-wise representation probing analysis, NeuroBreak offers a\nnovel perspective on the model's decision-making process throughout its\ngeneration steps. Furthermore, the system supports the analysis of critical\nneurons from both semantic and functional perspectives, facilitating a deeper\nexploration of security mechanisms. We conduct quantitative evaluations and\ncase studies to verify the effectiveness of our system, offering mechanistic\ninsights for developing next-generation defense strategies against evolving\njailbreak attacks.\n","authors":["Chuhan Zhang","Ye Zhang","Bowen Shi","Yuyou Gan","Tianyu Du","Shouling Ji","Dazhan Deng","Yingcai Wu"],"pdf_url":"https://arxiv.org/pdf/2509.03985v1.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2509.03973v1","updated":"2025-09-04T07:58:52Z","published":"2025-09-04T07:58:52Z","title":"SAC-MIL: Spatial-Aware Correlated Multiple Instance Learning for\n  Histopathology Whole Slide Image Classification","summary":"  We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for\nperforming WSI classification. SAC-MIL consists of a positional encoding module\nto encode position information and a SAC block to perform full instance\ncorrelations. The positional encoding module utilizes the instance coordinates\nwithin the slide to encode the spatial relationships instead of the instance\nindex in the input WSI sequence. The positional encoding module can also handle\nthe length extrapolation issue where the training and testing sequences have\ndifferent lengths. The SAC block is an MLP-based method that performs full\ninstance correlation in linear time complexity with respect to the sequence\nlength. Due to the simple structure of MLP, it is easy to deploy since it does\nnot require custom CUDA kernels, compared to Transformer-based methods for WSI\nclassification. SAC-MIL has achieved state-of-the-art performance on the\nCAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon\nacceptance.\n","authors":["Yu Bai","Zitong Yu","Haowen Tian","Xijing Wang","Shuo Yan","Lin Wang","Honglin Li","Xitong Ling","Bo Zhang","Zheng Zhang","Wufan Wang","Hui Gao","Xiangyang Gong","Wendong Wang"],"pdf_url":"https://arxiv.org/pdf/2509.03973v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03972v1","updated":"2025-09-04T07:56:24Z","published":"2025-09-04T07:56:24Z","title":"Expanding Foundational Language Capabilities in Open-Source LLMs through\n  a Korean Case Study","summary":"  We introduce Llama-3-Motif, a language model consisting of 102 billion\nparameters, specifically designed to enhance Korean capabilities while\nretaining strong performance in English. Developed on the Llama 3 architecture,\nLlama-3-Motif employs advanced training techniques, including LlamaPro and\nMasked Structure Growth, to effectively scale the model without altering its\ncore Transformer architecture. Using the MoAI platform for efficient training\nacross hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully\ncurated dataset that maintains a balanced ratio of Korean and English data.\nLlama-3-Motif shows decent performance on Korean-specific benchmarks,\noutperforming existing models and achieving results comparable to GPT-4.\n","authors":["Junghwan Lim","Gangwon Jo","Sungmin Lee","Jiyoung Park","Dongseok Kim","Jihwan Kim","Junhyeok Lee","Wai Ting Cheung","Dahye Choi","Kibong Choi","Jaeyeon Huh","Beomgyu Kim","Jangwoong Kim","Taehyun Kim","Haesol Lee","Jeesoo Lee","Dongpin Oh","Changseok Song","Daewon Suh"],"pdf_url":"https://arxiv.org/pdf/2509.03972v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.08715v3","updated":"2025-09-04T07:56:00Z","published":"2025-08-12T07:58:48Z","title":"MultiGen: Child-Friendly Multilingual Speech Generator with LLMs","summary":"  Generative speech models have demonstrated significant potential in improving\nhuman-machine interactions, offering valuable real-world applications such as\nlanguage learning for children. However, achieving high-quality, child-friendly\nspeech generation remains challenging, particularly for low-resource languages\nacross diverse languages and cultural contexts. In this paper, we propose\nMultiGen, a multilingual speech generation model with child-friendly\ninteraction, leveraging LLM architecture for speech generation tailored for\nlow-resource languages. We propose to integrate age-appropriate multilingual\nspeech generation using LLM architectures, which can be used to facilitate\nyoung children's communication with AI systems through culturally relevant\ncontext in three low-resource languages: Singaporean accent Mandarin, Malay,\nand Tamil. Experimental results from both objective metrics and subjective\nevaluations demonstrate the superior performance of the proposed MultiGen\ncompared to baseline methods.\n","authors":["Xiaoxue Gao","Huayun Zhang","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2508.08715v3.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2509.00813v2","updated":"2025-09-04T07:41:31Z","published":"2025-08-31T12:14:30Z","title":"AImoclips: A Benchmark for Evaluating Emotion Conveyance in\n  Text-to-Music Generation","summary":"  Recent advances in text-to-music (TTM) generation have enabled controllable\nand expressive music creation using natural language prompts. However, the\nemotional fidelity of TTM systems remains largely underexplored compared to\nhuman preference or text alignment. In this study, we introduce AImoclips, a\nbenchmark for evaluating how well TTM systems convey intended emotions to human\nlisteners, covering both open-source and commercial models. We selected 12\nemotion intents spanning four quadrants of the valence-arousal space, and used\nsix state-of-the-art TTM systems to generate over 1,000 music clips. A total of\n111 participants rated the perceived valence and arousal of each clip on a\n9-point Likert scale. Our results show that commercial systems tend to produce\nmusic perceived as more pleasant than intended, while open-source systems tend\nto perform the opposite. Emotions are more accurately conveyed under\nhigh-arousal conditions across all models. Additionally, all systems exhibit a\nbias toward emotional neutrality, highlighting a key limitation in affective\ncontrollability. This benchmark offers valuable insights into model-specific\nemotion rendering characteristics and supports future development of\nemotionally aligned TTM systems.\n","authors":["Gyehun Go","Satbyul Han","Ahyeon Choi","Eunjin Choi","Juhan Nam","Jeong Mi Park"],"pdf_url":"https://arxiv.org/pdf/2509.00813v2.pdf","comment":"to be published in HCMIR25: 3rd Workshop on Human-Centric Music\n  Information Research"},{"id":"http://arxiv.org/abs/2509.03961v1","updated":"2025-09-04T07:39:18Z","published":"2025-09-04T07:39:18Z","title":"Multimodal Feature Fusion Network with Text Difference Enhancement for\n  Remote Sensing Change Detection","summary":"  Although deep learning has advanced remote sensing change detection (RSCD),\nmost methods rely solely on image modality, limiting feature representation,\nchange pattern modeling, and generalization especially under illumination and\nnoise disturbances. To address this, we propose MMChange, a multimodal RSCD\nmethod that combines image and text modalities to enhance accuracy and\nrobustness. An Image Feature Refinement (IFR) module is introduced to highlight\nkey regions and suppress environmental noise. To overcome the semantic\nlimitations of image features, we employ a vision language model (VLM) to\ngenerate semantic descriptions of bitemporal images. A Textual Difference\nEnhancement (TDE) module then captures fine grained semantic shifts, guiding\nthe model toward meaningful changes. To bridge the heterogeneity between\nmodalities, we design an Image Text Feature Fusion (ITFF) module that enables\ndeep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and\nSYSUCD demonstrate that MMChange consistently surpasses state of the art\nmethods across multiple metrics, validating its effectiveness for multimodal\nRSCD. Code is available at: https://github.com/yikuizhai/MMChange.\n","authors":["Yijun Zhou","Yikui Zhai","Zilu Ying","Tingfeng Xian","Wenlve Zhou","Zhiheng Zhou","Xiaolin Tian","Xudong Jia","Hongsheng Zhang","C. L. Philip Chen"],"pdf_url":"https://arxiv.org/pdf/2509.03961v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.06289v2","updated":"2025-09-04T07:35:48Z","published":"2025-02-10T09:31:39Z","title":"Is an Ultra Large Natural Image-Based Foundation Model Superior to a\n  Retina-Specific Model for Detecting Ocular and Systemic Diseases?","summary":"  The advent of foundation models (FMs) is transforming medical domain. In\nophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4\nmillion natural images and 1.6 million retinal images, has demonstrated high\nadaptability across clinical applications. Conversely, DINOv2, a\ngeneral-purpose vision FM pre-trained on 142 million natural images, has shown\npromise in non-medical domains. However, its applicability to clinical tasks\nremains underexplored. To address this, we conducted head-to-head evaluations\nby fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular\ndisease detection and systemic disease prediction tasks, across eight\nstandardized open-source ocular datasets, as well as the Moorfields AlzEye and\nthe UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting\ndiabetic retinopathy (AUROC=0.850-0.952 vs 0.823-0.944, across three datasets,\nall P<=0.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In\nglaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940,\nP<0.001). Conversely, RETFound achieved superior performance over all DINOv2\nmodels in predicting heart failure, myocardial infarction, and ischaemic stroke\n(AUROC=0.732-0.796 vs 0.663-0.771, all P<0.001). These trends persisted even\nwith 10% of the fine-tuning data. These findings showcase the distinct\nscenarios where general-purpose and domain-specific FMs excel, highlighting the\nimportance of aligning FM selection with task-specific requirements to optimise\nclinical performance.\n","authors":["Qingshan Hou","Yukun Zhou","Jocelyn Hui Lin Goh","Ke Zou","Samantha Min Er Yew","Sahana Srinivasan","Meng Wang","Thaddaeus Lo","Xiaofeng Lei","Siegfried K. Wagner","Mark A. Chia","Dawei Yang","Hongyang Jiang","An Ran Ran","Rui Santos","Gabor Mark Somfai","Juan Helen Zhou","Haoyu Chen","Qingyu Chen","Carol Y. Cheung","Pearse A. Keane","Yih Chung Tham"],"pdf_url":"https://arxiv.org/pdf/2502.06289v2.pdf","comment":"Accepted by Ophthalmology Science and is currently in press"},{"id":"http://arxiv.org/abs/2509.03957v1","updated":"2025-09-04T07:33:44Z","published":"2025-09-04T07:33:44Z","title":"CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese\n  Misinformation Fact-Checking","summary":"  The effectiveness of large language models (LLMs) to fact-check\nmisinformation remains uncertain, despite their growing use. To this end, we\npresent CANDY, a benchmark designed to systematically evaluate the capabilities\nand limitations of LLMs in fact-checking Chinese misinformation. Specifically,\nwe curate a carefully annotated dataset of ~20k instances. Our analysis shows\nthat current LLMs exhibit limitations in generating accurate fact-checking\nconclusions, even when enhanced with chain-of-thought reasoning and few-shot\nprompting. To understand these limitations, we develop a taxonomy to categorize\nflawed LLM-generated explanations for their conclusions and identify factual\nfabrication as the most common failure mode. Although LLMs alone are unreliable\nfor fact-checking, our findings indicate their considerable potential to\naugment human performance when deployed as assistive tools in scenarios. Our\ndataset and code can be accessed at https://github.com/SCUNLP/CANDY\n","authors":["Ruiling Guo","Xinwei Yang","Chen Huang","Tong Zhang","Yong Hu"],"pdf_url":"https://arxiv.org/pdf/2509.03957v1.pdf","comment":"Findings of EMNLP 2025"},{"id":"http://arxiv.org/abs/2509.03956v1","updated":"2025-09-04T07:32:16Z","published":"2025-09-04T07:32:16Z","title":"World Model Implanting for Test-time Adaptation of Embodied Agents","summary":"  In embodied AI, a persistent challenge is enabling agents to robustly adapt\nto novel domains without requiring extensive data collection or retraining. To\naddress this, we present a world model implanting framework (WorMI) that\ncombines the reasoning capabilities of large language models (LLMs) with\nindependently learned, domain-specific world models through test-time\ncomposition. By allowing seamless implantation and removal of the world models,\nthe embodied agent's policy achieves and maintains cross-domain adaptability.\nIn the WorMI framework, we employ a prototype-based world model retrieval\napproach, utilizing efficient trajectory-based abstract representation\nmatching, to incorporate relevant models into test-time composition. We also\ndevelop a world-wise compound attention method that not only integrates the\nknowledge from the retrieved world models but also aligns their intermediate\nrepresentations with the reasoning model's representation within the agent's\npolicy. This framework design effectively fuses domain-specific knowledge from\nmultiple world models, ensuring robust adaptation to unseen domains. We\nevaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating\nsuperior zero-shot and few-shot performance compared to several LLM-based\napproaches across a range of unseen domains. These results highlight the\nframeworks potential for scalable, real-world deployment in embodied agent\nscenarios where adaptability and data efficiency are essential.\n","authors":["Minjong Yoo","Jinwoo Jang","Sihyung Yoon","Honguk Woo"],"pdf_url":"https://arxiv.org/pdf/2509.03956v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03953v1","updated":"2025-09-04T07:27:27Z","published":"2025-09-04T07:27:27Z","title":"Handling Infinite Domain Parameters in Planning Through Best-First\n  Search with Delayed Partial Expansions","summary":"  In automated planning, control parameters extend standard action\nrepresentations through the introduction of continuous numeric decision\nvariables. Existing state-of-the-art approaches have primarily handled control\nparameters as embedded constraints alongside other temporal and numeric\nrestrictions, and thus have implicitly treated them as additional constraints\nrather than as decision points in the search space. In this paper, we propose\nan efficient alternative that explicitly handles control parameters as true\ndecision points within a systematic search scheme. We develop a best-first,\nheuristic search algorithm that operates over infinite decision spaces defined\nby control parameters and prove a notion of completeness in the limit under\ncertain conditions. Our algorithm leverages the concept of delayed partial\nexpansion, where a state is not fully expanded but instead incrementally\nexpands a subset of its successors. Our results demonstrate that this novel\nsearch algorithm is a competitive alternative to existing approaches for\nsolving planning problems involving control parameters.\n","authors":["Ángel Aso-Mollar","Diego Aineto","Enrico Scala","Eva Onaindia"],"pdf_url":"https://arxiv.org/pdf/2509.03953v1.pdf","comment":"To appear in the Proceedings of the Thirty-Fourth International Joint\n  Conference on Artificial Intelligence (IJCAI 2025)"},{"id":"http://arxiv.org/abs/2509.03950v1","updated":"2025-09-04T07:21:37Z","published":"2025-09-04T07:21:37Z","title":"Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer\n  Learning in a U-Net Architecture","summary":"  Pneumothorax, the abnormal accumulation of air in the pleural space, can be\nlife-threatening if undetected. Chest X-rays are the first-line diagnostic\ntool, but small cases may be subtle. We propose an automated deep-learning\npipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax\nregions. Trained on the SIIM-ACR dataset with data augmentation and a combined\nbinary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and\nDice score of 0.8241 on the independent PTX-498 dataset. These results\ndemonstrate that the model can accurately localize pneumothoraces and support\nradiologists.\n","authors":["Alvaro Aranibar Roque","Helga Sebastian"],"pdf_url":"https://arxiv.org/pdf/2509.03950v1.pdf","comment":"10 page, 5 figures"},{"id":"http://arxiv.org/abs/2509.03940v1","updated":"2025-09-04T07:03:46Z","published":"2025-09-04T07:03:46Z","title":"VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based\n  Role-Playing Agents","summary":"  Recent significant advancements in Large Language Models (LLMs) have greatly\npropelled the development of Role-Playing Conversational Agents (RPCAs). These\nsystems aim to create immersive user experiences through consistent persona\nadoption. However, current RPCA research faces dual limitations. First,\nexisting work predominantly focuses on the textual modality, entirely\noverlooking critical paralinguistic features including intonation, prosody, and\nrhythm in speech, which are essential for conveying character emotions and\nshaping vivid identities. Second, the speech-based role-playing domain suffers\nfrom a long-standing lack of standardized evaluation benchmarks. Most current\nspoken dialogue datasets target only fundamental capability assessments,\nfeaturing thinly sketched or ill-defined character profiles. Consequently, they\nfail to effectively quantify model performance on core competencies like\nlong-term persona consistency. To address this critical gap, we introduce\nVoxRole, the first comprehensive benchmark specifically designed for the\nevaluation of speech-based RPCAs. The benchmark comprises 13335 multi-turn\ndialogues, totaling 65.6 hours of speech from 1228 unique characters across 261\nmovies. To construct this resource, we propose a novel two-stage automated\npipeline that first aligns movie audio with scripts and subsequently employs an\nLLM to systematically build multi-dimensional profiles for each character.\nLeveraging VoxRole, we conduct a multi-dimensional evaluation of contemporary\nspoken dialogue models, revealing crucial insights into their respective\nstrengths and limitations in maintaining persona consistency.\n","authors":["Weihao Wu","Liang Cao","Xinyu Wu","Zhiwei Lin","Rui Niu","Jingbei Li","Zhiyong Wu"],"pdf_url":"https://arxiv.org/pdf/2509.03940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03937v1","updated":"2025-09-04T06:55:46Z","published":"2025-09-04T06:55:46Z","title":"SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by\n  Self-Play Fine-Tuning","summary":"  Despite the significant advancements of self-play fine-tuning (SPIN), which\ncan transform a weak large language model (LLM) into a strong one through\ncompetitive interactions between models of varying capabilities, it still faces\nchallenges in the Text-to-SQL task. SPIN does not generate new information, and\nthe large number of correct SQL queries produced by the opponent model during\nself-play reduces the main model's ability to generate accurate SQL queries. To\naddress this challenge, we propose a new self-play fine-tuning method tailored\nfor the Text-to-SQL task, called SPFT-SQL. Prior to self-play, we introduce a\nverification-based iterative fine-tuning approach, which synthesizes\nhigh-quality fine-tuning data iteratively based on the database schema and\nvalidation feedback to enhance model performance, while building a model base\nwith varying capabilities. During the self-play fine-tuning phase, we propose\nan error-driven loss method that incentivizes incorrect outputs from the\nopponent model, enabling the main model to distinguish between correct SQL and\nerroneous SQL generated by the opponent model, thereby improving its ability to\ngenerate correct SQL. Extensive experiments and in-depth analyses on six\nopen-source LLMs and five widely used benchmarks demonstrate that our approach\noutperforms existing state-of-the-art (SOTA) methods.\n","authors":["Yuhao Zhang","Shaoming Duan","Jinhang Su","Chuanyi Liu","Peiyi Han"],"pdf_url":"https://arxiv.org/pdf/2509.03937v1.pdf","comment":"EMNLP 2025 Findings"},{"id":"http://arxiv.org/abs/2509.03934v1","updated":"2025-09-04T06:50:47Z","published":"2025-09-04T06:50:47Z","title":"SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented\n  Generation via Distribution Self-Alignment","summary":"  Recent advancements in large language models (LLMs) have revolutionized\nnatural language processing through their remarkable capabilities in\nunderstanding and executing diverse tasks. While supervised fine-tuning,\nparticularly in Retrieval-Augmented Generation (RAG) scenarios, effectively\nenhances task-specific performance, it often leads to catastrophic forgetting,\nwhere models lose their previously acquired knowledge and general capabilities.\nExisting solutions either require access to general instruction data or face\nlimitations in preserving the model's original distribution. To overcome these\nlimitations, we propose SelfAug, a self-distribution alignment method that\naligns input sequence logits to preserve the model's semantic distribution,\nthereby mitigating catastrophic forgetting and improving downstream\nperformance. Extensive experiments demonstrate that SelfAug achieves a superior\nbalance between downstream learning and general capability retention. Our\ncomprehensive empirical analysis reveals a direct correlation between\ndistribution shifts and the severity of catastrophic forgetting in RAG\nscenarios, highlighting how the absence of RAG capabilities in general\ninstruction tuning leads to significant distribution shifts during fine-tuning.\nOur findings not only advance the understanding of catastrophic forgetting in\nRAG contexts but also provide a practical solution applicable across diverse\nfine-tuning scenarios. Our code is publicly available at\nhttps://github.com/USTC-StarTeam/SelfAug.\n","authors":["Yuqing Huang","Rongyang Zhang","Qimeng Wang","Chengqiang Lu","Yan Gao","Yi Wu","Yao Hu","Xuyang Zhi","Guiquan Liu","Xin Li","Hao Wang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2509.03934v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.12722v2","updated":"2025-09-04T06:43:22Z","published":"2024-12-17T09:38:58Z","title":"Defending LVLMs Against Vision Attacks through Partial-Perception\n  Supervision","summary":"  Recent studies have raised significant concerns regarding the vulnerability\nof Large Vision Language Models (LVLMs) to maliciously injected or perturbed\ninput images, which can mislead their responses. Existing defense methods show\nthat such vision attacks are sensitive to image modifications especially\ncropping, using majority voting across responses of modified images as\ncorrected responses. However, these modifications often result in partial\nimages and distort the semantics, which reduces response quality on clean\nimages after voting. Instead of directly using responses from partial images\nfor voting, we investigate using them to supervise the LVLM's responses to the\noriginal images. We propose a black-box, training-free method called DPS\n(Defense through Partial-Perception Supervision). In this approach, the model\nis prompted using the responses generated by a model that perceives only a\npartial image. With DPS, the model can adjust its response based on partial\nimage understanding when under attack, while confidently maintaining its\noriginal response for clean input. Our findings show that the weak model can\nsupervise the strong model: when faced with an attacked input, the strong model\nbecomes less confident and adjusts its response based on the weak model's\npartial understanding, effectively defending against the attack. With clean\ninput, it confidently maintains its original response. Empirical experiments\nshow our method outperforms the baseline, cutting the average attack success\nrate by 76.3% across six datasets on three popular models.\n","authors":["Qi Zhou","Tianlin Li","Qing Guo","Dongxia Wang","Yun Lin","Yang Liu","Jin Song Dong"],"pdf_url":"https://arxiv.org/pdf/2412.12722v2.pdf","comment":"Accepted to ICML 2025"},{"id":"http://arxiv.org/abs/2508.20117v2","updated":"2025-09-04T06:26:17Z","published":"2025-08-21T11:17:24Z","title":"Is Artificial Intelligence Reshaping the Landscape of the International\n  Academic Community of Geosciences?","summary":"  Through bibliometric analysis and topic modeling, we find that artificial\nintelligence (AI) is positively transforming geosciences research, with a\nnotable increase in AI-related scientific output in recent years. We are\nencouraged to observe that earth scientists from developing countries have\ngained better visibility in the recent AI for Science (AI4S) paradigm and that\nAI is also improving the landscape of international collaboration in\ngeoscience-related research.\n","authors":["Liang Li","Yuntian Li","Wenxin Zhao","Shan Ye","Yun Lu"],"pdf_url":"https://arxiv.org/pdf/2508.20117v2.pdf","comment":"miscommunication in the authorization process from the first author"},{"id":"http://arxiv.org/abs/2509.03918v1","updated":"2025-09-04T06:13:28Z","published":"2025-09-04T06:13:28Z","title":"MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question\n  Answering","summary":"  Complex Question Answering (QA) is a fundamental and challenging task in NLP.\nWhile large language models (LLMs) exhibit impressive performance in QA, they\nsuffer from significant performance degradation when facing complex and\nabstract QA tasks due to insufficient reasoning capabilities. Works such as\nChain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning\nabilities, but they face issues such as in-layer redundancy in tree structures\nand single paths in chain structures. Although some studies utilize\nRetrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the\nchallenge of effectively utilizing large amounts of information involving\nmultiple entities and hops remains critical. To address this, we propose the\nMatrix of Thought (MoT), a novel and efficient LLM thought structure. MoT\nexplores the problem in both horizontal and vertical dimensions through the\n\"column-cell communication\" mechanism, enabling LLMs to actively engage in\nmulti-strategy and deep-level thinking, reducing redundancy within the column\ncells and enhancing reasoning capabilities. Furthermore, we develop a\nfact-correction mechanism by constructing knowledge units from retrieved\nknowledge graph triples and raw text to enhance the initial knowledge for LLM\nreasoning and correct erroneous answers. This leads to the development of an\nefficient and accurate QA framework (MTQA). Experimental results show that our\nframework outperforms state-of-the-art methods on four widely-used datasets in\nterms of F1 and EM scores, with reasoning time only 14.4\\% of the baseline\nmethods, demonstrating both its efficiency and accuracy. The code for this\nframework is available at https://github.com/lyfiter/mtqa.\n","authors":["Fengxiao Tang","Yufeng Li","Zongzong Wu","Ming Zhao"],"pdf_url":"https://arxiv.org/pdf/2509.03918v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.09523v2","updated":"2025-09-04T06:03:10Z","published":"2025-07-13T07:34:33Z","title":"An Analysis of Action-Value Temporal-Difference Methods That Learn State\n  Values","summary":"  The hallmark feature of temporal-difference (TD) learning is bootstrapping:\nusing value predictions to generate new value predictions. The vast majority of\nTD methods for control learn a policy by bootstrapping from a single\naction-value function (e.g., Q-learning and Sarsa). Significantly less\nattention has been given to methods that bootstrap from two asymmetric value\nfunctions: i.e., methods that learn state values as an intermediate step in\nlearning action values. Existing algorithms in this vein can be categorized as\neither QV-learning or AV-learning. Though these algorithms have been\ninvestigated to some degree in prior work, it remains unclear if and when it is\nadvantageous to learn two value functions instead of just one -- and whether\nsuch approaches are theoretically sound in general. In this paper, we analyze\nthese algorithmic families in terms of convergence and sample efficiency. We\nfind that while both families are more efficient than Expected Sarsa in the\nprediction setting, only AV-learning methods offer any major benefit over\nQ-learning in the control setting. Finally, we introduce a new AV-learning\nalgorithm called Regularized Dueling Q-learning (RDQ), which significantly\noutperforms Dueling DQN in the MinAtar benchmark.\n","authors":["Brett Daley","Prabhat Nagarajan","Martha White","Marlos C. Machado"],"pdf_url":"https://arxiv.org/pdf/2507.09523v2.pdf","comment":"Published in RLC/RLJ 2025. Camera-ready version"},{"id":"http://arxiv.org/abs/2509.03906v1","updated":"2025-09-04T06:00:04Z","published":"2025-09-04T06:00:04Z","title":"A Foundation Model for Chest X-ray Interpretation with Grounded\n  Reasoning via Online Reinforcement Learning","summary":"  Medical foundation models (FMs) have shown tremendous promise amid the rapid\nadvancements in artificial intelligence (AI) technologies. However, current\nmedical FMs typically generate answers in a black-box manner, lacking\ntransparent reasoning processes and locally grounded interpretability, which\nhinders their practical clinical deployments. To this end, we introduce\nDeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It\nleverages a sequential training pipeline: initially fine-tuned on curated CXR\ninstruction data to equip with fundamental CXR interpretation capabilities,\nthen exposed to high-quality synthetic reasoning samples to enable cold-start\nreasoning, and finally refined via online reinforcement learning to enhance\nboth grounded reasoning quality and generation performance. Thus, the model\nproduces both an answer and reasoning steps tied to the image's local regions\nfor each query. Quantitative evaluation demonstrates substantial improvements\nin report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and\nvisual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent)\ntasks. To facilitate robust assessment, we propose Report Arena, a benchmarking\nframework using advanced language models to evaluate answer quality, further\nhighlighting the superiority of DeepMedix-R1. Expert review of generated\nreasoning steps reveals greater interpretability and clinical plausibility\ncompared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall\npreference). Collectively, our work advances medical FM development toward\nholistic, transparent, and clinically actionable modeling for CXR\ninterpretation.\n","authors":["Qika Lin","Yifan Zhu","Bin Pu","Ling Huang","Haoran Luo","Jingying Ma","Zhen Peng","Tianzhe Zhao","Fangzhi Xu","Jian Zhang","Kai He","Zhonghong Ou","Swapnil Mishra","Mengling Feng"],"pdf_url":"https://arxiv.org/pdf/2509.03906v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2509.03898v1","updated":"2025-09-04T05:45:06Z","published":"2025-09-04T05:45:06Z","title":"Diffusion Generative Models Meet Compressed Sensing, with Applications\n  to Image Data and Financial Time Series","summary":"  This paper develops dimension reduction techniques for accelerating diffusion\nmodel inference in the context of synthetic data generation. The idea is to\nintegrate compressed sensing into diffusion models: (i) compress the data into\na latent space, (ii) train a diffusion model in the latent space, and (iii)\napply a compressed sensing algorithm to the samples generated in the latent\nspace, facilitating the efficiency of both model training and inference. Under\nsuitable sparsity assumptions on data, the proposed algorithm is proved to\nenjoy faster convergence by combining diffusion model inference with sparse\nrecovery. As a byproduct, we obtain an optimal value for the latent space\ndimension. We also conduct numerical experiments on a range of datasets,\nincluding image data (handwritten digits, medical images, and climate data) and\nfinancial time series for stress testing.\n","authors":["Zhengyi Guo","Jiatu Li","Wenpin Tang","David D. Yao"],"pdf_url":"https://arxiv.org/pdf/2509.03898v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03890v1","updated":"2025-09-04T05:22:25Z","published":"2025-09-04T05:22:25Z","title":"FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer\n  Marketplace","summary":"  The emergence of agentic AI, powered by Large Language Models (LLMs), marks a\nparadigm shift from reactive generative systems to proactive, goal-oriented\nautonomous agents capable of sophisticated planning, memory, and tool use. This\nevolution presents a novel opportunity to address long-standing challenges in\ncomplex digital environments. Core tasks on Consumer-to-Consumer (C2C)\ne-commerce platforms often require users to navigate complex Graphical User\nInterfaces (GUIs), making the experience time-consuming for both buyers and\nsellers. This paper introduces a novel approach to simplify these interactions\nthrough an LLM-powered agentic assistant. This agent functions as a new,\nconversational entry point to the marketplace, shifting the primary interaction\nmodel from a complex GUI to an intuitive AI agent. By interpreting natural\nlanguage commands, the agent automates key high-friction workflows. For\nsellers, this includes simplified updating and renewal of listings, and the\nability to send bulk messages. For buyers, the agent facilitates a more\nefficient product discovery process through conversational search. We present\nthe architecture for Facebook Marketplace Assistant (FaMA), arguing that this\nagentic, conversational paradigm provides a lightweight and more accessible\nalternative to traditional app interfaces, allowing users to manage their\nmarketplace activities with greater efficiency. Experiments show FaMA achieves\na 98% task success rate on solving complex tasks on the marketplace and enables\nup to a 2x speedup on interaction time.\n","authors":["Yineng Yan","Xidong Wang","Jin Seng Cheng","Ran Hu","Wentao Guan","Nahid Farahmand","Hengte Lin","Yue Li"],"pdf_url":"https://arxiv.org/pdf/2509.03890v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03889v1","updated":"2025-09-04T05:16:56Z","published":"2025-09-04T05:16:56Z","title":"Reactive In-Air Clothing Manipulation with Confidence-Aware Dense\n  Correspondence and Visuotactile Affordance","summary":"  Manipulating clothing is challenging due to complex configurations, variable\nmaterial dynamics, and frequent self-occlusion. Prior systems often flatten\ngarments or assume visibility of key features. We present a dual-arm\nvisuotactile framework that combines confidence-aware dense visual\ncorrespondence and tactile-supervised grasp affordance to operate directly on\ncrumpled and suspended garments. The correspondence model is trained on a\ncustom, high-fidelity simulated dataset using a distributional loss that\ncaptures cloth symmetries and generates correspondence confidence estimates.\nThese estimates guide a reactive state machine that adapts folding strategies\nbased on perceptual uncertainty. In parallel, a visuotactile grasp affordance\nnetwork, self-supervised using high-resolution tactile feedback, determines\nwhich regions are physically graspable. The same tactile classifier is used\nduring execution for real-time grasp validation. By deferring action in\nlow-confidence states, the system handles highly occluded table-top and in-air\nconfigurations. We demonstrate our task-agnostic grasp selection module in\nfolding and hanging tasks. Moreover, our dense descriptors provide a reusable\nintermediate representation for other planning modalities, such as extracting\ngrasp targets from human video demonstrations, paving the way for more\ngeneralizable and scalable garment manipulation.\n","authors":["Neha Sunil","Megha Tippur","Arnau Saumell","Edward Adelson","Alberto Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2509.03889v1.pdf","comment":"Accepted at CoRL 2025. Project website:\n  https://mhtippur.github.io/inairclothmanipulation/"},{"id":"http://arxiv.org/abs/2508.20293v2","updated":"2025-09-04T05:03:16Z","published":"2025-08-27T22:00:18Z","title":"Beacon: Post-Training Quantization with Integrated Grid Selection","summary":"  Quantization is a widely used compression technique for reducing the memory\nand computation costs of large pre-trained models. A key challenge in\nper-channel post-training quantization (PTQ) is selecting appropriate scaling\nfactors to replace weight values with values from a scaled integer grid.\nExisting methods typically fix the scale at the outset via heuristic tuning or\ngrid search. We propose Beacon, a simple and effective algorithm that\neliminates the need for such manual tuning. Beacon performs per-channel PTQ\ndirectly using an unscaled grid and automatically determines the optimal\nscaling factors by exploiting the geometry of scalar quantization. It does not\nrely on back-propagation or large calibration sets. Despite its simplicity and\ntuning-free nature, Beacon achieves competitive performance compared to\nstate-of-the-art methods, making it a practical solution for efficient model\ndeployment.\n","authors":["Shihao Zhang","Rayan Saab"],"pdf_url":"https://arxiv.org/pdf/2508.20293v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03884v1","updated":"2025-09-04T04:54:02Z","published":"2025-09-04T04:54:02Z","title":"Peptidomic-Based Prediction Model for Coronary Heart Disease Using a\n  Multilayer Perceptron Neural Network","summary":"  Coronary heart disease (CHD) is a leading cause of death worldwide and\ncontributes significantly to annual healthcare expenditures. To develop a\nnon-invasive diagnostic approach, we designed a model based on a multilayer\nperceptron (MLP) neural network, trained on 50 key urinary peptide biomarkers\nselected via genetic algorithms. Treatment and control groups, each comprising\n345 individuals, were balanced using the Synthetic Minority Over-sampling\nTechnique (SMOTE). The neural network was trained using a stratified validation\nstrategy. Using a network with three hidden layers of 60 neurons each and an\noutput layer of two neurons, the model achieved a precision, sensitivity, and\nspecificity of 95.67 percent, with an F1-score of 0.9565. The area under the\nROC curve (AUC) reached 0.9748 for both classes, while the Matthews correlation\ncoefficient (MCC) and Cohen's kappa coefficient were 0.9134 and 0.9131,\nrespectively, demonstrating its reliability in detecting CHD. These results\nindicate that the model provides a highly accurate and robust non-invasive\ndiagnostic tool for coronary heart disease.\n","authors":["Jesus Celis-Porras"],"pdf_url":"https://arxiv.org/pdf/2509.03884v1.pdf","comment":"14 pages, 6 figures, Submitted to arXiv for public dissemination"},{"id":"http://arxiv.org/abs/2509.03873v1","updated":"2025-09-04T04:22:36Z","published":"2025-09-04T04:22:36Z","title":"SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition","summary":"  Food recognition has gained significant attention, but the rapid emergence of\nnew dishes requires methods for recognizing unseen food categories, motivating\nZero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot\nFood Recognition (CZSFR), where cuisines and ingredients naturally align with\nattributes and objects in Compositional Zero-Shot learning (CZSL). However,\nCZSFR faces three challenges: (1) Redundant background information distracts\nmodels from learning meaningful food features, (2) Role confusion between\nstaple and side dishes leads to misclassification, and (3) Semantic bias in a\nsingle attribute can lead to confusion of understanding. Therefore, we propose\nSalientFusion, a context-aware CZSFR method with two components: SalientFormer,\nwhich removes background redundancy and uses depth features to resolve role\nconfusion; DebiasAT, which reduces the semantic bias by aligning prompts with\nvisual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we\nshow that SalientFusion achieves state-of-the-art results on these benchmarks\nand the most popular general datasets for the general CZSL. The code is\navaliable at https://github.com/Jiajun-RUC/SalientFusion.\n","authors":["Jiajun Song","Xiaoou Liu"],"pdf_url":"https://arxiv.org/pdf/2509.03873v1.pdf","comment":"34th International Conference on Artificial Neural Networks - ICANN\n  2025"},{"id":"http://arxiv.org/abs/2509.03871v1","updated":"2025-09-04T04:12:31Z","published":"2025-09-04T04:12:31Z","title":"A Comprehensive Survey on Trustworthiness in Reasoning with Large\n  Language Models","summary":"  The development of Long-CoT reasoning has advanced LLM performance across\nvarious tasks, including language understanding, complex problem solving, and\ncode generation. This paradigm enables models to generate intermediate\nreasoning steps, thereby improving both accuracy and interpretability. However,\ndespite these advancements, a comprehensive understanding of how CoT-based\nreasoning affects the trustworthiness of language models remains\nunderdeveloped. In this paper, we survey recent work on reasoning models and\nCoT techniques, focusing on five core dimensions of trustworthy reasoning:\ntruthfulness, safety, robustness, fairness, and privacy. For each aspect, we\nprovide a clear and structured overview of recent studies in chronological\norder, along with detailed analyses of their methodologies, findings, and\nlimitations. Future research directions are also appended at the end for\nreference and discussion. Overall, while reasoning techniques hold promise for\nenhancing model trustworthiness through hallucination mitigation, harmful\ncontent detection, and robustness improvement, cutting-edge reasoning models\nthemselves often suffer from comparable or even greater vulnerabilities in\nsafety, robustness, and privacy. By synthesizing these insights, we hope this\nwork serves as a valuable and timely resource for the AI safety community to\nstay informed on the latest progress in reasoning trustworthiness. A full list\nof related papers can be found at\n\\href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.\n","authors":["Yanbo Wang","Yongcan Yu","Jian Liang","Ran He"],"pdf_url":"https://arxiv.org/pdf/2509.03871v1.pdf","comment":"38 pages. This survey considers papers published up to June 30, 2025.\n  Work in progress"},{"id":"http://arxiv.org/abs/2505.03522v2","updated":"2025-09-04T04:08:12Z","published":"2025-05-06T13:35:59Z","title":"Optimization of Module Transferability in Single Image Super-Resolution:\n  Universality Assessment and Cycle Residual Blocks","summary":"  Deep learning has substantially advanced the field of Single Image\nSuper-Resolution (SISR). However, existing research has predominantly focused\non raw performance gains, with little attention paid to quantifying the\ntransferability of architectural components. In this paper, we introduce the\nconcept of \"Universality\" and its associated definitions, which extend the\ntraditional notion of \"Generalization\" to encompass the ease of transferability\nof modules. We then propose the Universality Assessment Equation (UAE), a\nmetric that quantifies how readily a given module can be transplanted across\nmodels and reveals the combined influence of multiple existing metrics on\ntransferability. Guided by the UAE results of standard residual blocks and\nother plug-and-play modules, we further design two optimized modules: the Cycle\nResidual Block (CRB) and the Depth-Wise Cycle Residual Block (DCRB). Through\ncomprehensive experiments on natural-scene benchmarks, remote-sensing datasets,\nand other low-level tasks, we demonstrate that networks embedded with the\nproposed plug-and-play modules outperform several state-of-the-art methods,\nachieving a PSNR improvement of up to 0.83 dB or enabling a 71.3% reduction in\nparameters with negligible loss in reconstruction fidelity. Similar\noptimization approaches could be applied to a broader range of basic modules,\noffering a new paradigm for the design of plug-and-play modules.\n","authors":["Haotong Cheng","Zhiqi Zhang","Hao Li","Xinshang Zhang"],"pdf_url":"https://arxiv.org/pdf/2505.03522v2.pdf","comment":"The paper has been accepted to IET Image Processing"},{"id":"http://arxiv.org/abs/2509.03863v1","updated":"2025-09-04T03:44:44Z","published":"2025-09-04T03:44:44Z","title":"Expedition & Expansion: Leveraging Semantic Representations for\n  Goal-Directed Exploration in Continuous Cellular Automata","summary":"  Discovering diverse visual patterns in continuous cellular automata (CA) is\nchallenging due to the vastness and redundancy of high-dimensional behavioral\nspaces. Traditional exploration methods like Novelty Search (NS) expand locally\nby mutating known novel solutions but often plateau when local novelty is\nexhausted, failing to reach distant, unexplored regions. We introduce\nExpedition and Expansion (E&E), a hybrid strategy where exploration alternates\nbetween local novelty-driven expansions and goal-directed expeditions. During\nexpeditions, E&E leverages a Vision-Language Model (VLM) to generate linguistic\ngoals--descriptions of interesting but hypothetical patterns that drive\nexploration toward uncharted regions. By operating in semantic spaces that\nalign with human perception, E&E both evaluates novelty and generates goals in\nconceptually meaningful ways, enhancing the interpretability and relevance of\ndiscovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich,\nemergent behaviors, E&E consistently uncovers more diverse solutions than\nexisting exploration methods. A genealogical analysis further reveals that\nsolutions originating from expeditions disproportionately influence long-term\nexploration, unlocking new behavioral niches that serve as stepping stones for\nsubsequent search. These findings highlight E&E's capacity to break through\nlocal novelty boundaries and explore behavioral landscapes in human-aligned,\ninterpretable ways, offering a promising template for open-ended exploration in\nartificial life and beyond.\n","authors":["Sina Khajehabdollahi","Gautier Hamon","Marko Cvjetko","Pierre-Yves Oudeyer","Clément Moulin-Frier","Cédric Colas"],"pdf_url":"https://arxiv.org/pdf/2509.03863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03857v1","updated":"2025-09-04T03:34:49Z","published":"2025-09-04T03:34:49Z","title":"Continuous Monitoring of Large-Scale Generative AI via Deterministic\n  Knowledge Graph Structures","summary":"  Generative AI (GEN AI) models have revolutionized diverse application domains\nbut present substantial challenges due to reliability concerns, including\nhallucinations, semantic drift, and inherent biases. These models typically\noperate as black-boxes, complicating transparent and objective evaluation.\nCurrent evaluation methods primarily depend on subjective human assessment,\nlimiting scalability, transparency, and effectiveness. This research proposes a\nsystematic methodology using deterministic and Large Language Model\n(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN\nAI reliability. We construct two parallel KGs: (i) a deterministic KG built\nusing explicit rule-based methods, predefined ontologies, domain-specific\ndictionaries, and structured entity-relation extraction rules, and (ii) an\nLLM-generated KG dynamically derived from real-time textual data streams such\nas live news articles. Utilizing real-time news streams ensures authenticity,\nmitigates biases from repetitive training, and prevents adaptive LLMs from\nbypassing predefined benchmarks through feedback memorization. To quantify\nstructural deviations and semantic discrepancies, we employ several established\nKG metrics, including Instantiated Class Ratio (ICR), Instantiated Property\nRatio (IPR), and Class Instantiation (CI). An automated real-time monitoring\nframework continuously computes deviations between deterministic and\nLLM-generated KGs. By establishing dynamic anomaly thresholds based on\nhistorical structural metric distributions, our method proactively identifies\nand flags significant deviations, thus promptly detecting semantic anomalies or\nhallucinations. This structured, metric-driven comparison between deterministic\nand dynamically generated KGs delivers a robust and scalable evaluation\nframework.\n","authors":["Kishor Datta Gupta","Mohd Ariful Haque","Hasmot Ali","Marufa Kamal","Syed Bahauddin Alam","Mohammad Ashiqur Rahman"],"pdf_url":"https://arxiv.org/pdf/2509.03857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03852v1","updated":"2025-09-04T03:28:42Z","published":"2025-09-04T03:28:42Z","title":"MillGNN: Learning Multi-Scale Lead-Lag Dependencies for Multi-Variate\n  Time Series Forecasting","summary":"  Multi-variate time series (MTS) forecasting is crucial for various\napplications. Existing methods have shown promising results owing to their\nstrong ability to capture intra- and inter-variate dependencies. However, these\nmethods often overlook lead-lag dependencies at multiple grouping scales,\nfailing to capture hierarchical lead-lag effects in complex systems. To this\nend, we propose MillGNN, a novel \\underline{g}raph \\underline{n}eural\n\\underline{n}etwork-based method that learns \\underline{m}ult\\underline{i}ple\ngrouping scale \\underline{l}ead-\\underline{l}ag dependencies for MTS\nforecasting, which can comprehensively capture lead-lag effects considering\nvariate-wise and group-wise dynamics and decays. Specifically, MillGNN\nintroduces two key innovations: (1) a scale-specific lead-lag graph learning\nmodule that integrates cross-correlation coefficients and dynamic decaying\nfeatures derived from real-time inputs and time lags to learn lead-lag\ndependencies for each scale, which can model evolving lead-lag dependencies\nwith statistical interpretability and data-driven flexibility; (2) a\nhierarchical lead-lag message passing module that passes lead-lag messages at\nmultiple grouping scales in a structured way to simultaneously propagate intra-\nand inter-scale lead-lag effects, which can capture multi-scale lead-lag\neffects with a balance of comprehensiveness and efficiency. Experimental\nresults on 11 datasets demonstrate the superiority of MillGNN for long-term and\nshort-term MTS forecasting, compared with 16 state-of-the-art methods.\n","authors":["Binqing Wu","Zongjiang Shang","Jianlong Huang","Ling Chen"],"pdf_url":"https://arxiv.org/pdf/2509.03852v1.pdf","comment":"Accepted by CIKM 2025"},{"id":"http://arxiv.org/abs/2509.03845v1","updated":"2025-09-04T03:13:11Z","published":"2025-09-04T03:13:11Z","title":"Meta-Inverse Reinforcement Learning for Mean Field Games via\n  Probabilistic Context Variables","summary":"  Designing suitable reward functions for numerous interacting intelligent\nagents is challenging in real-world applications. Inverse reinforcement\nlearning (IRL) in mean field games (MFGs) offers a practical framework to infer\nreward functions from expert demonstrations. While promising, the assumption of\nagent homogeneity limits the capability of existing methods to handle\ndemonstrations with heterogeneous and unknown objectives, which are common in\npractice. To this end, we propose a deep latent variable MFG model and an\nassociated IRL method. Critically, our method can infer rewards from different\nyet structurally similar tasks without prior knowledge about underlying\ncontexts or modifying the MFG model itself. Our experiments, conducted on\nsimulated scenarios and a real-world spatial taxi-ride pricing problem,\ndemonstrate the superiority of our approach over state-of-the-art IRL methods\nin MFGs.\n","authors":["Yang Chen","Xiao Lin","Bo Yan","Libo Zhang","Jiamou Liu","Neset Özkan Tan","Michael Witbrock"],"pdf_url":"https://arxiv.org/pdf/2509.03845v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2509.03842v1","updated":"2025-09-04T03:08:01Z","published":"2025-09-04T03:08:01Z","title":"INGRID: Intelligent Generative Robotic Design Using Large Language\n  Models","summary":"  The integration of large language models (LLMs) into robotic systems has\naccelerated progress in embodied artificial intelligence, yet current\napproaches remain constrained by existing robotic architectures, particularly\nserial mechanisms. This hardware dependency fundamentally limits the scope of\nrobotic intelligence. Here, we present INGRID (Intelligent Generative Robotic\nDesign), a framework that enables the automated design of parallel robotic\nmechanisms through deep integration with reciprocal screw theory and kinematic\nsynthesis methods. We decompose the design challenge into four progressive\ntasks: constraint analysis, kinematic joint generation, chain construction, and\ncomplete mechanism design. INGRID demonstrates the ability to generate novel\nparallel mechanisms with both fixed and variable mobility, discovering\nkinematic configurations not previously documented in the literature. We\nvalidate our approach through three case studies demonstrating how INGRID\nassists users in designing task-specific parallel robots based on desired\nmobility requirements. By bridging the gap between mechanism theory and machine\nlearning, INGRID enables researchers without specialized robotics training to\ncreate custom parallel mechanisms, thereby decoupling advances in robotic\nintelligence from hardware constraints. This work establishes a foundation for\nmechanism intelligence, where AI systems actively design robotic hardware,\npotentially transforming the development of embodied AI systems.\n","authors":["Guanglu Jia","Ceng Zhang","Gregory S. Chirikjian"],"pdf_url":"https://arxiv.org/pdf/2509.03842v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.14891v3","updated":"2025-09-04T02:53:35Z","published":"2025-02-17T03:20:52Z","title":"CoDiff: Conditional Diffusion Model for Collaborative 3D Object\n  Detection","summary":"  Collaborative 3D object detection holds significant importance in the field\nof autonomous driving, as it greatly enhances the perception capabilities of\neach individual agent by facilitating information exchange among multiple\nagents. However, in practice, due to pose estimation errors and time delays,\nthe fusion of information across agents often results in feature\nrepresentations with spatial and temporal noise, leading to detection errors.\nDiffusion models naturally have the ability to denoise noisy samples to the\nideal data, which motivates us to explore the use of diffusion models to\naddress the noise problem between multi-agent systems. In this work, we propose\nCoDiff, a novel robust collaborative perception framework that leverages the\npotential of diffusion models to generate more comprehensive and clearer\nfeature representations. To the best of our knowledge, this is the first work\nto apply diffusion models to multi-agent collaborative perception.\nSpecifically, we project high-dimensional feature map into the latent space of\na powerful pre-trained autoencoder. Within this space, individual agent\ninformation serves as a condition to guide the diffusion model's sampling. This\nprocess denoises coarse feature maps and progressively refines the fused\nfeatures. Experimental study on both simulated and real-world datasets\ndemonstrates that the proposed framework CoDiff consistently outperforms\nexisting relevant methods in terms of the collaborative object detection\nperformance, and exhibits highly desired robustness when the pose and delay\ninformation of agents is with high-level noise. The code is released at\nhttps://github.com/HuangZhe885/CoDiff\n","authors":["Zhe Huang","Shuo Wang","Yongcai Wang","Lei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.14891v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03834v1","updated":"2025-09-04T02:50:54Z","published":"2025-09-04T02:50:54Z","title":"From Leiden to Pleasure Island: The Constant Potts Model for Community\n  Detection as a Hedonic Game","summary":"  Community detection is one of the fundamental problems in data science which\nconsists of partitioning nodes into disjoint communities. We present a\ngame-theoretic perspective on the Constant Potts Model (CPM) for partitioning\nnetworks into disjoint communities, emphasizing its efficiency, robustness, and\naccuracy. Efficiency: We reinterpret CPM as a potential hedonic game by\ndecomposing its global Hamiltonian into local utility functions, where the\nlocal utility gain of each agent matches the corresponding increase in global\nutility. Leveraging this equivalence, we prove that local optimization of the\nCPM objective via better-response dynamics converges in pseudo-polynomial time\nto an equilibrium partition. Robustness: We introduce and relate two stability\ncriteria: a strict criterion based on a novel notion of robustness, requiring\nnodes to simultaneously maximize neighbors and minimize non-neighbors within\ncommunities, and a relaxed utility function based on a weighted sum of these\nobjectives, controlled by a resolution parameter. Accuracy: In community\ntracking scenarios, where initial partitions are used to bootstrap the Leiden\nalgorithm with partial ground-truth information, our experiments reveal that\nrobust partitions yield higher accuracy in recovering ground-truth communities.\n","authors":["Lucas Lopes Felipe","Konstantin Avrachenkov","Daniel Sadoc Menasche"],"pdf_url":"https://arxiv.org/pdf/2509.03834v1.pdf","comment":"Manuscript submitted to Physica A: Statistical Mechanics and its\n  Applications"},{"id":"http://arxiv.org/abs/2509.03832v1","updated":"2025-09-04T02:41:35Z","published":"2025-09-04T02:41:35Z","title":"Gravity Well Echo Chamber Modeling With An LLM-Based Confirmation Bias\n  Model","summary":"  Social media echo chambers play a central role in the spread of\nmisinformation, yet existing models often overlook the influence of individual\nconfirmation bias. An existing model of echo chambers is the \"gravity well\"\nmodel, which creates an analog between echo chambers and spatial gravity wells.\nWe extend this established model by introducing a dynamic confirmation bias\nvariable that adjusts the strength of pull based on a user's susceptibility to\nbelief-reinforcing content. This variable is calculated for each user through\ncomparisons between their posting history and their responses to posts of a\nwide range of viewpoints.\n  Incorporating this factor produces a confirmation-bias-integrated gravity\nwell model that more accurately identifies echo chambers and reveals\ncommunity-level markers of information health. We validated the approach on\nnineteen Reddit communities, demonstrating improved detection of echo chambers.\n  Our contribution is a framework for systematically capturing the role of\nconfirmation bias in online group dynamics, enabling more effective\nidentification of echo chambers. By flagging these high-risk environments, the\nmodel supports efforts to curb the spread of misinformation at its most common\npoints of amplification.\n","authors":["Joseph Jackson","Georgiy Lapin","Jeremy E. Thompson"],"pdf_url":"https://arxiv.org/pdf/2509.03832v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03830v1","updated":"2025-09-04T02:35:14Z","published":"2025-09-04T02:35:14Z","title":"A Multidimensional AI-powered Framework for Analyzing Tourist Perception\n  in Historic Urban Quarters: A Case Study in Shanghai","summary":"  Historic urban quarters play a vital role in preserving cultural heritage\nwhile serving as vibrant spaces for tourism and everyday life. Understanding\nhow tourists perceive these environments is essential for sustainable,\nhuman-centered urban planning. This study proposes a multidimensional\nAI-powered framework for analyzing tourist perception in historic urban\nquarters using multimodal data from social media. Applied to twelve historic\nquarters in central Shanghai, the framework integrates focal point extraction,\ncolor theme analysis, and sentiment mining. Visual focus areas are identified\nfrom tourist-shared photos using a fine-tuned semantic segmentation model. To\nassess aesthetic preferences, dominant colors are extracted using a clustering\nmethod, and their spatial distribution across quarters is analyzed. Color\nthemes are further compared between social media photos and real-world street\nviews, revealing notable shifts. This divergence highlights potential gaps\nbetween visual expectations and the built environment, reflecting both\nstylistic preferences and perceptual bias. Tourist reviews are evaluated\nthrough a hybrid sentiment analysis approach combining a rule-based method and\na multi-task BERT model. Satisfaction is assessed across four dimensions:\ntourist activities, built environment, service facilities, and business\nformats. The results reveal spatial variations in aesthetic appeal and\nemotional response. Rather than focusing on a single technical innovation, this\nframework offers an integrated, data-driven approach to decoding tourist\nperception and contributes to informed decision-making in tourism, heritage\nconservation, and the design of aesthetically engaging public spaces.\n","authors":["Kaizhen Tan","Yufan Wu","Yuxuan Liu","Haoran Zeng"],"pdf_url":"https://arxiv.org/pdf/2509.03830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.22524v2","updated":"2025-09-04T02:32:37Z","published":"2025-03-28T15:28:36Z","title":"Robust Offline Imitation Learning Through State-level Trajectory\n  Stitching","summary":"  Imitation learning (IL) has proven effective for enabling robots to acquire\nvisuomotor skills through expert demonstrations. However, traditional IL\nmethods are limited by their reliance on high-quality, often scarce, expert\ndata, and suffer from covariate shift. To address these challenges, recent\nadvances in offline IL have incorporated suboptimal, unlabeled datasets into\nthe training. In this paper, we propose a novel approach to enhance policy\nlearning from mixed-quality offline datasets by leveraging task-relevant\ntrajectory fragments and rich environmental dynamics. Specifically, we\nintroduce a state-based search framework that stitches state-action pairs from\nimperfect demonstrations, generating more diverse and informative training\ntrajectories. Experimental results on standard IL benchmarks and real-world\nrobotic tasks showcase that our proposed method significantly improves both\ngeneralization and performance.\n","authors":["Shuze Wang","Yunpeng Mei","Hongjie Cao","Yetian Yuan","Gang Wang","Jian Sun","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2503.22524v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03828v1","updated":"2025-09-04T02:32:22Z","published":"2025-09-04T02:32:22Z","title":"An Agentic Model Context Protocol Framework for Medical Concept\n  Standardization","summary":"  The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)\nprovides a standardized representation of heterogeneous health data to support\nlarge-scale, multi-institutional research. One critical step in data\nstandardization using OMOP CDM is the mapping of source medical terms to OMOP\nstandard concepts, a procedure that is resource-intensive and error-prone.\nWhile large language models (LLMs) have the potential to facilitate this\nprocess, their tendency toward hallucination makes them unsuitable for clinical\ndeployment without training and expert validation. Here, we developed a\nzero-training, hallucination-preventive mapping system based on the Model\nContext Protocol (MCP), a standardized and secure framework allowing LLMs to\ninteract with external resources and tools. The system enables explainable\nmapping and significantly improves efficiency and accuracy with minimal effort.\nIt provides real-time vocabulary lookups and structured reasoning outputs\nsuitable for immediate use in both exploratory and production environments.\n","authors":["Jaerong Ahn","Andrew Wen","Nan Wang","Heling Jia","Zhiyi Yue","Sunyang Fu","Hongfang Liu"],"pdf_url":"https://arxiv.org/pdf/2509.03828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03827v1","updated":"2025-09-04T02:28:58Z","published":"2025-09-04T02:28:58Z","title":"What Would an LLM Do? Evaluating Policymaking Capabilities of Large\n  Language Models","summary":"  Large language models (LLMs) are increasingly being adopted in high-stakes\ndomains. Their capacity to process vast amounts of unstructured data, explore\nflexible scenarios, and handle a diversity of contextual factors can make them\nuniquely suited to provide new insights for the complexity of social\npolicymaking. This article evaluates whether LLMs' are aligned with domain\nexperts (and among themselves) to inform social policymaking on the subject of\nhomelessness alleviation - a challenge affecting over 150 million people\nworldwide. We develop a novel benchmark comprised of decision scenarios with\npolicy choices across four geographies (South Bend, USA; Barcelona, Spain;\nJohannesburg, South Africa; Macau SAR, China). The policies in scope are\ngrounded in the conceptual framework of the Capability Approach for human\ndevelopment. We also present an automated pipeline that connects the\nbenchmarked policies to an agent-based model, and we explore the social impact\nof the recommended policies through simulated social scenarios. The paper\nresults reveal promising potential to leverage LLMs for social policy making.\nIf responsible guardrails and contextual calibrations are introduced in\ncollaboration with local domain experts, LLMs can provide humans with valuable\ninsights, in the form of alternative policies at scale.\n","authors":["Pierre Le Coz","Jia An Liu","Debarun Bhattacharjya","Georgina Curto","Serge Stinckwich"],"pdf_url":"https://arxiv.org/pdf/2509.03827v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08949v3","updated":"2025-09-04T02:10:45Z","published":"2024-10-11T16:17:20Z","title":"Transferable Belief Model on Quantum Circuits","summary":"  The transferable belief model, as a semantic interpretation of\nDempster-Shafer theory, enables agents to perform reasoning and decision making\nin imprecise and incomplete environments. The model offers distinct semantics\nfor handling unreliable testimonies, allowing for a more reasonable and general\nprocess of belief transfer compared to the Bayesian approach. However, because\nboth the belief masses and the structure of focal sets must be considered when\nupdating belief functions-leading to extra computational complexity during\nreasoning-the transferable belief model has gradually lost favor among\nresearchers in recent developments. In this paper, we implement the\ntransferable belief model on quantum circuits and demonstrate that belief\nfunctions offer a more concise and effective alternative to Bayesian approaches\nwithin the quantum computing framework. Furthermore, leveraging the unique\ncharacteristics of quantum computing, we propose several novel belief transfer\napproaches. More broadly, this paper introduces a new perspective on basic\ninformation representation for quantum AI models, suggesting that belief\nfunctions are more suitable than Bayesian approach for handling uncertainty on\nquantum circuits.\n","authors":["Qianli Zhou","Hao Luo","Lipeng Pan","Yong Deng","Eloi Bosse"],"pdf_url":"https://arxiv.org/pdf/2410.08949v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03817v1","updated":"2025-09-04T02:06:06Z","published":"2025-09-04T02:06:06Z","title":"Learning to Deliberate: Meta-policy Collaboration for Agentic LLMs with\n  Multi-agent Reinforcement Learning","summary":"  Multi-agent systems of large language models (LLMs) show promise for complex\nreasoning, but their effectiveness is often limited by fixed collaboration\nprotocols. These frameworks typically focus on macro-level orchestration while\noverlooking agents' internal deliberative capabilities. This critical\nmeta-cognitive blindspot treats agents as passive executors unable to adapt\ntheir strategy based on internal cognitive states like uncertainty or\nconfidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where\nagents learn a decentralized policy over a set of high-level meta-cognitive\nactions: Persist, Refine, and Concede. To overcome the instability of\ntraditional policy gradients in this setting, we develop SoftRankPO, a novel\nreinforcement learning algorithm. SoftRankPO stabilizes training by shaping\nadvantages based on the rank of rewards mapped through smooth normal quantiles,\nmaking the learning process robust to reward variance. Experiments show that\nMPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across\nfive mathematical and general reasoning benchmarks compared to six\nstate-of-the-art heuristic and learning-based multi-agent reasoning algorithms.\nOur work presents a paradigm for learning adaptive, meta-cognitive policies for\nmulti-agent LLM systems, shifting the focus from designing fixed protocols to\nlearning dynamic, deliberative strategies.\n","authors":["Wei Yang","Jesse Thomason"],"pdf_url":"https://arxiv.org/pdf/2509.03817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03811v1","updated":"2025-09-04T01:55:58Z","published":"2025-09-04T01:55:58Z","title":"Leveraging LLM-Based Agents for Intelligent Supply Chain Planning","summary":"  In supply chain management, planning is a critical concept. The movement of\nphysical products across different categories, from suppliers to warehouse\nmanagement, to sales, and logistics transporting them to customers, entails the\ninvolvement of many entities. It covers various aspects such as demand\nforecasting, inventory management, sales operations, and replenishment. How to\ncollect relevant data from an e-commerce platform's perspective, formulate\nlong-term plans, and dynamically adjust them based on environmental changes,\nwhile ensuring interpretability, efficiency, and reliability, is a practical\nand challenging problem. In recent years, the development of AI technologies,\nespecially the rapid progress of large language models, has provided new tools\nto address real-world issues. In this work, we construct a Supply Chain\nPlanning Agent (SCPA) framework that can understand domain knowledge,\ncomprehend the operator's needs, decompose tasks, leverage or create new tools,\nand return evidence-based planning reports. We deploy this framework in\nJD.com's real-world scenario, demonstrating the feasibility of LLM-agent\napplications in the supply chain. It effectively reduced labor and improved\naccuracy, stock availability, and other key metrics.\n","authors":["Yongzhi Qi","Jiaheng Yin","Jianshen Zhang","Dongyang Geng","Zhengyu Chen","Hao Hu","Wei Qi","Zuo-Jun Max Shen"],"pdf_url":"https://arxiv.org/pdf/2509.03811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03809v1","updated":"2025-09-04T01:50:20Z","published":"2025-09-04T01:50:20Z","title":"Align-then-Slide: A complete evaluation framework for Ultra-Long\n  Document-Level Machine Translation","summary":"  Large language models (LLMs) have ushered in a new era for document-level\nmachine translation (\\textit{doc}-mt), yet their whole-document outputs\nchallenge existing evaluation methods that assume sentence-by-sentence\nalignment. We introduce \\textit{\\textbf{Align-then-Slide}}, a complete\nevaluation framework for ultra-long doc-mt. In the Align stage, we\nautomatically infer sentence-level source-target correspondences and rebuild\nthe target to match the source sentence number, resolving omissions and\nmany-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we\ncalculate averaged metric scores under 1-, 2-, 3- and 4-chunk for\nmulti-granularity assessment. Experiments on the WMT benchmark show a Pearson\ncorrelation of 0.929 between our method with expert MQM rankings. On a newly\ncurated real-world test set, our method again aligns closely with human\njudgments. Furthermore, preference data produced by Align-then-Slide enables\neffective CPO training and its direct use as a reward model for GRPO, both\nyielding translations preferred over a vanilla SFT baseline. The results\nvalidate our framework as an accurate, robust, and actionable evaluation tool\nfor doc-mt systems.\n","authors":["Jiaxin Guo","Daimeng Wei","Yuanchang Luo","Xiaoyu Chen","Zhanglin Wu","Huan Yang","Hengchao Shang","Zongyao Li","Zhiqiang Rao","Jinlong Yang","Hao Yang"],"pdf_url":"https://arxiv.org/pdf/2509.03809v1.pdf","comment":"under preview"},{"id":"http://arxiv.org/abs/2509.03805v1","updated":"2025-09-04T01:43:49Z","published":"2025-09-04T01:43:49Z","title":"Measuring How (Not Just Whether) VLMs Build Common Ground","summary":"  Large vision language models (VLMs) increasingly claim reasoning skills, yet\ncurrent benchmarks evaluate them in single-turn or question answering settings.\nHowever, grounding is an interactive process in which people gradually develop\nshared understanding through ongoing communication. We introduce a four-metric\nsuite (grounding efficiency, content alignment, lexical adaptation, and\nhuman-likeness) to systematically evaluate VLM performance in interactive\ngrounding contexts. We deploy the suite on 150 self-play sessions of\ninteractive referential games between three proprietary VLMs and compare them\nwith human dyads. All three models diverge from human patterns on at least\nthree metrics, while GPT4o-mini is the closest overall. We find that (i) task\nsuccess scores do not indicate successful grounding and (ii) high\nimage-utterance alignment does not necessarily predict task success. Our metric\nsuite and findings offer a framework for future research on VLM grounding.\n","authors":["Saki Imai","Mert İnan","Anthony Sicilia","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2509.03805v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17882v2","updated":"2025-09-04T01:28:51Z","published":"2025-02-25T06:08:48Z","title":"Science Across Languages: Assessing LLM Multilingual Translation of\n  Scientific Papers","summary":"  Scientific research is inherently global. However, the vast majority of\nacademic journals are published exclusively in English, creating barriers for\nnon-native-English-speaking researchers. In this study, we leverage large\nlanguage models (LLMs) to translate published scientific articles while\npreserving their native JATS XML formatting, thereby developing a practical,\nautomated approach for implementation by academic journals. Using our approach,\nwe translate articles across multiple scientific disciplines into 28 languages.\nTo evaluate translation accuracy, we introduce a novel question-and-answer (QA)\nbenchmarking method, in which an LLM generates comprehension-based questions\nfrom the original text and then answers them based on the translated text. Our\nbenchmark results show an average performance of 95.9%, showing that the key\nscientific details are accurately conveyed. In a user study, we translate the\nscientific papers of 15 researchers into their native languages, finding that\nthe authors consistently found the translations to accurately capture the\noriginal information in their articles. Interestingly, a third of the authors\nfound many technical terms \"overtranslated,\" expressing a preference to keep\nterminology more familiar in English untranslated. Finally, we demonstrate how\nin-context learning techniques can be used to align translations with\ndomain-specific preferences such as mitigating overtranslation, highlighting\nthe adaptability and utility of LLM-driven scientific translation. The code and\ntranslated articles are available at https://hankleid.github.io/ProjectMundo.\n","authors":["Hannah Calzi Kleidermacher","James Zou"],"pdf_url":"https://arxiv.org/pdf/2502.17882v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15869v2","updated":"2025-09-04T01:16:59Z","published":"2024-07-18T08:43:12Z","title":"Long Input Sequence Network for Long Time Series Forecasting","summary":"  Short fixed-length inputs are the main bottleneck of deep learning methods in\nlong time-series forecasting tasks. Prolonging input length causes overfitting,\nrapidly deteriorating accuracy. Our research indicates that the overfitting is\na combination reaction of the multi-scale pattern coupling in time series and\nthe fixed focusing scale of current models. First, we find that the patterns\nexhibited by a time series across various scales are reflective of its\nmulti-periodic nature, where each scale corresponds to specific period length.\nSecond, We find that the token size predominantly dictates model behavior, as\nit determines the scale at which the model focuses and the context size it can\naccommodate. Our idea is to decouple the multi-scale temporal patterns of time\nseries and to model each pattern with its corresponding period length as token\nsize. We introduced a novel series-decomposition module(MPSD), and a\nMulti-Token Pattern Recognition neural network(MTPR), enabling the model to\nhandle \\textit{inputs up to $10\\times$ longer}. Sufficient context enhances\nperformance(\\textit{38% maximum precision improvement}), and the decoupling\napproach offers \\textit{Low complexity($0.22\\times$ cost)} and \\textit{high\ninterpretability}.\n","authors":["Chao Ma","Yikai Hou","Xiang Li","Yinggang Sun","Haining Yu"],"pdf_url":"https://arxiv.org/pdf/2407.15869v2.pdf","comment":"rewrite and authorship changing"},{"id":"http://arxiv.org/abs/2412.16572v2","updated":"2025-09-04T01:16:44Z","published":"2024-12-21T10:29:34Z","title":"Breaking the Context Bottleneck on Long Time Series Forecasting","summary":"  Long-term time-series forecasting is essential for planning and\ndecision-making in economics, energy, and transportation, where long foresight\nis required. To obtain such long foresight, models must be both efficient and\neffective in processing long sequence. Recent advancements have enhanced the\nefficiency of these models; however, the challenge of effectively leveraging\nlonger sequences persists. This is primarily due to the tendency of these\nmodels to overfit when presented with extended inputs, necessitating the use of\nshorter input lengths to maintain tolerable error margins. In this work, we\ninvestigate the multiscale modeling method and propose the Logsparse\nDecomposable Multiscaling (LDM) framework for the efficient and effective\nprocessing of long sequences. We demonstrate that by decoupling patterns at\ndifferent scales in time series, we can enhance predictability by reducing\nnon-stationarity, improve efficiency through a compact long input\nrepresentation, and simplify the architecture by providing clear task\nassignments. Experimental results demonstrate that LDM not only outperforms all\nbaselines in long-term forecasting benchmarks, but also reducing both training\ntime and memory costs.\n","authors":["Chao Ma","Yikai Hou","Xiang Li","Yinggang Sun","Haining Yu","Zhou Fang","Jiaxing Qu"],"pdf_url":"https://arxiv.org/pdf/2412.16572v2.pdf","comment":"rewrite and authorship changing"},{"id":"http://arxiv.org/abs/2509.01153v2","updated":"2025-09-04T01:07:56Z","published":"2025-09-01T06:10:30Z","title":"EZhouNet:A framework based on graph neural network and anchor interval\n  for the respiratory sound event detection","summary":"  Auscultation is a key method for early diagnosis of respiratory and pulmonary\ndiseases, relying on skilled healthcare professionals. However, the process is\noften subjective, with variability between experts. As a result, numerous deep\nlearning-based automatic classification methods have emerged, most of which\nfocus on respiratory sound classification. In contrast, research on respiratory\nsound event detection remains limited. Existing sound event detection methods\ntypically rely on frame-level predictions followed by post-processing to\ngenerate event-level outputs, making interval boundaries challenging to learn\ndirectly. Furthermore, many approaches can only handle fixed-length audio,\nlimiting their applicability to variable-length respiratory sounds.\nAdditionally, the impact of respiratory sound location information on detection\nperformance has not been extensively explored. To address these issues, we\npropose a graph neural network-based framework with anchor intervals, capable\nof handling variable-length audio and providing more precise temporal\nlocalization for abnormal respiratory sound events. Our method improves both\nthe flexibility and applicability of respiratory sound detection. Experiments\non the SPRSound 2024 and HF Lung V1 datasets demonstrate the effectiveness of\nthe proposed approach, and incorporating respiratory position information\nenhances the discrimination between abnormal sounds. The reference\nimplementation is available at https://github.com/chumingqian/EzhouNet.\n","authors":["Yun Chu","Qiuhao Wang","Enze Zhou","Qian Liu","Gang Zheng"],"pdf_url":"https://arxiv.org/pdf/2509.01153v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03793v1","updated":"2025-09-04T01:04:44Z","published":"2025-09-04T01:04:44Z","title":"SAMVAD: A Multi-Agent System for Simulating Judicial Deliberation\n  Dynamics in India","summary":"  Understanding the complexities of judicial deliberation is crucial for\nassessing the efficacy and fairness of a justice system. However, empirical\nstudies of judicial panels are constrained by significant ethical and practical\nbarriers. This paper introduces SAMVAD, an innovative Multi-Agent System (MAS)\ndesigned to simulate the deliberation process within the framework of the\nIndian justice system.\n  Our system comprises agents representing key judicial roles: a Judge, a\nProsecution Counsel, a Defense Counsel, and multiple Adjudicators (simulating a\njudicial bench), all powered by large language models (LLMs). A primary\ncontribution of this work is the integration of Retrieval-Augmented Generation\n(RAG), grounded in a domain-specific knowledge base of landmark Indian legal\ndocuments, including the Indian Penal Code and the Constitution of India. This\nRAG functionality enables the Judge and Counsel agents to generate legally\nsound instructions and arguments, complete with source citations, thereby\nenhancing both the fidelity and transparency of the simulation.\n  The Adjudicator agents engage in iterative deliberation rounds, processing\ncase facts, legal instructions, and arguments to reach a consensus-based\nverdict. We detail the system architecture, agent communication protocols, the\nRAG pipeline, the simulation workflow, and a comprehensive evaluation plan\ndesigned to assess performance, deliberation quality, and outcome consistency.\n  This work provides a configurable and explainable MAS platform for exploring\nlegal reasoning and group decision-making dynamics in judicial simulations,\nspecifically tailored to the Indian legal context and augmented with verifiable\nlegal grounding via RAG.\n","authors":["Prathamesh Devadiga","Omkaar Jayadev Shetty","Pooja Agarwal"],"pdf_url":"https://arxiv.org/pdf/2509.03793v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00616v2","updated":"2025-09-04T01:01:04Z","published":"2025-08-30T21:48:51Z","title":"TimeCopilot","summary":"  We introduce TimeCopilot, the first open-source agentic framework for\nforecasting that combines multiple Time Series Foundation Models (TSFMs) with\nLarge Language Models (LLMs) through a single unified API. TimeCopilot\nautomates the forecasting pipeline: feature analysis, model selection,\ncross-validation, and forecast generation, while providing natural language\nexplanations and supporting direct queries about the future. The framework is\nLLM-agnostic, compatible with both commercial and open-source models, and\nsupports ensembles across diverse forecasting families. Results on the\nlarge-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art\nprobabilistic forecasting performance at low cost. Our framework provides a\npractical foundation for reproducible, explainable, and accessible agentic\nforecasting systems.\n","authors":["Azul Garza","Reneé Rosillo"],"pdf_url":"https://arxiv.org/pdf/2509.00616v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02591v2","updated":"2025-09-04T00:59:33Z","published":"2025-08-29T03:24:57Z","title":"Ensemble of Pathology Foundation Models for MIDOG 2025 Track 2: Atypical\n  Mitosis Classification","summary":"  Mitotic figures are classified into typical and atypical variants, with\natypical counts correlating strongly with tumor aggressiveness. Accurate\ndifferentiation is therefore essential for patient prognostication and resource\nallocation, yet remains challenging even for expert pathologists. Here, we\nleveraged Pathology Foundation Models (PFMs) pre-trained on large\nhistopathology datasets and applied parameter-efficient fine-tuning via\nlow-rank adaptation. In addition, we incorporated ConvNeXt V2, a\nstate-of-the-art convolutional neural network architecture, to complement PFMs.\nDuring training, we employed a fisheye transform to emphasize mitoses and\nFourier Domain Adaptation using ImageNet target images. Finally, we ensembled\nmultiple PFMs to integrate complementary morphological insights, achieving\ncompetitive balanced accuracy on the Preliminary Evaluation Phase dataset.\n","authors":["Mieko Ochi","Bae Yuan"],"pdf_url":"https://arxiv.org/pdf/2509.02591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03791v1","updated":"2025-09-04T00:58:43Z","published":"2025-09-04T00:58:43Z","title":"SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation\n  Evaluation","summary":"  Evaluating sign language generation is often done through back-translation,\nwhere generated signs are first recognized back to text and then compared to a\nreference using text-based metrics. However, this two-step evaluation pipeline\nintroduces ambiguity: it not only fails to capture the multimodal nature of\nsign language-such as facial expressions, spatial grammar, and prosody-but also\nmakes it hard to pinpoint whether evaluation errors come from sign generation\nmodel or the translation system used to assess it. In this work, we propose\nSiLVERScore, a novel semantically-aware embedding-based evaluation metric that\nassesses sign language generation in a joint embedding space. Our contributions\ninclude: (1) identifying limitations of existing metrics, (2) introducing\nSiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness\nto semantic and prosodic variations, and (4) exploring generalization\nchallenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore\nachieves near-perfect discrimination between correct and random pairs (ROC AUC\n= 0.99, overlap < 7%), substantially outperforming traditional metrics.\n","authors":["Saki Imai","Mert İnan","Anthony Sicilia","Malihe Alikhani"],"pdf_url":"https://arxiv.org/pdf/2509.03791v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03790v1","updated":"2025-09-04T00:53:02Z","published":"2025-09-04T00:53:02Z","title":"What Fundamental Structure in Reward Functions Enables Efficient\n  Sparse-Reward Learning?","summary":"  What fundamental properties of reward functions enable efficient\nsparse-reward reinforcement learning? We address this question through the lens\nof low-rank structure in reward matrices, showing that such structure induces a\nsharp transition from exponential to polynomial sample complexity, the first\nresult of this kind for sparse-reward RL. We introduce Policy-Aware Matrix\nCompletion (PAMC), which connects matrix completion theory with reinforcement\nlearning via a new analysis of policy-dependent sampling. Our framework\nprovides: (i) impossibility results for general sparse reward observation, (ii)\nreward-free representation learning from dynamics, (iii) distribution-free\nconfidence sets via conformal prediction, and (iv) robust completion guarantees\nthat degrade gracefully when low-rank structure is only approximate.\nEmpirically, we conduct a pre-registered evaluation across 100 systematically\nsampled domains, finding exploitable structure in over half. PAMC improves\nsample efficiency by factors between 1.6 and 2.1 compared to strong\nexploration, structured, and representation-learning baselines, while adding\nonly about 20 percent computational overhead.These results establish structural\nreward learning as a promising new paradigm, with immediate implications for\nrobotics, healthcare, and other safety-critical, sample-expensive applications.\n","authors":["Ibne Farabi Shihab","Sanjeda Akter","Anuj Sharma"],"pdf_url":"https://arxiv.org/pdf/2509.03790v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.18102v5","updated":"2025-09-04T00:35:33Z","published":"2025-05-23T16:57:34Z","title":"How Can I Publish My LLM Benchmark Without Giving the True Answers Away?","summary":"  Publishing a large language model (LLM) benchmark on the Internet risks\ncontaminating future LLMs: the benchmark may be unintentionally (or\nintentionally) used to train or select a model. A common mitigation is to keep\nthe benchmark private and let participants submit their models or predictions\nto the organizers. However, this strategy will require trust in a single\norganization and still permits test-set overfitting through repeated queries.\nTo overcome this issue, we propose a way to publish benchmarks without\ncompletely disclosing the ground-truth answers to the questions, while still\nmaintaining the ability to openly evaluate LLMs. Our main idea is to inject\nrandomness to the answers by preparing several logically correct answers, and\nonly include one of them as the solution in the benchmark. This reduces the\nbest possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is\nthis helpful to keep us from disclosing the ground truth, but this approach\nalso offers a test for detecting data contamination. In principle, even fully\ncapable models should not surpass the Bayes accuracy. If a model surpasses this\nceiling despite this expectation, this is a strong signal of data\ncontamination. We present experimental evidence that our method can detect data\ncontamination accurately on a wide range of benchmarks, models, and training\nmethodologies.\n","authors":["Takashi Ishida","Thanawat Lodkaew","Ikko Yamane"],"pdf_url":"https://arxiv.org/pdf/2505.18102v5.pdf","comment":"Extended version of the paper presented as an Oral at the ICML 2025\n  Workshop on the Impact of Memorization on Trustworthy Foundation Models"}]},"2025-09-03T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2506.20553v2","updated":"2025-09-03T22:57:48Z","published":"2025-06-25T15:51:07Z","title":"Sim2Val: Leveraging Correlation Across Test Platforms for\n  Variance-Reduced Metric Estimation","summary":"  Learning-based robotic systems demand rigorous validation to assure reliable\nperformance, but extensive real-world testing is often prohibitively expensive,\nand if conducted may still yield insufficient data for high-confidence\nguarantees. In this work we introduce Sim2Val, a general estimation framework\nthat leverages paired data across test platforms, e.g., paired simulation and\nreal-world observations, to achieve better estimates of real-world metrics via\nthe method of control variates. By incorporating cheap and abundant auxiliary\nmeasurements (for example, simulator outputs) as control variates for costly\nreal-world samples, our method provably reduces the variance of Monte Carlo\nestimates and thus requires significantly fewer real-world samples to attain a\nspecified confidence bound on the mean performance. We provide theoretical\nanalysis characterizing the variance and sample-efficiency improvement, and\ndemonstrate empirically in autonomous driving and quadruped robotics settings\nthat our approach achieves high-probability bounds with markedly improved\nsample efficiency. Our technique can lower the real-world testing burden for\nvalidating the performance of the stack, thereby enabling more efficient and\ncost-effective experimental evaluation of robotic systems.\n","authors":["Rachel Luo","Heng Yang","Michael Watson","Apoorva Sharma","Sushant Veer","Edward Schmerling","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2506.20553v2.pdf","comment":"Conference on Robot Learning (CoRL) 2025"},{"id":"http://arxiv.org/abs/2509.03753v1","updated":"2025-09-03T22:45:50Z","published":"2025-09-03T22:45:50Z","title":"Memory Optimization for Convex Hull Support Point Queries","summary":"  This paper evaluates several improvements to the memory layout of convex\nhulls to improve computation times for support point queries. The support point\nquery is a fundamental part of common collision algorithms, and the work\npresented achieves a significant speedup depending on the number of vertices of\nthe convex hull.\n","authors":["Michael Greer"],"pdf_url":"https://arxiv.org/pdf/2509.03753v1.pdf","comment":"6 pages, 15 figures"},{"id":"http://arxiv.org/abs/2509.03721v1","updated":"2025-09-03T21:03:43Z","published":"2025-09-03T21:03:43Z","title":"Avoidance of an unexpected obstacle without reinforcement learning: Why\n  not using advanced control-theoretic tools?","summary":"  This communication on collision avoidance with unexpected obstacles is\nmotivated by some critical appraisals on reinforcement learning (RL) which\n\"requires ridiculously large numbers of trials to learn any new task\" (Yann\nLeCun). We use the classic Dubins' car in order to replace RL with\nflatness-based control, combined with the HEOL feedback setting, and the latest\nmodel-free predictive control approach. The two approaches lead to convincing\ncomputer experiments where the results with the model-based one are only\nslightly better. They exhibit a satisfactory robustness with respect to\nrandomly generated mismatches/disturbances, which become excellent in the\nmodel-free case. Those properties would have been perhaps difficult to obtain\nwith today's popular machine learning techniques in AI. Finally, we should\nemphasize that our two methods require a low computational burden.\n","authors":["Cédric Join","Michel Fliess"],"pdf_url":"https://arxiv.org/pdf/2509.03721v1.pdf","comment":"IEEE 2025 - 13th International Conference on Systems and Control\n  (ICSC) - October 22-24, 2025 - Marrakesh, Morocco"},{"id":"http://arxiv.org/abs/2509.03690v1","updated":"2025-09-03T20:13:19Z","published":"2025-09-03T20:13:19Z","title":"Low-Cost Open-Source Ambidextrous Robotic Hand with 23 Direct-Drive\n  servos for American Sign Language Alphabet","summary":"  Accessible communication through sign language is vital for deaf communities,\n1 yet robotic solutions are often costly and limited. This study presents\nVulcanV3, a low- 2 cost, open-source, 3D-printed ambidextrous robotic hand\ncapable of reproducing the full 3 American Sign Language (ASL) alphabet (52\nsigns for right- and left-hand configurations). 4 The system employs 23\ndirect-drive servo actuators for precise finger and wrist movements, 5\ncontrolled by an Arduino Mega with dual PCA9685 modules. Unlike most humanoid\nupper- 6 limb systems, which rarely employ direct-drive actuation, VulcanV3\nachieves complete ASL 7 coverage with a reversible design. All CAD files and\ncode are released under permissive 8 open-source licenses to enable\nreplication. Empirical tests confirmed accurate reproduction 9 of all 52 ASL\nhandshapes, while a participant study (n = 33) achieved 96.97% recognition 10\naccuracy, improving to 98.78% after video demonstration. VulcanV3 advances\nassistive 11 robotics by combining affordability, full ASL coverage, and\nambidexterity in an openly 12 shared platform, contributing to accessible\ncommunication technologies and inclusive 13 innovation.\n","authors":["Kelvin Daniel Gonzalez Amador"],"pdf_url":"https://arxiv.org/pdf/2509.03690v1.pdf","comment":"9 pages, 8 figures, 4 tables. Submitted as preprint"},{"id":"http://arxiv.org/abs/2509.03658v1","updated":"2025-09-03T19:18:02Z","published":"2025-09-03T19:18:02Z","title":"Efficient Virtuoso: A Latent Diffusion Transformer Model for\n  Goal-Conditioned Trajectory Planning","summary":"  The ability to generate a diverse and plausible distribution of future\ntrajectories is a critical capability for autonomous vehicle planning systems.\nWhile recent generative models have shown promise, achieving high fidelity,\ncomputational efficiency, and precise control remains a significant challenge.\nIn this paper, we present the \\textbf{Efficient Virtuoso}, a conditional latent\ndiffusion model for goal-conditioned trajectory planning. Our approach\nintroduces a novel two-stage normalization pipeline that first scales\ntrajectories to preserve their geometric aspect ratio and then normalizes the\nresulting PCA latent space to ensure a stable training target. The denoising\nprocess is performed efficiently in this low-dimensional latent space by a\nsimple MLP denoiser, which is conditioned on a rich scene context fused by a\npowerful Transformer-based StateEncoder. We demonstrate that our method\nachieves state-of-the-art performance on the Waymo Open Motion Dataset,\nreaching a \\textbf{minADE of 0.25}. Furthermore, through a rigorous ablation\nstudy on goal representation, we provide a key insight: while a single endpoint\ngoal can resolve strategic ambiguity, a richer, multi-step sparse route is\nessential for enabling the precise, high-fidelity tactical execution that\nmirrors nuanced human driving behavior.\n","authors":["Antonio Guillen-Perez"],"pdf_url":"https://arxiv.org/pdf/2509.03658v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03638v1","updated":"2025-09-03T18:44:42Z","published":"2025-09-03T18:44:42Z","title":"Cooperative Grasping for Collective Object Transport in Constrained\n  Environments","summary":"  We propose a novel framework for decision-making in cooperative grasping for\ntwo-robot object transport in constrained environments. The core of the\nframework is a Conditional Embedding (CE) model consisting of two neural\nnetworks that map grasp configuration information into an embedding space. The\nresulting embedding vectors are then used to identify feasible grasp\nconfigurations that allow two robots to collaboratively transport an object. To\nensure generalizability across diverse environments and object geometries, the\nneural networks are trained on a dataset comprising a range of environment maps\nand object shapes. We employ a supervised learning approach with negative\nsampling to ensure that the learned embeddings effectively distinguish between\nfeasible and infeasible grasp configurations. Evaluation results across a wide\nrange of environments and objects in simulations demonstrate the model's\nability to reliably identify feasible grasp configurations. We further validate\nthe framework through experiments on a physical robotic platform, confirming\nits practical applicability.\n","authors":["David Alvear","George Turkiyyah","Shinkyu Park"],"pdf_url":"https://arxiv.org/pdf/2509.03638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03515v1","updated":"2025-09-03T17:56:46Z","published":"2025-09-03T17:56:46Z","title":"Can the Waymo Open Motion Dataset Support Realistic Behavioral Modeling?\n  A Validation Study with Naturalistic Trajectories","summary":"  The Waymo Open Motion Dataset (WOMD) has become a popular resource for\ndata-driven modeling of autonomous vehicles (AVs) behavior. However, its\nvalidity for behavioral analysis remains uncertain due to proprietary\npost-processing, the absence of error quantification, and the segmentation of\ntrajectories into 20-second clips. This study examines whether WOMD accurately\ncaptures the dynamics and interactions observed in real-world AV operations.\nLeveraging an independently collected naturalistic dataset from Level 4 AV\noperations in Phoenix, Arizona (PHX), we perform comparative analyses across\nthree representative urban driving scenarios: discharging at signalized\nintersections, car-following, and lane-changing behaviors. For the discharging\nanalysis, headways are manually extracted from aerial video to ensure\nnegligible measurement error. For the car-following and lane-changing cases, we\napply the Simulation-Extrapolation (SIMEX) method to account for empirically\nestimated error in the PHX data and use Dynamic Time Warping (DTW) distances to\nquantify behavioral differences. Results across all scenarios consistently show\nthat behavior in PHX falls outside the behavioral envelope of WOMD. Notably,\nWOMD underrepresents short headways and abrupt decelerations. These findings\nsuggest that behavioral models calibrated solely on WOMD may systematically\nunderestimate the variability, risk, and complexity of naturalistic driving.\nCaution is therefore warranted when using WOMD for behavior modeling without\nproper validation against independently collected data.\n","authors":["Yanlin Zhang","Sungyong Chung","Nachuan Li","Dana Monzer","Hani S. Mahmassani","Samer H. Hamdar","Alireza Talebpour"],"pdf_url":"https://arxiv.org/pdf/2509.03515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00117v2","updated":"2025-09-03T17:55:11Z","published":"2025-08-28T17:59:07Z","title":"Embodied AI: Emerging Risks and Opportunities for Policy Action","summary":"  The field of embodied AI (EAI) is rapidly advancing. Unlike virtual AI, EAI\nsystems can exist in, learn from, reason about, and act in the physical world.\nWith recent advances in AI models and hardware, EAI systems are becoming\nincreasingly capable across wider operational domains. While EAI systems can\noffer many benefits, they also pose significant risks, including physical harm\nfrom malicious use, mass surveillance, as well as economic and societal\ndisruption. These risks require urgent attention from policymakers, as existing\npolicies governing industrial robots and autonomous vehicles are insufficient\nto address the full range of concerns EAI systems present. To help address this\nissue, this paper makes three contributions. First, we provide a taxonomy of\nthe physical, informational, economic, and social risks EAI systems pose.\nSecond, we analyze policies in the US, EU, and UK to assess how existing\nframeworks address these risks and to identify critical gaps. We conclude by\noffering policy recommendations for the safe and beneficial deployment of EAI\nsystems, such as mandatory testing and certification schemes, clarified\nliability frameworks, and strategies to manage EAI's potentially transformative\neconomic and societal impacts.\n","authors":["Jared Perlo","Alexander Robey","Fazl Barez","Luciano Floridi","Jakob Mökander"],"pdf_url":"https://arxiv.org/pdf/2509.00117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03500v1","updated":"2025-09-03T17:32:15Z","published":"2025-09-03T17:32:15Z","title":"Real-Time Instrument Planning and Perception for Novel Measurements of\n  Dynamic Phenomena","summary":"  Advancements in onboard computing mean remote sensing agents can employ\nstate-of-the-art computer vision and machine learning at the edge. These\ncapabilities can be leveraged to unlock new rare, transient, and pinpoint\nmeasurements of dynamic science phenomena. In this paper, we present an\nautomated workflow that synthesizes the detection of these dynamic events in\nlook-ahead satellite imagery with autonomous trajectory planning for a\nfollow-up high-resolution sensor to obtain pinpoint measurements. We apply this\nworkflow to the use case of observing volcanic plumes. We analyze\nclassification approaches including traditional machine learning algorithms and\nconvolutional neural networks. We present several trajectory planning\nalgorithms that track the morphological features of a plume and integrate these\nalgorithms with the classifiers. We show through simulation an order of\nmagnitude increase in the utility return of the high-resolution instrument\ncompared to baselines while maintaining efficient runtimes.\n","authors":["Itai Zilberstein","Alberto Candela","Steve Chien"],"pdf_url":"https://arxiv.org/pdf/2509.03500v1.pdf","comment":"Appears in Proceedings of 18th Symposium on Advanced Space\n  Technologies in Robotics and Automation"},{"id":"http://arxiv.org/abs/2507.20800v3","updated":"2025-09-03T17:09:49Z","published":"2025-07-28T13:08:33Z","title":"LanternNet: A Hub-and-Spoke System to Seek and Suppress Spotted\n  Lanternfly Populations","summary":"  The invasive spotted lanternfly (SLF) poses a significant threat to\nagriculture and ecosystems, causing widespread damage. Current control methods,\nsuch as egg scraping, pesticides, and quarantines, prove labor-intensive,\nenvironmentally hazardous, and inadequate for long-term SLF suppression. This\nresearch introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system\ndesigned for scalable detection and suppression of SLF populations. A central,\ntree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF\nidentification. Three specialized robotic spokes perform targeted tasks: pest\nneutralization, environmental monitoring, and navigation/mapping. Field\ndeployment across multiple infested sites over 5 weeks demonstrated\nLanternNet's efficacy. Quantitative analysis revealed significant reductions (p\n< 0.01, paired t-tests) in SLF populations and corresponding improvements in\ntree health indicators across the majority of test sites. Compared to\nconventional methods, LanternNet offers substantial cost advantages and\nimproved scalability. Furthermore, the system's adaptability for enhanced\nautonomy and targeting of other invasive species presents significant potential\nfor broader ecological impact. LanternNet demonstrates the transformative\npotential of integrating robotics and AI for advanced invasive species\nmanagement and improved environmental outcomes.\n","authors":["Vinil Polepalli"],"pdf_url":"https://arxiv.org/pdf/2507.20800v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16114v2","updated":"2025-09-03T16:59:53Z","published":"2024-02-25T15:26:16Z","title":"Controlling Deformable Objects with Non-negligible Dynamics: a\n  Shape-Regulation Approach to End-Point Positioning","summary":"  Model-based manipulation of deformable objects has traditionally dealt with\nobjects while neglecting their dynamics, thus mostly focusing on very\nlightweight objects at steady state. At the same time, soft robotic research\nhas made considerable strides toward general modeling and control, despite soft\nrobots and deformable objects being very similar from a mechanical standpoint.\nIn this work, we leverage these recent results to develop a control-oriented,\nfully dynamic framework of slender deformable objects grasped at one end by a\nrobotic manipulator. We introduce a dynamic model of this system using\nfunctional strain parameterizations and describe the manipulation challenge as\na regulation control problem. This enables us to define a fully model-based\ncontrol architecture, for which we can prove analytically closed-loop stability\nand provide sufficient conditions for steady state convergence to the desired\nstate. The nature of this work is intended to be markedly experimental. We\nprovide an extensive experimental validation of the proposed ideas, tasking a\nrobot arm with controlling the distal end of six different cables, in a given\nplanar position and orientation in space.\n","authors":["Sebastien Tiburzio","Tomás Coleman","Daniel Feliu-Talegon","Cosimo Della Santina"],"pdf_url":"https://arxiv.org/pdf/2402.16114v2.pdf","comment":"15 pages, 18 figures. Accepted for publication as a Regular Paper in\n  the IEEE Transactions on Robotics (T-RO)"},{"id":"http://arxiv.org/abs/2509.03462v1","updated":"2025-09-03T16:37:49Z","published":"2025-09-03T16:37:49Z","title":"sam-llm: interpretable lane change trajectoryprediction via parametric\n  finetuning","summary":"  This work introduces SAM-LLM, a novel hybrid architecture that bridges the\ngap between the contextual reasoning of Large Language Models (LLMs) and the\nphysical precision of kinematic lane change models for autonomous driving. The\nsystem is designed for interpretable lane change trajectory prediction by\nfinetuning an LLM to output the core physical parameters of a trajectory model\ninstead of raw coordinates. For lane-keeping scenarios, the model predicts\ndiscrete coordinates, but for lane change maneuvers, it generates the\nparameters for an enhanced Sinusoidal Acceleration Model (SAM), including\nlateral displacement, maneuver duration, initial lateral velocity, and\nlongitudinal velocity change. This parametric approach yields a complete,\ncontinuous, and physically plausible trajectory model that is inherently\ninterpretable and computationally efficient, achieving an 80% reduction in\noutput size compared to coordinate-based methods. The SAM-LLM achieves a\nstate-of-the-art overall intention prediction accuracy of 98.73%, demonstrating\nperformance equivalent to traditional LLM predictors while offering significant\nadvantages in explainability and resource efficiency.\n","authors":["Zhuo Cao","Yunxiao Shi","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2509.03462v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2509.03451v1","updated":"2025-09-03T16:16:55Z","published":"2025-09-03T16:16:55Z","title":"SmartPoser: Arm Pose Estimation with a Smartphone and Smartwatch Using\n  UWB and IMU Data","summary":"  The ability to track a user's arm pose could be valuable in a wide range of\napplications, including fitness, rehabilitation, augmented reality input, life\nlogging, and context-aware assistants. Unfortunately, this capability is not\nreadily available to consumers. Systems either require cameras, which carry\nprivacy issues, or utilize multiple worn IMUs or markers. In this work, we\ndescribe how an off-the-shelf smartphone and smartwatch can work together to\naccurately estimate arm pose. Moving beyond prior work, we take advantage of\nmore recent ultra-wideband (UWB) functionality on these devices to capture\nabsolute distance between the two devices. This measurement is the perfect\ncomplement to inertial data, which is relative and suffers from drift. We\nquantify the performance of our software-only approach using off-the-shelf\ndevices, showing it can estimate the wrist and elbow joints with a \\hl{median\npositional error of 11.0~cm}, without the user having to provide training data.\n","authors":["Nathan DeVrio","Vimal Mollyn","Chris Harrison"],"pdf_url":"https://arxiv.org/pdf/2509.03451v1.pdf","comment":"The first two listed authors contributed equally. Published at UIST\n  2023"},{"id":"http://arxiv.org/abs/2509.03436v1","updated":"2025-09-03T16:06:39Z","published":"2025-09-03T16:06:39Z","title":"Cost-Optimized Systems Engineering for IoT-Enabled Robot Nurse in\n  Infectious Pandemic Management","summary":"  The utilization of robotic technology has gained traction in healthcare\nfacilities due to progress in the field that enables time and cost savings,\nminimizes waste, and improves patient care. Digital healthcare technologies\nthat leverage automation, such as robotics and artificial intelligence, have\nthe potential to enhance the sustainability and profitability of healthcare\nsystems in the long run. However, the recent COVID-19 pandemic has amplified\nthe need for cyber-physical robots to automate check-ups and medication\nadministration. A robot nurse is controlled by the Internet of Things (IoT) and\ncan serve as an automated medical assistant while also allowing supervisory\ncontrol based on custom commands. This system helps reduce infection risk and\nimproves outcomes in pandemic settings. This research presents a test case with\na nurse robot that can assess a patient's health status and take action\naccordingly. We also evaluate the system's performance in medication\nadministration, health-status monitoring, and life-cycle considerations.\n","authors":["Md Mhamud Hussen Sifat","Md Maruf","Md Rokunuzzaman"],"pdf_url":"https://arxiv.org/pdf/2509.03436v1.pdf","comment":"11 pages, 10 figures, 4 tables, 1 algorithm. Corresponding author: Md\n  Maruf (maruf.mte.17@gmail.com)"},{"id":"http://arxiv.org/abs/2509.03430v1","updated":"2025-09-03T15:59:28Z","published":"2025-09-03T15:59:28Z","title":"EclipseTouch: Touch Segmentation on Ad Hoc Surfaces using Worn Infrared\n  Shadow Casting","summary":"  The ability to detect touch events on uninstrumented, everyday surfaces has\nbeen a long-standing goal for mixed reality systems. Prior work has shown that\nvirtual interfaces bound to physical surfaces offer performance and ergonomic\nbenefits over tapping at interfaces floating in the air. A wide variety of\napproaches have been previously developed, to which we contribute a new\nheadset-integrated technique called \\systemname. We use a combination of a\ncomputer-triggered camera and one or more infrared emitters to create\nstructured shadows, from which we can accurately estimate hover distance (mean\nerror of 6.9~mm) and touch contact (98.0\\% accuracy). We discuss how our\ntechnique works across a range of conditions, including surface material,\ninteraction orientation, and environmental lighting.\n","authors":["Vimal Mollyn","Nathan DeVrio","Chris Harrison"],"pdf_url":"https://arxiv.org/pdf/2509.03430v1.pdf","comment":"Accepted to UIST 2025"},{"id":"http://arxiv.org/abs/2509.03563v1","updated":"2025-09-03T15:11:41Z","published":"2025-09-03T15:11:41Z","title":"Self-Organizing Aerial Swarm Robotics for Resilient Load Transportation\n  : A Table-Mechanics-Inspired Approach","summary":"  In comparison with existing approaches, which struggle with scalability,\ncommunication dependency, and robustness against dynamic failures, cooperative\naerial transportation via robot swarms holds transformative potential for\nlogistics and disaster response. Here, we present a physics-inspired\ncooperative transportation approach for flying robot swarms that imitates the\ndissipative mechanics of table-leg load distribution. By developing a\ndecentralized dissipative force model, our approach enables autonomous\nformation stabilization and adaptive load allocation without the requirement of\nexplicit communication. Based on local neighbor robots and the suspended\npayload, each robot dynamically adjusts its position. This is similar to\nenergy-dissipating table leg reactions. The stability of the resultant control\nsystem is rigorously proved. Simulations demonstrate that the tracking errors\nof the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches\nunder the cases of capability variation, cable uncertainty, limited vision, and\npayload variation, respectively. In real-world experiments with six flying\nrobots, the cooperative aerial transportation system achieved a 94% success\nrate under single-robot failure, disconnection events, 25% payload variation,\nand 40% cable length uncertainty, demonstrating strong robustness under outdoor\nwinds up to Beaufort scale 4. Overall, this physics-inspired approach bridges\nswarm intelligence and mechanical stability principles, offering a scalable\nframework for heterogeneous aerial systems to collectively handle complex\ntransportation tasks in communication-constrained environments.\n","authors":["Quan Quan","Jiwen Xu","Runxiao Liu","Yi Ding","Jiaxing Che","Kai-Yuan Cai"],"pdf_url":"https://arxiv.org/pdf/2509.03563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03383v1","updated":"2025-09-03T15:00:28Z","published":"2025-09-03T15:00:28Z","title":"ANNIE: Be Careful of Your Robots","summary":"  The integration of vision-language-action (VLA) models into embodied AI (EAI)\nrobots is rapidly advancing their ability to perform complex, long-horizon\ntasks in humancentric environments. However, EAI systems introduce critical\nsecurity risks: a compromised VLA model can directly translate adversarial\nperturbations on sensory input into unsafe physical actions. Traditional safety\ndefinitions and methodologies from the machine learning community are no longer\nsufficient. EAI systems raise new questions, such as what constitutes safety,\nhow to measure it, and how to design effective attack and defense mechanisms in\nphysically grounded, interactive settings. In this work, we present the first\nsystematic study of adversarial safety attacks on embodied AI systems, grounded\nin ISO standards for human-robot interactions. We (1) formalize a principled\ntaxonomy of safety violations (critical, dangerous, risky) based on physical\nconstraints such as separation distance, velocity, and collision boundaries;\n(2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with\n2,400 video-action sequences for evaluating embodied safety; and (3)\nANNIE-Attack, a task-aware adversarial framework with an attack leader model\nthat decomposes long-horizon goals into frame-level perturbations. Our\nevaluation across representative EAI models shows attack success rates\nexceeding 50% across all safety categories. We further demonstrate sparse and\nadaptive attack strategies and validate the real-world impact through physical\nrobot experiments. These results expose a previously underexplored but highly\nconsequential attack surface in embodied AI systems, highlighting the urgent\nneed for security-driven defenses in the physical AI era. Code is available at\nhttps://github.com/RLCLab/Annie.\n","authors":["Yiyang Huang","Zixuan Wang","Zishen Wan","Yapeng Tian","Haobo Xu","Yinhe Han","Yiming Gan"],"pdf_url":"https://arxiv.org/pdf/2509.03383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03381v1","updated":"2025-09-03T14:57:21Z","published":"2025-09-03T14:57:21Z","title":"Dependency Chain Analysis of ROS 2 DDS QoS Policies: From Lifecycle\n  Tutorial to Static Verification","summary":"  Robot Operating System 2 (ROS 2) relies on the Data Distribution Service\n(DDS), which offers more than 20 Quality of Service (QoS) policies governing\navailability, reliability, and resource usage. Yet ROS 2 users lack clear\nguidance on safe policy combinations and validation processes prior to\ndeployment, which often leads to trial-and-error tuning and unexpected runtime\nfailures. To address these challenges, we analyze DDS Publisher-Subscriber\ncommunication over a life cycle divided into Discovery, Data Exchange, and\nDisassociation, and provide a user oriented tutorial explaining how 16 QoS\npolicies operate in each phase. Building on this analysis, we derive a QoS\ndependency chain that formalizes inter-policy relationships and classifies 41\ndependency violation rules, capturing constraints that commonly cause\ncommunication failures in practice. Finally, we introduce QoS Guard, a ROS 2\npackage that statically validates DDS XML profiles offline, flags conflicts,\nand enables safe, predeployment tuning without establishing a live ROS 2\nsession. Together, these contributions give ROS 2 users both conceptual insight\nand a concrete tool that enables early detection of misconfigurations,\nimproving the reliability and resource efficiency of ROS 2 based robotic\nsystems.\n","authors":["Sanghoon Lee","Junha Kang","Kyung-Joon Park"],"pdf_url":"https://arxiv.org/pdf/2509.03381v1.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2409.11017v3","updated":"2025-09-03T14:39:20Z","published":"2024-09-17T09:30:30Z","title":"Stretchable Electrohydraulic Artificial Muscle for Full Motion Ranges in\n  Musculoskeletal Antagonistic Joints","summary":"  Artificial muscles play a crucial role in musculoskeletal robotics and\nprosthetics to approximate the force-generating functionality of biological\nmuscle. However, current artificial muscle systems are typically limited to\neither contraction or extension, not both. This limitation hinders the\ndevelopment of fully functional artificial musculoskeletal systems. We address\nthis challenge by introducing an artificial antagonistic muscle system capable\nof both contraction and extension. Our design integrates non-stretchable\nelectrohydraulic soft actuators (HASELs) with electrostatic clutches within an\nantagonistic musculoskeletal framework. This configuration enables an\nantagonistic joint to achieve a full range of motion without displacement loss\ndue to tendon slack. We implement a synchronization method to coordinate muscle\nand clutch units, ensuring smooth motion profiles and speeds. This approach\nfacilitates seamless transitions between antagonistic muscles at operational\nfrequencies of up to 3.2 Hz. While our prototype utilizes electrohydraulic\nactuators, this muscle-clutch concept is adaptable to other non-stretchable\nartificial muscles, such as McKibben actuators, expanding their capability for\nextension and full range of motion in antagonistic setups. Our design\nrepresents a significant advancement in the development of fundamental\ncomponents for more functional and efficient artificial musculoskeletal\nsystems, bringing their capabilities closer to those of their biological\ncounterparts.\n","authors":["Amirhossein Kazemipour","Ronan Hinchet","Robert K. Katzschmann"],"pdf_url":"https://arxiv.org/pdf/2409.11017v3.pdf","comment":"This paper has been accepted to the IEEE International Conference on\n  Robotics and Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2505.02476v2","updated":"2025-09-03T13:01:19Z","published":"2025-05-05T09:00:16Z","title":"Point Cloud Recombination: Systematic Real Data Augmentation Using\n  Robotic Targets for LiDAR Perception Validation","summary":"  The validation of LiDAR-based perception of intelligent mobile systems\noperating in open-world applications remains a challenge due to the variability\nof real environmental conditions. Virtual simulations allow the generation of\narbitrary scenes under controlled conditions but lack physical sensor\ncharacteristics, such as intensity responses or material-dependent effects. In\ncontrast, real-world data offers true sensor realism but provides less control\nover influencing factors, hindering sufficient validation. Existing approaches\naddress this problem with augmentation of real-world point cloud data by\ntransferring objects between scenes. However, these methods do not consider\nvalidation and remain limited in controllability because they rely on empirical\ndata. We solve these limitations by proposing Point Cloud Recombination, which\nsystematically augments captured point cloud scenes by integrating point clouds\nacquired from physical target objects measured in controlled laboratory\nenvironments. Thus enabling the creation of vast amounts and varieties of\nrepeatable, physically accurate test scenes with respect to phenomena-aware\nocclusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we\ndemonstrate the augmentation of real-world urban and rural scenes with humanoid\ntargets featuring varied clothing and poses, for repeatable positioning. We\nshow that the recombined scenes closely match real sensor outputs, enabling\ntargeted testing, scalable failure analysis, and improved system safety. By\nproviding controlled yet sensor-realistic data, our method enables trustworthy\nconclusions about the limitations of specific sensors in compound with their\nalgorithms, e.g., object detection.\n","authors":["Hubert Padusinski","Christian Steinhauser","Christian Scherl","Julian Gaal","Jacob Langner"],"pdf_url":"https://arxiv.org/pdf/2505.02476v2.pdf","comment":"Pre-print for IEEE IAVVC 2025"},{"id":"http://arxiv.org/abs/2509.03270v1","updated":"2025-09-03T12:41:25Z","published":"2025-09-03T12:41:25Z","title":"AI Safety Assurance in Electric Vehicles: A Case Study on AI-Driven SOC\n  Estimation","summary":"  Integrating Artificial Intelligence (AI) technology in electric vehicles (EV)\nintroduces unique challenges for safety assurance, particularly within the\nframework of ISO 26262, which governs functional safety in the automotive\ndomain. Traditional assessment methodologies are not geared toward evaluating\nAI-based functions and require evolving standards and practices. This paper\nexplores how an independent assessment of an AI component in an EV can be\nachieved when combining ISO 26262 with the recently released ISO/PAS 8800,\nwhose scope is AI safety for road vehicles. The AI-driven State of Charge (SOC)\nbattery estimation exemplifies the process. Key features relevant to the\nindependent assessment of this extended evaluation approach are identified. As\npart of the evaluation, robustness testing of the AI component is conducted\nusing fault injection experiments, wherein perturbed sensor inputs are\nsystematically introduced to assess the component's resilience to input\nvariance.\n","authors":["Martin Skoglund","Fredrik Warg","Aria Mirzai","Anders Thorsen","Karl Lundgren","Peter Folkesson","Bastian Havers-zulka"],"pdf_url":"https://arxiv.org/pdf/2509.03270v1.pdf","comment":"12 pages, 9 figures, EVS38,\n  https://evs38-program.org/en/evs-38-proceedings/all"},{"id":"http://arxiv.org/abs/2509.03261v1","updated":"2025-09-03T12:23:46Z","published":"2025-09-03T12:23:46Z","title":"Parallel-Constraint Model Predictive Control: Exploiting Parallel\n  Computation for Improving Safety","summary":"  Ensuring constraint satisfaction is a key requirement for safety-critical\nsystems, which include most robotic platforms. For example, constraints can be\nused for modeling joint position/velocity/torque limits and collision\navoidance. Constrained systems are often controlled using Model Predictive\nControl, because of its ability to naturally handle constraints, relying on\nnumerical optimization. However, ensuring constraint satisfaction is\nchallenging for nonlinear systems/constraints. A well-known tool to make\ncontrollers safe is the so-called control-invariant set (a.k.a. safe set). In\nour previous work, we have shown that safety can be improved by letting the\nsafe-set constraint recede along the MPC horizon. In this paper, we push that\nidea further by exploiting parallel computation to improve safety. We solve\nseveral MPC problems at the same time, where each problem instantiates the\nsafe-set constraint at a different time step along the horizon. Finally, the\ncontroller can select the best solution according to some user-defined\ncriteria. We validated this idea through extensive simulations with a 3-joint\nrobotic arm, showing that significant improvements can be achieved in terms of\nsafety and performance, even using as little as 4 computational cores.\n","authors":["Elias Fontanari","Gianni Lunardi","Matteo Saveriano","Andrea Del Prete"],"pdf_url":"https://arxiv.org/pdf/2509.03261v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01197v2","updated":"2025-09-03T12:05:59Z","published":"2025-08-02T05:05:50Z","title":"A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding","summary":"  Visual grounding aims to identify objects or regions in a scene based on\nnatural language descriptions, essential for spatially aware perception in\nautonomous driving. However, existing visual grounding tasks typically depend\non bounding boxes that often fail to capture fine-grained details. Not all\nvoxels within a bounding box are occupied, resulting in inaccurate object\nrepresentations. To address this, we introduce a benchmark for 3D occupancy\ngrounding in challenging outdoor scenes. Built on the nuScenes dataset, it\nintegrates natural language with voxel-level occupancy annotations, offering\nmore precise object perception compared to the traditional grounding task.\nMoreover, we propose GroundingOcc, an end-to-end model designed for 3D\noccupancy grounding through multi-modal learning. It combines visual, textual,\nand point cloud features to predict object location and occupancy information\nfrom coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder\nfor feature extraction, an occupancy head for voxel-wise predictions, and a\ngrounding head to refine localization. Additionally, a 2D grounding module and\na depth estimation module enhance geometric understanding, thereby boosting\nmodel performance. Extensive experiments on the benchmark demonstrate that our\nmethod outperforms existing baselines on 3D occupancy grounding. The dataset is\navailable at https://github.com/RONINGOD/GroundingOcc.\n","authors":["Zhan Shi","Song Wang","Junbo Chen","Jianke Zhu"],"pdf_url":"https://arxiv.org/pdf/2508.01197v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03238v1","updated":"2025-09-03T11:52:06Z","published":"2025-09-03T11:52:06Z","title":"Vibration Damping in Underactuated Cable-suspended Artwork -- Flying\n  Belt Motion Control","summary":"  This paper presents a comprehensive refurbishment of the interactive robotic\nart installation Standards and Double Standards by Rafael Lozano-Hemmer. The\ninstallation features an array of belts suspended from the ceiling, each\nactuated by stepper motors and dynamically oriented by a vision-based tracking\nsystem that follows the movements of exhibition visitors. The original system\nwas limited by oscillatory dynamics, resulting in torsional and pendulum-like\nvibrations that constrained rotational speed and reduced interactive\nresponsiveness. To address these challenges, the refurbishment involved\nsignificant upgrades to both hardware and motion control algorithms. A detailed\nmathematical model of the flying belt system was developed to accurately\ncapture its dynamic behavior, providing a foundation for advanced control\ndesign. An input shaping method, formulated as a convex optimization problem,\nwas implemented to effectively suppress vibrations, enabling smoother and\nfaster belt movements. Experimental results demonstrate substantial\nimprovements in system performance and audience interaction. This work\nexemplifies the integration of robotics, control engineering, and interactive\nart, offering new solutions to technical challenges in real-time motion control\nand vibration damping for large-scale kinetic installations.\n","authors":["Martin Goubej","Lauria Clarke","Martin Hrabačka","David Tolar"],"pdf_url":"https://arxiv.org/pdf/2509.03238v1.pdf","comment":"10 pages, 10 figures"},{"id":"http://arxiv.org/abs/2509.03231v1","updated":"2025-09-03T11:42:24Z","published":"2025-09-03T11:42:24Z","title":"Exploring persuasive Interactions with generative social robots: An\n  experimental framework","summary":"  Integrating generative AI such as large language models into social robots\nhas improved their ability to engage in natural, human-like communication. This\nstudy presents a method to examine their persuasive capabilities. We designed\nan experimental framework focused on decision making and tested it in a pilot\nthat varied robot appearance and self-knowledge. Using qualitative analysis, we\nevaluated interaction quality, persuasion effectiveness, and the robot's\ncommunicative strategies. Participants generally experienced the interaction\npositively, describing the robot as competent, friendly, and supportive, while\nnoting practical limits such as delayed responses and occasional\nspeech-recognition errors. Persuasiveness was highly context dependent and\nshaped by robot behavior: participants responded well to polite, reasoned\nsuggestions and expressive gestures, but emphasized the need for more\npersonalized, context-aware arguments and clearer social roles. These findings\nsuggest that generative social robots can influence user decisions, but their\neffectiveness depends on communicative nuance and contextual relevance. We\npropose refinements to the framework to further study persuasive dynamics\nbetween robots and human users.\n","authors":["Stephan Vonschallen","Larissa Julia Corina Finsler","Theresa Schmiedel","Friederike Eyssel"],"pdf_url":"https://arxiv.org/pdf/2509.03231v1.pdf","comment":"A shortened version of this paper was accepted as poster for the\n  Thirteenth International Conference on Human-Agent Interaction (HAI2025)"},{"id":"http://arxiv.org/abs/2509.03222v1","updated":"2025-09-03T11:25:36Z","published":"2025-09-03T11:25:36Z","title":"The Role of Embodiment in Intuitive Whole-Body Teleoperation for Mobile\n  Manipulation","summary":"  Intuitive Teleoperation interfaces are essential for mobile manipulation\nrobots to ensure high quality data collection while reducing operator workload.\nA strong sense of embodiment combined with minimal physical and cognitive\ndemands not only enhances the user experience during large-scale data\ncollection, but also helps maintain data quality over extended periods. This\nbecomes especially crucial for challenging long-horizon mobile manipulation\ntasks that require whole-body coordination. We compare two distinct robot\ncontrol paradigms: a coupled embodiment integrating arm manipulation and base\nnavigation functions, and a decoupled embodiment treating these systems as\nseparate control entities. Additionally, we evaluate two visual feedback\nmechanisms: immersive virtual reality and conventional screen-based\nvisualization of the robot's field of view. These configurations were\nsystematically assessed across a complex, multi-stage task sequence requiring\nintegrated planning and execution. Our results show that the use of VR as a\nfeedback modality increases task completion time, cognitive workload, and\nperceived effort of the teleoperator. Coupling manipulation and navigation\nleads to a comparable workload on the user as decoupling the embodiments, while\npreliminary experiments suggest that data acquired by coupled teleoperation\nleads to better imitation learning performance. Our holistic view on intuitive\nteleoperation interfaces provides valuable insight into collecting\nhigh-quality, high-dimensional mobile manipulation data at scale with the human\noperator in mind. Project\nwebsite:https://sophiamoyen.github.io/role-embodiment-wbc-moma-teleop/\n","authors":["Sophia Bianchi Moyen","Rickmer Krohn","Sophie Lueth","Kay Pompetzki","Jan Peters","Vignesh Prasad","Georgia Chalvatzaki"],"pdf_url":"https://arxiv.org/pdf/2509.03222v1.pdf","comment":"8 pages, 8 figures, Accepted at the IEEE-RAS International Conference\n  on Humanoid Robots (Humanoids) 2025"},{"id":"http://arxiv.org/abs/2509.03211v1","updated":"2025-09-03T11:00:17Z","published":"2025-09-03T11:00:17Z","title":"Efficient Active Training for Deep LiDAR Odometry","summary":"  Robust and efficient deep LiDAR odometry models are crucial for accurate\nlocalization and 3D reconstruction, but typically require extensive and diverse\ntraining data to adapt to diverse environments, leading to inefficiencies. To\ntackle this, we introduce an active training framework designed to selectively\nextract training data from diverse environments, thereby reducing the training\nload and enhancing model generalization. Our framework is based on two key\nstrategies: Initial Training Set Selection (ITSS) and Active Incremental\nSelection (AIS). ITSS begins by breaking down motion sequences from general\nweather into nodes and edges for detailed trajectory analysis, prioritizing\ndiverse sequences to form a rich initial training dataset for training the base\nmodel. For complex sequences that are difficult to analyze, especially under\nchallenging snowy weather conditions, AIS uses scene reconstruction and\nprediction inconsistency to iteratively select training samples, refining the\nmodel to handle a wide range of real-world scenarios. Experiments across\ndatasets and weather conditions validate our approach's effectiveness. Notably,\nour method matches the performance of full-dataset training with just 52\\% of\nthe sequence volume, demonstrating the training efficiency and robustness of\nour active training paradigm. By optimizing the training process, our approach\nsets the stage for more agile and reliable LiDAR odometry systems, capable of\nnavigating diverse environmental conditions with greater precision.\n","authors":["Beibei Zhou","Zhiyuan Zhang","Zhenbo Song","Jianhui Guo","Hui Kong"],"pdf_url":"https://arxiv.org/pdf/2509.03211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00024v2","updated":"2025-09-03T10:51:34Z","published":"2024-03-20T18:01:20Z","title":"Hey, Teacher, (Don't) Leave Those Kids Alone: Standardizing HRI\n  Education","summary":"  Creating a standardized introduction course becomes more critical as the\nfield of human-robot interaction (HRI) becomes more established. This paper\noutlines the key components necessary to provide an undergraduate with a\nsufficient foundational understanding of the interdisciplinary nature of this\nfield and provides proposed course content. It emphasizes the importance of\ncreating a course with theoretical and experimental components to accommodate\nall different learning preferences. This manuscript also advocates creating or\nadopting a universal platform to standardize the hands-on component of\nintroductory HRI courses, regardless of university funding or size. Next, it\nrecommends formal training in how to read scientific articles and staying\nup-to-date with the latest relevant papers. Finally, it provides detailed\nlecture content and project milestones for a 15-week semester. By creating a\nstandardized course, researchers can ensure consistency and quality are\nmaintained across institutions, which will help students as well as industrial\nand academic employers understand what foundational knowledge is expected.\n","authors":["Alexis E. Block"],"pdf_url":"https://arxiv.org/pdf/2404.00024v2.pdf","comment":"Presented at the Designing an Intro to HRI Course Workshop at HRI\n  2024 (arXiv:2403.05588)"},{"id":"http://arxiv.org/abs/2508.01415v3","updated":"2025-09-03T09:32:27Z","published":"2025-08-02T15:39:42Z","title":"RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems","summary":"  We present RoboMemory, a brain-inspired multi-memory framework for lifelong\nlearning in physical embodied systems, addressing critical challenges in\nreal-world environments: continuous learning, multi-module memory latency, task\ncorrelation capture, and infinite-loop mitigation in closed-loop planning.\nGrounded in cognitive neuroscience, it integrates four core modules: the\nInformation Preprocessor (thalamus-like), the Lifelong Embodied Memory System\n(hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and\nthe Low-Level Executer (cerebellum-like) to enable long-term planning and\ncumulative learning. The Lifelong Embodied Memory System, central to the\nframework, alleviates inference speed issues in complex memory frameworks via\nparallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic\nsubmodules. It incorporates a dynamic Knowledge Graph (KG) and consistent\narchitectural design to enhance memory consistency and scalability. Evaluations\non EmbodiedBench show RoboMemory outperforms the open-source baseline\n(Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the\nclosed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing\nnew SOTA. Ablation studies validate key components (critic, spatial memory,\nlong-term memory), while real-world deployment confirms its lifelong learning\ncapability with significantly improved success rates across repeated tasks.\nRoboMemory alleviates high latency challenges with scalability, serving as a\nfoundational reference for integrating multi-modal memory systems in physical\nrobots.\n","authors":["Mingcong Lei","Honghao Cai","Binbin Que","Zezhou Cui","Liangchen Tan","Junkun Hong","Gehan Hu","Shuangyu Zhu","Yimou Wu","Shaohan Jiang","Ge Wang","Zhen Li","Shuguang Cui","Yiming Zhao","Yatong Han"],"pdf_url":"https://arxiv.org/pdf/2508.01415v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.18840v2","updated":"2025-09-03T09:31:59Z","published":"2025-04-26T07:56:52Z","title":"Distributed Lloyd-Based Algorithm for Uncertainty-Aware Multi-Robot\n  Under-Canopy Flocking","summary":"  In this letter, we present a distributed algorithm for flocking in complex\nenvironments that operates at constant altitude, without explicit\ncommunication, no a priori information about the environment, and by using only\non-board sensing and computation capabilities. We provide sufficient conditions\nto guarantee that each robot reaches its goal region in a finite time, avoiding\ncollisions with obstacles and other robots without exceeding a desired maximum\ndistance from a predefined set of neighbors (flocking or proximity constraint).\nThe proposed approach allows to operate in crowded scenarios and to deal with\ntracking errors and on-board sensing errors, without violating safety and\nproximity constraints. The algorithm was verified through simulations with\nvarying number of UAVs and also through numerous real-world experiments in a\ndense forest involving up to four UAVs.\n","authors":["Manuel Boldrer","Vit Kratky","Viktor Walter","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2504.18840v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03140v1","updated":"2025-09-03T08:50:41Z","published":"2025-09-03T08:50:41Z","title":"Decentralised self-organisation of pivoting cube ensembles using\n  geometric deep learning","summary":"  We present a decentralized model for autonomous reconfiguration of\nhomogeneous pivoting cube modular robots in two dimensions. Each cube in the\nensemble is controlled by a neural network that only gains information from\nother cubes in its local neighborhood, trained using reinforcement learning.\nFurthermore, using geometric deep learning, we include the grid symmetries of\nthe cube ensemble in the neural network architecture. We find that even the\nmost localized versions succeed in reconfiguring to the target shape, although\nreconfiguration happens faster the more information about the whole ensemble is\navailable to individual cubes. Near-optimal reconfiguration is achieved with\nonly nearest neighbor interactions by using multiple information passing\nbetween cubes, allowing them to accumulate more global information about the\nensemble. Compared to standard neural network architectures, using geometric\ndeep learning approaches provided only minor benefits. Overall, we successfully\ndemonstrate mostly local control of a modular self-assembling system, which is\ntransferable to other space-relevant systems with different action spaces, such\nas sliding cube modular robots and CubeSat swarms.\n","authors":["Nadezhda Dobreva","Emmanuel Blazquez","Jai Grover","Dario Izzo","Yuzhen Qin","Dominik Dold"],"pdf_url":"https://arxiv.org/pdf/2509.03140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03119v1","updated":"2025-09-03T08:20:55Z","published":"2025-09-03T08:20:55Z","title":"Forbal: Force Balanced 2-5 Degree of Freedom Robot Manipulator Built\n  from a Five Bar Linkage","summary":"  A force balanced manipulator design based on the closed chain planar five bar\nlinkage is developed and experimentally validated. We present 2 variants as a\nmodular design: Forbal-2, a planar 2-DOF manipulator, and its extension to\n5-DOF spatial motion called Forbal-5. The design considerations in terms of\ngeometric, kinematic, and dynamic design that fulfill the force balance\nconditions while maximizing workspace are discussed. Then, the inverse\nkinematics of both variants are derived from geometric principles.\n  We validate the improvements from force balancing the manipulator through\ncomparative experiments with counter mass balanced and unbalanced\nconfigurations. The results show how the balanced configuration yields a\nreduction in the average reaction moments of up to 66\\%, a reduction of average\njoint torques of up to 79\\%, as well as a noticeable reduction in position\nerror for Forbal-2. For Forbal-5, which has a higher end effector payload mass,\nthe joint torques are reduced up to 84\\% for the balanced configuration.\nExperimental results validate that the balanced manipulator design is suitable\nfor applications where the reduction of joint torques and reaction\nforces/moments helps achieve millimeter level precision.\n","authors":["Yash Vyas","Matteo Bottin"],"pdf_url":"https://arxiv.org/pdf/2509.03119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03030v1","updated":"2025-09-03T05:33:46Z","published":"2025-09-03T05:33:46Z","title":"Population-aware Online Mirror Descent for Mean-Field Games with Common\n  Noise by Deep Reinforcement Learning","summary":"  Mean Field Games (MFGs) offer a powerful framework for studying large-scale\nmulti-agent systems. Yet, learning Nash equilibria in MFGs remains a\nchallenging problem, particularly when the initial distribution is unknown or\nwhen the population is subject to common noise. In this paper, we introduce an\nefficient deep reinforcement learning (DRL) algorithm designed to achieve\npopulation-dependent Nash equilibria without relying on averaging or historical\nsampling, inspired by Munchausen RL and Online Mirror Descent. The resulting\npolicy is adaptable to various initial distributions and sources of common\nnoise. Through numerical experiments on seven canonical examples, we\ndemonstrate that our algorithm exhibits superior convergence properties\ncompared to state-of-the-art algorithms, particularly a DRL version of\nFictitious Play for population-dependent policies. The performance in the\npresence of common noise underscores the robustness and adaptability of our\napproach.\n","authors":["Zida Wu","Mathieu Lauriere","Matthieu Geist","Olivier Pietquin","Ankur Mehta"],"pdf_url":"https://arxiv.org/pdf/2509.03030v1.pdf","comment":"2025 IEEE 64rd Conference on Decision and Control (CDC)"},{"id":"http://arxiv.org/abs/2509.02283v2","updated":"2025-09-03T05:06:33Z","published":"2025-09-02T13:07:02Z","title":"Sem-RaDiff: Diffusion-Based 3D Radar Semantic Perception in Cluttered\n  Agricultural Environments","summary":"  Accurate and robust environmental perception is crucial for robot autonomous\nnavigation. While current methods typically adopt optical sensors (e.g.,\ncamera, LiDAR) as primary sensing modalities, their susceptibility to visual\nocclusion often leads to degraded performance or complete system failure. In\nthis paper, we focus on agricultural scenarios where robots are exposed to the\nrisk of onboard sensor contamination. Leveraging radar's strong penetration\ncapability, we introduce a radar-based 3D environmental perception framework as\na viable alternative. It comprises three core modules designed for dense and\naccurate semantic perception: 1) Parallel frame accumulation to enhance\nsignal-to-noise ratio of radar raw data. 2) A diffusion model-based\nhierarchical learning framework that first filters radar sidelobe artifacts\nthen generates fine-grained 3D semantic point clouds. 3) A specifically\ndesigned sparse 3D network optimized for processing large-scale radar raw data.\nWe conducted extensive benchmark comparisons and experimental evaluations on a\nself-built dataset collected in real-world agricultural field scenes. Results\ndemonstrate that our method achieves superior structural and semantic\nprediction performance compared to existing methods, while simultaneously\nreducing computational and memory costs by 51.3% and 27.5%, respectively.\nFurthermore, our approach achieves complete reconstruction and accurate\nclassification of thin structures such as poles and wires-which existing\nmethods struggle to perceive-highlighting its potential for dense and accurate\n3D radar perception.\n","authors":["Ruibin Zhang","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2509.02283v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03012v1","updated":"2025-09-03T04:41:43Z","published":"2025-09-03T04:41:43Z","title":"Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly\n  Domain Adaptive Dense Regression","summary":"  Deep neural networks (DNNs) are increasingly being used in autonomous\nsystems. However, DNNs do not generalize well to domain shift. Adapting to a\ncontinuously evolving environment is a safety-critical challenge inevitably\nfaced by all autonomous systems deployed to the real world. Recent work on\ntest-time training proposes methods that adapt to a new test distribution on\nthe fly by optimizing the DNN model for each test input using self-supervision.\nHowever, these techniques result in a sharp increase in inference time as\nmultiple forward and backward passes are required for a single test sample (for\ntest-time training) before finally making the prediction based on the\nfine-tuned features. This is undesirable for real-world robotics applications\nwhere these models may be deployed to resource constraint hardware with strong\nlatency requirements. In this work, we propose a new framework (called UT$^3$)\nthat leverages test-time training for improved performance in the presence of\ncontinuous domain shift while also decreasing the inference time, making it\nsuitable for real-world applications. Our method proposes an uncertainty-aware\nself-supervision task for efficient test-time training that leverages the\nquantified uncertainty to selectively apply the training leading to sharp\nimprovements in the inference time while performing comparably to standard\ntest-time training protocol. Our proposed protocol offers a continuous setting\nto identify the selected keyframes, allowing the end-user to control how often\nto apply test-time training. We demonstrate the efficacy of our method on a\ndense regression task - monocular depth estimation.\n","authors":["Uddeshya Upadhyay"],"pdf_url":"https://arxiv.org/pdf/2509.03012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.08624v2","updated":"2025-09-03T04:32:06Z","published":"2025-08-12T04:27:12Z","title":"Communication Efficient Robotic Mixed Reality with Gaussian Splatting\n  Cross-Layer Optimization","summary":"  Realizing low-cost communication in robotic mixed reality (RoboMR) systems\npresents a challenge, due to the necessity of uploading high-resolution images\nthrough wireless channels. This paper proposes Gaussian splatting (GS) RoboMR\n(GSMR), which enables the simulator to opportunistically render a\nphoto-realistic view from the robot's pose by calling ``memory'' from a GS\nmodel, thus reducing the need for excessive image uploads. However, the GS\nmodel may involve discrepancies compared to the actual environments. To this\nend, a GS cross-layer optimization (GSCLO) framework is further proposed, which\njointly optimizes content switching (i.e., deciding whether to upload image or\nnot) and power allocation (i.e., adjusting to content profiles) across\ndifferent frames by minimizing a newly derived GSMR loss function. The GSCLO\nproblem is addressed by an accelerated penalty optimization (APO) algorithm\nthat reduces computational complexity by over $10$x compared to traditional\nbranch-and-bound and search algorithms. Moreover, variants of GSCLO are\npresented to achieve robust, low-power, and multi-robot GSMR. Extensive\nexperiments demonstrate that the proposed GSMR paradigm and GSCLO method\nachieve significant improvements over existing benchmarks on both wheeled and\nlegged robots in terms of diverse metrics in various scenarios. For the first\ntime, it is found that RoboMR can be achieved with ultra-low communication\ncosts, and mixture of data is useful for enhancing GS performance in dynamic\nscenarios.\n","authors":["Chenxuan Liu","He Li","Zongze Li","Shuai Wang","Wei Xu","Kejiang Ye","Derrick Wing Kwan Ng","Chengzhong Xu"],"pdf_url":"https://arxiv.org/pdf/2508.08624v2.pdf","comment":"14 pages, 18 figures, to appear in IEEE Transactions on Cognitive\n  Communications and Networking"},{"id":"http://arxiv.org/abs/2408.16206v2","updated":"2025-09-03T04:01:03Z","published":"2024-08-29T01:57:57Z","title":"RMMI: Reactive Mobile Manipulation using an Implicit Neural Map","summary":"  Mobile manipulator robots operating in complex domestic and industrial\nenvironments must effectively coordinate their base and arm motions while\navoiding obstacles. While current reactive control methods gracefully achieve\nthis coordination, they rely on simplified and idealised geometric\nrepresentations of the environment to avoid collisions. This limits their\nperformance in cluttered environments. To address this problem, we introduce\nRMMI, a reactive control framework that leverages the ability of neural Signed\nDistance Fields (SDFs) to provide a continuous and differentiable\nrepresentation of the environment's geometry. RMMI formulates a quadratic\nprogram that optimises jointly for robot base and arm motion, maximises the\nmanipulability, and avoids collisions through a set of inequality constraints.\nThese constraints are constructed by querying the SDF for the distance and\ndirection to the closest obstacle for a large number of sampling points on the\nrobot. We evaluate RMMI both in simulation and in a set of real-world\nexperiments. For reaching in cluttered environments, we observe a 25% increase\nin success rate. For additional details, code, and experiment videos, please\nvisit https://rmmi.github.io/.\n","authors":["Nicolas Marticorena","Tobias Fischer","Jesse Haviland","Niko Suenderhauf"],"pdf_url":"https://arxiv.org/pdf/2408.16206v2.pdf","comment":"8 pages, 6 figures, accepted to the IEEE/RSJ International Conference\n  on Intelligent Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2504.18829v2","updated":"2025-09-03T03:54:22Z","published":"2025-04-26T07:32:59Z","title":"Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy","summary":"  Generalizable dexterous grasping with suitable grasp types is a fundamental\nskill for intelligent robots. Developing such skills requires a large-scale and\nhigh-quality dataset that covers numerous grasp types (i.e., at least those\ncategorized by the GRASP taxonomy), but collecting such data is extremely\nchallenging. Existing automatic grasp synthesis methods are often limited to\nspecific grasp types or object categories, hindering scalability. This work\nproposes an efficient pipeline capable of synthesizing contact-rich,\npenetration-free, and physically plausible grasps for any grasp type, object,\nand articulated hand. Starting from a single human-annotated template for each\nhand and grasp type, our pipeline tackles the complicated synthesis problem\nwith two stages: optimize the object to fit the hand template first, and then\nlocally refine the hand to fit the object in simulation. To validate the\nsynthesized grasps, we introduce a contact-aware control strategy that allows\nthe hand to apply the appropriate force at each contact point to the object.\nThose validated grasps can also be used as new grasp templates to facilitate\nfuture synthesis. Experiments show that our method significantly outperforms\nprevious type-unaware grasp synthesis baselines in simulation. Using our\nalgorithm, we construct a dataset containing 10.7k objects and 9.5M grasps,\ncovering 31 grasp types in the GRASP taxonomy. Finally, we train a\ntype-conditional generative model that successfully performs the desired grasp\ntype from single-view object point clouds, achieving an 82.3% success rate in\nreal-world experiments. Project page: https://pku-epic.github.io/Dexonomy.\n","authors":["Jiayi Chen","Yubin Ke","Lin Peng","He Wang"],"pdf_url":"https://arxiv.org/pdf/2504.18829v2.pdf","comment":"Accepted by Robotics: Science and Systems (RSS 2025)"},{"id":"http://arxiv.org/abs/2509.02986v1","updated":"2025-09-03T03:46:43Z","published":"2025-09-03T03:46:43Z","title":"CTBC: Contact-Triggered Blind Climbing for Wheeled Bipedal Robots with\n  Instruction Learning and Reinforcement Learning","summary":"  In recent years, wheeled bipedal robots have gained increasing attention due\nto their advantages in mobility, such as high-speed locomotion on flat terrain.\nHowever, their performance on complex environments (e.g., staircases) remains\ninferior to that of traditional legged robots. To overcome this limitation, we\npropose a general contact-triggered blind climbing (CTBC) framework for wheeled\nbipedal robots. Upon detecting wheel-obstacle contact, the robot triggers a\nleg-lifting motion to overcome the obstacle. By leveraging a strongly-guided\nfeedforward trajectory, our method enables the robot to rapidly acquire agile\nleg-lifting skills, significantly enhancing its capability to traverse\nunstructured terrains. The approach has been experimentally validated and\nsuccessfully deployed on LimX Dynamics' wheeled bipedal robot, Tron1.\nReal-world tests demonstrate that Tron1 can reliably climb obstacles well\nbeyond its wheel radius using only proprioceptive feedback.\n","authors":["Rankun Li","Hao Wang","Qi Li","Zhuo Han","Yifei Chu","Linqi Ye","Wende Xie","Wenlong Liao"],"pdf_url":"https://arxiv.org/pdf/2509.02986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02983v1","updated":"2025-09-03T03:43:12Z","published":"2025-09-03T03:43:12Z","title":"DUViN: Diffusion-Based Underwater Visual Navigation via\n  Knowledge-Transferred Depth Features","summary":"  Autonomous underwater navigation remains a challenging problem due to limited\nsensing capabilities and the difficulty of constructing accurate maps in\nunderwater environments. In this paper, we propose a Diffusion-based Underwater\nVisual Navigation policy via knowledge-transferred depth features, named DUViN,\nwhich enables vision-based end-to-end 4-DoF motion control for underwater\nvehicles in unknown environments. DUViN guides the vehicle to avoid obstacles\nand maintain a safe and perception awareness altitude relative to the terrain\nwithout relying on pre-built maps. To address the difficulty of collecting\nlarge-scale underwater navigation datasets, we propose a method that ensures\nrobust generalization under domain shifts from in-air to underwater\nenvironments by leveraging depth features and introducing a novel model\ntransfer strategy. Specifically, our training framework consists of two phases:\nwe first train the diffusion-based visual navigation policy on in-air datasets\nusing a pre-trained depth feature extractor. Secondly, we retrain the extractor\non an underwater depth estimation task and integrate the adapted extractor into\nthe trained navigation policy from the first step. Experiments in both\nsimulated and real-world underwater environments demonstrate the effectiveness\nand generalization of our approach. The experimental videos are available at\nhttps://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.\n","authors":["Jinghe Yang","Minh-Quan Le","Mingming Gong","Ye Pu"],"pdf_url":"https://arxiv.org/pdf/2509.02983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16490v3","updated":"2025-09-03T03:39:08Z","published":"2024-12-21T05:22:53Z","title":"BODex: Scalable and Efficient Robotic Dexterous Grasp Synthesis Using\n  Bilevel Optimization","summary":"  Robotic dexterous grasping is important for interacting with the environment.\nTo unleash the potential of data-driven models for dexterous grasping, a\nlarge-scale, high-quality dataset is essential. While gradient-based\noptimization offers a promising way for constructing such datasets, previous\nworks suffer from limitations, such as inefficiency, strong assumptions in the\ngrasp quality energy, or limited object sets for experiments. Moreover, the\nlack of a standard benchmark for comparing different methods and datasets\nhinders progress in this field. To address these challenges, we develop a\nhighly efficient synthesis system and a comprehensive benchmark with MuJoCo for\ndexterous grasping. We formulate grasp synthesis as a bilevel optimization\nproblem, combining a novel lower-level quadratic programming (QP) with an\nupper-level gradient descent process. By leveraging recent advances in\nCUDA-accelerated robotic libraries and GPU-based QP solvers, our system can\nparallelize thousands of grasps and synthesize over 49 grasps per second on a\nsingle 3090 GPU. Our synthesized grasps for Shadow, Allegro, and Leap hands all\nachieve a success rate above 75% in simulation, with a penetration depth under\n1 mm, outperforming existing baselines on nearly all metrics. Compared to the\nprevious large-scale dataset, DexGraspNet, our dataset significantly improves\nthe performance of learning models, with a success rate from around 40% to 80%\nin simulation. Real-world testing of the trained model on the Shadow Hand\nachieves an 81% success rate across 20 diverse objects. The codes and datasets\nare released on our project page: https://pku-epic.github.io/BODex.\n","authors":["Jiayi Chen","Yubin Ke","He Wang"],"pdf_url":"https://arxiv.org/pdf/2412.16490v3.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2509.02972v1","updated":"2025-09-03T03:19:09Z","published":"2025-09-03T03:19:09Z","title":"IL-SLAM: Intelligent Line-assisted SLAM Based on Feature Awareness for\n  Dynamic Environments","summary":"  Visual Simultaneous Localization and Mapping (SLAM) plays a crucial role in\nautonomous systems. Traditional SLAM methods, based on static environment\nassumptions, struggle to handle complex dynamic environments. Recent dynamic\nSLAM systems employ geometric constraints and deep learning to remove dynamic\nfeatures, yet this creates a new challenge: insufficient remaining point\nfeatures for subsequent SLAM processes. Existing solutions address this by\ncontinuously introducing additional line and plane features to supplement point\nfeatures, achieving robust tracking and pose estimation. However, current\nmethods continuously introduce additional features regardless of necessity,\ncausing two problems: unnecessary computational overhead and potential\nperformance degradation from accumulated low-quality additional features and\nnoise. To address these issues, this paper proposes a feature-aware mechanism\nthat evaluates whether current features are adequate to determine if line\nfeature support should be activated. This decision mechanism enables the system\nto introduce line features only when necessary, significantly reducing\ncomputational complexity of additional features while minimizing the\nintroduction of low-quality features and noise. In subsequent processing, the\nintroduced line features assist in obtaining better initial camera poses\nthrough tracking, local mapping, and loop closure, but are excluded from global\noptimization to avoid potential negative impacts from low-quality additional\nfeatures in long-term process. Extensive experiments on TUM datasets\ndemonstrate substantial improvements in both ATE and RPE metrics compared to\nORB-SLAM3 baseline and superior performance over other dynamic SLAM and\nmulti-feature methods.\n","authors":["Haolan Zhang","Thanh Nguyen Canh","Chenghao Li","Ruidong Yang","Yonghoon Ji","Nak Young Chong"],"pdf_url":"https://arxiv.org/pdf/2509.02972v1.pdf","comment":"submitted to International Conference on Robotic Computing and\n  Communication(IEEE IRC)"},{"id":"http://arxiv.org/abs/2509.02930v1","updated":"2025-09-03T01:53:29Z","published":"2025-09-03T01:53:29Z","title":"VendiRL: A Framework for Self-Supervised Reinforcement Learning of\n  Diversely Diverse Skills","summary":"  In self-supervised reinforcement learning (RL), one of the key challenges is\nlearning a diverse set of skills to prepare agents for unknown future tasks.\nDespite impressive advances, scalability and evaluation remain prevalent\nissues. Regarding scalability, the search for meaningful skills can be obscured\nby high-dimensional feature spaces, where relevant features may vary across\ndownstream task domains. For evaluating skill diversity, defining what\nconstitutes \"diversity\" typically requires a hard commitment to a specific\nnotion of what it means for skills to be diverse, potentially leading to\ninconsistencies in how skill diversity is understood, making results across\ndifferent approaches hard to compare, and leaving many forms of diversity\nunexplored. To address these issues, we adopt a measure of sample diversity\nthat translates ideas from ecology to machine learning -- the Vendi Score --\nallowing the user to specify and evaluate any desired form of diversity. We\ndemonstrate how this metric facilitates skill evaluation and introduce VendiRL,\na unified framework for learning diversely diverse sets of skills. Given\ndistinct similarity functions, VendiRL motivates distinct forms of diversity,\nwhich could support skill-diversity pretraining in new and richly interactive\nenvironments where optimising for various forms of diversity may be desirable.\n","authors":["Erik M. Lintunen"],"pdf_url":"https://arxiv.org/pdf/2509.02930v1.pdf","comment":"17 pages including appendices"},{"id":"http://arxiv.org/abs/2507.00917v3","updated":"2025-09-03T01:44:58Z","published":"2025-07-01T16:23:00Z","title":"A Survey: Learning Embodied Intelligence from Physical Simulators and\n  World Models","summary":"  The pursuit of artificial general intelligence (AGI) has placed embodied\nintelligence at the forefront of robotics research. Embodied intelligence\nfocuses on agents capable of perceiving, reasoning, and acting within the\nphysical world. Achieving robust embodied intelligence requires not only\nadvanced perception and control, but also the ability to ground abstract\ncognition in real-world interactions. Two foundational technologies, physical\nsimulators and world models, have emerged as critical enablers in this quest.\nPhysical simulators provide controlled, high-fidelity environments for training\nand evaluating robotic agents, allowing safe and efficient development of\ncomplex behaviors. In contrast, world models empower robots with internal\nrepresentations of their surroundings, enabling predictive planning and\nadaptive decision-making beyond direct sensory input. This survey\nsystematically reviews recent advances in learning embodied AI through the\nintegration of physical simulators and world models. We analyze their\ncomplementary roles in enhancing autonomy, adaptability, and generalization in\nintelligent robots, and discuss the interplay between external simulation and\ninternal modeling in bridging the gap between simulated training and real-world\ndeployment. By synthesizing current progress and identifying open challenges,\nthis survey aims to provide a comprehensive perspective on the path toward more\ncapable and generalizable embodied AI systems. We also maintain an active\nrepository that contains up-to-date literature and open-source projects at\nhttps://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey.\n","authors":["Xiaoxiao Long","Qingrui Zhao","Kaiwen Zhang","Zihao Zhang","Dingrui Wang","Yumeng Liu","Zhengjie Shu","Yi Lu","Shouzheng Wang","Xinzhe Wei","Wei Li","Wei Yin","Yao Yao","Jia Pan","Qiu Shen","Ruigang Yang","Xun Cao","Qionghai Dai"],"pdf_url":"https://arxiv.org/pdf/2507.00917v3.pdf","comment":"Update with recent progresses. 49pages, 25figures, 6tables, github\n  repository avalible in\n  https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey"},{"id":"http://arxiv.org/abs/2509.02922v1","updated":"2025-09-03T01:19:40Z","published":"2025-09-03T01:19:40Z","title":"Approximate constrained stochastic optimal control via parameterized\n  input inference","summary":"  Approximate methods to solve stochastic optimal control (SOC) problems have\nreceived significant interest from researchers in the past decade.\nProbabilistic inference approaches to SOC have been developed to solve\nnonlinear quadratic Gaussian problems. In this work, we propose an\nExpectation-Maximization (EM) based inference procedure to generate\nstate-feedback controls for constrained SOC problems. We consider the\ninequality constraints for the state and controls and also the structural\nconstraints for the controls. We employ barrier functions to address state and\ncontrol constraints. We show that the expectation step leads to smoothing of\nthe state-control pair while the the maximization step on the non-zero subsets\nof the control parameters allows inference of structured stochastic optimal\ncontrollers. We demonstrate the effectiveness of the algorithm on unicycle\nobstacle avoidance, four-unicycle formation control, and quadcopter navigation\nin windy environment examples. In these examples, we perform an empirical study\non the parametric effect of barrier functions on the state constraint\nsatisfaction. We also present a comparative study of smoothing algorithms on\nthe performance of the proposed approach.\n","authors":["Shahbaz P Qadri Syed","He Bai"],"pdf_url":"https://arxiv.org/pdf/2509.02922v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2509.03775v1","updated":"2025-09-03T23:40:17Z","published":"2025-09-03T23:40:17Z","title":"ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast,\n  Memory-Efficient Reconstruction","summary":"  3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world\nscenes with high quality and real-time rendering. Typically, a higher quality\nrepresentation can be achieved by using a large number of 3D Gaussians.\nHowever, using large 3D Gaussian counts significantly increases the GPU device\nmemory for storing model parameters. A large model thus requires powerful GPUs\nwith high memory capacities for training and has slower training/rendering\nlatencies due to the inefficiencies of memory access and data movement. In this\nwork, we introduce ContraGS, a method to enable training directly on compressed\n3DGS representations without reducing the Gaussian Counts, and thus with a\nlittle loss in model quality. ContraGS leverages codebooks to compactly store a\nset of Gaussian parameter vectors throughout the training process, thereby\nsignificantly reducing memory consumption. While codebooks have been\ndemonstrated to be highly effective at compressing fully trained 3DGS models,\ndirectly training using codebook representations is an unsolved challenge.\nContraGS solves the problem of learning non-differentiable parameters in\ncodebook-compressed representations by posing parameter estimation as a\nBayesian inference problem. To this end, ContraGS provides a framework that\neffectively uses MCMC sampling to sample over a posterior distribution of these\ncompressed representations. With ContraGS, we demonstrate that ContraGS\nsignificantly reduces the peak memory during training (on average 3.49X) and\naccelerated training and rendering (1.36X and 1.88X on average, respectively),\nwhile retraining close to state-of-art quality.\n","authors":["Sankeerth Durvasula","Sharanshangar Muhunthan","Zain Moustafa","Richard Chen","Ruofan Liang","Yushi Guan","Nilesh Ahuja","Nilesh Jain","Selvakumar Panneer","Nandita Vijaykumar"],"pdf_url":"https://arxiv.org/pdf/2509.03775v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03754v1","updated":"2025-09-03T22:46:20Z","published":"2025-09-03T22:46:20Z","title":"STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight\n  Plant Disease Classification","summary":"  Responding to rising global food security needs, precision agriculture and\ndeep learning-based plant disease diagnosis have become crucial. Yet, deploying\nhigh-precision models on edge devices is challenging. Most lightweight networks\nuse attention mechanisms designed for generic object recognition, which poorly\ncapture subtle pathological features like irregular lesion shapes and complex\ntextures. To overcome this, we propose a twofold solution: first, using a\ntraining-free neural architecture search method (DeepMAD) to create an\nefficient network backbone for edge devices; second, introducing the\nShape-Texture Attention Module (STAM). STAM splits attention into two branches\n-- one using deformable convolutions (DCNv4) for shape awareness and the other\nusing a Gabor filter bank for texture awareness. On the public CCMT plant\ndisease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)\nreached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm\nSTAM significantly improves performance over baseline and standard attention\nmodels. Integrating domain knowledge via decoupled attention thus presents a\npromising path for edge-deployed precision agriculture AI. The source code is\navailable at https://github.com/RzMY/STA-Net.\n","authors":["Zongsen Qiu"],"pdf_url":"https://arxiv.org/pdf/2509.03754v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03749v1","updated":"2025-09-03T22:24:55Z","published":"2025-09-03T22:24:55Z","title":"Mapping on a Budget: Optimizing Spatial Data Collection for ML","summary":"  In applications across agriculture, ecology, and human development, machine\nlearning with satellite imagery (SatML) is limited by the sparsity of labeled\ntraining data. While satellite data cover the globe, labeled training datasets\nfor SatML are often small, spatially clustered, and collected for other\npurposes (e.g., administrative surveys or field measurements). Despite the\npervasiveness of this issue in practice, past SatML research has largely\nfocused on new model architectures and training algorithms to handle scarce\ntraining data, rather than modeling data conditions directly. This leaves\nscientists and policymakers who wish to use SatML for large-scale monitoring\nuncertain about whether and how to collect additional data to maximize\nperformance. Here, we present the first problem formulation for the\noptimization of spatial training data in the presence of heterogeneous data\ncollection costs and realistic budget constraints, as well as novel methods for\naddressing this problem. In experiments simulating different problem settings\nacross three continents and four tasks, our strategies reveal substantial gains\nfrom sample optimization. Further experiments delineate settings for which\noptimized sampling is particularly effective. The problem formulation and\nmethods we introduce are designed to generalize across application domains for\nSatML; we put special emphasis on a specific problem setting where our\ncoauthors can immediately use our findings to augment clustered agricultural\nsurveys for SatML monitoring in Togo.\n","authors":["Livia Betti","Farooq Sanni","Gnouyaro Sogoyou","Togbe Agbagla","Cullen Molitor","Tamma Carleton","Esther Rolf"],"pdf_url":"https://arxiv.org/pdf/2509.03749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.09465v2","updated":"2025-09-03T22:09:05Z","published":"2024-12-12T17:14:58Z","title":"OFTSR: One-Step Flow for Image Super-Resolution with Tunable\n  Fidelity-Realism Trade-offs","summary":"  Recent advances in diffusion and flow-based generative models have\ndemonstrated remarkable success in image restoration tasks, achieving superior\nperceptual quality compared to traditional deep learning approaches. However,\nthese methods either require numerous sampling steps to generate high-quality\nimages, resulting in significant computational overhead, or rely on common\nmodel distillation, which usually imposes a fixed fidelity-realism trade-off\nand thus lacks flexibility. In this paper, we introduce OFTSR, a novel\nflow-based framework for one-step image super-resolution that can produce\noutputs with tunable levels of fidelity and realism. Our approach first trains\na conditional flow-based super-resolution model to serve as a teacher model. We\nthen distill this teacher model by applying a specialized constraint.\nSpecifically, we force the predictions from our one-step student model for same\ninput to lie on the same sampling ODE trajectory of the teacher model. This\nalignment ensures that the student model's single-step predictions from initial\nstates match the teacher's predictions from a closer intermediate state.\nThrough extensive experiments on datasets including FFHQ (256$\\times$256),\nDIV2K, and ImageNet (256$\\times$256), we demonstrate that OFTSR achieves\nstate-of-the-art performance for one-step image super-resolution, while having\nthe ability to flexibly tune the fidelity-realism trade-off. Codes:\n\\href{https://github.com/yuanzhi-zhu/OFTSR}{https://github.com/yuanzhi-zhu/OFTSR}.\n","authors":["Yuanzhi Zhu","Ruiqing Wang","Shilin Lu","Junnan Li","Hanshu Yan","Kai Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.09465v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03740v1","updated":"2025-09-03T22:00:23Z","published":"2025-09-03T22:00:23Z","title":"Singular Value Few-shot Adaptation of Vision-Language Models","summary":"  Vision-language models (VLMs) like CLIP have shown impressive zero-shot and\nfew-shot learning capabilities across diverse applications. However, adapting\nthese models to new fine-grained domains remains difficult due to reliance on\nprompt engineering and the high cost of full model fine-tuning. Existing\nadaptation approaches rely on augmented components, such as prompt tokens and\nadapter modules, which could limit adaptation quality, destabilize the model,\nand compromise the rich knowledge learned during pretraining. In this work, we\npresent \\textbf{CLIP-SVD}, a novel \\textit{multi-modal} and\n\\textit{parameter-efficient} adaptation technique that leverages Singular Value\nDecomposition (SVD) to modify the internal parameter space of CLIP without\ninjecting additional modules. Specifically, we fine-tune only the singular\nvalues of the CLIP parameter matrices to rescale the basis vectors for domain\nadaptation while retaining the pretrained model. This design enables enhanced\nadaptation performance using only \\textbf{0.04\\%} of the model's total\nparameters and better preservation of its generalization ability. CLIP-SVD\nachieves state-of-the-art classification results on 11 natural and 10\nbiomedical datasets, outperforming previous methods in both accuracy and\ngeneralization under few-shot settings. Additionally, we leverage a natural\nlanguage-based approach to analyze the effectiveness and dynamics of the CLIP\nadaptation to allow interpretability of CLIP-SVD. The code is publicly\navailable at https://github.com/HealthX-Lab/CLIP-SVD.\n","authors":["Taha Koleilat","Hassan Rivaz","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2509.03740v1.pdf","comment":"10 pages, 2 figures, 8 tables"},{"id":"http://arxiv.org/abs/2509.03737v1","updated":"2025-09-03T21:56:16Z","published":"2025-09-03T21:56:16Z","title":"LayoutGKN: Graph Similarity Learning of Floor Plans","summary":"  Floor plans depict building layouts and are often represented as graphs to\ncapture the underlying spatial relationships. Comparison of these graphs is\ncritical for applications like search, clustering, and data visualization. The\nmost successful methods to compare graphs \\ie, graph matching networks, rely on\ncostly intermediate cross-graph node-level interactions, therefore being slow\nin inference time. We introduce \\textbf{LayoutGKN}, a more efficient approach\nthat postpones the cross-graph node-level interactions to the end of the joint\nembedding architecture. We do so by using a differentiable graph kernel as a\ndistance function on the final learned node-level embeddings. We show that\nLayoutGKN computes similarity comparably or better than graph matching networks\nwhile significantly increasing the speed.\n\\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are\nopen.\n","authors":["Casper van Engelenburg","Jan van Gemert","Seyran Khademi"],"pdf_url":"https://arxiv.org/pdf/2509.03737v1.pdf","comment":"BMVC (2025)"},{"id":"http://arxiv.org/abs/2503.13756v2","updated":"2025-09-03T21:35:27Z","published":"2025-03-17T22:26:54Z","title":"Fast rigid alignment of heterogeneous images in sliced Wasserstein\n  distance","summary":"  Many applications of computer vision rely on the alignment of similar but\nnon-identical images. We present a fast algorithm for aligning heterogeneous\nimages based on optimal transport. Our approach combines the speed of fast\nFourier methods with the robustness of sliced probability metrics and allows us\nto efficiently compute the alignment between two $L \\times L$ images using the\nsliced 2-Wasserstein distance in $O(L^2 \\log L)$ operations. We show that our\nmethod is robust to translations, rotations and deformations in the images.\n","authors":["Yunpeng Shi","Amit Singer","Eric J. Verbeke"],"pdf_url":"https://arxiv.org/pdf/2503.13756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03729v1","updated":"2025-09-03T21:23:09Z","published":"2025-09-03T21:23:09Z","title":"Transfer Learning-Based CNN Models for Plant Species Identification\n  Using Leaf Venation Patterns","summary":"  This study evaluates the efficacy of three deep learning architectures:\nResNet50, MobileNetV2, and EfficientNetB0 for automated plant species\nclassification based on leaf venation patterns, a critical morphological\nfeature with high taxonomic relevance. Using the Swedish Leaf Dataset\ncomprising images from 15 distinct species (75 images per species, totalling\n1,125 images), the models were demonstrated using standard performance metrics\nduring training and testing phases. ResNet50 achieved a training accuracy of\n94.11% but exhibited overfitting, reflected by a reduced testing accuracy of\n88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better\ngeneralization capabilities, attaining a testing accuracy of 93.34% and an F1\nscore of 93.23%, indicating its suitability for lightweight, real-time\napplications. EfficientNetB0 outperformed both models, achieving a testing\naccuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,\nhighlighting its robustness in venation-based classification. The findings\nunderscore the potential of deep learning, particularly EfficientNetB0, in\ndeveloping scalable and accurate tools for automated plant taxonomy using\nvenation traits.\n","authors":["Bandita Bharadwaj","Ankur Mishra","Saurav Bharadwaj"],"pdf_url":"https://arxiv.org/pdf/2509.03729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.10823v2","updated":"2025-09-03T20:41:40Z","published":"2025-05-16T03:39:46Z","title":"From Embeddings to Accuracy: Comparing Foundation Models for\n  Radiographic Classification","summary":"  Foundation models provide robust embeddings for diverse tasks, including\nmedical imaging. We evaluate embeddings from seven general and medical-specific\nfoundation models (e.g., DenseNet121, BiomedCLIP, MedImageInsight, Rad-DINO,\nCXR-Foundation) for training lightweight adapters in multi-class radiography\nclassification. Using a dataset of 8,842 radiographs across seven classes, we\ntrained adapters with algorithms like K-Nearest Neighbors, logistic regression,\nSVM, random forest, and MLP. The combination of MedImageInsight embeddings with\nan SVM or MLP adapter achieved the highest mean area under the curve (mAUC) of\n93.1%. This performance was statistically superior to other models, including\nMedSigLIP with an MLP (91.0%), Rad-DINO with an SVM (90.7%), and CXR-Foundation\nwith logistic regression (88.6%). In contrast, models like BiomedCLIP (82.8%)\nand Med-Flamingo (78.5%) showed lower performance. Crucially, these lightweight\nadapters are computationally efficient, training in minutes and performing\ninference in seconds on a CPU, making them practical for clinical use. A\nfairness analysis of the top-performing MedImageInsight adapter revealed\nminimal performance disparities across patient gender (within 1.8%) and age\ngroups (std. dev < 1.4%), with no significant statistical differences. These\nfindings confirm that embeddings from specialized foundation models,\nparticularly MedImageInsight, can power accurate, efficient, and equitable\ndiagnostic tools using simple, lightweight adapters.\n","authors":["Xue Li","Jameson Merkow","Noel C. F. Codella","Alberto Santamaria-Pang","Naiteek Sangani","Alexander Ersoy","Christopher Burt","John W. Garrett","Richard J. Bruce","Joshua D. Warner","Tyler Bradshaw","Ivan Tarapov","Matthew P. Lungren","Alan B. McMillan"],"pdf_url":"https://arxiv.org/pdf/2505.10823v2.pdf","comment":"12 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2509.03704v1","updated":"2025-09-03T20:39:03Z","published":"2025-09-03T20:39:03Z","title":"QuantV2X: A Fully Quantized Multi-Agent System for Cooperative\n  Perception","summary":"  Cooperative perception through Vehicle-to-Everything (V2X) communication\noffers significant potential for enhancing vehicle perception by mitigating\nocclusions and expanding the field of view. However, past research has\npredominantly focused on improving accuracy metrics without addressing the\ncrucial system-level considerations of efficiency, latency, and real-world\ndeployability. Noticeably, most existing systems rely on full-precision models,\nwhich incur high computational and transmission costs, making them impractical\nfor real-time operation in resource-constrained environments. In this paper, we\nintroduce \\textbf{QuantV2X}, the first fully quantized multi-agent system\ndesigned specifically for efficient and scalable deployment of multi-modal,\nmulti-agent V2X cooperative perception. QuantV2X introduces a unified\nend-to-end quantization strategy across both neural network models and\ntransmitted message representations that simultaneously reduces computational\nload and transmission bandwidth. Remarkably, despite operating under low-bit\nconstraints, QuantV2X achieves accuracy comparable to full-precision systems.\nMore importantly, when evaluated under deployment-oriented metrics, QuantV2X\nreduces system-level latency by 3.2$\\times$ and achieves a +9.5 improvement in\nmAP30 over full-precision baselines. Furthermore, QuantV2X scales more\neffectively, enabling larger and more capable models to fit within strict\nmemory budgets. These results highlight the viability of a fully quantized\nmulti-agent intermediate fusion system for real-world deployment. The system\nwill be publicly released to promote research in this field:\nhttps://github.com/ucla-mobility/QuantV2X.\n","authors":["Seth Z. Zhao","Huizhi Zhang","Zhaowei Li","Juntong Peng","Anthony Chui","Zewei Zhou","Zonglin Meng","Hao Xiang","Zhiyu Huang","Fujia Wang","Ran Tian","Chenfeng Xu","Bolei Zhou","Jiaqi Ma"],"pdf_url":"https://arxiv.org/pdf/2509.03704v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.11249v3","updated":"2025-09-03T20:25:50Z","published":"2025-04-15T14:46:25Z","title":"Cryo-em images are intrinsically low dimensional","summary":"  Simulation-based inference provides a powerful framework for cryo-electron\nmicroscopy, employing neural networks in methods like CryoSBI to infer\nbiomolecular conformations via learned latent representations. This latent\nspace represents a rich opportunity, encoding valuable information about the\nphysical system and the inference process. Harnessing this potential hinges on\nunderstanding the underlying geometric structure of these representations. We\ninvestigate this structure by applying manifold learning techniques to CryoSBI\nrepresentations of hemagglutinin (simulated and experimental). We reveal that\nthese high-dimensional data inherently populate low-dimensional, smooth\nmanifolds, with simulated data effectively covering the experimental\ncounterpart. By characterizing the manifold's geometry using Diffusion Maps and\nidentifying its principal axes of variation via coordinate interpretation\nmethods, we establish a direct link between the latent structure and key\nphysical parameters. Discovering this intrinsic low-dimensionality and\ninterpretable geometric organization not only validates the CryoSBI approach\nbut enables us to learn more from the data structure and provides opportunities\nfor improving future inference strategies by exploiting this revealed manifold\ngeometry.\n","authors":["Luke Evans","Octavian-Vlad Murad","Lars Dingeldein","Pilar Cossio","Roberto Covino","Marina Meila"],"pdf_url":"https://arxiv.org/pdf/2504.11249v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03680v1","updated":"2025-09-03T19:59:20Z","published":"2025-09-03T19:59:20Z","title":"LuxDiT: Lighting Estimation with Video Diffusion Transformer","summary":"  Estimating scene lighting from a single image or video remains a longstanding\nchallenge in computer vision and graphics. Learning-based approaches are\nconstrained by the scarcity of ground-truth HDR environment maps, which are\nexpensive to capture and limited in diversity. While recent generative models\noffer strong priors for image synthesis, lighting estimation remains difficult\ndue to its reliance on indirect visual cues, the need to infer global\n(non-local) context, and the recovery of high-dynamic-range outputs. We propose\nLuxDiT, a novel data-driven approach that fine-tunes a video diffusion\ntransformer to generate HDR environment maps conditioned on visual input.\nTrained on a large synthetic dataset with diverse lighting conditions, our\nmodel learns to infer illumination from indirect visual cues and generalizes\neffectively to real-world scenes. To improve semantic alignment between the\ninput and the predicted environment map, we introduce a low-rank adaptation\nfinetuning strategy using a collected dataset of HDR panoramas. Our method\nproduces accurate lighting predictions with realistic angular high-frequency\ndetails, outperforming existing state-of-the-art techniques in both\nquantitative and qualitative evaluations.\n","authors":["Ruofan Liang","Kai He","Zan Gojcic","Igor Gilitschenski","Sanja Fidler","Nandita Vijaykumar","Zian Wang"],"pdf_url":"https://arxiv.org/pdf/2509.03680v1.pdf","comment":"Project page: https://research.nvidia.com/labs/toronto-ai/LuxDiT/"},{"id":"http://arxiv.org/abs/2509.03677v1","updated":"2025-09-03T19:54:23Z","published":"2025-09-03T19:54:23Z","title":"Insights from Gradient Dynamics: Gradient Autoscaled Normalization","summary":"  Gradient dynamics play a central role in determining the stability and\ngeneralization of deep neural networks. In this work, we provide an empirical\nanalysis of how variance and standard deviation of gradients evolve during\ntraining, showing consistent changes across layers and at the global scale in\nconvolutional networks. Motivated by these observations, we propose a\nhyperparameter-free gradient normalization method that aligns gradient scaling\nwith their natural evolution. This approach prevents unintended amplification,\nstabilizes optimization, and preserves convergence guarantees. Experiments on\nthe challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN\ndemonstrate that our method maintains or improves test accuracy even under\nstrong generalization. Beyond practical performance, our study highlights the\nimportance of directly tracking gradient dynamics, aiming to bridge the gap\nbetween theoretical expectations and empirical behaviors, and to provide\ninsights for future optimization research.\n","authors":["Vincent-Daniel Yun"],"pdf_url":"https://arxiv.org/pdf/2509.03677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07681v2","updated":"2025-09-03T19:44:03Z","published":"2025-01-13T20:41:52Z","title":"Dataset Distillation as Pushforward Optimal Quantization","summary":"  Dataset distillation aims to find a synthetic training set such that training\non the synthetic data achieves similar performance to training on real data,\nwith orders of magnitude less computational requirements. Existing methods can\nbe broadly categorized as either bi-level optimization problems that have\nneural network training heuristics as the lower level problem, or disentangled\nmethods that bypass the bi-level optimization by matching distributions of\ndata. The latter method has the major advantages of speed and scalability in\nterms of size of both training and distilled datasets. We demonstrate that when\nequipped with an encoder-decoder structure, the empirically successful\ndisentangled methods can be reformulated as an optimal quantization problem,\nwhere a finite set of points is found to approximate the underlying probability\nmeasure by minimizing the expected projection distance. In particular, we link\nexisting disentangled dataset distillation methods to the classical optimal\nquantization and Wasserstein barycenter problems, demonstrating consistency of\ndistilled datasets for diffusion-based generative priors. We propose Dataset\nDistillation by Optimal Quantization, based on clustering in a latent space.\nCompared to the previous SOTA method D\\textsuperscript{4}M, we achieve better\nperformance and inter-model generalization on the ImageNet-1K dataset with\ntrivial additional computation, and SOTA performance in higher image-per-class\nsettings. Using the distilled noise initializations in a stronger diffusion\ntransformer model, we obtain SOTA distillation performance on ImageNet-1K and\nits subsets, outperforming diffusion guidance methods.\n","authors":["Hong Ye Tan","Emma Slade"],"pdf_url":"https://arxiv.org/pdf/2501.07681v2.pdf","comment":"Modified abstract, additional experiments based on diffusion\n  transformers"},{"id":"http://arxiv.org/abs/2509.03635v1","updated":"2025-09-03T18:36:44Z","published":"2025-09-03T18:36:44Z","title":"Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene\n  Understanding","summary":"  The rapid development of Large Multimodal Models (LMMs) has led to remarkable\nprogress in 2D visual understanding; however, extending these capabilities to\n3D scene understanding remains a significant challenge. Existing approaches\npredominantly rely on text-only supervision, which fails to provide the\ngeometric constraints required for learning robust 3D spatial representations.\nIn this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction\nTuning framework that addresses this limitation by incorporating geometry-aware\nsupervision directly into the training process. Our key insight is that\neffective 3D understanding necessitates reconstructing underlying geometric\nstructures rather than merely describing them. Unlike existing methods that\ninject 3D information solely at the input level, Reg3D adopts a\ndual-supervision paradigm that leverages 3D geometric information both as input\nand as explicit learning targets. Specifically, we design complementary\nobject-level and frame-level reconstruction tasks within a dual-encoder\narchitecture, enforcing geometric consistency to encourage the development of\nspatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,\nScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance\nimprovements, establishing a new training paradigm for spatially aware\nmultimodal models.\n","authors":["Hongpei Zheng","Lintao Xiang","Qijun Yang","Qian Lin","Hujun Yin"],"pdf_url":"https://arxiv.org/pdf/2509.03635v1.pdf","comment":"16 pages, 6 figures"},{"id":"http://arxiv.org/abs/2509.03633v1","updated":"2025-09-03T18:35:20Z","published":"2025-09-03T18:35:20Z","title":"treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point\n  Clouds","summary":"  Close-range laser scanning provides detailed 3D captures of forest stands but\nrequires efficient software for processing 3D point cloud data and extracting\nindividual trees. Although recent studies have introduced deep learning methods\nfor tree instance segmentation, these approaches require large annotated\ndatasets and substantial computational resources. As a resource-efficient\nalternative, we present a revised version of the treeX algorithm, an\nunsupervised method that combines clustering-based stem detection with region\ngrowing for crown delineation. While the original treeX algorithm was developed\nfor personal laser scanning (PLS) data, we provide two parameter presets, one\nfor ground-based laser scanning (stationary terrestrial - TLS and PLS), and one\nfor UAV-borne laser scanning (ULS). We evaluated the method on six public\ndatasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham\nWoods) and compared it to six open-source methods (original treeX, treeiso,\nRayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original\ntreeX algorithm, our revision reduces runtime and improves accuracy, with\ninstance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.\nFor ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original\nalgorithm fails to segment any correct instances. For TLS and PLS data, our\nalgorithm achieves accuracy similar to recent open-source methods, including\ndeep learning. Given its algorithmic design, we see two main applications for\nour method: (1) as a resource-efficient alternative to deep learning approaches\nin scenarios where the data characteristics align with the method design\n(sufficient stem visibility and point density), and (2) for the semi-automatic\ngeneration of labels for deep learning models. To enable broader adoption, we\nprovide an open-source Python implementation in the pointtree package.\n","authors":["Josafat-Mattias Burmeister","Andreas Tockner","Stefan Reder","Markus Engel","Rico Richter","Jan-Peter Mund","Jürgen Döllner"],"pdf_url":"https://arxiv.org/pdf/2509.03633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03631v1","updated":"2025-09-03T18:33:28Z","published":"2025-09-03T18:33:28Z","title":"Lightweight image segmentation for echocardiography","summary":"  Accurate segmentation of the left ventricle in echocardiography can enable\nfully automatic extraction of clinical measurements such as volumes and\nejection fraction. While models configured by nnU-Net perform well, they are\nlarge and slow, thus limiting real-time use. We identified the most effective\ncomponents of nnU-Net for cardiac segmentation through an ablation study,\nincrementally evaluating data augmentation schemes, architectural\nmodifications, loss functions, and post-processing techniques. Our analysis\nrevealed that simple affine augmentations and deep supervision drive\nperformance, while complex augmentations and large model capacity offer\ndiminishing returns. Based on these insights, we developed a lightweight U-Net\n(2M vs 33M parameters) that achieves statistically equivalent performance to\nnnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89\nfor LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster\n(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.\nCross-dataset evaluation on an internal dataset (N=311) confirms comparable\ngeneralization.\n","authors":["Anders Kjelsrud","Lasse Løvstakken","Erik Smistad","Håvard Dalen","Gilles Van De Vyver"],"pdf_url":"https://arxiv.org/pdf/2509.03631v1.pdf","comment":"4 pages, 6 figures, The 2025 IEEE International Ultrasonics Symposium"},{"id":"http://arxiv.org/abs/2509.03623v1","updated":"2025-09-03T18:18:16Z","published":"2025-09-03T18:18:16Z","title":"Revealing Fine Structure in Protoplanetary Disks with Physics\n  Constrained Neural Fields","summary":"  Protoplanetary disks are the birthplaces of planets, and resolving their\nthree-dimensional structure is key to understanding disk evolution. The\nunprecedented resolution of ALMA demands modeling approaches that capture\nfeatures beyond the reach of traditional methods. We introduce a computational\nframework that integrates physics-constrained neural fields with differentiable\nrendering and present RadJAX, a GPU-accelerated, fully differentiable line\nradiative transfer solver achieving up to 10,000x speedups over conventional\nray tracers, enabling previously intractable, high-dimensional neural\nreconstructions. Applied to ALMA CO observations of HD 163296, this framework\nrecovers the vertical morphology of the CO-rich layer, revealing a pronounced\nnarrowing and flattening of the emission surface beyond 400 au - a feature\nmissed by existing approaches. Our work establish a new paradigm for extracting\ncomplex disk structure and advancing our understanding of protoplanetary\nevolution.\n","authors":["Aviad Levis","Nhan Luong","Richard Teague","Katherine. L. Bouman","Marcelo Barraza-Alfaro","Kevin Flaherty"],"pdf_url":"https://arxiv.org/pdf/2509.03623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03616v1","updated":"2025-09-03T18:08:59Z","published":"2025-09-03T18:08:59Z","title":"Multi Attribute Bias Mitigation via Representation Learning","summary":"  Real world images frequently exhibit multiple overlapping biases, including\ntextures, watermarks, gendered makeup, scene object pairings, etc. These biases\ncollectively impair the performance of modern vision models, undermining both\ntheir robustness and fairness. Addressing these biases individually proves\ninadequate, as mitigating one bias often permits or intensifies others. We\ntackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a\nlean two stage framework that needs group labels only while training and\nminimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)\ndeliberately identifies the influence of known shortcuts by training encoders\nfor each attribute and integrating them with the main backbone, compelling the\nclassifier to explicitly recognize these biases. Then Gradient Suppression Fine\nTuning prunes those very bias directions from the backbone's gradients, leaving\na single compact network that ignores all the shortcuts it just learned to\nrecognize. Moreover we find that existing bias metrics break under subgroup\nimbalance and train test distribution shifts, so we introduce Scaled Bias\nAmplification (SBA): a test time measure that disentangles model induced bias\namplification from distributional differences. We validate GMBM on FB CMNIST,\nCelebA, and COCO, where we boost worst group accuracy, halve multi attribute\nbias amplification, and set a new low in SBA even as bias complexity and\ndistribution shifts intensify, making GMBM the first practical, end to end\nmultibias solution for visual recognition. Project page:\nhttp://visdomlab.github.io/GMBM/\n","authors":["Rajeev Ranjan Dwivedi","Ankur Kumar","Vinod K Kurmi"],"pdf_url":"https://arxiv.org/pdf/2509.03616v1.pdf","comment":"ECAI 2025 (28th European Conference on Artificial Intelligence)"},{"id":"http://arxiv.org/abs/2509.03614v1","updated":"2025-09-03T18:08:11Z","published":"2025-09-03T18:08:11Z","title":"Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG\n  2025 Challenge","summary":"  Counting mitotic figures is time-intensive for pathologists and leads to\ninter-observer variability. Artificial intelligence (AI) promises a solution by\nautomatically detecting mitotic figures while maintaining decision consistency.\nHowever, AI tools are susceptible to domain shift, where a significant drop in\nperformance can occur due to differences in the training and testing sets,\nincluding morphological diversity between organs, species, and variations in\nstaining protocols. Furthermore, the number of mitoses is much less than the\ncount of normal nuclei, which introduces severely imbalanced data for the\ndetection task. In this work, we formulate mitosis detection as a pixel-level\nsegmentation and propose a teacher-student model that simultaneously addresses\nmitosis detection (Track 1) and atypical mitosis classification (Track 2). Our\nmethod is based on a UNet segmentation backbone that integrates domain\ngeneralization modules, namely contrastive representation learning and\ndomain-adversarial training. A teacher-student strategy is employed to generate\npixel-level pseudo-masks not only for annotated mitoses and hard negatives but\nalso for normal nuclei, thereby enhancing feature discrimination and improving\nrobustness against domain shift. For the classification task, we introduce a\nmulti-scale CNN classifier that leverages feature maps from the segmentation\nmodel within a multi-task learning paradigm. On the preliminary test set, the\nalgorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of\n0.8414 in Track 2, demonstrating the effectiveness of integrating\nsegmentation-based detection and classification into a unified framework for\nrobust mitosis analysis.\n","authors":["Seungho Choe","Xiaoli Qin","Abubakr Shafique","Amanda Dy","Susan Done","Dimitrios Androutsos","April Khademi"],"pdf_url":"https://arxiv.org/pdf/2509.03614v1.pdf","comment":"4 pages, 1 figures, final submission for MIDOG 2025 challenge"},{"id":"http://arxiv.org/abs/2509.03609v1","updated":"2025-09-03T18:05:02Z","published":"2025-09-03T18:05:02Z","title":"Towards Efficient General Feature Prediction in Masked Skeleton Modeling","summary":"  Recent advances in the masked autoencoder (MAE) paradigm have significantly\npropelled self-supervised skeleton-based action recognition. However, most\nexisting approaches limit reconstruction targets to raw joint coordinates or\ntheir simple variants, resulting in computational redundancy and limited\nsemantic representation. To address this, we propose a novel General Feature\nPrediction framework (GFP) for efficient mask skeleton modeling. Our key\ninnovation is replacing conventional low-level reconstruction with high-level\nfeature prediction that spans from local motion patterns to global semantic\nrepresentations. Specifically, we introduce a collaborative learning framework\nwhere a lightweight target generation network dynamically produces diversified\nsupervision signals across spatial-temporal hierarchies, avoiding reliance on\npre-computed offline features. The framework incorporates constrained\noptimization to ensure feature diversity while preventing model collapse.\nExperiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits\nof our approach: Computational efficiency (with 6.2$\\times$ faster training\nthan standard masked skeleton modeling methods) and superior representation\nquality, achieving state-of-the-art performance in various downstream tasks.\n","authors":["Shengkai Sun","Zefan Zhang","Jianfeng Dong","Zhiyong Cheng","Xiaojun Chang","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2509.03609v1.pdf","comment":"Accepted by ICCV 2025"},{"id":"http://arxiv.org/abs/2508.10868v2","updated":"2025-09-03T18:01:47Z","published":"2025-08-14T17:43:25Z","title":"TexVerse: A Universe of 3D Objects with High-Resolution Textures","summary":"  We introduce TexVerse, a large-scale 3D dataset featuring high-resolution\ntextures. While recent advances in large-scale 3D datasets have enhanced\nhigh-resolution geometry generation, creating high-resolution textures\nend-to-end remains underexplored due to the lack of suitable datasets. TexVerse\nfills this gap with a curated collection of over 858K unique high-resolution 3D\nmodels sourced from Sketchfab, including more than 158K models with physically\nbased rendering (PBR) materials. Each model encompasses all of its\nhigh-resolution variants, bringing the total to 1.6M 3D instances. TexVerse\nalso includes specialized subsets: TexVerse-Skeleton, with 69K rigged models,\nand TexVerse-Animation, with 54K animated models, both preserving original\nskeleton and animation data uploaded by the user. We also provide detailed\nmodel annotations describing overall characteristics, structural components,\nand intricate features. TexVerse offers a high-quality data resource with\nwide-ranging potential applications in texture synthesis, PBR material\ndevelopment, animation, and various 3D vision and graphics tasks.\n","authors":["Yibo Zhang","Li Zhang","Rui Ma","Nan Cao"],"pdf_url":"https://arxiv.org/pdf/2508.10868v2.pdf","comment":"https://github.com/yiboz2001/TexVerse"},{"id":"http://arxiv.org/abs/2412.00177v3","updated":"2025-09-03T17:59:08Z","published":"2024-11-29T18:59:11Z","title":"LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene\n  Relighting","summary":"  We introduce LumiNet, a novel architecture that leverages generative models\nand latent intrinsic representations for effective lighting transfer. Given a\nsource image and a target lighting image, LumiNet synthesizes a relit version\nof the source scene that captures the target's lighting. Our approach makes two\nkey contributions: a data curation strategy from the StyleGAN-based relighting\nmodel for our training, and a modified diffusion-based ControlNet that\nprocesses both latent intrinsic properties from the source image and latent\nextrinsic properties from the target image. We further improve lighting\ntransfer through a learned adaptor (MLP) that injects the target's latent\nextrinsic properties via cross-attention and fine-tuning.\n  Unlike traditional ControlNet, which generates images with conditional maps\nfrom a single scene, LumiNet processes latent representations from two\ndifferent images - preserving geometry and albedo from the source while\ntransferring lighting characteristics from the target. Experiments demonstrate\nthat our method successfully transfers complex lighting phenomena including\nspecular highlights and indirect illumination across scenes with varying\nspatial layouts and materials, outperforming existing approaches on challenging\nindoor scenes using only images as input.\n","authors":["Xiaoyan Xing","Konrad Groh","Sezer Karaoglu","Theo Gevers","Anand Bhattad"],"pdf_url":"https://arxiv.org/pdf/2412.00177v3.pdf","comment":"Corrects an evaluation bug in Table 1 due to a data normalization\n  error. Thanks to the Sony PlayStation team for discovering and reporting the\n  issue. The paper's core contributions, qualitative results, and user study\n  are unaffected. We also include a minor update to the method to further\n  improve result quality. Project page: https://luminet-relight.github.io/"},{"id":"http://arxiv.org/abs/2509.03516v1","updated":"2025-09-03T17:58:12Z","published":"2025-09-03T17:58:12Z","title":"Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?","summary":"  Text-to-image (T2I) generation aims to synthesize images from textual\nprompts, which jointly specify what must be shown and imply what can be\ninferred, thereby corresponding to two core capabilities: composition and\nreasoning. However, with the emerging advances of T2I models in reasoning\nbeyond composition, existing benchmarks reveal clear limitations in providing\ncomprehensive evaluations across and within these capabilities. Meanwhile,\nthese advances also enable models to handle more complex prompts, whereas\ncurrent benchmarks remain limited to low scene density and simplified\none-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a\ncomprehensive and complex benchmark that evaluates both composition and\nreasoning capabilities of T2I models. To ensure comprehensiveness, we structure\ncomposition around scene graph elements (instance, attribute, and relation) and\nreasoning around the philosophical framework of inference (deductive,\ninductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To\nincrease complexity, driven by the inherent complexities of real-world\nscenarios, we curate each prompt with high compositional density for\ncomposition and multi-step inference for reasoning. We also pair each prompt\nwith a checklist that specifies individual yes/no questions to assess each\nintended element independently to facilitate fine-grained and reliable\nevaluation. In statistics, our benchmark comprises 1,080 challenging prompts\nand around 13,500 checklist questions. Experiments across 27 current T2I models\nreveal that their composition capability still remains limited in complex\nhigh-density scenarios, while the reasoning capability lags even further behind\nas a critical bottleneck, with all models struggling to infer implicit elements\nfrom prompts. Our project page: https://t2i-corebench.github.io/.\n","authors":["Ouxiang Li","Yuan Wang","Xinting Hu","Huijuan Huang","Rui Chen","Jiarong Ou","Xin Tao","Pengfei Wan","Fuli Feng"],"pdf_url":"https://arxiv.org/pdf/2509.03516v1.pdf","comment":"Project Page: https://t2i-corebench.github.io/"},{"id":"http://arxiv.org/abs/2509.03510v1","updated":"2025-09-03T17:47:23Z","published":"2025-09-03T17:47:23Z","title":"A comprehensive Persian offline handwritten database for investigating\n  the effects of heritability and family relationships on handwriting","summary":"  This paper introduces a comprehensive database for research and investigation\non the effects of inheritance on handwriting. A database has been created that\ncan be used to answer questions such as: Is there a genetic component to\nhandwriting? Is handwriting inherited? Do family relationships affect\nhandwriting? Varieties of samples of handwritten components such as: digits,\nletters, shapes and free paragraphs of 210 families including (grandparents,\nparents, uncles, aunts, siblings, cousins, nephews and nieces) have been\ncollected using specially designed forms, and family relationships of all\nwriters are captured. To the best of our knowledge, no such database is\npresently available. Based on comparisons and investigation of features of\nhandwritings of family members, similarities among their features and writing\nstyles are detected. Our database is freely available to the pattern\nrecognition community and hope it will pave the way for investigations on the\neffects of inheritance and family relationships on handwritings.\n","authors":["Abbas Zohrevand","Javad Sadri","Zahra Imani"],"pdf_url":"https://arxiv.org/pdf/2509.03510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.03501v1","updated":"2025-09-03T17:33:20Z","published":"2025-09-03T17:33:20Z","title":"Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning\n  via Synthetic Instruction Data","summary":"  Next-generation AI companions must go beyond general video understanding to\nresolve spatial and temporal references in dynamic, real-world environments.\nExisting Video Large Language Models (Video LLMs), while capable of\ncoarse-level comprehension, struggle with fine-grained, spatiotemporal\nreasoning, especially when user queries rely on time-based event references for\ntemporal anchoring, or gestural cues for spatial anchoring to clarify object\nreferences and positions. To bridge this critical gap, we introduce Strefer, a\nsynthetic instruction data generation framework designed to equip Video LLMs\nwith spatiotemporal referring and reasoning capabilities. Strefer produces\ndiverse instruction-tuning data using a data engine that pseudo-annotates\ntemporally dense, fine-grained video metadata, capturing rich spatial and\ntemporal information in a structured manner, including subjects, objects, their\nlocations as masklets, and their action descriptions and timelines. Our\napproach enhances the ability of Video LLMs to interpret spatial and temporal\nreferences, fostering more versatile, space-time-aware reasoning essential for\nreal-world AI companions. Without using proprietary models, costly human\nannotation, or the need to annotate large volumes of new videos, experimental\nevaluations show that models trained with data produced by Strefer outperform\nbaselines on tasks requiring spatial and temporal disambiguation. Additionally,\nthese models exhibit enhanced space-time-aware reasoning, establishing a new\nfoundation for perceptually grounded, instruction-tuned Video LLMs.\n","authors":["Honglu Zhou","Xiangyu Peng","Shrikant Kendre","Michael S. Ryoo","Silvio Savarese","Caiming Xiong","Juan Carlos Niebles"],"pdf_url":"https://arxiv.org/pdf/2509.03501v1.pdf","comment":"This technical report serves as the archival version of our paper\n  accepted at the ICCV 2025 Workshop. For more information, please visit our\n  project website: https://strefer.github.io/"},{"id":"http://arxiv.org/abs/2509.03499v1","updated":"2025-09-03T17:30:53Z","published":"2025-09-03T17:30:53Z","title":"DeepSea MOT: A benchmark dataset for multi-object tracking on deep-sea\n  video","summary":"  Benchmarking multi-object tracking and object detection model performance is\nan essential step in machine learning model development, as it allows\nresearchers to evaluate model detection and tracker performance on\nhuman-generated 'test' data, facilitating consistent comparisons between models\nand trackers and aiding performance optimization. In this study, a novel\nbenchmark video dataset was developed and used to assess the performance of\nseveral Monterey Bay Aquarium Research Institute object detection models and a\nFathomNet single-class object detection model together with several trackers.\nThe dataset consists of four video sequences representing midwater and benthic\ndeep-sea habitats. Performance was evaluated using Higher Order Tracking\nAccuracy, a metric that balances detection, localization, and association\naccuracy. To the best of our knowledge, this is the first publicly available\nbenchmark for multi-object tracking in deep-sea video footage. We provide the\nbenchmark data, a clearly documented workflow for generating additional\nbenchmark videos, as well as example Python notebooks for computing metrics.\n","authors":["Kevin Barnard","Elaine Liu","Kristine Walz","Brian Schlining","Nancy Jacobsen Stout","Lonny Lundsten"],"pdf_url":"https://arxiv.org/pdf/2509.03499v1.pdf","comment":"5 pages, 3 figures, dataset available at\n  https://huggingface.co/datasets/MBARI-org/DeepSea-MOT"},{"id":"http://arxiv.org/abs/2509.03498v1","updated":"2025-09-03T17:29:50Z","published":"2025-09-03T17:29:50Z","title":"OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and\n  Generation","summary":"  We introduce OneCAT, a unified multimodal model that seamlessly integrates\nunderstanding, generation, and editing within a novel, pure decoder-only\ntransformer architecture. Our framework uniquely eliminates the need for\nexternal components such as Vision Transformers (ViT) or vision tokenizer\nduring inference, leading to significant efficiency gains, especially for\nhigh-resolution inputs. This is achieved through a modality-specific\nMixture-of-Experts (MoE) structure trained with a single autoregressive (AR)\nobjective, which also natively supports dynamic resolutions. Furthermore, we\npioneer a multi-scale visual autoregressive mechanism within the Large Language\nModel (LLM) that drastically reduces decoding steps compared to diffusion-based\nmethods while maintaining state-of-the-art performance. Our findings\ndemonstrate the powerful potential of pure autoregressive modeling as a\nsufficient and elegant foundation for unified multimodal intelligence. As a\nresult, OneCAT sets a new performance standard, outperforming existing\nopen-source unified multimodal models across benchmarks for multimodal\ngeneration, editing, and understanding.\n","authors":["Han Li","Xinyu Peng","Yaoming Wang","Zelin Peng","Xin Chen","Rongxiang Weng","Jingang Wang","Xunliang Cai","Wenrui Dai","Hongkai Xiong"],"pdf_url":"https://arxiv.org/pdf/2509.03498v1.pdf","comment":"technical report"}]},"2025-09-02T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2509.02876v1","updated":"2025-09-02T22:46:27Z","published":"2025-09-02T22:46:27Z","title":"Generalizable Skill Learning for Construction Robots with Crowdsourced\n  Natural Language Instructions, Composable Skills Standardization, and Large\n  Language Model","summary":"  The quasi-repetitive nature of construction work and the resulting lack of\ngeneralizability in programming construction robots presents persistent\nchallenges to the broad adoption of robots in the construction industry. Robots\ncannot achieve generalist capabilities as skills learnt from one domain cannot\nreadily transfer to another work domain or be directly used to perform a\ndifferent set of tasks. Human workers have to arduously reprogram their\nscene-understanding, path-planning, and manipulation components to enable the\nrobots to perform alternate work tasks. The methods presented in this paper\nresolve a significant proportion of such reprogramming workload by proposing a\ngeneralizable learning architecture that directly teaches robots versatile\ntask-performance skills through crowdsourced online natural language\ninstructions. A Large Language Model (LLM), a standardized and modularized\nhierarchical modeling approach, and Building Information Modeling-Robot sematic\ndata pipeline are developed to address the multi-task skill transfer problem.\nThe proposed skill standardization scheme and LLM-based hierarchical skill\nlearning framework were tested with a long-horizon drywall installation\nexperiment using a full-scale industrial robotic manipulator. The resulting\nrobot task learning scheme achieves multi-task reprogramming with minimal\neffort and high quality.\n","authors":["Hongrui Yu","Vineet R. Kamat","Carol C. Menassa"],"pdf_url":"https://arxiv.org/pdf/2509.02876v1.pdf","comment":"Under review for ASCE OPEN: Multidisciplinary Journal of Civil\n  Engineering"},{"id":"http://arxiv.org/abs/2509.02870v1","updated":"2025-09-02T22:36:10Z","published":"2025-09-02T22:36:10Z","title":"Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms","summary":"  The small scale of urban farms and the commercial availability of low-cost\nrobots (such as the FarmBot) that automate simple tending tasks enable an\naccessible platform for plant phenotyping. We have used a FarmBot with a custom\ncamera end-effector to estimate strawberry plant flower pose (for robotic\npollination) from acquired 3D point cloud models. We describe a novel algorithm\nthat translates individual occupancy grids along orthogonal axes of a point\ncloud to obtain 2D images corresponding to the six viewpoints. For each image,\n2D object detection models for flowers are used to identify 2D bounding boxes\nwhich can be converted into the 3D space to extract flower point clouds. Pose\nestimation is performed by fitting three shapes (superellipsoids, paraboloids\nand planes) to the flower point clouds and compared with manually labeled\nground truth. Our method successfully finds approximately 80% of flowers\nscanned using our customized FarmBot platform and has a mean flower pose error\nof 7.7 degrees, which is sufficient for robotic pollination and rivals previous\nresults. All code will be made available at\nhttps://github.com/harshmuriki/flowerPose.git.\n","authors":["Harsh Muriki","Hong Ray Teo","Ved Sengupta","Ai-Ping Hu"],"pdf_url":"https://arxiv.org/pdf/2509.02870v1.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2208.13266v4","updated":"2025-09-02T22:12:53Z","published":"2022-08-28T18:30:46Z","title":"JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for\n  Conversational Embodied Agents","summary":"  Building a conversational embodied agent to execute real-life tasks has been\na long-standing yet quite challenging research goal, as it requires effective\nhuman-agent communication, multi-modal understanding, long-range sequential\ndecision making, etc. Traditional symbolic methods have scaling and\ngeneralization issues, while end-to-end deep learning models suffer from data\nscarcity and high task complexity, and are often hard to explain. To benefit\nfrom both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning\nframework for modular, generalizable, and interpretable conversational embodied\nagents. First, it acquires symbolic representations by prompting large language\nmodels (LLMs) for language understanding and sub-goal planning, and by\nconstructing semantic maps from visual observations. Then the symbolic module\nreasons for sub-goal planning and action generation based on task- and\naction-level common sense. Extensive experiments on the TEACh dataset validate\nthe efficacy and efficiency of our JARVIS framework, which achieves\nstate-of-the-art (SOTA) results on all three dialog-based embodied tasks,\nincluding Execution from Dialog History (EDH), Trajectory from Dialog (TfD),\nand Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen\nSuccess Rate on EDH from 6.1\\% to 15.8\\%). Moreover, we systematically analyze\nthe essential factors that affect the task performance and also demonstrate the\nsuperiority of our method in few-shot settings. Our JARVIS model ranks first in\nthe Alexa Prize SimBot Public Benchmark Challenge.\n","authors":["Kaizhi Zheng","Kaiwen Zhou","Jing Gu","Yue Fan","Jialu Wang","Zonglin Di","Xuehai He","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2208.13266v4.pdf","comment":"19th International Conference on Neurosymbolic Learning and Reasoning"},{"id":"http://arxiv.org/abs/2506.13265v3","updated":"2025-09-02T20:38:10Z","published":"2025-06-16T09:03:51Z","title":"Open-Set LiDAR Panoptic Segmentation Guided by Uncertainty-Aware\n  Learning","summary":"  Autonomous vehicles that navigate in open-world environments may encounter\npreviously unseen object classes. However, most existing LiDAR panoptic\nsegmentation models rely on closed-set assumptions, failing to detect unknown\nobject instances. In this work, we propose ULOPS, an uncertainty-guided\nopen-set panoptic segmentation framework that leverages Dirichlet-based\nevidential learning to model predictive uncertainty. Our architecture\nincorporates separate decoders for semantic segmentation with uncertainty\nestimation, embedding with prototype association, and instance center\nprediction. During inference, we leverage uncertainty estimates to identify and\nsegment unknown instances. To strengthen the model's ability to differentiate\nbetween known and unknown objects, we introduce three uncertainty-driven loss\nfunctions. Uniform Evidence Loss to encourage high uncertainty in unknown\nregions. Adaptive Uncertainty Separation Loss ensures a consistent difference\nin uncertainty estimates between known and unknown objects at a global scale.\nContrastive Uncertainty Loss refines this separation at the fine-grained level.\nTo evaluate open-set performance, we extend benchmark settings on KITTI-360 and\nintroduce a new open-set evaluation for nuScenes. Extensive experiments\ndemonstrate that ULOPS consistently outperforms existing open-set LiDAR\npanoptic segmentation methods.\n","authors":["Rohit Mohan","Julia Hindel","Florian Drews","Claudius Gläser","Daniele Cattaneo","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2506.13265v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02815v1","updated":"2025-09-02T20:32:02Z","published":"2025-09-02T20:32:02Z","title":"Multi-Embodiment Locomotion at Scale with extreme Embodiment\n  Randomization","summary":"  We present a single, general locomotion policy trained on a diverse\ncollection of 50 legged robots. By combining an improved embodiment-aware\narchitecture (URMAv2) with a performance-based curriculum for extreme\nEmbodiment Randomization, our policy learns to control millions of\nmorphological variations. Our policy achieves zero-shot transfer to unseen\nreal-world humanoid and quadruped robots.\n","authors":["Nico Bohlinger","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2509.02815v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02808v1","updated":"2025-09-02T20:22:54Z","published":"2025-09-02T20:22:54Z","title":"Improving the Resilience of Quadrotors in Underground Environments by\n  Combining Learning-based and Safety Controllers","summary":"  Autonomously controlling quadrotors in large-scale subterranean environments\nis applicable to many areas such as environmental surveying, mining operations,\nand search and rescue. Learning-based controllers represent an appealing\napproach to autonomy, but are known to not generalize well to\n`out-of-distribution' environments not encountered during training. In this\nwork, we train a normalizing flow-based prior over the environment, which\nprovides a measure of how far out-of-distribution the quadrotor is at any given\ntime. We use this measure as a runtime monitor, allowing us to switch between a\nlearning-based controller and a safe controller when we are sufficiently\nout-of-distribution. Our methods are benchmarked on a point-to-point navigation\ntask in a simulated 3D cave environment based on real-world point cloud data\nfrom the DARPA Subterranean Challenge Final Event Dataset. Our experimental\nresults show that our combined controller simultaneously possesses the liveness\nof the learning-based controller (completing the task quickly) and the safety\nof the safety controller (avoiding collision).\n","authors":["Isaac Ronald Ward","Mark Paral","Kristopher Riordan","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2509.02808v1.pdf","comment":"Accepted and awarded best paper at the 11th International Conference\n  on Control, Decision and Information Technologies (CoDIT 2025 -\n  https://codit2025.org/)"},{"id":"http://arxiv.org/abs/2504.00969v3","updated":"2025-09-02T19:48:21Z","published":"2025-04-01T17:08:27Z","title":"HDVIO2.0: Wind and Disturbance Estimation with Hybrid Dynamics VIO","summary":"  Visual-inertial odometry (VIO) is widely used for state estimation in\nautonomous micro aerial vehicles using onboard sensors. Current methods improve\nVIO by incorporating a model of the translational vehicle dynamics, yet their\nperformance degrades when faced with low-accuracy vehicle models or continuous\nexternal disturbances, like wind. Additionally, incorporating rotational\ndynamics in these models is computationally intractable when they are deployed\nin online applications, e.g., in a closed-loop control system. We present\nHDVIO2.0, which models full 6-DoF, translational and rotational, vehicle\ndynamics and tightly incorporates them into a VIO with minimal impact on the\nruntime. HDVIO2.0 builds upon the previous work, HDVIO, and addresses these\nchallenges through a hybrid dynamics model combining a point-mass vehicle model\nwith a learning-based component, with access to control commands and IMU\nhistory, to capture complex aerodynamic effects. The key idea behind modeling\nthe rotational dynamics is to represent them with continuous-time functions.\nHDVIO2.0 leverages the divergence between the actual motion and the predicted\nmotion from the hybrid dynamics model to estimate external forces as well as\nthe robot state. Our system surpasses the performance of state-of-the-art\nmethods in experiments using public and new drone dynamics datasets, as well as\nreal-world flights in winds up to 25 km/h. Unlike existing approaches, we also\nshow that accurate vehicle dynamics predictions are achievable without precise\nknowledge of the full vehicle state.\n","authors":["Giovanni Cioffi","Leonard Bauersfeld","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2504.00969v3.pdf","comment":"Transactions on Robotics (T-RO) 2025"},{"id":"http://arxiv.org/abs/2504.09833v2","updated":"2025-09-02T19:36:04Z","published":"2025-04-14T03:02:02Z","title":"PPF: Pre-training and Preservative Fine-tuning of Humanoid Locomotion\n  via Model-Assumption-based Regularization","summary":"  Humanoid locomotion is a challenging task due to its inherent complexity and\nhigh-dimensional dynamics, as well as the need to adapt to diverse and\nunpredictable environments. In this work, we introduce a novel learning\nframework for effectively training a humanoid locomotion policy that imitates\nthe behavior of a model-based controller while extending its capabilities to\nhandle more complex locomotion tasks, such as more challenging terrain and\nhigher velocity commands. Our framework consists of three key components:\npre-training through imitation of the model-based controller, fine-tuning via\nreinforcement learning, and model-assumption-based regularization (MAR) during\nfine-tuning. In particular, MAR aligns the policy with actions from the\nmodel-based controller only in states where the model assumption holds to\nprevent catastrophic forgetting. We evaluate the proposed framework through\ncomprehensive simulation tests and hardware experiments on a full-size humanoid\nrobot, Digit, demonstrating a forward speed of 1.5 m/s and robust locomotion\nacross diverse terrains, including slippery, sloped, uneven, and sandy\nterrains.\n","authors":["Hyunyoung Jung","Zhaoyuan Gu","Ye Zhao","Hae-Won Park","Sehoon Ha"],"pdf_url":"https://arxiv.org/pdf/2504.09833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.17985v2","updated":"2025-09-02T19:01:33Z","published":"2025-08-25T12:55:30Z","title":"Integration of Computer Vision with Adaptive Control for Autonomous\n  Driving Using ADORE","summary":"  Ensuring safety in autonomous driving requires a seamless integration of\nperception and decision making under uncertain conditions. Although computer\nvision (CV) models such as YOLO achieve high accuracy in detecting traffic\nsigns and obstacles, their performance degrades in drift scenarios caused by\nweather variations or unseen objects. This work presents a simulated autonomous\ndriving system that combines a context aware CV model with adaptive control\nusing the ADORE framework. The CARLA simulator was integrated with ADORE via\nthe ROS bridge, allowing real-time communication between perception, decision,\nand control modules. A simulated test case was designed in both clear and drift\nweather conditions to demonstrate the robust detection performance of the\nperception model while ADORE successfully adapted vehicle behavior to speed\nlimits and obstacles with low response latency. The findings highlight the\npotential of coupling deep learning-based perception with rule-based adaptive\ndecision making to improve automotive safety critical system.\n","authors":["Abu Shad Ahammed","Md Shahi Amran Hossain","Sayeri Mukherjee","Roman Obermaisser","Md. Ziaur Rahman"],"pdf_url":"https://arxiv.org/pdf/2508.17985v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02749v1","updated":"2025-09-02T18:56:44Z","published":"2025-09-02T18:56:44Z","title":"The Impact of Adaptive Emotional Alignment on Mental State Attribution\n  and User Empathy in HRI","summary":"  The paper presents an experiment on the effects of adaptive emotional\nalignment between agents, considered a prerequisite for empathic communication,\nin Human-Robot Interaction (HRI). Using the NAO robot, we investigate the\nimpact of an emotionally aligned, empathic, dialogue on these aspects: (i) the\nrobot's persuasive effectiveness, (ii) the user's communication style, and\n(iii) the attribution of mental states and empathy to the robot. In an\nexperiment with 42 participants, two conditions were compared: one with neutral\ncommunication and another where the robot provided responses adapted to the\nemotions expressed by the users. The results show that emotional alignment does\nnot influence users' communication styles or have a persuasive effect. However,\nit significantly influences attribution of mental states to the robot and its\nperceived empathy\n","authors":["Giorgia Buracchio","Ariele Callegari","Massimo Donini","Cristina Gena","Antonio Lieto","Alberto Lillo","Claudio Mattutino","Alessandro Mazzei","Linda Pigureddu","Manuel Striani","Fabiana Vernero"],"pdf_url":"https://arxiv.org/pdf/2509.02749v1.pdf","comment":"autohor copy of the paper accepted at ROMAN2025"},{"id":"http://arxiv.org/abs/2509.02727v1","updated":"2025-09-02T18:23:17Z","published":"2025-09-02T18:23:17Z","title":"Acrobotics: A Generalist Approahc To Quadrupedal Robots' Parkour","summary":"  Climbing, crouching, bridging gaps, and walking up stairs are just a few of\nthe advantages that quadruped robots have over wheeled robots, making them more\nsuitable for navigating rough and unstructured terrain. However, executing such\nmanoeuvres requires precise temporal coordination and complex agent-environment\ninteractions. Moreover, legged locomotion is inherently more prone to slippage\nand tripping, and the classical approach of modeling such cases to design a\nrobust controller thus quickly becomes impractical. In contrast, reinforcement\nlearning offers a compelling solution by enabling optimal control through trial\nand error. We present a generalist reinforcement learning algorithm for\nquadrupedal agents in dynamic motion scenarios. The learned policy rivals\nstate-of-the-art specialist policies trained using a mixture of experts\napproach, while using only 25% as many agents during training. Our experiments\nalso highlight the key components of the generalist locomotion policy and the\nprimary factors contributing to its success.\n","authors":["Guillaume Gagné-Labelle","Vassil Atanassov","Ioannis Havoutis"],"pdf_url":"https://arxiv.org/pdf/2509.02727v1.pdf","comment":"Supplementary material can be found here:\n  https://drive.google.com/drive/folders/18h25azbCFfPF4fhSsRfxKrnZo3dPKs_j?usp=sharing"},{"id":"http://arxiv.org/abs/2509.02659v1","updated":"2025-09-02T17:52:29Z","published":"2025-09-02T17:52:29Z","title":"2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous\n  Driving Using Vision Language Model","summary":"  End-to-end autonomous driving has drawn tremendous attention recently. Many\nworks focus on using modular deep neural networks to construct the end-to-end\narchi-tecture. However, whether using powerful large language models (LLM),\nespecially multi-modality Vision Language Models (VLM) could benefit the\nend-to-end driving tasks remain a question. In our work, we demonstrate that\ncombining end-to-end architectural design and knowledgeable VLMs yield\nimpressive performance on the driving tasks. It is worth noting that our method\nonly uses a single camera and is the best camera-only solution across the\nleaderboard, demonstrating the effectiveness of vision-based driving approach\nand the potential for end-to-end driving tasks.\n","authors":["Zilong Guo","Yi Luo","Long Sha","Dongxu Wang","Panqu Wang","Chenyang Xu","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2509.02659v1.pdf","comment":"2nd place in CVPR 2024 End-to-End Driving at Scale Challenge"},{"id":"http://arxiv.org/abs/2509.02530v1","updated":"2025-09-02T17:29:38Z","published":"2025-09-02T17:29:38Z","title":"Manipulation as in Simulation: Enabling Accurate Geometry Perception in\n  Robots","summary":"  Modern robotic manipulation primarily relies on visual observations in a 2D\ncolor space for skill learning but suffers from poor generalization. In\ncontrast, humans, living in a 3D world, depend more on physical properties-such\nas distance, size, and shape-than on texture when interacting with objects.\nSince such 3D geometric information can be acquired from widely available depth\ncameras, it appears feasible to endow robots with similar perceptual\ncapabilities. Our pilot study found that using depth cameras for manipulation\nis challenging, primarily due to their limited accuracy and susceptibility to\nvarious types of noise. In this work, we propose Camera Depth Models (CDMs) as\na simple plugin on daily-use depth cameras, which take RGB images and raw depth\nsignals as input and output denoised, accurate metric depth. To achieve this,\nwe develop a neural data engine that generates high-quality paired data from\nsimulation by modeling a depth camera's noise pattern. Our results show that\nCDMs achieve nearly simulation-level accuracy in depth prediction, effectively\nbridging the sim-to-real gap for manipulation tasks. Notably, our experiments\ndemonstrate, for the first time, that a policy trained on raw simulated depth,\nwithout the need for adding noise or real-world fine-tuning, generalizes\nseamlessly to real-world robots on two challenging long-horizon tasks involving\narticulated, reflective, and slender objects, with little to no performance\ndegradation. We hope our findings will inspire future research in utilizing\nsimulation data and 3D information in general robot policies.\n","authors":["Minghuan Liu","Zhengbang Zhu","Xiaoshen Han","Peng Hu","Haotong Lin","Xinyao Li","Jingxiao Chen","Jiafeng Xu","Yichu Yang","Yunfeng Lin","Xinghang Li","Yong Yu","Weinan Zhang","Tao Kong","Bingyi Kang"],"pdf_url":"https://arxiv.org/pdf/2509.02530v1.pdf","comment":"32 pages, 18 figures, project page:\n  https://manipulation-as-in-simulation.github.io/"},{"id":"http://arxiv.org/abs/2509.02527v1","updated":"2025-09-02T17:28:38Z","published":"2025-09-02T17:28:38Z","title":"Fault-tolerant Model Predictive Control for Spacecraft","summary":"  Given the cost and critical functions of satellite constellations, ensuring\nmission longevity and safe decommissioning is essential for space\nsustainability. This article presents a Model Predictive Control for spacecraft\ntrajectory and setpoint stabilization under multiple actuation failures. The\nproposed solution allows us to efficiently control the faulty spacecraft\nenabling safe navigation towards servicing or collision-free trajectories. The\nproposed scheme ensures closed-loop asymptotic stability and is shown to be\nrecursively feasible. We demonstrate its efficacy through open-source numerical\nresults and realistic experiments using the ATMOS platform.\n","authors":["Raphael Stöckner","Pedro Roque","Maria Charitidou","Dimos V. Dimarogonas"],"pdf_url":"https://arxiv.org/pdf/2509.02527v1.pdf","comment":"The paper has been submitted to CDC2025"},{"id":"http://arxiv.org/abs/2503.23760v3","updated":"2025-09-02T17:11:45Z","published":"2025-03-31T06:23:14Z","title":"Towards a cognitive architecture to enable natural language interaction\n  in co-constructive task learning","summary":"  This research addresses the question, which characteristics a cognitive\narchitecture must have to leverage the benefits of natural language in\nCo-Constructive Task Learning (CCTL). To provide context, we first discuss\nInteractive Task Learning (ITL), the mechanisms of the human memory system, and\nthe significance of natural language and multi-modality. Next, we examine the\ncurrent state of cognitive architectures, analyzing their capabilities to\ninform a concept of CCTL grounded in multiple sources. We then integrate\ninsights from various research domains to develop a unified framework. Finally,\nwe conclude by identifying the remaining challenges and requirements necessary\nto achieve CCTL in Human-Robot Interaction (HRI).\n","authors":["Manuel Scheibl","Birte Richter","Alissa Müller","Michael Beetz","Britta Wrede"],"pdf_url":"https://arxiv.org/pdf/2503.23760v3.pdf","comment":"8 pages, 5 figures, The paper has been accepted by the 2025 34th IEEE\n  International Conference on Robot and Human Interactive Communication\n  (ROMAN), IEEE Copyright Policy:\n  https://www.ieee.org/publications/rights/copyright-policy"},{"id":"http://arxiv.org/abs/2412.20519v2","updated":"2025-09-02T16:54:57Z","published":"2024-12-29T16:42:30Z","title":"Goal-Conditioned Data Augmentation for Offline Reinforcement Learning","summary":"  Offline reinforcement learning (RL) enables policy learning from\npre-collected offline datasets, relaxing the need to interact directly with the\nenvironment. However, limited by the quality of offline datasets, it generally\nfails to learn well-qualified policies in suboptimal datasets. To address\ndatasets with insufficient optimal demonstrations, we introduce\nGoal-cOnditioned Data Augmentation (GODA), a novel goal-conditioned\ndiffusion-based method for augmenting samples with higher quality. Leveraging\nrecent advancements in generative modelling, GODA incorporates a novel\nreturn-oriented goal condition with various selection mechanisms. Specifically,\nwe introduce a controllable scaling technique to provide enhanced return-based\nguidance during data sampling. GODA learns a comprehensive distribution\nrepresentation of the original offline datasets while generating new data with\nselectively higher-return goals, thereby maximizing the utility of limited\noptimal demonstrations. Furthermore, we propose a novel adaptive gated\nconditioning method for processing noisy inputs and conditions, enhancing the\ncapture of goal-oriented guidance. We conduct experiments on the D4RL benchmark\nand real-world challenges, specifically traffic signal control (TSC) tasks, to\ndemonstrate GODA's effectiveness in enhancing data quality and superior\nperformance compared to state-of-the-art data augmentation methods across\nvarious offline RL algorithms.\n","authors":["Xingshuai Huang","Di Wu","Benoit Boulet"],"pdf_url":"https://arxiv.org/pdf/2412.20519v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.11775v2","updated":"2025-09-02T16:23:53Z","published":"2025-06-13T13:35:49Z","title":"ExoStart: Efficient learning for dexterous manipulation with sensorized\n  exoskeleton demonstrations","summary":"  Recent advancements in teleoperation systems have enabled high-quality data\ncollection for robotic manipulators, showing impressive results in learning\nmanipulation at scale. This progress suggests that extending these capabilities\nto robotic hands could unlock an even broader range of manipulation skills,\nespecially if we could achieve the same level of dexterity that human hands\nexhibit. However, teleoperating robotic hands is far from a solved problem, as\nit presents a significant challenge due to the high degrees of freedom of\nrobotic hands and the complex dynamics occurring during contact-rich settings.\nIn this work, we present ExoStart, a general and scalable learning framework\nthat leverages human dexterity to improve robotic hand control. In particular,\nwe obtain high-quality data by collecting direct demonstrations without a robot\nin the loop using a sensorized low-cost wearable exoskeleton, capturing the\nrich behaviors that humans can demonstrate with their own hands. We also\npropose a simulation-based dynamics filter that generates dynamically feasible\ntrajectories from the collected demonstrations and use the generated\ntrajectories to bootstrap an auto-curriculum reinforcement learning method that\nrelies only on simple sparse rewards. The ExoStart pipeline is generalizable\nand yields robust policies that transfer zero-shot to the real robot. Our\nresults demonstrate that ExoStart can generate dexterous real-world hand\nskills, achieving a success rate above 50% on a wide range of complex tasks\nsuch as opening an AirPods case or inserting and turning a key in a lock. More\ndetails and videos can be found in https://sites.google.com/view/exostart.\n","authors":["Zilin Si","Jose Enrique Chen","M. Emre Karagozler","Antonia Bronars","Jonathan Hutchinson","Thomas Lampe","Nimrod Gileadi","Taylor Howell","Stefano Saliceti","Lukasz Barczyk","Ilan Olivarez Correa","Tom Erez","Mohit Shridhar","Murilo Fernandes Martins","Konstantinos Bousmalis","Nicolas Heess","Francesco Nori","Maria Bauza Villalonga"],"pdf_url":"https://arxiv.org/pdf/2506.11775v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02453v1","updated":"2025-09-02T16:04:15Z","published":"2025-09-02T16:04:15Z","title":"Coral: A Unifying Abstraction Layer for Composable Robotics Software","summary":"  Despite the multitude of excellent software components and tools available in\nthe robotics and broader software engineering communities, successful\nintegration of software for robotic systems remains a time-consuming and\nchallenging task for users of all knowledge and skill levels. And with robotics\nsoftware often being built into tightly coupled, monolithic systems, even minor\nalterations to improve performance, adjust to changing task requirements, or\ndeploy to new hardware can require significant engineering investment. To help\nsolve this problem, this paper presents Coral, an abstraction layer for\nbuilding, deploying, and coordinating independent software components that\nmaximizes composability to allow for rapid system integration without modifying\nlow-level code. Rather than replacing existing tools, Coral complements them by\nintroducing a higher-level abstraction that constrains the integration process\nto semantically meaningful choices, reducing the configuration burden without\nlimiting adaptability to diverse domains, systems, and tasks. We describe Coral\nin detail and demonstrate its utility in integrating software for scenarios of\nincreasing complexity, including LiDAR-based SLAM and multi-robot corrosion\nmitigation tasks. By enabling practical composability in robotics software,\nCoral offers a scalable solution to a broad range of robotics system\nintegration challenges, improving component reusability, system\nreconfigurability, and accessibility to both expert and non-expert users. We\nrelease Coral open source.\n","authors":["Steven Swanbeck","Mitch Pryor"],"pdf_url":"https://arxiv.org/pdf/2509.02453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.13048v3","updated":"2025-09-02T15:47:35Z","published":"2025-03-17T10:53:42Z","title":"Multi-Touch and Bending Perception Using Electrical Impedance Tomography\n  for Robotics","summary":"  Electrical Impedance Tomography (EIT) offers a promising solution for\ndistributed tactile sensing with minimal wiring and full-surface coverage in\nrobotic applications. However, EIT-based tactile sensors face significant\nchallenges during surface bending. Deformation alters the baseline impedance\ndistribution and couples with touch-induced conductivity variations,\ncomplicating signal interpretation. To address this challenge, we present a\nnovel sensing framework that integrates a deep neural network for interaction\nstate classification with a dynamic adaptive reference strategy to decouple\ntouch and deformation signals, while a data-driven regression model translates\nEIT voltage changes into continuous bending angles. The framework is validated\nusing a magnetic hydrogel composite sensor that conforms to bendable surfaces.\nExperimental evaluations demonstrate that the proposed framework achieves\nprecise and robust bending angle estimation, high accuracy in distinguishing\ntouch, bending, and idle states, and significantly improves touch localization\nquality under bending deformation compared to conventional fixed-reference\nmethods. Real-time experiments confirm the system's capability to reliably\ndetect multi-touch interactions and track bending angles across varying\ndeformation conditions. This work paves the way for flexible EIT-based robotic\nskins capable of rich multimodal sensing in robotics and human-robot\ninteraction.\n","authors":["Haofeng Chen","Bedrich Himmel","Bin Li","Xiaojie Wang","Matej Hoffmann"],"pdf_url":"https://arxiv.org/pdf/2503.13048v3.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2509.02437v1","updated":"2025-09-02T15:39:38Z","published":"2025-09-02T15:39:38Z","title":"U-ARM : Ultra low-cost general teleoperation interface for robot\n  manipulation","summary":"  We propose U-Arm, a low-cost and rapidly adaptable leader-follower\nteleoperation framework designed to interface with most of commercially\navailable robotic arms. Our system supports teleoperation through three\nstructurally distinct 3D-printed leader arms that share consistent control\nlogic, enabling seamless compatibility with diverse commercial robot\nconfigurations. Compared with previous open-source leader-follower interfaces,\nwe further optimized both the mechanical design and servo selection, achieving\na bill of materials (BOM) cost of only \\$50.5 for the 6-DoF leader arm and\n\\$56.8 for the 7-DoF version. To enhance usability, we mitigate the common\nchallenge in controlling redundant degrees of freedom by %engineering methods\nmechanical and control optimizations. Experimental results demonstrate that\nU-Arm achieves 39\\% higher data collection efficiency and comparable task\nsuccess rates across multiple manipulation scenarios compared with Joycon,\nanother low-cost teleoperation interface. We have open-sourced all CAD models\nof three configs and also provided simulation support for validating\nteleoperation workflows. We also open-sourced real-world manipulation data\ncollected with U-Arm. The project website is\nhttps://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.\n","authors":["Yanwen Zou","Zhaoye Zhou","Chenyang Shi","Zewei Ye","Junda Huang","Yan Ding","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2509.02437v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02425v1","updated":"2025-09-02T15:27:12Z","published":"2025-09-02T15:27:12Z","title":"OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals\n  with Visual Impairments","summary":"  Indoor built environments like homes and offices often present complex and\ncluttered layouts that pose significant challenges for individuals who are\nblind or visually impaired, especially when performing tasks that involve\nlocating and gathering multiple objects. While many existing assistive\ntechnologies focus on basic navigation or obstacle avoidance, few systems\nprovide scalable and efficient multi-object search capabilities in real-world,\npartially observable settings. To address this gap, we introduce OpenGuide, an\nassistive mobile robot system that combines natural language understanding with\nvision-language foundation models (VLM), frontier-based exploration, and a\nPartially Observable Markov Decision Process (POMDP) planner. OpenGuide\ninterprets open-vocabulary requests, reasons about object-scene relationships,\nand adaptively navigates and localizes multiple target items in novel\nenvironments. Our approach enables robust recovery from missed detections\nthrough value decay and belief-space reasoning, resulting in more effective\nexploration and object localization. We validate OpenGuide in simulated and\nreal-world experiments, demonstrating substantial improvements in task success\nrate and search efficiency over prior methods. This work establishes a\nfoundation for scalable, human-centered robotic assistance in assisted living\nenvironments.\n","authors":["Yifan Xu","Qianwei Wang","Vineet Kamat","Carol Menassa"],"pdf_url":"https://arxiv.org/pdf/2509.02425v1.pdf","comment":"32 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.16551v2","updated":"2025-09-02T15:23:47Z","published":"2025-03-19T14:16:37Z","title":"SafeLink: Safety-Critical Control Under Dynamic and Irregular Unsafe\n  Regions","summary":"  Control barrier functions (CBFs) provide a theoretical foundation for\nsafety-critical control in robotic systems. However, most existing methods rely\non the analytical expressions of unsafe state regions, which are often\nimpractical for irregular and dynamic unsafe regions. This paper introduces\nSafeLink, a novel CBF construction method based on cost-sensitive incremental\nrandom vector functional-link (RVFL) neural networks. By designing a valid cost\nfunction, SafeLink assigns different sensitivities to safe and unsafe state\npoints, thereby eliminating false negatives in classification of unsafe state\npoints. Furthermore, an incremental update theorem is established, enabling\nprecise real-time adaptation to changes in unsafe regions. An analytical\nexpression for the gradient of SafeLink is also derived to facilitate control\ninput computation. The proposed method is validated on the endpoint position\ncontrol task of a nonlinear two-link manipulator. Experimental results\ndemonstrate that the method effectively learns the unsafe regions and rapidly\nadapts as these regions change, achieving an update speed significantly faster\nthan comparison methods, while safely reaching the target position. The source\ncode is available at https://github.com/songqiaohu/SafeLink.\n","authors":["Songqiao Hu","Zidong Wang","Zeyi Liu","Zhen Shen","Xiao He"],"pdf_url":"https://arxiv.org/pdf/2503.16551v2.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2507.17376v2","updated":"2025-09-02T14:33:10Z","published":"2025-07-23T10:19:22Z","title":"An Exploratory Study on Human-Robot Interaction using Semantics-based\n  Situational Awareness","summary":"  In this paper, we investigate the impact of high-level semantics (evaluation\nof the environment) on Human-Robot Teams (HRT) and Human-Robot Interaction\n(HRI) in the context of mobile robot deployments. Although semantics has been\nwidely researched in AI, how high-level semantics can benefit the HRT paradigm\nis underexplored, often fuzzy, and intractable. We applied a semantics-based\nframework that could reveal different indicators of the environment (i.e. how\nmuch semantic information exists) in a mock-up disaster response mission. In\nsuch missions, semantics are crucial as the HRT should handle complex\nsituations and respond quickly with correct decisions, where humans might have\na high workload and stress. Especially when human operators need to shift their\nattention between robots and other tasks, they will struggle to build\nSituational Awareness (SA) quickly. The experiment suggests that the presented\nsemantics: 1) alleviate the perceived workload of human operators; 2) increase\nthe operator's trust in the SA; and 3) help to reduce the reaction time in\nswitching the level of autonomy when needed. Additionally, we find that\nparticipants with higher trust in the system are encouraged by high-level\nsemantics to use teleoperation mode more.\n","authors":["Tianshu Ruan","Aniketh Ramesh","Rustam Stolkin","Manolis Chiou"],"pdf_url":"https://arxiv.org/pdf/2507.17376v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.20425v3","updated":"2025-09-02T14:25:18Z","published":"2025-03-26T10:59:08Z","title":"Perspective-Shifted Neuro-Symbolic World Models: A Framework for\n  Socially-Aware Robot Navigation","summary":"  Navigating in environments alongside humans requires agents to reason under\nuncertainty and account for the beliefs and intentions of those around them.\nUnder a sequential decision-making framework, egocentric navigation can\nnaturally be represented as a Markov Decision Process (MDP). However, social\nnavigation additionally requires reasoning about the hidden beliefs of others,\ninherently leading to a Partially Observable Markov Decision Process (POMDP),\nwhere agents lack direct access to others' mental states. Inspired by Theory of\nMind and Epistemic Planning, we propose (1) a neuro-symbolic model-based\nreinforcement learning architecture for social navigation, addressing the\nchallenge of belief tracking in partially observable environments; and (2) a\nperspective-shift operator for belief estimation, leveraging recent work on\nInfluence-based Abstractions (IBA) in structured multi-agent settings.\n","authors":["Kevin Alcedo","Pedro U. Lima","Rachid Alami"],"pdf_url":"https://arxiv.org/pdf/2503.20425v3.pdf","comment":"Accepted as a regular paper at the 2025 IEEE International Conference\n  on Robot & Human Interactive Communication (RO-MAN). \\c{opyright} 2025 IEEE.\n  The final version will appear in IEEE Xplore"},{"id":"http://arxiv.org/abs/2509.02343v1","updated":"2025-09-02T14:11:26Z","published":"2025-09-02T14:11:26Z","title":"Physics-Informed Machine Learning with Adaptive Grids for Optical\n  Microrobot Depth Estimation","summary":"  Optical microrobots actuated by optical tweezers (OT) offer great potential\nfor biomedical applications such as cell manipulation and microscale assembly.\nThese tasks demand accurate three-dimensional perception to ensure precise\ncontrol in complex and dynamic biological environments. However, the\ntransparent nature of microrobots and low-contrast microscopic imaging\nchallenge conventional deep learning methods, which also require large\nannotated datasets that are costly to obtain. To address these challenges, we\npropose a physics-informed, data-efficient framework for depth estimation of\noptical microrobots. Our method augments convolutional feature extraction with\nphysics-based focus metrics, such as entropy, Laplacian of Gaussian, and\ngradient sharpness, calculated using an adaptive grid strategy. This approach\nallocates finer grids over microrobot regions and coarser grids over background\nareas, enhancing depth sensitivity while reducing computational complexity. We\nevaluate our framework on multiple microrobot types and demonstrate significant\nimprovements over baseline models. Specifically, our approach reduces mean\nsquared error (MSE) by over 60% and improves the coefficient of determination\n(R^2) across all test cases. Notably, even when trained on only 20% of the\navailable data, our model outperforms ResNet50 trained on the full dataset,\nhighlighting its robustness under limited data conditions. Our code is\navailable at: https://github.com/LannWei/CBS2025.\n","authors":["Lan Wei","Lou Genoud","Dandan Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.02343v1.pdf","comment":"2025 IEEE International Conference on Cyborg and Bionic Systems (CBS\n  2025)"},{"id":"http://arxiv.org/abs/2509.02324v1","updated":"2025-09-02T13:50:45Z","published":"2025-09-02T13:50:45Z","title":"Language-Guided Long Horizon Manipulation with LLM-based Planning and\n  Visual Perception","summary":"  Language-guided long-horizon manipulation of deformable objects presents\nsignificant challenges due to high degrees of freedom, complex dynamics, and\nthe need for accurate vision-language grounding. In this work, we focus on\nmulti-step cloth folding, a representative deformable-object manipulation task\nthat requires both structured long-horizon planning and fine-grained visual\nperception. To this end, we propose a unified framework that integrates a Large\nLanguage Model (LLM)-based planner, a Vision-Language Model (VLM)-based\nperception system, and a task execution module. Specifically, the LLM-based\nplanner decomposes high-level language instructions into low-level action\nprimitives, bridging the semantic-execution gap, aligning perception with\naction, and enhancing generalization. The VLM-based perception module employs a\nSigLIP2-driven architecture with a bidirectional cross-attention fusion\nmechanism and weight-decomposed low-rank adaptation (DoRA) fine-tuning to\nachieve language-conditioned fine-grained visual grounding. Experiments in both\nsimulation and real-world settings demonstrate the method's effectiveness. In\nsimulation, it outperforms state-of-the-art baselines by 2.23, 1.87, and 33.3\non seen instructions, unseen instructions, and unseen tasks, respectively. On a\nreal robot, it robustly executes multi-step folding sequences from language\ninstructions across diverse cloth materials and configurations, demonstrating\nstrong generalization in practical scenarios. Project page:\nhttps://language-guided.netlify.app/\n","authors":["Changshi Zhou","Haichuan Xu","Ningquan Gu","Zhipeng Wang","Bin Cheng","Pengpeng Zhang","Yanchao Dong","Mitsuhiro Hayashibe","Yanmin Zhou","Bin He"],"pdf_url":"https://arxiv.org/pdf/2509.02324v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12731v2","updated":"2025-09-02T13:35:51Z","published":"2024-06-18T15:55:02Z","title":"Tactile SoftHand-A: 3D-Printed, Tactile, Highly-underactuated,\n  Anthropomorphic Robot Hand with an Antagonistic Tendon Mechanism","summary":"  A challenging and important problem for tendon-driven multi-fingered robotic\nhands is to ensure grasping adaptivity while minimizing the number of actuators\nneeded to provide human-like functionality. Inspired by the Pisa/IIT SoftHand,\nthis paper introduces a 3D-printed, highly-underactuated, tactile-sensorized,\nfive-finger robotic hand named the Tactile SoftHand-A, which features an\nantagonistic mechanism to actively open and close the hand. Our proposed\ndual-tendon design gives options that allow active control of specific (distal\nor proximal interphalangeal) joints; for example, to adjust from an enclosing\nto fingertip grasp or to manipulate an object with a fingertip. We also develop\nand integrate a new design of fully 3D-printed vision-based tactile sensor\nwithin the fingers that requires minimal hand assembly. A control scheme based\non analytically extracting contact location and slip from the tactile images is\nused to coordinate the antagonistic tendon mechanism (using a marker\ndisplacement density map, suitable for TacTip-based sensors). We perform\nextensive testing of a single finger, the entire hand, and the tactile\ncapabilities to show the improvements in reactivity, load-bearing, and\nmanipulability in comparison to a SoftHand that lacks the antagonistic\nmechanism. We also demonstrate the hand's reactivity to contact disturbances\nincluding slip, and how this enables teleoperated control from human hand\ngestures. Overall, this study points the way towards a class of low-cost,\naccessible, 3D-printable, tactile, underactuated human-like robotic hands, and\nwe openly release the designs to facilitate others to build upon this work. The\ndesigns are open-sourced at https://github.com/HaoranLi-Data/Tactile_SoftHand_A\n","authors":["Haoran Li","Christopher J. Ford","Chenghua Lu","Yijiong Lin","Matteo Bianchi","Manuel G. Catalano","Efi Psomopoulou","Nathan F. Lepora"],"pdf_url":"https://arxiv.org/pdf/2406.12731v2.pdf","comment":"17 pages, 13 figures"},{"id":"http://arxiv.org/abs/2409.10931v2","updated":"2025-09-02T13:21:35Z","published":"2024-09-17T07:07:08Z","title":"Frontier Shepherding: A Bio-inspired Multi-robot Framework for\n  Large-Scale Exploration","summary":"  Efficient exploration of large-scale environments remains a critical\nchallenge in robotics, with applications ranging from environmental monitoring\nto search and rescue operations. This article proposes Frontier Shepherding\n(FroShe), a bio-inspired multi-robot framework for large-scale exploration. The\nframework heuristically models frontier exploration based on the shepherding\nbehavior of herding dogs, where frontiers are treated as a swarm of sheep\nreacting to robots modeled as shepherding dogs. FroShe is robust across varying\nenvironment sizes and obstacle densities, requiring minimal parameter tuning\nfor deployment across multiple agents. Simulation results demonstrate that the\nproposed method performs consistently, regardless of environment complexity,\nand outperforms state-of-the-art exploration strategies by an average of 20%\nwith three UAVs. The approach was further validated in real-world experiments\nusing single- and dual-drone deployments in a forest-like environment.\n","authors":["John Lewis","Meysam Basiri","Pedro U. Lima"],"pdf_url":"https://arxiv.org/pdf/2409.10931v2.pdf","comment":"8 page article accepted at IEEE/RSJ International Conferenceo on\n  Intelligent Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2509.02275v1","updated":"2025-09-02T12:52:53Z","published":"2025-09-02T12:52:53Z","title":"Human-Inspired Soft Anthropomorphic Hand System for Neuromorphic Object\n  and Pose Recognition Using Multimodal Signals","summary":"  The human somatosensory system integrates multimodal sensory feedback,\nincluding tactile, proprioceptive, and thermal signals, to enable comprehensive\nperception and effective interaction with the environment. Inspired by the\nbiological mechanism, we present a sensorized soft anthropomorphic hand\nequipped with diverse sensors designed to emulate the sensory modalities of the\nhuman hand. This system incorporates biologically inspired encoding schemes\nthat convert multimodal sensory data into spike trains, enabling\nhighly-efficient processing through Spiking Neural Networks (SNNs). By\nutilizing these neuromorphic signals, the proposed framework achieves 97.14%\naccuracy in object recognition across varying poses, significantly\noutperforming previous studies on soft hands. Additionally, we introduce a\nnovel differentiator neuron model to enhance material classification by\ncapturing dynamic thermal responses. Our results demonstrate the benefits of\nmultimodal sensory fusion and highlight the potential of neuromorphic\napproaches for achieving efficient, robust, and human-like perception in\nrobotic systems.\n","authors":["Fengyi Wang","Xiangyu Fu","Nitish Thakor","Gordon Cheng"],"pdf_url":"https://arxiv.org/pdf/2509.02275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13266v2","updated":"2025-09-02T12:20:09Z","published":"2024-03-20T03:03:22Z","title":"Enhancing Security in Multi-Robot Systems through Co-Observation\n  Planning, Reachability Analysis, and Network Flow","summary":"  This paper addresses security challenges in multi-robot systems (MRS) where\nadversaries may compromise robot control, risking unauthorized access to\nforbidden areas. We propose a novel multi-robot optimal planning algorithm that\nintegrates mutual observations and introduces reachability constraints for\nenhanced security. This ensures that, even with adversarial movements,\ncompromised robots cannot breach forbidden regions without missing scheduled\nco-observations. The reachability constraint uses ellipsoidal\nover-approximation for efficient intersection checking and gradient\ncomputation. To enhance system resilience and tackle feasibility challenges, we\nalso introduce sub-teams. These cohesive units replace individual robot\nassignments along each route, enabling redundant robots to deviate for\nco-observations across different trajectories, securing multiple sub-teams\nwithout requiring modifications. We formulate the cross-trajectory\nco-observation plan by solving a network flow coverage problem on the\ncheckpoint graph generated from the original unsecured MRS trajectories,\nproviding the same security guarantees against plan-deviation attacks. We\ndemonstrate the effectiveness and robustness of our proposed algorithm, which\nsignificantly strengthens the security of multi-robot systems in the face of\nadversarial threats.\n","authors":["Ziqi Yang","Roberto Tron"],"pdf_url":"https://arxiv.org/pdf/2403.13266v2.pdf","comment":"12 pages, 6 figures, submitted to IEEE Transactions on Control of\n  Network Systems"},{"id":"http://arxiv.org/abs/2506.12779v3","updated":"2025-09-02T12:06:20Z","published":"2025-06-15T09:09:34Z","title":"From Experts to a Generalist: Toward General Whole-Body Control for\n  Humanoid Robots","summary":"  Achieving general agile whole-body control on humanoid robots remains a major\nchallenge due to diverse motion demands and data conflicts. While existing\nframeworks excel in training single motion-specific policies, they struggle to\ngeneralize across highly varied behaviors due to conflicting control\nrequirements and mismatched data distributions. In this work, we propose\nBumbleBee (BB), an expert-generalist learning framework that combines motion\nclustering and sim-to-real adaptation to overcome these challenges. BB first\nleverages an autoencoder-based clustering method to group behaviorally similar\nmotions using motion features and motion descriptions. Expert policies are then\ntrained within each cluster and refined with real-world data through iterative\ndelta action modeling to bridge the sim-to-real gap. Finally, these experts are\ndistilled into a unified generalist controller that preserves agility and\nrobustness across all motion types. Experiments on two simulations and a real\nhumanoid robot demonstrate that BB achieves state-of-the-art general whole-body\ncontrol, setting a new benchmark for agile, robust, and generalizable humanoid\nperformance in the real world. The project webpage is available at\nhttps://beingbeyond.github.io/BumbleBee/.\n","authors":["Yuxuan Wang","Ming Yang","Ziluo Ding","Yu Zhang","Weishuai Zeng","Xinrun Xu","Haobin Jiang","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2506.12779v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02204v1","updated":"2025-09-02T11:18:16Z","published":"2025-09-02T11:18:16Z","title":"Adaptive Navigation Strategy for Low-Thrust Proximity Operations in\n  Circular Relative Orbit","summary":"  This paper presents an adaptive observer-based navigation strategy for\nspacecraft in Circular Relative Orbit (CRO) scenarios, addressing challenges in\nproximity operations like formation flight and uncooperative target inspection.\nThe proposed method adjusts observer gains based on the estimated state to\nachieve fast convergence and low noise sensitivity in state estimation. A\nLyapunov-based analysis ensures stability and accuracy, while simulations using\nvision-based sensor data validate the approach under realistic conditions.\nCompared to classical observers with time-invariant gains, the proposed method\nenhances trajectory tracking precision and reduces control input switching,\nmaking it a promising solution for autonomous spacecraft localization and\ncontrol.\n","authors":["Dario Ruggiero","Mauro Mancini","Elisa Capello"],"pdf_url":"https://arxiv.org/pdf/2509.02204v1.pdf","comment":"This work has been accepted and presented at the 35th AAS/AIAA Space\n  Flight Mechanics Meeting, 2025, Kaua'i, Hawai"},{"id":"http://arxiv.org/abs/2502.21057v4","updated":"2025-09-02T10:25:02Z","published":"2025-02-28T13:58:22Z","title":"Robust Deterministic Policy Gradient for Disturbance Attenuation and Its\n  Application to Quadrotor Control","summary":"  Practical control systems pose significant challenges in identifying optimal\ncontrol policies due to uncertainties in the system model and external\ndisturbances. While $H_\\infty$ control techniques are commonly used to design\nrobust controllers that mitigate the effects of disturbances, these methods\noften require complex and computationally intensive calculations. To address\nthis issue, this paper proposes a reinforcement learning algorithm called\nRobust Deterministic Policy Gradient (RDPG), which formulates the $H_\\infty$\ncontrol problem as a two-player zero-sum dynamic game. In this formulation, one\nplayer (the user) aims to minimize the cost, while the other player (the\nadversary) seeks to maximize it. We then employ deterministic policy gradient\n(DPG) and its deep reinforcement learning counterpart to train a robust control\npolicy with effective disturbance attenuation. In particular, for practical\nimplementation, we introduce an algorithm called robust deep deterministic\npolicy gradient (RDDPG), which employs a deep neural network architecture and\nintegrates techniques from the twin-delayed deep deterministic policy gradient\n(TD3) to enhance stability and learning efficiency. To evaluate the proposed\nalgorithm, we implement it on an unmanned aerial vehicle (UAV) tasked with\nfollowing a predefined path in a disturbance-prone environment. The\nexperimental results demonstrate that the proposed method outperforms other\ncontrol approaches in terms of robustness against disturbances, enabling\nprecise real-time tracking of moving targets even under severe disturbance\nconditions.\n","authors":["Taeho Lee","Donghwan Lee"],"pdf_url":"https://arxiv.org/pdf/2502.21057v4.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2509.02163v1","updated":"2025-09-02T10:14:28Z","published":"2025-09-02T10:14:28Z","title":"Enhancing Reliability in LLM-Integrated Robotic Systems: A Unified\n  Approach to Security and Safety","summary":"  Integrating large language models (LLMs) into robotic systems has\nrevolutionised embodied artificial intelligence, enabling advanced\ndecision-making and adaptability. However, ensuring reliability, encompassing\nboth security against adversarial attacks and safety in complex environments,\nremains a critical challenge. To address this, we propose a unified framework\nthat mitigates prompt injection attacks while enforcing operational safety\nthrough robust validation mechanisms. Our approach combines prompt assembling,\nstate management, and safety validation, evaluated using both performance and\nsecurity metrics. Experiments show a 30.8% improvement under injection attacks\nand up to a 325% improvement in complex environment settings under adversarial\nconditions compared to baseline scenarios. This work bridges the gap between\nsafety and security in LLM-based robotic systems, offering actionable insights\nfor deploying reliable LLM-integrated mobile robots in real-world settings. The\nframework is open-sourced with simulation and physical deployment demos at\nhttps://llmeyesim.vercel.app/\n","authors":["Wenxiao Zhang","Xiangrui Kong","Conan Dewitt","Thomas Bräunl","Jin B. Hong"],"pdf_url":"https://arxiv.org/pdf/2509.02163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.08611v2","updated":"2025-09-02T09:53:05Z","published":"2022-09-18T17:27:08Z","title":"Autonomous Task Planning for Heterogeneous Multi-Agent Systems","summary":"  This paper presents a solution to the automatic task planning problem for\nmulti-agent systems. A formal framework is developed based on the\nNondeterministic Finite Automata with $\\epsilon$-transitions, where given the\ncapabilities, constraints and failure modes of the agents involved, an initial\nstate of the system and a task specification, an optimal solution is generated\nthat satisfies the system constraints and the task specification. The resulting\nsolution is guaranteed to be complete and optimal; moreover a heuristic\nsolution that offers significant reduction of the computational requirements\nwhile relaxing the completeness and optimality requirements is proposed. The\nconstructed system model is independent from the initial condition and the task\nspecification, alleviating the need to repeat the costly pre-processing cycle\nfor solving other scenarios, while allowing the incorporation of failure modes\non-the-fly. Two case studies are provided: a simple one to showcase the\nconcepts of the proposed methodology and a more elaborate one to demonstrate\nthe effectiveness and validity of the methodology.\n","authors":["Anatoli A. Tziola","Savvas G. Loizou"],"pdf_url":"https://arxiv.org/pdf/2209.08611v2.pdf","comment":"Long version of paper submitted to the IEEE ICRA 2023 Conference, in\n  IEEE Transactions on Automatic Control, 2025"},{"id":"http://arxiv.org/abs/2509.02146v1","updated":"2025-09-02T09:49:21Z","published":"2025-09-02T09:49:21Z","title":"Systematic Evaluation of Trade-Offs in Motion Planning Algorithms for\n  Optimal Industrial Robotic Work Cell Design","summary":"  The performance of industrial robotic work cells depends on optimizing\nvarious hyperparameters referring to the cell layout, such as robot base\nplacement, tool placement, and kinematic design. Achieving this requires a\nbilevel optimization approach, where the high-level optimization adjusts these\nhyperparameters, and the low-level optimization computes robot motions.\nHowever, computing the optimal robot motion is computationally infeasible,\nintroducing trade-offs in motion planning to make the problem tractable. These\ntrade-offs significantly impact the overall performance of the bilevel\noptimization, but their effects still need to be systematically evaluated. In\nthis paper, we introduce metrics to assess these trade-offs regarding\noptimality, time gain, robustness, and consistency. Through extensive\nsimulation studies, we investigate how simplifications in motion-level\noptimization affect the high-level optimization outcomes, balancing\ncomputational complexity with solution quality. The proposed algorithms are\napplied to find the time-optimal kinematic design for a modular robot in two\npalletization scenarios.\n","authors":["G. de Mathelin","C. Hartl-Nesic","A. Kugi"],"pdf_url":"https://arxiv.org/pdf/2509.02146v1.pdf","comment":"This work has been accepted to IFAC for publication under a Creative\n  Commons Licence CC-BY-NC-ND"},{"id":"http://arxiv.org/abs/2509.02134v1","updated":"2025-09-02T09:36:11Z","published":"2025-09-02T09:36:11Z","title":"Learning Social Heuristics for Human-Aware Path Planning","summary":"  Social robotic navigation has been at the center of numerous studies in\nrecent years. Most of the research has focused on driving the robotic agent\nalong obstacle-free trajectories, respecting social distances from humans, and\npredicting their movements to optimize navigation. However, in order to really\nbe socially accepted, the robots must be able to attain certain social norms\nthat cannot arise from conventional navigation, but require a dedicated\nlearning process. We propose Heuristic Planning with Learned Social Value\n(HPLSV), a method to learn a value function encapsulating the cost of social\nnavigation, and use it as an additional heuristic in heuristic-search path\nplanning. In this preliminary work, we apply the methodology to the common\nsocial scenario of joining a queue of people, with the intention of\ngeneralizing to further human activities.\n","authors":["Andrea Eirale","Matteo Leonetti","Marcello Chiaberge"],"pdf_url":"https://arxiv.org/pdf/2509.02134v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.01752v2","updated":"2025-09-02T08:28:07Z","published":"2025-05-03T09:02:35Z","title":"NMPCB: A Lightweight and Safety-Critical Motion Control Framework for\n  Ackermann Mobile Robot","summary":"  In multi-obstacle environments, real-time performance and safety in robot\nmotion control have long been challenging issues, as conventional methods often\nstruggle to balance the two. In this paper, we propose a novel motion control\nframework composed of a Neural network-based path planner and a Model\nPredictive Control (MPC) controller based on control Barrier function (NMPCB) .\nThe planner predicts the next target point through a lightweight neural network\nand generates a reference trajectory for the controller. In the design of the\ncontroller, we introduce the dual problem of control barrier function (CBF) as\nthe obstacle avoidance constraint, enabling it to ensure robot motion safety\nwhile significantly reducing computation time. The controller directly outputs\ncontrol commands to the robot by tracking the reference trajectory. This\nframework achieves a balance between real-time performance and safety. We\nvalidate the feasibility of the framework through numerical simulations and\nreal-world experiments.\n","authors":["Longze Zheng","Qinghe Liu"],"pdf_url":"https://arxiv.org/pdf/2505.01752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02071v1","updated":"2025-09-02T08:20:34Z","published":"2025-09-02T08:20:34Z","title":"A Geometric Method for Base Parameter Analysis in Robot Inertia\n  Identification Based on Projective Geometric Algebra","summary":"  This paper proposes a novel geometric method for analytically determining the\nbase inertial parameters of robotic systems. The rigid body dynamics is\nreformulated using projective geometric algebra, leading to a new\nidentification model named ``tetrahedral-point (TP)\" model. Based on the rigid\nbody TP model, coefficients in the regresoor matrix of the identification model\nare derived in closed-form, exhibiting clear geometric interpretations.\nBuilding directly from the dynamic model, three foundational principles for\nbase parameter analysis are proposed: the shared points principle, fixed points\nprinciple, and planar rotations principle. With these principles, algorithms\nare developed to automatically determine all the base parameters. The core\nalgorithm, referred to as Dynamics Regressor Nullspace Generator (DRNG),\nachieves $O(1)$-complexity theoretically following an $O(N)$-complexity\npreprocessing stage, where $N$ is the number of rigid bodies. The proposed\nmethod and algorithms are validated across four robots: Puma560, Unitree Go2, a\n2RRU-1RRS parallel kinematics mechanism (PKM), and a 2PRS-1PSR PKM. In all\ncases, the algorithms successfully identify the complete set of base\nparameters. Notably, the approach demonstrates high robustness and\ncomputational efficiency, particularly in the cases of PKMs. Through the\ncomprehensive demonstrations, the method is shown to be general, robust, and\nefficient.\n","authors":["Guangzhen Sun","Ye Ding","Xiangyang Zhu"],"pdf_url":"https://arxiv.org/pdf/2509.02071v1.pdf","comment":"20 pages, 10 figures"},{"id":"http://arxiv.org/abs/2405.14314v3","updated":"2025-09-02T08:13:50Z","published":"2024-05-23T08:33:19Z","title":"Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration","summary":"  Grounding the reasoning ability of large language models (LLMs) for embodied\ntasks is challenging due to the complexity of the physical world. Especially,\nLLM planning for multi-agent collaboration requires communication of agents or\ncredit assignment as the feedback to re-adjust the proposed plans and achieve\neffective coordination. However, existing methods that overly rely on physical\nverification or self-reflection suffer from excessive and inefficient querying\nof LLMs. In this paper, we propose a novel framework for multi-agent\ncollaboration that introduces Reinforced Advantage feedback (ReAd) for\nefficient self-refinement of plans. Specifically, we perform critic regression\nto learn a sequential advantage function from LLM-planned data, and then treat\nthe LLM planner as an optimizer to generate actions that maximize the advantage\nfunction. It endows the LLM with the foresight to discern whether the action\ncontributes to accomplishing the final task. We provide theoretical analysis by\nextending advantage-weighted regression in reinforcement learning to\nmulti-agent systems. Experiments on Overcooked-AI and a difficult variant of\nRoCoBench show that ReAd surpasses baselines in success rate, and also\nsignificantly decreases the interaction steps of agents and query rounds of\nLLMs, demonstrating its high efficiency for grounding LLMs. More results are\ngiven at https://read-llm.github.io.\n","authors":["Yang Zhang","Shixin Yang","Chenjia Bai","Fei Wu","Xiu Li","Zhen Wang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2405.14314v3.pdf","comment":"accepted by ACL'2025"},{"id":"http://arxiv.org/abs/2506.02286v2","updated":"2025-09-02T07:57:49Z","published":"2025-06-02T21:57:53Z","title":"Efficient Manipulation-Enhanced Semantic Mapping With\n  Uncertainty-Informed Action Selection","summary":"  Service robots operating in cluttered human environments such as homes,\noffices, and schools cannot rely on predefined object arrangements and must\ncontinuously update their semantic and spatial estimates while dealing with\npossible frequent rearrangements. Efficient and accurate mapping under such\nconditions demands selecting informative viewpoints and targeted manipulations\nto reduce occlusions and uncertainty. In this work, we present a\nmanipulation-enhanced semantic mapping framework for occlusion-heavy shelf\nscenes that integrates evidential metric-semantic mapping with\nreinforcement-learning-based next-best view planning and targeted action\nselection. Our method thereby exploits uncertainty estimates from Dirichlet and\nBeta distributions in the map prediction networks to guide both active sensor\nplacement and object manipulation, focusing on areas with high uncertainty and\nselecting actions with high expected information gain. Furthermore, we\nintroduce an uncertainty-informed push strategy that targets occlusion-critical\nobjects and generates minimally invasive actions to reveal hidden regions by\nreducing overall uncertainty in the scene. The experimental evaluation shows\nthat our framework enables to accurately map cluttered scenes, while\nsubstantially reducing object displacement and achieving a 95% reduction in\nplanning time compared to the state-of-the-art, thereby realizing real-world\napplicability.\n","authors":["Nils Dengler","Jesper Mücke","Rohit Menon","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2506.02286v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02055v1","updated":"2025-09-02T07:51:59Z","published":"2025-09-02T07:51:59Z","title":"Align-Then-stEer: Adapting the Vision-Language Action Models through\n  Unified Latent Guidance","summary":"  Vision-Language-Action (VLA) models pre-trained on large, diverse datasets\nshow remarkable potential for general-purpose robotic manipulation. However, a\nprimary bottleneck remains in adapting these models to downstream tasks,\nespecially when the robot's embodiment or the task itself differs from the\npre-training data. This discrepancy leads to a significant mismatch in action\ndistributions, demanding extensive data and compute for effective fine-tuning.\nTo address this challenge, we introduce \\textbf{Align-Then-stEer\n(\\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation\nframework. \\texttt{ATE} first aligns disparate action spaces by constructing a\nunified latent space, where a variational autoencoder constrained by reverse KL\ndivergence embeds adaptation actions into modes of the pre-training action\nlatent distribution. Subsequently, it steers the diffusion- or flow-based VLA's\ngeneration process during fine-tuning via a guidance mechanism that pushes the\nmodel's output distribution towards the target domain. We conduct extensive\nexperiments on cross-embodiment and cross-task manipulation in both simulation\nand real world. Compared to direct fine-tuning of representative VLAs, our\nmethod improves the average multi-task success rate by up to \\textbf{9.8\\%} in\nsimulation and achieves a striking \\textbf{32\\% success rate gain} in a\nreal-world cross-embodiment setting. Our work presents a general and\nlightweight solution that greatly enhances the practicality of deploying VLA\nmodels to new robotic platforms and tasks.\n","authors":["Yang Zhang","Chenwei Wang","Ouyang Lu","Yuan Zhao","Yunfei Ge","Zhenglong Sun","Xiu Li","Chi Zhang","Chenjia Bai","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2509.02055v1.pdf","comment":"The first three authors contributed equally"},{"id":"http://arxiv.org/abs/2509.02011v1","updated":"2025-09-02T06:52:25Z","published":"2025-09-02T06:52:25Z","title":"Generalizing Unsupervised Lidar Odometry Model from Normal to Snowy\n  Weather Conditions","summary":"  Deep learning-based LiDAR odometry is crucial for autonomous driving and\nrobotic navigation, yet its performance under adverse weather, especially\nsnowfall, remains challenging. Existing models struggle to generalize across\nconditions due to sensitivity to snow-induced noise, limiting real-world use.\nIn this work, we present an unsupervised LiDAR odometry model to close the gap\nbetween clear and snowy weather conditions. Our approach focuses on effective\ndenoising to mitigate the impact of snowflake noise and outlier points on pose\nestimation, while also maintaining computational efficiency for real-time\napplications.\n  To achieve this, we introduce a Patch Spatial Measure (PSM) module that\nevaluates the dispersion of points within each patch, enabling effective\ndetection of sparse and discrete noise.\n  We further propose a Patch Point Weight Predictor (PPWP) to assign adaptive\npoint-wise weights, enhancing their discriminative capacity within local\nregions. To support real-time performance, we first apply an intensity\nthreshold mask to quickly suppress dense snowflake clusters near the LiDAR, and\nthen perform multi-modal feature fusion to refine the point-wise weight\nprediction, improving overall robustness under adverse weather. Our model is\ntrained in clear weather conditions and rigorously tested across various\nscenarios, including snowy and dynamic. Extensive experimental results confirm\nthe effectiveness of our method, demonstrating robust performance in both clear\nand snowy weather. This advancement enhances the model's generalizability and\npaves the way for more reliable autonomous systems capable of operating across\na wider range of environmental conditions.\n","authors":["Beibei Zhou","Zhiyuan Zhang","Zhenbo Song","Jianhui Guo","Hui Kong"],"pdf_url":"https://arxiv.org/pdf/2509.02011v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01996v1","updated":"2025-09-02T06:20:28Z","published":"2025-09-02T06:20:28Z","title":"MIRAGE: Multimodal Intention Recognition and Admittance-Guided\n  Enhancement in VR-based Multi-object Teleoperation","summary":"  Effective human-robot interaction (HRI) in multi-object teleoperation tasks\nfaces significant challenges due to perceptual ambiguities in virtual reality\n(VR) environments and the limitations of single-modality intention recognition.\nThis paper proposes a shared control framework that combines a virtual\nadmittance (VA) model with a Multimodal-CNN-based Human Intention Perception\nNetwork (MMIPN) to enhance teleoperation performance and user experience. The\nVA model employs artificial potential fields to guide operators toward target\nobjects by adjusting admittance force and optimizing motion trajectories. MMIPN\nprocesses multimodal inputs, including gaze movement, robot motions, and\nenvironmental context, to estimate human grasping intentions, helping to\novercome depth perception challenges in VR. Our user study evaluated four\nconditions across two factors, and the results showed that MMIPN significantly\nimproved grasp success rates, while the VA model enhanced movement efficiency\nby reducing path lengths. Gaze data emerged as the most crucial input modality.\nThese findings demonstrate the effectiveness of combining multimodal cues with\nimplicit guidance in VR-based teleoperation, providing a robust solution for\nmulti-object grasping tasks and enabling more natural interactions across\nvarious applications in the future.\n","authors":["Chi Sun","Xian Wang","Abhishek Kumar","Chengbin Cui","Lik-Hang Lee"],"pdf_url":"https://arxiv.org/pdf/2509.01996v1.pdf","comment":"Accepted by ISMAR 2025"},{"id":"http://arxiv.org/abs/2509.01985v1","updated":"2025-09-02T06:06:21Z","published":"2025-09-02T06:06:21Z","title":"Geometric Control of Mechanical Systems with Symmetries Based on Sliding\n  Modes","summary":"  In this paper, we propose a framework for designing sliding mode controllers\nfor a class of mechanical systems with symmetry, both unconstrained and\nconstrained, that evolve on principal fiber bundles. Control laws are developed\nbased on the reduced motion equations by exploring symmetries, leading to a\nsliding mode control strategy where the reaching stage is executed on the base\nspace, and the sliding stage is performed on the structure group. Thus, design\ncomplexity is reduced, and difficult choices for coordinate representations\nwhen working with a particular Lie group are avoided. For this purpose, a\nsliding subgroup is constructed on the structure group based on a kinematic\ncontroller, and the sliding variable will converge to the identity of the state\nmanifold upon reaching the sliding subgroup. A reaching law based on a general\nsliding vector field is then designed on the base space using the local form of\nthe mechanical connection to drive the sliding variable to the sliding\nsubgroup, and its time evolution is given according to the appropriate\ncovariant derivative. Almost global asymptotic stability and local exponential\nstability are demonstrated using a Lyapunov analysis. We apply the results to a\nfully actuated system (a rigid spacecraft actuated by reaction wheels) and a\nsubactuated nonholonomic system (unicycle mobile robot actuated by wheels),\nwhich is also simulated for illustration.\n","authors":["Eduardo Espindola","Yu Tang"],"pdf_url":"https://arxiv.org/pdf/2509.01985v1.pdf","comment":"32 pages, 3 figures, journal submission"},{"id":"http://arxiv.org/abs/2509.01980v1","updated":"2025-09-02T05:47:14Z","published":"2025-09-02T05:47:14Z","title":"Hybrid Autonomy Framework for a Future Mars Science Helicopter","summary":"  Autonomous aerial vehicles, such as NASA's Ingenuity, enable rapid planetary\nsurface exploration beyond the reach of ground-based robots. Thus, NASA is\nstudying a Mars Science Helicopter (MSH), an advanced concept capable of\nperforming long-range science missions and autonomously navigating challenging\nMartian terrain. Given significant Earth-Mars communication delays and mission\ncomplexity, an advanced autonomy framework is required to ensure safe and\nefficient operation by continuously adapting behavior based on mission\nobjectives and real-time conditions, without human intervention. This study\npresents a deterministic high-level control framework for aerial exploration,\nintegrating a Finite State Machine (FSM) with Behavior Trees (BTs) to achieve a\nscalable, robust, and computationally efficient autonomy solution for critical\nscenarios like deep space exploration. In this paper we outline key\ncapabilities of a possible MSH and detail the FSM-BT hybrid autonomy framework\nwhich orchestrates them to achieve the desired objectives. Monte Carlo\nsimulations and real field tests validate the framework, demonstrating its\nrobustness and adaptability to both discrete events and real-time system\nfeedback. These inputs trigger state transitions or dynamically adjust behavior\nexecution, enabling reactive and context-aware responses. The framework is\nmiddleware-agnostic, supporting integration with systems like F-Prime and\nextending beyond aerial robotics.\n","authors":["Luca Di Pierno","Robert Hewitt","Stephan Weiss","Roland Brockers"],"pdf_url":"https://arxiv.org/pdf/2509.01980v1.pdf","comment":"8 pages, IEEE CASE 2025 Conference"},{"id":"http://arxiv.org/abs/2505.20404v3","updated":"2025-09-02T05:43:33Z","published":"2025-05-26T18:00:09Z","title":"Co-Design of Soft Gripper with Neural Physics","summary":"  For robot manipulation, both the controller and end-effector design are\ncrucial. Soft grippers are generalizable by deforming to different geometries,\nbut designing such a gripper and finding its grasp pose remains challenging. In\nthis paper, we propose a co-design framework that generates an optimized soft\ngripper's block-wise stiffness distribution and its grasping pose, using a\nneural physics model trained in simulation. We derived a uniform-pressure\ntendon model for a flexure-based soft finger, then generated a diverse dataset\nby randomizing both gripper pose and design parameters. A neural network is\ntrained to approximate this forward simulation, yielding a fast, differentiable\nsurrogate. We embed that surrogate in an end-to-end optimization loop to\noptimize the ideal stiffness configuration and best grasp pose. Finally, we\n3D-print the optimized grippers of various stiffness by changing the structural\nparameters. We demonstrate that our co-designed grippers significantly\noutperform baseline designs in both simulation and hardware experiments. More\ninfo: http://yswhynot.github.io/codesign-soft/\n","authors":["Sha Yi","Xueqian Bai","Adabhav Singh","Jianglong Ye","Michael T Tolley","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2505.20404v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01968v1","updated":"2025-09-02T05:17:07Z","published":"2025-09-02T05:17:07Z","title":"Ensemble-Based Event Camera Place Recognition Under Varying Illumination","summary":"  Compared to conventional cameras, event cameras provide a high dynamic range\nand low latency, offering greater robustness to rapid motion and challenging\nlighting conditions. Although the potential of event cameras for visual place\nrecognition (VPR) has been established, developing robust VPR frameworks under\nsevere illumination changes remains an open research problem. In this paper, we\nintroduce an ensemble-based approach to event camera place recognition that\ncombines sequence-matched results from multiple event-to-frame reconstructions,\nVPR feature extractors, and temporal resolutions. Unlike previous event-based\nensemble methods, which only utilise temporal resolution, our broader fusion\nstrategy delivers significantly improved robustness under varied lighting\nconditions (e.g., afternoon, sunset, night), achieving a 57% relative\nimprovement in Recall@1 across day-night transitions. We evaluate our approach\non two long-term driving datasets (with 8 km per traverse) without metric\nsubsampling, thereby preserving natural variations in speed and stop duration\nthat influence event density. We also conduct a comprehensive analysis of key\ndesign choices, including binning strategies, polarity handling, reconstruction\nmethods, and feature extractors, to identify the most critical components for\nrobust performance. Additionally, we propose a modification to the standard\nsequence matching framework that enhances performance at longer sequence\nlengths. To facilitate future research, we will release our codebase and\nbenchmarking framework.\n","authors":["Therese Joseph","Tobias Fischer","Michael Milford"],"pdf_url":"https://arxiv.org/pdf/2509.01968v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.14589v2","updated":"2025-09-02T05:09:17Z","published":"2025-06-17T14:52:50Z","title":"NetRoller: Interfacing General and Specialized Models for End-to-End\n  Autonomous Driving","summary":"  Integrating General Models (GMs) such as Large Language Models (LLMs), with\nSpecialized Models (SMs) in autonomous driving tasks presents a promising\napproach to mitigating challenges in data diversity and model capacity of\nexisting specialized driving models. However, this integration leads to\nproblems of asynchronous systems, which arise from the distinct characteristics\ninherent in GMs and SMs. To tackle this challenge, we propose NetRoller, an\nadapter that incorporates a set of novel mechanisms to facilitate the seamless\nintegration of GMs and specialized driving models. Specifically, our mechanisms\nfor interfacing the asynchronous GMs and SMs are organized into three key\nstages. NetRoller first harvests semantically rich and computationally\nefficient representations from the reasoning processes of LLMs using an early\nstopping mechanism, which preserves critical insights on driving context while\nmaintaining low overhead. It then applies learnable query embeddings,\nnonsensical embeddings, and positional layer embeddings to facilitate robust\nand efficient cross-modality translation. At last, it employs computationally\nefficient Query Shift and Feature Shift mechanisms to enhance the performance\nof SMs through few-epoch fine-tuning. Based on the mechanisms formalized in\nthese three stages, NetRoller enables specialized driving models to operate at\ntheir native frequencies while maintaining situational awareness of the GM.\nExperiments conducted on the nuScenes dataset demonstrate that integrating GM\nthrough NetRoller significantly improves human similarity and safety in\nplanning tasks, and it also achieves noticeable precision improvements in\ndetection and mapping tasks for end-to-end autonomous driving. The code and\nmodels are available at https://github.com/Rex-sys-hk/NetRoller .\n","authors":["Ren Xin","Hongji Liu","Xiaodong Mei","Wenru Liu","Maosheng Ye","Zhili Chen","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2506.14589v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2509.01952v1","updated":"2025-09-02T04:47:21Z","published":"2025-09-02T04:47:21Z","title":"Robustness Enhancement for Multi-Quadrotor Centralized Transportation\n  System via Online Tuning and Learning","summary":"  This paper introduces an adaptive-neuro geometric control for a centralized\nmulti-quadrotor cooperative transportation system, which enhances both\nadaptivity and disturbance rejection. Our strategy is to coactively tune the\nmodel parameters and learn the external disturbances in real-time. To realize\nthis, we augmented the existing geometric control with multiple neural networks\nand adaptive laws, where the estimated model parameters and the weights of the\nneural networks are simultaneously tuned and adjusted online. The\nLyapunov-based adaptation guarantees bounded estimation errors without\nrequiring either pre-training or the persistent excitation (PE) condition. The\nproposed control system has been proven to be stable in the sense of Lyapunov\nunder certain preconditions, and its enhanced robustness under scenarios of\ndisturbed environment and model-unmatched plant was demonstrated by numerical\nsimulations.\n","authors":["Tianhua Gao","Kohji Tomita","Akiya Kamimura"],"pdf_url":"https://arxiv.org/pdf/2509.01952v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01951v1","updated":"2025-09-02T04:45:35Z","published":"2025-09-02T04:45:35Z","title":"Online Identification using Adaptive Laws and Neural Networks for\n  Multi-Quadrotor Centralized Transportation System","summary":"  This paper introduces an adaptive-neuro identification method that enhances\nthe robustness of a centralized multi-quadrotor transportation system. This\nmethod leverages online tuning and learning on decomposed error subspaces,\nenabling efficient real-time compensation to time-varying disturbances and\nmodel uncertainties acting on the payload. The strategy is to decompose the\nhigh-dimensional error space into a set of low-dimensional subspaces. In this\nway, the identification problem for unseen features is naturally transformed\ninto submappings (``slices'') addressed by multiple adaptive laws and shallow\nneural networks, which are updated online via Lyapunov-based adaptation without\nrequiring persistent excitation (PE) and offline training. Due to the\nmodel-free nature of neural networks, this approach can be well adapted to\nhighly coupled and nonlinear centralized transportation systems. It serves as a\nfeedforward compensator for the payload controller without explicitly relying\non the dynamics coupled with the payload, such as cables and quadrotors. The\nproposed control system has been proven to be stable in the sense of Lyapunov,\nand its enhanced robustness under time-varying disturbances and model\nuncertainties was demonstrated by numerical simulations.\n","authors":["Tianhua Gao","Kohji Tomita","Akiya Kamimura"],"pdf_url":"https://arxiv.org/pdf/2509.01951v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01944v1","updated":"2025-09-02T04:32:24Z","published":"2025-09-02T04:32:24Z","title":"AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity\n  for VLA Model in Autonomous Driving","summary":"  Vision-Language-Action (VLA) models in autonomous driving systems have\nrecently demonstrated transformative potential by integrating multimodal\nperception with decision-making capabilities. However, the interpretability and\ncoherence of the decision process and the plausibility of action sequences\nremain largely underexplored. To address these issues, we propose\nAutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and\nself-reflection capabilities of autonomous driving systems through\nchain-of-thought (CoT) processing and reinforcement learning (RL).\nSpecifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K\nfor supervised fine-tuning, which effectively builds cognitive bridges between\ninput information and output trajectories through a four-step logical chain\nwith self-reflection for validation. Moreover, to maximize both reasoning and\nself-reflection during the RL stage, we further employ the Group Relative\nPolicy Optimization (GRPO) algorithm within a physics-grounded reward framework\nthat incorporates spatial alignment, vehicle dynamic, and temporal smoothness\ncriteria to ensure reliable and realistic trajectory planning. Extensive\nevaluation results across both nuScenes and Waymo datasets demonstrates the\nstate-of-the-art performance and robust generalization capacity of our proposed\nmethod.\n","authors":["Zhenlong Yuan","Jing Tang","Jinguo Luo","Rui Chen","Chengxuan Qian","Lei Sun","Xiangxiang Chu","Yujun Cai","Dapeng Zhang","Shuo Li"],"pdf_url":"https://arxiv.org/pdf/2509.01944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01878v1","updated":"2025-09-02T01:51:31Z","published":"2025-09-02T01:51:31Z","title":"AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and\n  Ecosystem Monitoring","summary":"  Marine ecosystems face increasing pressure due to climate change, driving the\nneed for scalable, AI-powered monitoring solutions. This paper examines the\nrapid emergence of underwater AI as a major research frontier and analyzes the\nfactors that have transformed marine perception from a niche application into a\ncatalyst for AI innovation. We identify three convergent drivers: environmental\nnecessity for ecosystem-scale monitoring, democratization of underwater\ndatasets through citizen science platforms, and researcher migration from\nsaturated terrestrial computer vision domains. Our analysis reveals how unique\nunderwater challenges - turbidity, cryptic species detection, expert annotation\nbottlenecks, and cross-ecosystem generalization - are driving fundamental\nadvances in weakly supervised learning, open-set recognition, and robust\nperception under degraded conditions. We survey emerging trends in datasets,\nscene understanding and 3D reconstruction, highlighting the paradigm shift from\npassive observation toward AI-driven, targeted intervention capabilities. The\npaper demonstrates how underwater constraints are pushing the boundaries of\nfoundation models, self-supervised learning, and perception, with\nmethodological innovations that extend far beyond marine applications to\nbenefit general computer vision, robotics, and environmental monitoring.\n","authors":["Scarlett Raine","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2509.01878v1.pdf","comment":"9 pages, 3 figures"}]},"2025-09-01T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2507.16124v2","updated":"2025-09-01T23:52:31Z","published":"2025-07-22T00:36:59Z","title":"Benchmarking LLM Privacy Recognition for Social Robot Decision Making","summary":"  While robots have previously utilized rule-based systems or probabilistic\nmodels for user interaction, the rapid evolution of large language models\n(LLMs) presents new opportunities to develop LLM-powered robots for enhanced\nhuman-robot interaction (HRI). To fully realize these capabilities, however,\nrobots need to collect data such as audio, fine-grained images, video, and\nlocations. As a result, LLMs often process sensitive personal information,\nparticularly within private environments, such as homes. Given the tension\nbetween utility and privacy risks, evaluating how current LLMs manage sensitive\ndata is critical. Specifically, we aim to explore the extent to which\nout-of-the-box LLMs are privacy-aware in the context of household robots. In\nthis work, we present a set of privacy-relevant scenarios developed using the\nContextual Integrity (CI) framework. We first surveyed users' privacy\npreferences regarding in-home robot behaviors and then examined how their\nprivacy orientations affected their choices of these behaviors (N = 450). We\nthen provided the same set of scenarios and questions to state-of-the-art LLMs\n(N = 10) and found that the agreement between humans and LLMs was generally\nlow. To further investigate the capabilities of LLMs as potential privacy\ncontrollers, we implemented four additional prompting strategies and compared\ntheir results. We discuss the performance of the evaluated models as well as\nthe implications and potential of AI privacy awareness in human-robot\ninteraction.\n","authors":["Dakota Sullivan","Shirley Zhang","Jennica Li","Heather Kirkorian","Bilge Mutlu","Kassem Fawaz"],"pdf_url":"https://arxiv.org/pdf/2507.16124v2.pdf","comment":"18 pages, 7 figures. Dakota Sullivan and Shirley Zhang contributed\n  equally to this work"},{"id":"http://arxiv.org/abs/2509.01836v1","updated":"2025-09-01T23:38:01Z","published":"2025-09-01T23:38:01Z","title":"Multi-vessel Interaction-Aware Trajectory Prediction and Collision Risk\n  Assessment","summary":"  Accurate vessel trajectory prediction is essential for enhancing situational\nawareness and preventing collisions. Still, existing data-driven models are\nconstrained mainly to single-vessel forecasting, overlooking vessel\ninteractions, navigation rules, and explicit collision risk assessment. We\npresent a transformer-based framework for multi-vessel trajectory prediction\nwith integrated collision risk analysis. For a given target vessel, the\nframework identifies nearby vessels. It jointly predicts their future\ntrajectories through parallel streams encoding kinematic and derived physical\nfeatures, causal convolutions for temporal locality, spatial transformations\nfor positional encoding, and hybrid positional embeddings that capture both\nlocal motion patterns and long-range dependencies. Evaluated on large-scale\nreal-world AIS data using joint multi-vessel metrics, the model demonstrates\nsuperior forecasting capabilities beyond traditional single-vessel displacement\nerrors. By simulating interactions among predicted trajectories, the framework\nfurther quantifies potential collision risks, offering actionable insights to\nstrengthen maritime safety and decision support.\n","authors":["Md Mahbub Alam","Jose F. Rodrigues-Jr","Gabriel Spadon"],"pdf_url":"https://arxiv.org/pdf/2509.01836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20396v2","updated":"2025-09-01T23:08:08Z","published":"2025-02-27T18:59:52Z","title":"Sim-to-Real Reinforcement Learning for Vision-Based Dexterous\n  Manipulation on Humanoids","summary":"  Learning generalizable robot manipulation policies, especially for complex\nmulti-fingered humanoids, remains a significant challenge. Existing approaches\nprimarily rely on extensive data collection and imitation learning, which are\nexpensive, labor-intensive, and difficult to scale. Sim-to-real reinforcement\nlearning (RL) offers a promising alternative, but has mostly succeeded in\nsimpler state-based or single-hand setups. How to effectively extend this to\nvision-based, contact-rich bimanual manipulation tasks remains an open\nquestion. In this paper, we introduce a practical sim-to-real RL recipe that\ntrains a humanoid robot to perform three challenging dexterous manipulation\ntasks: grasp-and-reach, box lift and bimanual handover. Our method features an\nautomated real-to-sim tuning module, a generalized reward formulation based on\ncontact and object goals, a divide-and-conquer policy distillation framework,\nand a hybrid object representation strategy with modality-specific\naugmentation. We demonstrate high success rates on unseen objects and robust,\nadaptive policy behaviors -- highlighting that vision-based dexterous\nmanipulation via sim-to-real RL is not only viable, but also scalable and\nbroadly applicable to real-world humanoid manipulation tasks.\n","authors":["Toru Lin","Kartik Sachdev","Linxi Fan","Jitendra Malik","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2502.20396v2.pdf","comment":"Published at CoRL 2025. Project page can be found at\n  https://toruowo.github.io/recipe/"},{"id":"http://arxiv.org/abs/2509.01820v1","updated":"2025-09-01T22:50:56Z","published":"2025-09-01T22:50:56Z","title":"Nonlinear Model Predictive Control-Based Reverse Path-Planning and\n  Path-Tracking Control of a Vehicle with Trailer System","summary":"  Reverse parking maneuvers of a vehicle with trailer system is a challenging\ntask to complete for human drivers due to the unstable nature of the system and\nunintuitive controls required to orientate the trailer properly. This paper\nhence proposes an optimization-based automation routine to handle the\npath-planning and path-tracking control process of such type of maneuvers. The\nproposed approach utilizes nonlinear model predictive control (NMPC) to\nrobustly guide the vehicle-trailer system into the desired parking space, and\nan optional forward repositioning maneuver can be added as an additional stage\nof the parking process to obtain better system configurations, before backward\nmotion can be attempted again to get a good final pose. The novelty of the\nproposed approach is the simplicity of its formulation, as the path-planning\nand path-tracking operations are only conducted on the trailer being viewed as\na standalone vehicle, before the control inputs are propagated to the tractor\nvehicle via inverse kinematic relationships also derived in this paper.\nSimulation case studies and hardware-in-the-loop tests are performed, and the\nresults demonstrate the efficacy of the proposed approach.\n","authors":["Xincheng Cao","Haochong Chen","Bilin Aksun-Guvenc","Levent Guvenc","Brian Link","Peter J Richmond","Dokyung Yim","Shihong Fan","John Harber"],"pdf_url":"https://arxiv.org/pdf/2509.01820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01819v1","updated":"2025-09-01T22:50:55Z","published":"2025-09-01T22:50:55Z","title":"ManiFlow: A General Robot Manipulation Policy via Consistency Flow\n  Training","summary":"  This paper introduces ManiFlow, a visuomotor imitation learning policy for\ngeneral robot manipulation that generates precise, high-dimensional actions\nconditioned on diverse visual, language and proprioceptive inputs. We leverage\nflow matching with consistency training to enable high-quality dexterous action\ngeneration in just 1-2 inference steps. To handle diverse input modalities\nefficiently, we propose DiT-X, a diffusion transformer architecture with\nadaptive cross-attention and AdaLN-Zero conditioning that enables fine-grained\nfeature interactions between action tokens and multi-modal observations.\nManiFlow demonstrates consistent improvements across diverse simulation\nbenchmarks and nearly doubles success rates on real-world tasks across\nsingle-arm, bimanual, and humanoid robot setups with increasing dexterity. The\nextensive evaluation further demonstrates the strong robustness and\ngeneralizability of ManiFlow to novel objects and background changes, and\nhighlights its strong scaling capability with larger-scale datasets. Our\nwebsite: maniflow-policy.github.io.\n","authors":["Ge Yan","Jiyue Zhu","Yuquan Deng","Shiqi Yang","Ri-Zhao Qiu","Xuxin Cheng","Marius Memmel","Ranjay Krishna","Ankit Goyal","Xiaolong Wang","Dieter Fox"],"pdf_url":"https://arxiv.org/pdf/2509.01819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01786v1","updated":"2025-09-01T21:32:30Z","published":"2025-09-01T21:32:30Z","title":"EgoTouch: On-Body Touch Input Using AR/VR Headset Cameras","summary":"  In augmented and virtual reality (AR/VR) experiences, a user's arms and hands\ncan provide a convenient and tactile surface for touch input. Prior work has\nshown on-body input to have significant speed, accuracy, and ergonomic benefits\nover in-air interfaces, which are common today. In this work, we demonstrate\nhigh accuracy, bare hands (i.e., no special instrumentation of the user) skin\ninput using just an RGB camera, like those already integrated into all modern\nXR headsets. Our results show this approach can be accurate, and robust across\ndiverse lighting conditions, skin tones, and body motion (e.g., input while\nwalking). Finally, our pipeline also provides rich input metadata including\ntouch force, finger identification, angle of attack, and rotation. We believe\nthese are the requisite technical ingredients to more fully unlock on-skin\ninterfaces that have been well motivated in the HCI literature but have lacked\nrobust and practical methods.\n","authors":["Vimal Mollyn","Chris Harrison"],"pdf_url":"https://arxiv.org/pdf/2509.01786v1.pdf","comment":"Published at UIST 2024. More info at\n  https://www.figlab.com/research/2024/egotouch"},{"id":"http://arxiv.org/abs/2509.01765v1","updated":"2025-09-01T20:52:41Z","published":"2025-09-01T20:52:41Z","title":"Non-conflicting Energy Minimization in Reinforcement Learning based\n  Robot Control","summary":"  Efficient robot control often requires balancing task performance with energy\nexpenditure. A common approach in reinforcement learning (RL) is to penalize\nenergy use directly as part of the reward function. This requires carefully\ntuning weight terms to avoid undesirable trade-offs where energy minimization\nharms task success. In this work, we propose a hyperparameter-free gradient\noptimization method to minimize energy expenditure without conflicting with\ntask performance. Inspired by recent works in multitask learning, our method\napplies policy gradient projection between task and energy objectives to derive\npolicy updates that minimize energy expenditure in ways that do not impact task\nperformance. We evaluate this technique on standard locomotion benchmarks of\nDM-Control and HumanoidBench and demonstrate a reduction of 64% energy usage\nwhile maintaining comparable task performance. Further, we conduct experiments\non a Unitree GO2 quadruped showcasing Sim2Real transfer of energy efficient\npolicies. Our method is easy to implement in standard RL pipelines with minimal\ncode changes, is applicable to any policy gradient method, and offers a\nprincipled alternative to reward shaping for energy efficient control policies.\n","authors":["Skand Peri","Akhil Perincherry","Bikram Pandit","Stefan Lee"],"pdf_url":"https://arxiv.org/pdf/2509.01765v1.pdf","comment":"17 pages, 6 figures. Accepted as Oral presentation at Conference on\n  Robot Learning (CoRL) 2025"},{"id":"http://arxiv.org/abs/2412.02176v2","updated":"2025-09-01T20:13:35Z","published":"2024-12-03T05:20:29Z","title":"Self-Supervised Learning-Based Path Planning and Obstacle Avoidance\n  Using PPO and B-Splines in Unknown Environments","summary":"  This paper introduces SmartBSP, an advanced self-supervised learning\nframework for real-time path planning and obstacle avoidance in autonomous\nrobotics navigating through complex environments. The proposed system\nintegrates Proximal Policy Optimization (PPO) with Convolutional Neural\nNetworks (CNN) and Actor-Critic architecture to process limited LIDAR inputs\nand compute spatial decision-making probabilities. The robot's perceptual field\nis discretized into a grid format, which the CNN analyzes to produce a spatial\nprobability distribution. During the training process a nuanced cost function\nis minimized that accounts for path curvature, endpoint proximity, and obstacle\navoidance. Simulations results in different scenarios validate the algorithm's\nresilience and adaptability across diverse operational scenarios. Subsequently,\nReal-time experiments, employing the Robot Operating System (ROS), were carried\nout to assess the efficacy of the proposed algorithm.\n","authors":["Shahab Shokouhi","Oguzhan Oruc","May-Win Thein"],"pdf_url":"https://arxiv.org/pdf/2412.02176v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01746v1","updated":"2025-09-01T20:00:56Z","published":"2025-09-01T20:00:56Z","title":"Fail2Progress: Learning from Real-World Robot Failures with Stein\n  Variational Inference","summary":"  Skill effect models for long-horizon manipulation tasks are prone to failures\nin conditions not covered by training data distributions. Therefore, enabling\nrobots to reason about and learn from failures is necessary. We investigate the\nproblem of efficiently generating a dataset targeted to observed failures.\nAfter fine-tuning a skill effect model on this dataset, we evaluate the extent\nto which the model can recover from failures and minimize future failures. We\npropose Fail2Progress, an approach that leverages Stein variational inference\nto generate multiple simulation environments in parallel, enabling efficient\ndata sample generation similar to observed failures. Our method is capable of\nhandling several challenging mobile manipulation tasks, including transporting\nmultiple objects, organizing a constrained shelf, and tabletop organization.\nThrough large-scale simulation and real-world experiments, we demonstrate that\nour approach excels at learning from failures across different numbers of\nobjects. Furthermore, we show that Fail2Progress outperforms several baselines.\n","authors":["Yixuan Huang","Novella Alvina","Mohanraj Devendran Shanthi","Tucker Hermans"],"pdf_url":"https://arxiv.org/pdf/2509.01746v1.pdf","comment":"Project page: sites.google.com/view/fail2progress. 25 pages, 8\n  figures. Accepted to the Conference on Robot Learning (CoRL) 2025"},{"id":"http://arxiv.org/abs/2509.01728v1","updated":"2025-09-01T19:17:40Z","published":"2025-09-01T19:17:40Z","title":"Constrained Decoding for Robotics Foundation Models","summary":"  Recent advances in the development of robotic foundation models have led to\npromising end-to-end and general-purpose capabilities in robotic systems. These\nmodels are pretrained on vast datasets of robot trajectories to process\nmulti-modal inputs and directly output a sequence of action that the system\nthen executes in the real world. Although this approach is attractive from the\nperspective of improved generalization across diverse tasks, these models are\nstill data-driven and, therefore, lack explicit notions of behavioral\ncorrectness and safety constraints. We address these limitations by introducing\na constrained decoding framework for robotics foundation models that enforces\nlogical constraints on action trajectories in dynamical systems. Our method\nensures that generated actions provably satisfy signal temporal logic (STL)\nspecifications at runtime without retraining, while remaining agnostic of the\nunderlying foundation model. We perform comprehensive evaluation of our\napproach across state-of-the-art navigation foundation models and we show that\nour decoding-time interventions are useful not only for filtering unsafe\nactions but also for conditional action-generation. Videos available on our\nwebsite: https://constrained-robot-fms.github.io\n","authors":["Parv Kapoor","Akila Ganlath","Changliu Liu","Sebastian Scherer","Eunsuk Kang"],"pdf_url":"https://arxiv.org/pdf/2509.01728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.05287v2","updated":"2025-09-01T18:49:59Z","published":"2025-05-08T14:29:00Z","title":"Morphologically Symmetric Reinforcement Learning for Ambidextrous\n  Bimanual Manipulation","summary":"  Humans naturally exhibit bilateral symmetry in their gross manipulation\nskills, effortlessly mirroring simple actions between left and right hands.\nBimanual robots-which also feature bilateral symmetry-should similarly exploit\nthis property to perform tasks with either hand. Unlike humans, who often favor\na dominant hand for fine dexterous skills, robots should ideally execute\nambidextrous manipulation with equal proficiency. To this end, we introduce\nSYMDEX (SYMmetric DEXterity), a reinforcement learning framework for\nambidextrous bi-manipulation that leverages the robot's inherent bilateral\nsymmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation\ntasks into per-hand subtasks and trains dedicated policies for each. By\nexploiting bilateral symmetry via equivariant neural networks, experience from\none arm is inherently leveraged by the opposite arm. We then distill the\nsubtask policies into a global ambidextrous policy that is independent of the\nhand-task assignment. We evaluate SYMDEX on six challenging simulated\nmanipulation tasks and demonstrate successful real-world deployment on two of\nthem. Our approach strongly outperforms baselines on complex task in which the\nleft and right hands perform different roles. We further demonstrate SYMDEX's\nscalability by extending it to a four-arm manipulation setup, where our\nsymmetry-aware policies enable effective multi-arm collaboration and\ncoordination. Our results highlight how structural symmetry as inductive bias\nin policy learning enhances sample efficiency, robustness, and generalization\nacross diverse dexterous manipulation tasks.\n","authors":["Zechu Li","Yufeng Jin","Daniel Ordonez Apraez","Claudio Semini","Puze Liu","Georgia Chalvatzaki"],"pdf_url":"https://arxiv.org/pdf/2505.05287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01708v1","updated":"2025-09-01T18:34:17Z","published":"2025-09-01T18:34:17Z","title":"Articulated Object Estimation in the Wild","summary":"  Understanding the 3D motion of articulated objects is essential in robotic\nscene understanding, mobile manipulation, and motion planning. Prior methods\nfor articulation estimation have primarily focused on controlled settings,\nassuming either fixed camera viewpoints or direct observations of various\nobject states, which tend to fail in more realistic unconstrained environments.\nIn contrast, humans effortlessly infer articulation by watching others\nmanipulate objects. Inspired by this, we introduce ArtiPoint, a novel\nestimation framework that can infer articulated object models under dynamic\ncamera motion and partial observability. By combining deep point tracking with\na factor graph optimization framework, ArtiPoint robustly estimates articulated\npart trajectories and articulation axes directly from raw RGB-D videos. To\nfoster future research in this domain, we introduce Arti4D, the first\nego-centric in-the-wild dataset that captures articulated object interactions\nat a scene level, accompanied by articulation labels and ground-truth camera\nposes. We benchmark ArtiPoint against a range of classical and learning-based\nbaselines, demonstrating its superior performance on Arti4D. We make code and\nArti4D publicly available at https://artipoint.cs.uni-freiburg.de.\n","authors":["Abdelrhman Werby","Martin Büchner","Adrian Röfer","Chenguang Huang","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2509.01708v1.pdf","comment":"9th Conference on Robot Learning (CoRL), 2025"},{"id":"http://arxiv.org/abs/2403.17231v2","updated":"2025-09-01T18:02:00Z","published":"2024-03-25T22:17:51Z","title":"Dyna-LfLH: Learning Agile Navigation in Dynamic Environments from\n  Learned Hallucination","summary":"  This paper introduces Dynamic Learning from Learned Hallucination\n(Dyna-LfLH), a self-supervised method for training motion planners to navigate\nenvironments with dense and dynamic obstacles. Classical planners struggle with\ndense, unpredictable obstacles due to limited computation, while learning-based\nplanners face challenges in acquiring high-quality demonstrations for imitation\nlearning or dealing with exploration inefficiencies in reinforcement learning.\nBuilding on Learning from Hallucination (LfH), which synthesizes training data\nfrom past successful navigation experiences in simpler environments, Dyna-LfLH\nincorporates dynamic obstacles by generating them through a learned latent\ndistribution. This enables efficient and safe motion planner training. We\nevaluate Dyna-LfLH on a ground robot in both simulated and real environments,\nachieving up to a 25% improvement in success rate compared to baselines.\n","authors":["Saad Abdul Ghani","Zizhao Wang","Peter Stone","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2403.17231v2.pdf","comment":"Accepted at International Conference on Intelligent Robots and\n  Systems (IROS) 2025 Hangzhou, China"},{"id":"http://arxiv.org/abs/2509.01658v1","updated":"2025-09-01T17:59:03Z","published":"2025-09-01T17:59:03Z","title":"MoTo: A Zero-shot Plug-in Interaction-aware Navigation for General\n  Mobile Manipulation","summary":"  Mobile manipulation stands as a core challenge in robotics, enabling robots\nto assist humans across varied tasks and dynamic daily environments.\nConventional mobile manipulation approaches often struggle to generalize across\ndifferent tasks and environments due to the lack of large-scale training.\nHowever, recent advances in manipulation foundation models demonstrate\nimpressive generalization capability on a wide range of fixed-base manipulation\ntasks, which are still limited to a fixed setting. Therefore, we devise a\nplug-in module named MoTo, which can be combined with any off-the-shelf\nmanipulation foundation model to empower them with mobile manipulation ability.\nSpecifically, we propose an interaction-aware navigation policy to generate\nrobot docking points for generalized mobile manipulation. To enable zero-shot\nability, we propose an interaction keypoints framework via vision-language\nmodels (VLM) under multi-view consistency for both target object and robotic\narm following instructions, where fixed-base manipulation foundation models can\nbe employed. We further propose motion planning objectives for the mobile base\nand robot arm, which minimize the distance between the two keypoints and\nmaintain the physical feasibility of trajectories. In this way, MoTo guides the\nrobot to move to the docking points where fixed-base manipulation can be\nsuccessfully performed, and leverages VLM generation and trajectory\noptimization to achieve mobile manipulation in a zero-shot manner, without any\nrequirement on mobile manipulation expert data. Extensive experimental results\non OVMM and real-world demonstrate that MoTo achieves success rates of 2.68%\nand 16.67% higher than the state-of-the-art mobile manipulation methods,\nrespectively, without requiring additional training data.\n","authors":["Zhenyu Wu","Angyuan Ma","Xiuwei Xu","Hang Yin","Yinan Liang","Ziwei Wang","Jiwen Lu","Haibin Yan"],"pdf_url":"https://arxiv.org/pdf/2509.01658v1.pdf","comment":"Accepted to CoRL 2025. Project Page: https://gary3410.github.io/MoTo/"},{"id":"http://arxiv.org/abs/2509.01657v1","updated":"2025-09-01T17:58:41Z","published":"2025-09-01T17:58:41Z","title":"Data Retrieval with Importance Weights for Few-Shot Imitation Learning","summary":"  While large-scale robot datasets have propelled recent progress in imitation\nlearning, learning from smaller task specific datasets remains critical for\ndeployment in new environments and unseen tasks. One such approach to few-shot\nimitation learning is retrieval-based imitation learning, which extracts\nrelevant samples from large, widely available prior datasets to augment a\nlimited demonstration dataset. To determine the relevant data from prior\ndatasets, retrieval-based approaches most commonly calculate a prior data\npoint's minimum distance to a point in the target dataset in latent space.\nWhile retrieval-based methods have shown success using this metric for data\nselection, we demonstrate its equivalence to the limit of a Gaussian kernel\ndensity (KDE) estimate of the target data distribution. This reveals two\nshortcomings of the retrieval rule used in prior work. First, it relies on\nhigh-variance nearest neighbor estimates that are susceptible to noise. Second,\nit does not account for the distribution of prior data when retrieving data. To\naddress these issues, we introduce Importance Weighted Retrieval (IWR), which\nestimates importance weights, or the ratio between the target and prior data\ndistributions for retrieval, using Gaussian KDEs. By considering the\nprobability ratio, IWR seeks to mitigate the bias of previous selection rules,\nand by using reasonable modeling parameters, IWR effectively smooths estimates\nusing all data points. Across both simulation environments and real-world\nevaluations on the Bridge dataset we find that our method, IWR, consistently\nimproves performance of existing retrieval-based methods, despite only\nrequiring minor modifications.\n","authors":["Amber Xie","Rahul Chand","Dorsa Sadigh","Joey Hejna"],"pdf_url":"https://arxiv.org/pdf/2509.01657v1.pdf","comment":"Conference on Robot Learning 2025"},{"id":"http://arxiv.org/abs/2509.01643v1","updated":"2025-09-01T17:36:43Z","published":"2025-09-01T17:36:43Z","title":"Speculative Design of Equitable Robotics: Queer Fictions and Futures","summary":"  This paper examines the speculative topic of equitable robots through an\nexploratory essay format. It focuses specifically on robots by and for LGBTQ+\npopulations. It aims to provoke thought and conversations in the field about\nwhat aspirational queer robotics futures may look like, both in the arts and\nsciences. First, it briefly reviews the state-of-the-art of queer robotics in\nfiction and science, drawing together threads from each. Then, it discusses\nqueering robots through three speculative design proposals for queer robot\nroles: 1) reflecting the queerness of their ''in-group'' queer users, building\nand celebrating ''in-group'' identity, 2) a new kind of queer activism by\nimplementing queer robot identity performance to interact with ''out-group''\nusers, with a goal of reducing bigotry through familiarisation, and 3) a\nnetwork of queer-owned robots, through which the community could reach each\nother, and distribute and access important resources. The paper then questions\nwhether robots should be queered, and what ethical implications this raises.\nFinally, the paper makes suggestions for what aspirational queer robotics\nfutures may look like, and what would be required to get there.\n","authors":["Minja Axelsson"],"pdf_url":"https://arxiv.org/pdf/2509.01643v1.pdf","comment":"Accepted at the British Computer Society's Special Interest Group in\n  Human Computer Interaction Conference (BCS HCI 2025), Futures track. 5 pages,\n  no figures"},{"id":"http://arxiv.org/abs/2509.01630v1","updated":"2025-09-01T17:17:05Z","published":"2025-09-01T17:17:05Z","title":"Learning to Coordinate: Distributed Meta-Trajectory Optimization Via\n  Differentiable ADMM-DDP","summary":"  Distributed trajectory optimization via ADMM-DDP is a powerful approach for\ncoordinating multi-agent systems, but it requires extensive tuning of tightly\ncoupled hyperparameters that jointly govern local task performance and global\ncoordination. In this paper, we propose Learning to Coordinate (L2C), a general\nframework that meta-learns these hyperparameters, modeled by lightweight\nagent-wise neural networks, to adapt across diverse tasks and agent\nconfigurations. L2C differentiates end-to-end through the ADMM-DDP pipeline in\na distributed manner. It also enables efficient meta-gradient computation by\nreusing DDP components such as Riccati recursions and feedback gains. These\ngradients correspond to the optimal solutions of distributed matrix-valued LQR\nproblems, coordinated across agents via an auxiliary ADMM framework that\nbecomes convex under mild assumptions. Training is further accelerated by\ntruncating iterations and meta-learning ADMM penalty parameters optimized for\nrapid residual reduction, with provable Lipschitz-bounded gradient errors. On a\nchallenging cooperative aerial transport task, L2C generates dynamically\nfeasible trajectories in high-fidelity simulation using IsaacSIM, reconfigures\nquadrotor formations for safe 6-DoF load manipulation in tight spaces, and\nadapts robustly to varying team sizes and task conditions, while achieving up\nto $88\\%$ faster gradient computation than state-of-the-art methods.\n","authors":["Bingheng Wang","Yichao Gao","Tianchen Sun","Lin Zhao"],"pdf_url":"https://arxiv.org/pdf/2509.01630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01611v1","updated":"2025-09-01T16:43:50Z","published":"2025-09-01T16:43:50Z","title":"A Hybrid Input based Deep Reinforcement Learning for Lane Change\n  Decision-Making of Autonomous Vehicle","summary":"  Lane change decision-making for autonomous vehicles is a complex but\nhigh-reward behavior. In this paper, we propose a hybrid input based deep\nreinforcement learning (DRL) algorithm, which realizes abstract lane change\ndecisions and lane change actions for autonomous vehicles within traffic flow.\nFirstly, a surrounding vehicles trajectory prediction method is proposed to\nreduce the risk of future behavior of surrounding vehicles to ego vehicle, and\nthe prediction results are input into the reinforcement learning model as\nadditional information. Secondly, to comprehensively leverage environmental\ninformation, the model extracts feature from high-dimensional images and\nlow-dimensional sensor data simultaneously. The fusion of surrounding vehicle\ntrajectory prediction and multi-modal information are used as state space of\nreinforcement learning to improve the rationality of lane change decision.\nFinally, we integrate reinforcement learning macro decisions with end-to-end\nvehicle control to achieve a holistic lane change process. Experiments were\nconducted within the CARLA simulator, and the results demonstrated that the\nutilization of a hybrid state space significantly enhances the safety of\nvehicle lane change decisions.\n","authors":["Ziteng Gao","Jiaqi Qu","Chaoyu Chen"],"pdf_url":"https://arxiv.org/pdf/2509.01611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01605v1","updated":"2025-09-01T16:36:23Z","published":"2025-09-01T16:36:23Z","title":"TransForSeg: A Multitask Stereo ViT for Joint Stereo Segmentation and 3D\n  Force Estimation in Catheterization","summary":"  Recently, the emergence of multitask deep learning models has enhanced\ncatheterization procedures by providing tactile and visual perception data\nthrough an end-to-end architecture. This information is derived from a\nsegmentation and force estimation head, which localizes the catheter in X-ray\nimages and estimates the applied pressure based on its deflection within the\nimage. These stereo vision architectures incorporate a CNN-based\nencoder-decoder that captures the dependencies between X-ray images from two\nviewpoints, enabling simultaneous 3D force estimation and stereo segmentation\nof the catheter. With these tasks in mind, this work approaches the problem\nfrom a new perspective. We propose a novel encoder-decoder Vision Transformer\nmodel that processes two input X-ray images as separate sequences. Given\nsequences of X-ray patches from two perspectives, the transformer captures\nlong-range dependencies without the need to gradually expand the receptive\nfield for either image. The embeddings generated by both the encoder and\ndecoder are fed into two shared segmentation heads, while a regression head\nemploys the fused information from the decoder for 3D force estimation. The\nproposed model is a stereo Vision Transformer capable of simultaneously\nsegmenting the catheter from two angles while estimating the generated forces\nat its tip in 3D. This model has undergone extensive experiments on synthetic\nX-ray images with various noise levels and has been compared against\nstate-of-the-art pure segmentation models, vision-based catheter force\nestimation methods, and a multitask catheter segmentation and force estimation\napproach. It outperforms existing models, setting a new state-of-the-art in\nboth catheter segmentation and force estimation.\n","authors":["Pedram Fekri","Mehrdad Zadeh","Javad Dargahi"],"pdf_url":"https://arxiv.org/pdf/2509.01605v1.pdf","comment":"Preprint version. This work is intended for future journal submission"},{"id":"http://arxiv.org/abs/2509.01583v1","updated":"2025-09-01T16:12:10Z","published":"2025-09-01T16:12:10Z","title":"Aleatoric Uncertainty from AI-based 6D Object Pose Predictors for\n  Object-relative State Estimation","summary":"  Deep Learning (DL) has become essential in various robotics applications due\nto excelling at processing raw sensory data to extract task specific\ninformation from semantic objects. For example, vision-based object-relative\nnavigation relies on a DL-based 6D object pose predictor to provide the\nrelative pose between the object and the robot as measurements to the robot's\nstate estimator. Accurately knowing the uncertainty inherent in such Deep\nNeural Network (DNN) based measurements is essential for probabilistic state\nestimators subsequently guiding the robot's tasks. Thus, in this letter, we\nshow that we can extend any existing DL-based object-relative pose predictor\nfor aleatoric uncertainty inference simply by including two multi-layer\nperceptrons detached from the translational and rotational part of the DL\npredictor. This allows for efficient training while freezing the existing\npre-trained predictor. We then use the inferred 6D pose and its uncertainty as\na measurement and corresponding noise covariance matrix in an extended Kalman\nfilter (EKF). Our approach induces minimal computational overhead such that the\nstate estimator can be deployed on edge devices while benefiting from the\ndynamically inferred measurement uncertainty. This increases the performance of\nthe object-relative state estimation task compared to a fix-covariance\napproach. We conduct evaluations on synthetic data and real-world data to\nunderline the benefits of aleatoric uncertainty inference for the\nobject-relative state estimation task.\n","authors":["Thomas Jantos","Stephan Weiss","Jan Steinbrener"],"pdf_url":"https://arxiv.org/pdf/2509.01583v1.pdf","comment":"Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)"},{"id":"http://arxiv.org/abs/2509.01582v1","updated":"2025-09-01T16:12:02Z","published":"2025-09-01T16:12:02Z","title":"Quantum game models for interaction-aware decision-making in automated\n  driving","summary":"  Decision-making in automated driving must consider interactions with\nsurrounding agents to be effective. However, traditional methods often neglect\nor oversimplify these interactions because they are difficult to model and\nsolve, which can lead to overly conservative behavior of the ego vehicle. To\naddress this gap, we propose two quantum game models, QG-U1 (Quantum Game -\nUnitary 1) and QG-G4 (Quantum Game - Gates 4), for interaction-aware\ndecision-making. These models extend classical game theory by incorporating\nprinciples of quantum mechanics, such as superposition, interference, and\nentanglement. Specifically, QG-U1 and QG-G4 are designed for two-player games\nwith two strategies per player and can be executed in real time on a standard\ncomputer without requiring quantum hardware. We evaluate both models in merging\nand roundabout scenarios and compare them with classical game-theoretic methods\nand baseline approaches (IDM, MOBIL, and a utility-based technique). Results\nshow that QG-G4 achieves lower collision rates and higher success rates\ncompared to baseline methods, while both quantum models yield higher expected\npayoffs than classical game approaches under certain parameter settings.\n","authors":["Karim Essalmi","Fernando Garrido","Fawzi Nashashibi"],"pdf_url":"https://arxiv.org/pdf/2509.01582v1.pdf","comment":"8 pages, 8 figures, submitted to ICAR 2025"},{"id":"http://arxiv.org/abs/2508.17481v2","updated":"2025-09-01T16:04:07Z","published":"2025-08-24T18:13:33Z","title":"SoK: Cybersecurity Assessment of Humanoid Ecosystem","summary":"  Humanoids are progressing toward practical deployment across healthcare,\nindustrial, defense, and service sectors. While typically considered\ncyber-physical systems (CPSs), their dependence on traditional networked\nsoftware stacks (e.g., Linux operating systems), robot operating system (ROS)\nmiddleware, and over-the-air update channels, creates a distinct security\nprofile that exposes them to vulnerabilities conventional CPS models do not\nfully address. Prior studies have mainly examined specific threats, such as\nLiDAR spoofing or adversarial machine learning (AML). This narrow focus\noverlooks how an attack targeting one component can cascade harm throughout the\nrobot's interconnected systems. We address this gap through a systematization\nof knowledge (SoK) that takes a comprehensive approach, consolidating\nfragmented research from robotics, CPS, and network security domains. We\nintroduce a seven-layer security model for humanoid robots, organizing 39 known\nattacks and 35 defenses across the humanoid ecosystem-from hardware to\nhuman-robot interaction. Building on this security model, we develop a\nquantitative 39x35 attack-defense matrix with risk-weighted scoring, validated\nthrough Monte Carlo analysis. We demonstrate our method by evaluating three\nreal-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed\nvarying security maturity levels, with scores ranging from 39.9% to 79.5%\nacross the platforms. This work introduces a structured, evidence-based\nassessment method that enables systematic security evaluation, supports\ncross-platform benchmarking, and guides prioritization of security investments\nin humanoid robotics.\n","authors":["Priyanka Prakash Surve","Asaf Shabtai","Yuval Elovici"],"pdf_url":"https://arxiv.org/pdf/2508.17481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01547v1","updated":"2025-09-01T15:20:41Z","published":"2025-09-01T15:20:41Z","title":"FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity\n  Radiance Field","summary":"  Visual SLAM has regained attention due to its ability to provide perceptual\ncapabilities and simulation test data for Embodied AI. However, traditional\nSLAM methods struggle to meet the demands of high-quality scene reconstruction,\nand Gaussian SLAM systems, despite their rapid rendering and high-quality\nmapping capabilities, lack effective pose optimization methods and face\nchallenges in geometric reconstruction. To address these issues, we introduce\nFGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as the\nscene representation to enhance geometric mapping performance. After initial\npose estimation, we apply global adjustment to optimize camera poses and sparse\npoint cloud, ensuring robust tracking of our approach. Additionally, we\nmaintain a globally consistent opacity radiance field based on 3D Gaussians and\nintroduce depth distortion and normal consistency terms to refine the scene\nrepresentation. Furthermore, after constructing tetrahedral grids, we identify\nlevel sets to directly extract surfaces from 3D Gaussians. Results across\nvarious real-world and large-scale synthetic datasets demonstrate that our\nmethod achieves state-of-the-art tracking accuracy and mapping performance.\n","authors":["Fan Zhu","Yifan Zhao","Ziyu Chen","Biao Yu","Hui Zhu"],"pdf_url":"https://arxiv.org/pdf/2509.01547v1.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2506.02353v2","updated":"2025-09-01T14:56:48Z","published":"2025-06-03T01:14:45Z","title":"SAVOR: Skill Affordance Learning from Visuo-Haptic Perception for\n  Robot-Assisted Bite Acquisition","summary":"  Robot-assisted feeding requires reliable bite acquisition, a challenging task\ndue to the complex interactions between utensils and food with diverse physical\nproperties. These interactions are further complicated by the temporal\nvariability of food properties-for example, steak becomes firm as it cools even\nduring a meal. To address this, we propose SAVOR, a novel approach for learning\nskill affordances for bite acquisition-how suitable a manipulation skill (e.g.,\nskewering, scooping) is for a given utensil-food interaction. In our\nformulation, skill affordances arise from the combination of tool affordances\n(what a utensil can do) and food affordances (what the food allows). Tool\naffordances are learned offline through calibration, where different utensils\ninteract with a variety of foods to model their functional capabilities. Food\naffordances are characterized by physical properties such as softness,\nmoisture, and viscosity, initially inferred through commonsense reasoning using\na visually-conditioned language model and then dynamically refined through\nonline multi-modal visuo-haptic perception using SAVOR-Net during interaction.\nOur method integrates these offline and online estimates to predict skill\naffordances in real time, enabling the robot to select the most appropriate\nskill for each food item. Evaluated on 20 single-item foods and 10 in-the-wild\nmeals, our approach improves bite acquisition success rate by 13% over\nstate-of-the-art (SOTA) category-based methods (e.g. use skewer for fruits).\nThese results highlight the importance of modeling interaction-driven skill\naffordances for generalizable and effective robot-assisted bite acquisition.\nWebsite: https://emprise.cs.cornell.edu/savor/\n","authors":["Zhanxin Wu","Bo Ai","Tom Silver","Tapomayukh Bhattacharjee"],"pdf_url":"https://arxiv.org/pdf/2506.02353v2.pdf","comment":"Conference on Robot Learning, Oral"},{"id":"http://arxiv.org/abs/2504.04991v3","updated":"2025-09-01T13:56:07Z","published":"2025-04-07T12:16:45Z","title":"Wavelet Policy: Imitation Policy Learning in the Scale Domain with\n  Wavelet Transforms","summary":"  Recent imitation learning policies, often framed as time series prediction\ntasks, directly map robotic observations into the action space, such as\nhigh-dimensional visual data and proprioception. When deploying at the edge, we\nfound the underutilization of frequency domain analysis in robotic manipulation\ntrajectory prediction leads to neglecting the inherent rhythm information\nembedded within action sequences, resulting in errors at critical moments. To\naddress this, we reframe imitation learning policies through the lens of\ntime-scale domain and introduce the Wavelet Policy. This novel approach employs\nwavelet transforms (WT) and new Features Extractor (FE) for feature\npreprocessing and extracts multi-scale features using the Single Encoder to\nMultiple Decoder (SE2MD) architecture. Furthermore, to enhance feature mapping\nin the scale domain and appropriately increase model capacity, we introduce a\nLearnable Scale Domain Filter (LSDF) after each decoder, improving adaptability\nunder different visual conditions. Our results show that the Wavelet Policy\nmaintaining a comparable parameter count outperforms SOTA end-to-end methods on\nfour challenging simulation robotic arm tasks and real tasks, especially at\ncritical moments and remote settings simultaneously. We release the source code\nand model checkpoint of simulation task at\nhttps://github.com/lurenjia384/Wavelet_Policy.\n","authors":["Changchuan Yang","Yuhang Dong","Guanzhong Tian","Haizhou Ge","Hongrui Zhu"],"pdf_url":"https://arxiv.org/pdf/2504.04991v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.01378v2","updated":"2025-09-01T13:53:03Z","published":"2025-07-02T05:44:17Z","title":"RALLY: Role-Adaptive LLM-Driven Yoked Navigation for Agentic UAV Swarms","summary":"  Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as\na critical research focus, and it typically requires the swarm to navigate\neffectively while avoiding obstacles and achieving continuous coverage over\nmultiple mission targets. Although traditional Multi-Agent Reinforcement\nLearning (MARL) approaches offer dynamic adaptability, they are hindered by the\nsemantic gap in numerical communication and the rigidity of homogeneous role\nstructures, resulting in poor generalization and limited task scalability.\nRecent advances in Large Language Model (LLM)-based control frameworks\ndemonstrate strong semantic reasoning capabilities by leveraging extensive\nprior knowledge. However, due to the lack of online learning and over-reliance\non static priors, these works often struggle with effective exploration,\nleading to reduced individual potential and overall system performance. To\naddress these limitations, we propose a Role-Adaptive LLM-Driven Yoked\nnavigation algorithm RALLY. Specifically, we first develop an LLM-driven\nsemantic decision framework that uses structured natural language for efficient\nsemantic communication and collaborative reasoning. Afterward, we introduce a\ndynamic role-heterogeneity mechanism for adaptive role switching and\npersonalized decision-making. Furthermore, we propose a Role-value Mixing\nNetwork (RMIX)-based assignment strategy that integrates LLM offline priors\nwith MARL online policies to enable semi-offline training of role selection\nstrategies. Experiments in the Multi-Agent Particle Environment (MPE)\nenvironment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY\noutperforms conventional approaches in terms of task coverage, convergence\nspeed, and generalization, highlighting its strong potential for collaborative\nnavigation in agentic multi-UAV systems.\n","authors":["Ziyao Wang","Rongpeng Li","Sizhao Li","Yuming Xiang","Haiping Wang","Zhifeng Zhao","Honggang Zhang"],"pdf_url":"https://arxiv.org/pdf/2507.01378v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.02624v1","updated":"2025-09-01T13:38:50Z","published":"2025-09-01T13:38:50Z","title":"Who Owns The Robot?: Four Ethical and Socio-technical Questions about\n  Wellbeing Robots in the Real World through Community Engagement","summary":"  Recent studies indicate that robotic coaches can play a crucial role in\npromoting wellbeing. However, the real-world deployment of wellbeing robots\nraises numerous ethical and socio-technical questions and concerns. To explore\nthese questions, we undertake a community-centered investigation to examine\nthree different communities' perspectives on using robotic wellbeing coaches in\nreal-world environments. We frame our work as an anticipatory ethical\ninvestigation, which we undertake to better inform the development of robotic\ntechnologies with communities' opinions, with the ultimate goal of aligning\nrobot development with public interest. We conducted workshops with three\ncommunities who are under-represented in robotics development: 1) members of\nthe public at a science festival, 2) women computer scientists at a conference,\nand 3) humanities researchers interested in history and philosophy of science.\nIn the workshops, we collected qualitative data using the Social Robot\nCo-Design Canvas on Ethics. We analysed the collected qualitative data with\nThematic Analysis, informed by notes taken during workshops. Through our\nanalysis, we identify four themes regarding key ethical and socio-technical\nquestions about the real-world use of wellbeing robots. We group participants'\ninsights and discussions around these broad thematic questions, discuss them in\nlight of state-of-the-art literature, and highlight areas for future\ninvestigation. Finally, we provide the four questions as a broad framework that\nroboticists can and should use during robotic development and deployment, in\norder to reflect on the ethics and socio-technical dimensions of their robotic\napplications, and to engage in dialogue with communities of robot users. The\nfour questions are: 1) Is the robot safe and how can we know that?, 2) Who is\nthe robot built for and with?, 3) Who owns the robot and the data?, and 4) Why\na robot?.\n","authors":["Minja Axelsson","Jiaee Cheong","Rune Nyrup","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2509.02624v1.pdf","comment":"Accepted at the 8th AAAI/ACM Conference on AI, Ethics, and Society.\n  23 pages, 1 figure"},{"id":"http://arxiv.org/abs/2509.01450v1","updated":"2025-09-01T13:08:09Z","published":"2025-09-01T13:08:09Z","title":"Analyzing Reluctance to Ask for Help When Cooperating With Robots:\n  Insights to Integrate Artificial Agents in HRC","summary":"  As robot technology advances, collaboration between humans and robots will\nbecome more prevalent in industrial tasks. When humans run into issues in such\nscenarios, a likely future involves relying on artificial agents or robots for\naid. This study identifies key aspects for the design of future user-assisting\nagents. We analyze quantitative and qualitative data from a user study\nexamining the impact of on-demand assistance received from a remote human in a\nhuman-robot collaboration (HRC) assembly task. We study scenarios in which\nusers require help and we assess their experiences in requesting and receiving\nassistance. Additionally, we investigate participants' perceptions of future\nnon-human assisting agents and whether assistance should be on-demand or\nunsolicited. Through a user study, we analyze the impact that such design\ndecisions (human or artificial assistant, on-demand or unsolicited help) can\nhave on elicited emotional responses, productivity, and preferences of humans\nengaged in HRC tasks.\n","authors":["Ane San Martin","Michael Hagenow","Julie Shah","Johan Kildal","Elena Lazkano"],"pdf_url":"https://arxiv.org/pdf/2509.01450v1.pdf","comment":"8 pages, 5 figures. Accepted for IEEE RO-MAN 2025"},{"id":"http://arxiv.org/abs/2509.01388v1","updated":"2025-09-01T11:33:30Z","published":"2025-09-01T11:33:30Z","title":"End-to-End Low-Level Neural Control of an Industrial-Grade 6D Magnetic\n  Levitation System","summary":"  Magnetic levitation is poised to revolutionize industrial automation by\nintegrating flexible in-machine product transport and seamless manipulation. It\nis expected to become the standard drive for automated manufacturing. However,\ncontrolling such systems is inherently challenging due to their complex,\nunstable dynamics. Traditional control approaches, which rely on hand-crafted\ncontrol engineering, typically yield robust but conservative solutions, with\ntheir performance closely tied to the expertise of the engineering team. In\ncontrast, neural control learning presents a promising alternative. This paper\npresents the first neural controller for 6D magnetic levitation. Trained\nend-to-end on interaction data from a proprietary controller, it directly maps\nraw sensor data and 6D reference poses to coil current commands. The neural\ncontroller can effectively generalize to previously unseen situations while\nmaintaining accurate and robust control. These results underscore the practical\nfeasibility of learning-based neural control in complex physical systems and\nsuggest a future where such a paradigm could enhance or even substitute\ntraditional engineering approaches in demanding real-world applications. The\ntrained neural controller, source code, and demonstration videos are publicly\navailable at https://sites.google.com/view/neural-maglev.\n","authors":["Philipp Hartmann","Jannick Stranghöner","Klaus Neumann"],"pdf_url":"https://arxiv.org/pdf/2509.01388v1.pdf","comment":"8 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2509.01364v1","updated":"2025-09-01T11:05:38Z","published":"2025-09-01T11:05:38Z","title":"TopoNav: Topological Graphs as a Key Enabler for Advanced Object\n  Navigation","summary":"  Object Navigation (ObjectNav) has made great progress with large language\nmodels (LLMs), but still faces challenges in memory management, especially in\nlong-horizon tasks and dynamic scenes. To address this, we propose TopoNav, a\nnew framework that leverages topological structures as spatial memory. By\nbuilding and updating a topological graph that captures scene connections,\nadjacency, and semantic meaning, TopoNav helps agents accumulate spatial\nknowledge over time, retrieve key information, and reason effectively toward\ndistant goals. Our experiments show that TopoNav achieves state-of-the-art\nperformance on benchmark ObjectNav datasets, with higher success rates and more\nefficient paths. It particularly excels in diverse and complex environments, as\nit connects temporary visual inputs with lasting spatial understanding.\n","authors":["Peiran Liu","Qiang Zhang","Daojie Peng","Lingfeng Zhang","Yihao Qin","Hang Zhou","Jun Ma","Renjing Xu","Yiding Ji"],"pdf_url":"https://arxiv.org/pdf/2509.01364v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.01622v4","updated":"2025-09-01T10:00:15Z","published":"2025-06-02T13:01:13Z","title":"General agents contain world models","summary":"  Are world models a necessary ingredient for flexible, goal-directed\nbehaviour, or is model-free learning sufficient? We provide a formal answer to\nthis question, showing that any agent capable of generalizing to multi-step\ngoal-directed tasks must have learned a predictive model of its environment. We\nshow that this model can be extracted from the agent's policy, and that\nincreasing the agents performance or the complexity of the goals it can achieve\nrequires learning increasingly accurate world models. This has a number of\nconsequences: from developing safe and general agents, to bounding agent\ncapabilities in complex environments, and providing new algorithms for\neliciting world models from agents.\n","authors":["Jonathan Richens","David Abel","Alexis Bellot","Tom Everitt"],"pdf_url":"https://arxiv.org/pdf/2506.01622v4.pdf","comment":"Accepted ICML 2025. Typos corrected"},{"id":"http://arxiv.org/abs/2509.01297v1","updated":"2025-09-01T09:33:52Z","published":"2025-09-01T09:33:52Z","title":"Disentangled Multi-Context Meta-Learning: Unlocking robust and\n  Generalized Task Learning","summary":"  In meta-learning and its downstream tasks, many methods rely on implicit\nadaptation to task variations, where multiple factors are mixed together in a\nsingle entangled representation. This makes it difficult to interpret which\nfactors drive performance and can hinder generalization. In this work, we\nintroduce a disentangled multi-context meta-learning framework that explicitly\nassigns each task factor to a distinct context vector. By decoupling these\nvariations, our approach improves robustness through deeper task understanding\nand enhances generalization by enabling context vector sharing across tasks\nwith shared factors. We evaluate our approach in two domains. First, on a\nsinusoidal regression task, our model outperforms baselines on\nout-of-distribution tasks and generalizes to unseen sine functions by sharing\ncontext vectors associated with shared amplitudes or phase shifts. Second, in a\nquadruped robot locomotion task, we disentangle the robot-specific properties\nand the characteristics of the terrain in the robot dynamics model. By\ntransferring disentangled context vectors acquired from the dynamics model into\nreinforcement learning, the resulting policy achieves improved robustness under\nout-of-distribution conditions, surpassing the baselines that rely on a single\nunified context. Furthermore, by effectively sharing context, our model enables\nsuccessful sim-to-real policy transfer to challenging terrains with\nout-of-distribution robot-specific properties, using just 20 seconds of real\ndata from flat terrain, a result not achievable with single-task adaptation.\n","authors":["Seonsoo Kim","Jun-Gill Kang","Taehong Kim","Seongil Hong"],"pdf_url":"https://arxiv.org/pdf/2509.01297v1.pdf","comment":"Accepted to The Conference on Robot Learning (CoRL) 2025 Project\n  Page: seonsoo-p1.github.io/DMCM"},{"id":"http://arxiv.org/abs/2509.01294v1","updated":"2025-09-01T09:30:35Z","published":"2025-09-01T09:30:35Z","title":"Metamorphic Testing of Multimodal Human Trajectory Prediction","summary":"  Context: Predicting human trajectories is crucial for the safety and\nreliability of autonomous systems, such as automated vehicles and mobile\nrobots. However, rigorously testing the underlying multimodal Human Trajectory\nPrediction (HTP) models, which typically use multiple input sources (e.g.,\ntrajectory history and environment maps) and produce stochastic outputs\n(multiple possible future paths), presents significant challenges. The primary\ndifficulty lies in the absence of a definitive test oracle, as numerous future\ntrajectories might be plausible for any given scenario. Objectives: This\nresearch presents the application of Metamorphic Testing (MT) as a systematic\nmethodology for testing multimodal HTP systems. We address the oracle problem\nthrough metamorphic relations (MRs) adapted for the complexities and stochastic\nnature of HTP. Methods: We present five MRs, targeting transformations of both\nhistorical trajectory data and semantic segmentation maps used as an\nenvironmental context. These MRs encompass: 1) label-preserving geometric\ntransformations (mirroring, rotation, rescaling) applied to both trajectory and\nmap inputs, where outputs are expected to transform correspondingly. 2)\nMap-altering transformations (changing semantic class labels, introducing\nobstacles) with predictable changes in trajectory distributions. We propose\nprobabilistic violation criteria based on distance metrics between probability\ndistributions, such as the Wasserstein or Hellinger distance. Conclusion: This\nstudy introduces tool, a MT framework for the oracle-less testing of\nmultimodal, stochastic HTP systems. It allows for assessment of model\nrobustness against input transformations and contextual changes without\nreliance on ground-truth trajectories.\n","authors":["Helge Spieker","Nadjib Lazaar","Arnaud Gotlieb","Nassim Belmecheri"],"pdf_url":"https://arxiv.org/pdf/2509.01294v1.pdf","comment":"Information and Software Technology"},{"id":"http://arxiv.org/abs/2509.01291v1","updated":"2025-09-01T09:22:07Z","published":"2025-09-01T09:22:07Z","title":"Toward a Holistic Multi-Criteria Trajectory Evaluation Framework for\n  Autonomous Driving in Mixed Traffic Environment","summary":"  This paper presents a unified framework for the evaluation and optimization\nof autonomous vehicle trajectories, integrating formal safety, comfort, and\nefficiency criteria. An innovative geometric indicator, based on the analysis\nof safety zones using adaptive ellipses, is used to accurately quantify\ncollision risks. Our method applies the Shoelace formula to compute the\nintersection area in the case of misaligned and time-varying configurations.\nComfort is modeled using indicators centered on longitudinal and lateral jerk,\nwhile efficiency is assessed by overall travel time. These criteria are\naggregated into a comprehensive objective function solved using a PSO based\nalgorithm. The approach was successfully validated under real traffic\nconditions via experiments conducted in an urban intersection involving an\nautonomous vehicle interacting with a human-operated vehicle, and in simulation\nusing data recorded from human driving in real traffic.\n","authors":["Nouhed Naidja","Stéphane Font","Marc Revilloud","Guillaume Sandou"],"pdf_url":"https://arxiv.org/pdf/2509.01291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11061v2","updated":"2025-09-01T09:18:50Z","published":"2024-09-17T10:43:06Z","title":"Force Myography based Torque Estimation in Human Knee and Ankle Joints","summary":"  The online adaptation of exoskeleton control based on muscle activity sensing\noffers a promising approach to personalizing exoskeleton behavior based on the\nuser's biosignals. While electromyography (EMG)-based methods have demonstrated\nimprovements in joint torque estimation, EMG sensors require direct skin\ncontact and extensive post-processing. In contrast, force myography (FMG)\nmeasures normal forces resulting from changes in muscle volume due to muscle\nactivity. We propose an FMG-based method to estimate knee and ankle joint\ntorques by integrating joint angles and velocities with muscle activity data.\nWe learn a model for joint torque estimation using Gaussian process regression\n(GPR). The effectiveness of the proposed FMG-based method is validated on\nisokinetic motions performed by ten participants. The model is compared to a\nbaseline model that uses only joint angle and velocity, as well as a model\naugmented by EMG data. The results indicate that incorporating FMG into\nexoskeleton control can improve the estimation of joint torque for the ankle\nand knee joints in novel task characteristics within a single participant.\nAlthough the findings suggest that this approach may not improve the\ngeneralizability of estimates between multiple participants, they highlight the\nneed for further research into its potential applications in exoskeleton\ncontrol.\n","authors":["Charlotte Marquardt","Arne Schulz","Miha Dezman","Gunther Kurz","Thorsten Stein","Tamim Asfour"],"pdf_url":"https://arxiv.org/pdf/2409.11061v2.pdf","comment":"This file corresponds to the manuscript presented at the IEEE\n  International Conference on Robotics and Automation (ICRA), May 2025"},{"id":"http://arxiv.org/abs/2509.01251v1","updated":"2025-09-01T08:42:28Z","published":"2025-09-01T08:42:28Z","title":"Towards Data-Driven Metrics for Social Robot Navigation Benchmarking","summary":"  This paper presents a joint effort towards the development of a data-driven\nSocial Robot Navigation metric to facilitate benchmarking and policy\noptimization. We provide our motivations for our approach and describe our\nproposal for storing rated social navigation trajectory datasets. Following\nthese guidelines, we compiled a dataset with 4427 trajectories -- 182 real and\n4245 simulated -- and presented it to human raters, yielding a total of 4402\nrated trajectories after data quality assurance. We also trained an RNN-based\nbaseline metric on the dataset and present quantitative and qualitative\nresults. All data, software, and model weights are publicly available.\n","authors":["Pilar Bachiller-Burgos","Ulysses Bernardet","Luis V. Calderita","Pranup Chhetri","Anthony Francis","Noriaki Hirose","Noé Pérez","Dhruv Shah","Phani T. Singamaneni","Xuesu Xiao","Luis J. Manso"],"pdf_url":"https://arxiv.org/pdf/2509.01251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01246v1","updated":"2025-09-01T08:38:54Z","published":"2025-09-01T08:38:54Z","title":"An AI-Based Shopping Assistant System to Support the Visually Impaired","summary":"  Shopping plays a significant role in shaping consumer identity and social\nintegration. However, for individuals with visual impairments, navigating in\nsupermarkets and identifying products can be an overwhelming and challenging\nexperience. This paper presents an AI-based shopping assistant prototype\ndesigned to enhance the autonomy and inclusivity of visually impaired\nindividuals in supermarket environments. The system integrates multiple\ntechnologies, including computer vision, speech recognition, text-to-speech\nsynthesis, and indoor navigation, into a single, user-friendly platform. Using\ncameras for ArUco marker detection and real-time environmental scanning, the\nsystem helps users navigate the store, identify product locations, provide\nreal-time auditory guidance, and gain context about their surroundings. The\nassistant interacts with the user through voice commands and multimodal\nfeedback, promoting a more dynamic and engaging shopping experience. The system\nwas evaluated through experiments, which demonstrated its ability to guide\nusers effectively and improve their shopping experience. This paper contributes\nto the development of inclusive AI-driven assistive technologies aimed at\nenhancing accessibility and user independence for the shopping experience.\n","authors":["Larissa R. de S. Shibata","Ankit A. Ravankar","Jose Victorio Salazar Luces","Yasuhisa Hirata"],"pdf_url":"https://arxiv.org/pdf/2509.01246v1.pdf","comment":"7 pages, Accepted for 2025 SICE-FES conference (IEEE)"},{"id":"http://arxiv.org/abs/2509.01228v1","updated":"2025-09-01T08:15:53Z","published":"2025-09-01T08:15:53Z","title":"OpenMulti: Open-Vocabulary Instance-Level Multi-Agent Distributed\n  Implicit Mapping","summary":"  Multi-agent distributed collaborative mapping provides comprehensive and\nefficient representations for robots. However, existing approaches lack\ninstance-level awareness and semantic understanding of environments, limiting\ntheir effectiveness for downstream applications. To address this issue, we\npropose OpenMulti, an open-vocabulary instance-level multi-agent distributed\nimplicit mapping framework. Specifically, we introduce a Cross-Agent Instance\nAlignment module, which constructs an Instance Collaborative Graph to ensure\nconsistent instance understanding across agents. To alleviate the degradation\nof mapping accuracy due to the blind-zone optimization trap, we leverage Cross\nRendering Supervision to enhance distributed learning of the scene.\nExperimental results show that OpenMulti outperforms related algorithms in both\nfine-grained geometric accuracy and zero-shot semantic accuracy. In addition,\nOpenMulti supports instance-level retrieval tasks, delivering semantic\nannotations for downstream applications. The project website of OpenMulti is\npublicly available at https://openmulti666.github.io/.\n","authors":["Jianyu Dou","Yinan Deng","Jiahui Wang","Xingsi Tang","Yi Yang","Yufeng Yue"],"pdf_url":"https://arxiv.org/pdf/2509.01228v1.pdf","comment":"Accepted to IEEE Robotics and Automation Letters. Project website:\n  https://openmulti666.github.io/"},{"id":"http://arxiv.org/abs/2508.13073v2","updated":"2025-09-01T08:10:01Z","published":"2025-08-18T16:45:48Z","title":"Large VLM-based Vision-Language-Action Models for Robotic Manipulation:\n  A Survey","summary":"  Robotic manipulation, a key frontier in robotics and embodied AI, requires\nprecise motor control and multimodal understanding, yet traditional rule-based\nmethods fail to scale or generalize in unstructured, novel environments. In\nrecent years, Vision-Language-Action (VLA) models, built upon Large\nVision-Language Models (VLMs) pretrained on vast image-text datasets, have\nemerged as a transformative paradigm. This survey provides the first\nsystematic, taxonomy-oriented review of large VLM-based VLA models for robotic\nmanipulation. We begin by clearly defining large VLM-based VLA models and\ndelineating two principal architectural paradigms: (1) monolithic models,\nencompassing single-system and dual-system designs with differing levels of\nintegration; and (2) hierarchical models, which explicitly decouple planning\nfrom execution via interpretable intermediate representations. Building on this\nfoundation, we present an in-depth examination of large VLM-based VLA models:\n(1) integration with advanced domains, including reinforcement learning,\ntraining-free optimization, learning from human videos, and world model\nintegration; (2) synthesis of distinctive characteristics, consolidating\narchitectural traits, operational strengths, and the datasets and benchmarks\nthat support their development; (3) identification of promising directions,\nincluding memory mechanisms, 4D perception, efficient adaptation, multi-agent\ncooperation, and other emerging capabilities. This survey consolidates recent\nadvances to resolve inconsistencies in existing taxonomies, mitigate research\nfragmentation, and fill a critical gap through the systematic integration of\nstudies at the intersection of large VLMs and robotic manipulation. We provide\na regularly updated project page to document ongoing progress:\nhttps://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation\n","authors":["Rui Shao","Wei Li","Lingsen Zhang","Renshan Zhang","Zhiyang Liu","Ran Chen","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2508.13073v2.pdf","comment":"Project Page:\n  https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation"},{"id":"http://arxiv.org/abs/2501.13919v3","updated":"2025-09-01T07:50:58Z","published":"2025-01-23T18:58:03Z","title":"Temporal Preference Optimization for Long-Form Video Understanding","summary":"  Despite significant advancements in video large multimodal models\n(video-LMMs), achieving effective temporal grounding in long-form videos\nremains a challenge for existing models. To address this limitation, we propose\nTemporal Preference Optimization (TPO), a novel post-training framework\ndesigned to enhance the temporal grounding capabilities of video-LMMs through\npreference learning. TPO adopts a self-training approach that enables models to\ndifferentiate between well-grounded and less accurate temporal responses by\nleveraging curated preference datasets at two granularities: localized temporal\ngrounding, which focuses on specific video segments, and comprehensive temporal\ngrounding, which captures extended temporal dependencies across entire video\nsequences. By optimizing on these preference datasets, TPO significantly\nenhances temporal understanding while reducing reliance on manually annotated\ndata. Extensive experiments on three long-form video understanding\nbenchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness\nof TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO\nestablishes itself as the leading 7B model on the Video-MME benchmark,\nunderscoring the potential of TPO as a scalable and efficient solution for\nadvancing temporal reasoning in long-form video understanding. Project page:\nhttps://ruili33.github.io/tpo_website.\n","authors":["Rui Li","Xiaohan Wang","Yuhui Zhang","Orr Zohar","Zeyu Wang","Serena Yeung-Levy"],"pdf_url":"https://arxiv.org/pdf/2501.13919v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.01235v3","updated":"2025-09-01T06:03:11Z","published":"2025-08-02T07:22:08Z","title":"NarraGuide: an LLM-based Narrative Mobile Robot for Remote Place\n  Exploration","summary":"  Robotic telepresence enables users to navigate and experience remote\nenvironments. However, effective navigation and situational awareness depend on\nusers' prior knowledge of the environment, limiting the usefulness of these\nsystems for exploring unfamiliar places. We explore how integrating\nlocation-aware LLM-based narrative capabilities into a mobile robot can support\nremote exploration. We developed a prototype system, called NarraGuide, that\nprovides narrative guidance for users to explore and learn about a remote place\nthrough a dialogue-based interface. We deployed our prototype in a geology\nmuseum, where remote participants (n=20) used the robot to tour the museum. Our\nfindings reveal how users perceived the robot's role, engaged in dialogue in\nthe tour, and expressed preferences for bystander encountering. Our work\ndemonstrates the potential of LLM-enabled robotic capabilities to deliver\nlocation-aware narrative guidance and enrich the experience of exploring remote\nenvironments.\n","authors":["Yaxin Hu","Arissa J. Sato","Jingxin Du","Chenming Ye","Anjun Zhu","Pragathi Praveena","Bilge Mutlu"],"pdf_url":"https://arxiv.org/pdf/2508.01235v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01145v1","updated":"2025-09-01T05:48:49Z","published":"2025-09-01T05:48:49Z","title":"Novel bio-inspired soft actuators for upper-limb exoskeletons: design,\n  fabrication and feasibility study","summary":"  Soft robots have been increasingly utilized as sophisticated tools in\nphysical rehabilitation, particularly for assisting patients with neuromotor\nimpairments. However, many soft robotics for rehabilitation applications are\ncharacterized by limitations such as slow response times, restricted range of\nmotion, and low output force. There are also limited studies on the precise\nposition and force control of wearable soft actuators. Furthermore, not many\nstudies articulate how bellow-structured actuator designs quantitatively\ncontribute to the robots' capability. This study introduces a paradigm of upper\nlimb soft actuator design. This paradigm comprises two actuators: the\nLobster-Inspired Silicone Pneumatic Robot (LISPER) for the elbow and the\nScallop-Shaped Pneumatic Robot (SCASPER) for the shoulder. LISPER is\ncharacterized by higher bandwidth, increased output force/torque, and high\nlinearity. SCASPER is characterized by high output force/torque and simplified\nfabrication processes. Comprehensive analytical models that describe the\nrelationship between pressure, bending angles, and output force for both\nactuators were presented so the geometric configuration of the actuators can be\nset to modify the range of motion and output forces. The preliminary test on a\ndummy arm is conducted to test the capability of the actuators.\n","authors":["Haiyun Zhang","Gabrielle Naquila","Jung Hyun Bae","Zonghuan Wu","Ashwin Hingwe","Ashish Deshpande"],"pdf_url":"https://arxiv.org/pdf/2509.01145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06156v2","updated":"2025-09-01T04:21:32Z","published":"2025-04-08T15:51:18Z","title":"ViTaMIn: Learning Contact-Rich Tasks Through Robot-Free Visuo-Tactile\n  Manipulation Interface","summary":"  Tactile information plays a crucial role for humans and robots to interact\neffectively with their environment, particularly for tasks requiring the\nunderstanding of contact properties. Solving such dexterous manipulation tasks\noften relies on imitation learning from demonstration datasets, which are\ntypically collected via teleoperation systems and often demand substantial time\nand effort. To address these challenges, we present ViTaMIn, an embodiment-free\nmanipulation interface that seamlessly integrates visual and tactile sensing\ninto a hand-held gripper, enabling data collection without the need for\nteleoperation. Our design employs a compliant Fin Ray gripper with tactile\nsensing, allowing operators to perceive force feedback during manipulation for\nmore intuitive operation. Additionally, we propose a multimodal representation\nlearning strategy to obtain pre-trained tactile representations, improving data\nefficiency and policy robustness. Experiments on seven contact-rich\nmanipulation tasks demonstrate that ViTaMIn significantly outperforms baseline\nmethods, demonstrating its effectiveness for complex manipulation tasks.\n","authors":["Fangchen Liu","Chuanyu Li","Yihua Qin","Jing Xu","Pieter Abbeel","Rui Chen"],"pdf_url":"https://arxiv.org/pdf/2504.06156v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01113v1","updated":"2025-09-01T04:15:25Z","published":"2025-09-01T04:15:25Z","title":"A novel parameter estimation method for pneumatic soft hand control\n  applying logarithmic decrement for pseudo rigid body modeling","summary":"  The rapid advancement in physical human-robot interaction (HRI) has\naccelerated the development of soft robot designs and controllers. Controlling\nsoft robots, especially soft hand grasping, is challenging due to their\ncontinuous deformation, motivating the use of reduced model-based controllers\nfor real-time dynamic performance. Most existing models, however, suffer from\ncomputational inefficiency and complex parameter identification, limiting their\nreal-time applicability. To address this, we propose a paradigm coupling\nPseudo-Rigid Body Modeling with the Logarithmic Decrement Method for parameter\nestimation (PRBM plus LDM). Using a soft robotic hand test bed, we validate\nPRBM plus LDM for predicting position and force output from pressure input and\nbenchmark its performance. We then implement PRBM plus LDM as the basis for\nclosed-loop position and force controllers. Compared to a simple PID\ncontroller, the PRBM plus LDM position controller achieves lower error (average\nmaximum error across all fingers: 4.37 degrees versus 20.38 degrees). For force\ncontrol, PRBM plus LDM outperforms constant pressure grasping in pinching tasks\non delicate objects: potato chip 86 versus 82.5, screwdriver 74.42 versus 70,\nbrass coin 64.75 versus 35. These results demonstrate PRBM plus LDM as a\ncomputationally efficient and accurate modeling technique for soft actuators,\nenabling stable and flexible grasping with precise force regulation.\n","authors":["Haiyun Zhang","Kelvin HoLam Heung","Gabrielle J. Naquila","Ashwin Hingwe","Ashish D. Deshpande"],"pdf_url":"https://arxiv.org/pdf/2509.01113v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.01111v1","updated":"2025-09-01T04:09:00Z","published":"2025-09-01T04:09:00Z","title":"SR-SLAM: Scene-reliability Based RGB-D SLAM in Diverse Environments","summary":"  Visual simultaneous localization and mapping (SLAM) plays a critical role in\nautonomous robotic systems, especially where accurate and reliable measurements\nare essential for navigation and sensing. In feature-based SLAM, the\nquantityand quality of extracted features significantly influence system\nperformance. Due to the variations in feature quantity and quality across\ndiverse environments, current approaches face two major challenges: (1) limited\nadaptability in dynamic feature culling and pose estimation, and (2)\ninsufficient environmental awareness in assessment and optimization strategies.\nTo address these issues, we propose SRR-SLAM, a scene-reliability based\nframework that enhances feature-based SLAM through environment-aware\nprocessing. Our method introduces a unified scene reliability assessment\nmechanism that incorporates multiple metrics and historical observations to\nguide system behavior. Based on this assessment, we develop: (i) adaptive\ndynamic region selection with flexible geometric constraints, (ii)\ndepth-assisted self-adjusting clustering for efficient dynamic feature removal\nin high-dimensional settings, and (iii) reliability-aware pose refinement that\ndynamically integrates direct methods when features are insufficient.\nFurthermore, we propose (iv) reliability-based keyframe selection and a\nweighted optimization scheme to reduce computational overhead while improving\nestimation accuracy. Extensive experiments on public datasets and real world\nscenarios show that SRR-SLAM outperforms state-of-the-art dynamic SLAM methods,\nachieving up to 90% improvement in accuracy and robustness across diverse\nenvironments. These improvements directly contribute to enhanced measurement\nprecision and reliability in autonomous robotic sensing systems.\n","authors":["Haolan Zhang","Chenghao Li","Thanh Nguyen Canh","Lijun Wang","Nak Young Chong"],"pdf_url":"https://arxiv.org/pdf/2509.01111v1.pdf","comment":"submitted"},{"id":"http://arxiv.org/abs/2509.01106v1","updated":"2025-09-01T03:53:47Z","published":"2025-09-01T03:53:47Z","title":"Robix: A Unified Model for Robot Interaction, Reasoning and Planning","summary":"  We introduce Robix, a unified model that integrates robot reasoning, task\nplanning, and natural language interaction within a single vision-language\narchitecture. Acting as the high-level cognitive layer in a hierarchical robot\nsystem, Robix dynamically generates atomic commands for the low-level\ncontroller and verbal responses for human interaction, enabling robots to\nfollow complex instructions, plan long-horizon tasks, and interact naturally\nwith human within an end-to-end framework. Robix further introduces novel\ncapabilities such as proactive dialogue, real-time interruption handling, and\ncontext-aware commonsense reasoning during task execution. At its core, Robix\nleverages chain-of-thought reasoning and adopts a three-stage training\nstrategy: (1) continued pretraining to enhance foundational embodied reasoning\nabilities including 3D spatial understanding, visual grounding, and\ntask-centric reasoning; (2) supervised finetuning to model human-robot\ninteraction and task planning as a unified reasoning-action sequence; and (3)\nreinforcement learning to improve reasoning-action consistency and long-horizon\ntask coherence. Extensive experiments demonstrate that Robix outperforms both\nopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in\ninteractive task execution, demonstrating strong generalization across diverse\ninstruction types (e.g., open-ended, multi-stage, constrained, invalid, and\ninterrupted) and various user-involved tasks such as table bussing, grocery\nshopping, and dietary filtering.\n","authors":["Huang Fang","Mengxi Zhang","Heng Dong","Wei Li","Zixuan Wang","Qifeng Zhang","Xueyun Tian","Yucheng Hu","Hang Li"],"pdf_url":"https://arxiv.org/pdf/2509.01106v1.pdf","comment":"Tech report. Project page: https://robix-seed.github.io/robix/"},{"id":"http://arxiv.org/abs/2410.02730v3","updated":"2025-09-01T03:33:43Z","published":"2024-10-03T17:49:28Z","title":"DivScene: Towards Open-Vocabulary Object Navigation with Large Vision\n  Language Models in Diverse Scenes","summary":"  Large Vision-Language Models (LVLMs) have achieved significant progress in\ntasks like visual question answering and document understanding. However, their\npotential to comprehend embodied environments and navigate within them remains\nunderexplored. In this work, we first study the challenge of open-vocabulary\nobject navigation by introducing DivScene, a large-scale dataset with 4,614\nhouses across 81 scene types and 5,707 kinds of target objects. Our dataset\nprovides a much greater diversity of target objects and scene types than\nexisting datasets, enabling a comprehensive task evaluation. We evaluated\nvarious methods with LVLMs and LLMs on our dataset and found that current\nmodels still fall short of open-vocab object navigation ability. Then, we\nfine-tuned LVLMs to predict the next action with CoT explanations. We observe\nthat LVLM's navigation ability can be improved substantially with only\nBFS-generated shortest paths without any human supervision, surpassing GPT-4o\nby over 20% in success rates.\n","authors":["Zhaowei Wang","Hongming Zhang","Tianqing Fang","Ye Tian","Yue Yang","Kaixin Ma","Xiaoman Pan","Yangqiu Song","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2410.02730v3.pdf","comment":"EMNLP 2025"},{"id":"http://arxiv.org/abs/2509.01065v1","updated":"2025-09-01T02:12:06Z","published":"2025-09-01T02:12:06Z","title":"Model Predictive Control for a Soft Robotic Finger with Stochastic\n  Behavior based on Fokker-Planck Equation","summary":"  The inherent flexibility of soft robots offers numerous advantages, such as\nenhanced adaptability and improved safety. However, this flexibility can also\nintroduce challenges regarding highly uncertain and nonlinear motion. These\nchallenges become particularly problematic when using open-loop control\nmethods, which lack a feedback mechanism and are commonly employed in soft\nrobot control. Though one potential solution is model-based control, typical\ndeterministic models struggle with uncertainty as mentioned above. The idea is\nto use the Fokker-Planck Equation (FPE), a master equation of a stochastic\nprocess, to control not the state of soft robots but the probabilistic\ndistribution. In this study, we propose and implement a stochastic-based\ncontrol strategy, termed FPE-based Model Predictive Control (FPE-MPC), for a\nsoft robotic finger. Two numerical simulation case studies examine the\nperformance and characteristics of this control method, revealing its efficacy\nin managing the uncertainty inherent in soft robotic systems.\n","authors":["Sumitaka Honji","Takahiro Wada"],"pdf_url":"https://arxiv.org/pdf/2509.01065v1.pdf","comment":"6 pages, 7 figures, presented/published at 2025 IEEE 8th\n  International Conference on Soft Robotics (RoboSoft)"},{"id":"http://arxiv.org/abs/2409.10049v3","updated":"2025-09-01T02:08:42Z","published":"2024-09-16T07:21:18Z","title":"Nav-SCOPE: Swarm Robot Cooperative Perception and Coordinated Navigation","summary":"  This paper proposes a lightweight systematic solution for multi-robot\ncoordinated navigation with decentralized cooperative perception. An\ninformation flow is first created to facilitate real-time observation sharing\nover unreliable ad-hoc networks. Then, the environmental uncertainties of each\nrobot are reduced by interaction fields that deliver complementary information.\nFinally, path optimization is achieved, enabling self-organized coordination\nwith effective convergence, divergence, and collision avoidance. Our method is\nfully interpretable and ready for deployment without gaps. Comprehensive\nsimulations and real-world experiments demonstrate reduced path redundancy,\nrobust performance across various tasks, and minimal demands on computation and\ncommunication.\n","authors":["Chenxi Li","Weining Lu","Qingquan Lin","Litong Meng","Haolu Li","Bin Liang"],"pdf_url":"https://arxiv.org/pdf/2409.10049v3.pdf","comment":"11 pages, 9 figures, accepted in IEEE Transactions on Automation\n  Science and Engineering"},{"id":"http://arxiv.org/abs/2509.01044v1","updated":"2025-09-01T00:52:01Z","published":"2025-09-01T00:52:01Z","title":"A Reactive Grasping Framework for Multi-DoF Grippers via Task Space\n  Velocity Fields and Joint Space QP","summary":"  We present a fast and reactive grasping framework for multi-DoF grippers that\ncombines task-space velocity fields with a joint-space Quadratic Program (QP)\nin a hierarchical structure. Reactive, collision-free global motion planning is\nparticularly challenging for high-DoF systems, since simultaneous increases in\nstate dimensionality and planning horizon trigger a combinatorial explosion of\nthe search space, making real-time planning intractable. To address this, we\nplan globally in a lower-dimensional task space, such as fingertip positions,\nand track locally in the full joint space while enforcing all constraints. This\napproach is realized by constructing velocity fields in multiple task-space\ncoordinates (or in some cases a subset of joint coordinates) and solving a\nweighted joint-space QP to compute joint velocities that track these fields\nwith appropriately assigned priorities. Through simulation experiments with\nprivileged knowledge and real-world tests using the recent pose-tracking\nalgorithm FoundationPose, we verify that our method enables high-DoF arm-hand\nsystems to perform real-time, collision-free reaching motions while adapting to\ndynamic environments and external disturbances.\n","authors":["Yonghyeon Lee","Tzu-Yuan Lin","Alexander Alexiev","Sangbae Kim"],"pdf_url":"https://arxiv.org/pdf/2509.01044v1.pdf","comment":"8 pages, 12 figures, under review"},{"id":"http://arxiv.org/abs/2509.01043v1","updated":"2025-09-01T00:50:20Z","published":"2025-09-01T00:50:20Z","title":"TARA: A Low-Cost 3D-Printed Robotic Arm for Accessible Robotics\n  Education","summary":"  The high cost of robotic platforms limits students' ability to gain practical\nskills directly applicable in real-world scenarios. To address this challenge,\nthis paper presents TARA, a low-cost, 3D-printed robotic arm designed for\naccessible robotics education. TARA includes an open-source repository with\ndesign files, assembly instructions, and baseline code, enabling users to build\nand customize the platform. The system balances affordability and\nfunctionality, offering a highly capable robotic arm for approximately 200 USD,\nsignificantly lower than industrial systems that often cost thousands of\ndollars. Experimental validation confirmed accurate performance in basic\nmanipulation tasks. Rather than focusing on performance benchmarking, this work\nprioritizes educational reproducibility, providing a platform that students and\neducators can reliably replicate and extend.\n","authors":["Thays Leach Mitre"],"pdf_url":"https://arxiv.org/pdf/2509.01043v1.pdf","comment":"6 pages, 5 figures. Preprint submission"},{"id":"http://arxiv.org/abs/2508.21112v2","updated":"2025-09-01T00:28:12Z","published":"2025-08-28T17:26:15Z","title":"EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control","summary":"  The human ability to seamlessly perform multimodal reasoning and physical\ninteraction in the open world is a core goal for general-purpose embodied\nintelligent systems. Recent vision-language-action (VLA) models, which are\nco-trained on large-scale robot and visual-text data, have demonstrated notable\nprogress in general robot control. However, they still fail to achieve\nhuman-level flexibility in interleaved reasoning and interaction. In this work,\nintroduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is\na unified embodied foundation model that achieves superior performance in\nmultimodal embodied reasoning and robot control through interleaved\nvision-text-action pre-training. The development of EO-1 is based on two key\npillars: (i) a unified architecture that processes multimodal inputs\nindiscriminately (image, text, video, and action), and (ii) a massive,\nhigh-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains\nover 1.5 million samples with emphasis on interleaved vision-text-action\ncomprehension. EO-1 is trained through synergies between auto-regressive\ndecoding and flow matching denoising on EO-Data1.5M, enabling seamless robot\naction generation and multimodal embodied reasoning. Extensive experiments\ndemonstrate the effectiveness of interleaved vision-text-action learning for\nopen-world understanding and generalization, validated through a variety of\nlong-horizon, dexterous manipulation tasks across multiple embodiments. This\npaper details the architecture of EO-1, the data construction strategy of\nEO-Data1.5M, and the training methodology, offering valuable insights for\ndeveloping advanced embodied foundation models.\n","authors":["Delin Qu","Haoming Song","Qizhi Chen","Zhaoqing Chen","Xianqiang Gao","Xinyi Ye","Qi Lv","Modi Shi","Guanghui Ren","Cheng Ruan","Maoqing Yao","Haoran Yang","Jiacheng Bao","Bin Zhao","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2508.21112v2.pdf","comment":null}]},"2025-08-31T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2509.01022v1","updated":"2025-08-31T23:27:27Z","published":"2025-08-31T23:27:27Z","title":"Symbolic Planning and Multi-Agent Path Finding in Extremely Dense\n  Environments with Movable Obstacles","summary":"  We introduce the Block Rearrangement Problem (BRaP), a challenging component\nof large warehouse management which involves rearranging storage blocks within\ndense grids to achieve a target state. We formally define the BRaP as a graph\nsearch problem. Building on intuitions from sliding puzzle problems, we propose\nfive search-based solution algorithms, leveraging joint configuration space\nsearch, classical planning, multi-agent pathfinding, and expert heuristics. We\nevaluate the five approaches empirically for plan quality and scalability.\nDespite the exponential relation between search space size and block number,\nour methods demonstrate efficiency in creating rearrangement plans for deeply\nburied blocks in up to 80x80 grids.\n","authors":["Bo Fu","Zhe Chen","Rahul Chandan","Alex Barbosa","Michael Caldara","Joey Durham","Federico Pecora"],"pdf_url":"https://arxiv.org/pdf/2509.01022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.19131v3","updated":"2025-08-31T23:13:05Z","published":"2025-04-27T07:07:12Z","title":"Making Physical Objects with Generative AI and Robotic Assembly:\n  Considering Fabrication Constraints, Sustainability, Time, Functionality, and\n  Accessibility","summary":"  3D generative AI enables rapid and accessible creation of 3D models from text\nor image inputs. However, translating these outputs into physical objects\nremains a challenge due to the constraints in the physical world. Recent\nstudies have focused on improving the capabilities of 3D generative AI to\nproduce fabricable outputs, with 3D printing as the main fabrication method.\nHowever, this workshop paper calls for a broader perspective by considering how\nfabrication methods align with the capabilities of 3D generative AI. As a case\nstudy, we present a novel system using discrete robotic assembly and 3D\ngenerative AI to make physical objects. Through this work, we identified five\nkey aspects to consider in a physical making process based on the capabilities\nof 3D generative AI. 1) Fabrication Constraints: Current text-to-3D models can\ngenerate a wide range of 3D designs, requiring fabrication methods that can\nadapt to the variability of generative AI outputs. 2) Time: While generative AI\ncan generate 3D models in seconds, fabricating physical objects can take hours\nor even days. Faster production could enable a closer iterative design loop\nbetween humans and AI in the making process. 3) Sustainability: Although\ntext-to-3D models can generate thousands of models in the digital world,\nextending this capability to the real world would be resource-intensive,\nunsustainable and irresponsible. 4) Functionality: Unlike digital outputs from\n3D generative AI models, the fabrication method plays a crucial role in the\nusability of physical objects. 5) Accessibility: While generative AI simplifies\n3D model creation, the need for fabrication equipment can limit participation,\nmaking AI-assisted creation less inclusive. These five key aspects provide a\nframework for assessing how well a physical making process aligns with the\ncapabilities of 3D generative AI and values in the world.\n","authors":["Alexander Htet Kyaw","Se Hwan Jeon","Miana Smith","Neil Gershenfeld"],"pdf_url":"https://arxiv.org/pdf/2504.19131v3.pdf","comment":"ACM CHI Conference on Human Factors in Computing Systems (CHI 2025),\n  Workshop on Generative AI and Human-Computer Interaction, Yokohama, Japan,\n  April 26 to May 1, 2025"},{"id":"http://arxiv.org/abs/2509.01019v1","updated":"2025-08-31T23:09:51Z","published":"2025-08-31T23:09:51Z","title":"AI-driven Dispensing of Coral Reseeding Devices for Broad-scale\n  Restoration of the Great Barrier Reef","summary":"  Coral reefs are on the brink of collapse, with climate change, ocean\nacidification, and pollution leading to a projected 70-90% loss of coral\nspecies within the next decade. Restoration efforts are crucial, but their\nsuccess hinges on introducing automation to upscale efforts. We present\nautomated deployment of coral re-seeding devices powered by artificial\nintelligence, computer vision, and robotics. Specifically, we perform automated\nsubstrate classification, enabling detection of areas of the seafloor suitable\nfor coral growth, thus significantly reducing reliance on human experts and\nincreasing the range and efficiency of restoration. Real-world testing of the\nalgorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%,\nsub-image patch classification of 89.1%, and real-time model inference at 5.5\nframes per second. Further, we present and publicly contribute a large\ncollection of annotated substrate image data to foster future research in this\narea.\n","authors":["Scarlett Raine","Benjamin Moshirian","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2509.01019v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2509.01010v1","updated":"2025-08-31T22:13:42Z","published":"2025-08-31T22:13:42Z","title":"A Robust Numerical Method for Solving Trigonometric Equations in Robotic\n  Kinematics","summary":"  This paper presents a robust numerical method for solving systems of\ntrigonometric equations commonly encountered in robotic kinematics. Our\napproach employs polynomial substitution techniques combined with eigenvalue\ndecomposition to handle singular matrices and edge cases effectively. The\nmethod demonstrates superior numerical stability compared to traditional\napproaches and has been implemented as an open-source Python package. For\nnon-singular matrices, we employ Weierstrass substitution to transform the\nsystem into a quartic polynomial, ensuring all analytical solutions are found.\nFor singular matrices, we develop specialized geometric constraint methods\nusing SVD analysis. The solver demonstrates machine precision accuracy ($<\n10^{-15}$ error) with 100\\% success rate on extensive test cases, making it\nparticularly valuable for robotics applications such as inverse kinematics\nproblems.\n","authors":["Hai-Jun Su"],"pdf_url":"https://arxiv.org/pdf/2509.01010v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00981v1","updated":"2025-08-31T20:24:53Z","published":"2025-08-31T20:24:53Z","title":"Enhanced Mean Field Game for Interactive Decision-Making with Varied\n  Stylish Multi-Vehicles","summary":"  This paper presents an MFG-based decision-making framework for autonomous\ndriving in heterogeneous traffic. To capture diverse human behaviors, we\npropose a quantitative driving style representation that maps abstract traits\nto parameters such as speed, safety factors, and reaction time. These\nparameters are embedded into the MFG through a spatial influence field model.\nTo ensure safe operation in dense traffic, we introduce a safety-critical\nlane-changing algorithm that leverages dynamic safety margins,\ntime-to-collision analysis, and multi-layered constraints. Real-world NGSIM\ndata is employed for style calibration and empirical validation. Experimental\nresults demonstrate zero collisions across six style combinations, two\n15-vehicle scenarios, and NGSIM-based trials, consistently outperforming\nconventional game-theoretic baselines. Overall, our approach provides a\nscalable, interpretable, and behavior-aware planning framework for real-world\nautonomous driving applications.\n","authors":["Liancheng Zheng","Zhen Tian","Yangfan He","Shuo Liu","Ke Gong","Huilin Chen","Zhihao Lin"],"pdf_url":"https://arxiv.org/pdf/2509.00981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.05410v2","updated":"2025-08-31T20:06:07Z","published":"2025-08-07T14:03:13Z","title":"Computational Design and Fabrication of Modular Robots with Untethered\n  Control","summary":"  Natural organisms utilize distributed actuation through their musculoskeletal\nsystems to adapt their gait for traversing diverse terrains or to morph their\nbodies for varied tasks. A longstanding challenge in robotics is to emulate\nthis capability of natural organisms, which has motivated the development of\nnumerous soft robotic systems. However, such systems are generally optimized\nfor a single functionality, lack the ability to change form or function on\ndemand, or remain tethered to bulky control systems. To address these\nlimitations, we present a framework for designing and controlling robots that\nutilize distributed actuation. We propose a novel building block that\nintegrates 3D-printed bones with liquid crystal elastomer (LCE) muscles as\nlightweight actuators, enabling the modular assembly of musculoskeletal robots.\nWe developed LCE rods that contract in response to infrared radiation, thereby\nproviding localized, untethered control over the distributed skeletal network\nand producing global deformations of the robot. To fully capitalize on the\nextensive design space, we introduce two computational tools: one for\noptimizing the robot's skeletal graph to achieve multiple target deformations,\nand another for co-optimizing skeletal designs and control gaits to realize\ndesired locomotion. We validate our framework by constructing several robots\nthat demonstrate complex shape morphing, diverse control schemes, and\nenvironmental adaptability. Our system integrates advances in modular material\nbuilding, untethered and distributed control, and computational design to\nintroduce a new generation of robots that brings us closer to the capabilities\nof living organisms.\n","authors":["Manas Bhargava","Takefumi Hiraki","Malina Strugaru","Yuhan Zhang","Michal Piovarci","Chiara Daraio","Daisuke Iwai","Bernd Bickel"],"pdf_url":"https://arxiv.org/pdf/2508.05410v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00836v1","updated":"2025-08-31T13:09:24Z","published":"2025-08-31T13:09:24Z","title":"One-Step Model Predictive Path Integral for Manipulator Motion Planning\n  Using Configuration Space Distance Fields","summary":"  Motion planning for robotic manipulators is a fundamental problem in\nrobotics. Classical optimization-based methods typically rely on the gradients\nof signed distance fields (SDFs) to impose collision-avoidance constraints.\nHowever, these methods are susceptible to local minima and may fail when the\nSDF gradients vanish. Recently, Configuration Space Distance Fields (CDFs) have\nbeen introduced, which directly model distances in the robot's configuration\nspace. Unlike workspace SDFs, CDFs are differentiable almost everywhere and\nthus provide reliable gradient information. On the other hand, gradient-free\napproaches such as Model Predictive Path Integral (MPPI) control leverage\nlong-horizon rollouts to achieve collision avoidance. While effective, these\nmethods are computationally expensive due to the large number of trajectory\nsamples, repeated collision checks, and the difficulty of designing cost\nfunctions with heterogeneous physical units. In this paper, we propose a\nframework that integrates CDFs with MPPI to enable direct navigation in the\nrobot's configuration space. Leveraging CDF gradients, we unify the MPPI cost\nin joint-space and reduce the horizon to one step, substantially cutting\ncomputation while preserving collision avoidance in practice. We demonstrate\nthat our approach achieves nearly 100% success rates in 2D environments and\nconsistently high success rates in challenging 7-DOF Franka manipulator\nsimulations with complex obstacles. Furthermore, our method attains control\nfrequencies exceeding 750 Hz, substantially outperforming both\noptimization-based and standard MPPI baselines. These results highlight the\neffectiveness and efficiency of the proposed CDF-MPPI framework for\nhigh-dimensional motion planning.\n","authors":["Yulin Li","Tetsuro Miyazaki","Kenji Kawashima"],"pdf_url":"https://arxiv.org/pdf/2509.00836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00828v1","updated":"2025-08-31T12:59:46Z","published":"2025-08-31T12:59:46Z","title":"An Effective Trajectory Planning and an Optimized Path Planning for a\n  6-Degree-of-Freedom Robot Manipulator","summary":"  An effective method for optimizing path planning for a specific model of a\n6-degree-of-freedom (6-DOF) robot manipulator is presented as part of the\nmotion planning of the manipulator using computer algebra. We assume that we\nare given a path in the form of a set of line segments that the end-effector\nshould follow. We also assume that we have a method to solve the inverse\nkinematic problem of the manipulator at each via-point of the trajectory. The\nproposed method consists of three steps. First, we calculate the feasible\nregion of the manipulator under a specific configuration of the end-effector.\nNext, we aim to find a trajectory on the line segments and a sequence of joint\nconfigurations the manipulator should follow to move the end-effector along the\nspecified trajectory. Finally, we find the optimal combination of solutions to\nthe inverse kinematic problem at each via-point along the trajectory by\nreducing the problem to a shortest-path problem of the graph and applying\nDijkstra's algorithm. We show the effectiveness of the proposed method by\nexperiments.\n","authors":["Takumu Okazaki","Akira Terui","Masahiko Mikawa"],"pdf_url":"https://arxiv.org/pdf/2509.00828v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2509.00823v1","updated":"2025-08-31T12:41:32Z","published":"2025-08-31T12:41:32Z","title":"Inverse Kinematics for a 6-Degree-of-Freedom Robot Manipulator Using\n  Comprehensive Gröbner Systems","summary":"  We propose an effective method for solving the inverse kinematic problem of a\nspecific model of 6-degree-of-freedom (6-DOF) robot manipulator using computer\nalgebra. It is known that when the rotation axes of three consecutive\nrotational joints of a manipulator intersect at a single point, the inverse\nkinematics problem can be divided into determining position and orientation. We\nextend this method to more general manipulators in which the rotational axes of\ntwo consecutive joints intersect. This extension broadens the class of 6-DOF\nmanipulators for which the inverse kinematics problem can be solved, and is\nexpected to enable more efficient solutions. The inverse kinematic problem is\nsolved using the Comprehensive Gr\\\"obner System (CGS) with joint parameters of\nthe robot appearing as parameters in the coefficients to prevent repetitive\ncalculations of the Gr\\\"obner bases. The effectiveness of the proposed method\nis shown by experiments.\n","authors":["Takumu Okazaki","Akira Terui","Masahiko Mikawa"],"pdf_url":"https://arxiv.org/pdf/2509.00823v1.pdf","comment":"24 pages"},{"id":"http://arxiv.org/abs/2507.13053v2","updated":"2025-08-31T12:00:35Z","published":"2025-07-17T12:26:03Z","title":"Efficient Online Learning and Adaptive Planning for Robotic Information\n  Gathering Based on Streaming Data","summary":"  Robotic information gathering (RIG) techniques refer to methods where mobile\nrobots are used to acquire data about the physical environment with a suite of\nsensors. Informative planning is an important part of RIG where the goal is to\nfind sequences of actions or paths that maximize efficiency or the quality of\ninformation collected. Many existing solutions solve this problem by assuming\nthat the environment is known in advance. However, real environments could be\nunknown or time-varying, and adaptive informative planning remains an active\narea of research. Adaptive planning and incremental online mapping are required\nfor mapping initially unknown or varying spatial fields. Gaussian process (GP)\nregression is a widely used technique in RIG for mapping continuous spatial\nfields. However, it falls short in many applications as its real-time\nperformance does not scale well to large datasets. To address these challenges,\nthis paper proposes an efficient adaptive informative planning approach for\nmapping continuous scalar fields with GPs with streaming sparse GPs. Simulation\nexperiments are performed with a synthetic dataset and compared against\nexisting benchmarks. Finally, it is also verified with a real-world dataset to\nfurther validate the efficacy of the proposed method. Results show that our\nmethod achieves similar mapping accuracy to the baselines while reducing\ncomputational complexity for longer missions.\n","authors":["Sanjeev Ramkumar Sudha","Joel Jose","Erlend M. Coates"],"pdf_url":"https://arxiv.org/pdf/2507.13053v2.pdf","comment":"Accepted for presentation at 2025 European Conference on Mobile\n  Robots"},{"id":"http://arxiv.org/abs/2502.10610v3","updated":"2025-08-31T08:57:35Z","published":"2025-02-14T23:57:54Z","title":"Safety-Critical Human-Machine Shared Driving for Vehicle Collision\n  Avoidance based on Hamilton-Jacobi reachability","summary":"  Road safety continues to be a pressing global issue, with vehicle collisions\nimposing significant human, societal, and economic burdens. Human-machine\nshared collision avoidance in critical collision scenarios aims to aid drivers'\naccident avoidance through intervening only when necessary. Existing methods\ncount on replanning collision-free trajectories and imposing human-machine\ntracking, which usually interrupts the driver's intent and increases the risk\nof conflict. This paper introduces a Reachability-Aware Reinforcement Learning\n(RL) framework for shared control, guided by Hamilton-Jacobi (HJ) reachability\nanalysis. Machine intervention is activated only when the vehicle approaches\nthe Collision Avoidance Reachable Set (CARS), which represents states where\ncollision is unavoidable. First, we precompute the reachability distributions\nand the CARS by solving the Bellman equation using offline data. To reduce\nhuman-machine conflicts, we develop a driver model for sudden obstacles and\npropose an authority allocation strategy considering key collision avoidance\nfeatures. Finally, we train a RL agent to reduce human-machine conflicts while\nenforcing the hard constraint of avoiding entry into the CARS. The proposed\nmethod was tested on a real vehicle platform. Results show that the controller\nintervenes effectively near CARS to prevent collisions while maintaining\nimproved original driving task performance. Robustness analysis further\nsupports its flexibility across different driver attributes.\n","authors":["Shiyue Zhao","Junzhi Zhang","Rui Zhou","Neda Masoud","Jianxiong Li","Helai Huang","Shijie Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.10610v3.pdf","comment":"36 pages, 15 figures"},{"id":"http://arxiv.org/abs/2507.10082v4","updated":"2025-08-31T08:53:28Z","published":"2025-07-14T09:08:29Z","title":"Unscented Kalman Filter with a Nonlinear Propagation Model for\n  Navigation Applications","summary":"  The unscented Kalman filter is a nonlinear estimation algorithm commonly used\nin navigation applications. The prediction of the mean and covariance matrix is\ncrucial to the stable behavior of the filter. This prediction is done by\npropagating the sigma points according to the dynamic model at hand. In this\npaper, we introduce an innovative method to propagate the sigma points\naccording to the nonlinear dynamic model of the navigation error state vector.\nThis improves the filter accuracy and navigation performance. We demonstrate\nthe benefits of our proposed approach using real sensor data recorded by an\nautonomous underwater vehicle during several scenarios.\n","authors":["Amit Levy","Itzik Klein"],"pdf_url":"https://arxiv.org/pdf/2507.10082v4.pdf","comment":"6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2405.11094v2","updated":"2025-08-31T08:44:54Z","published":"2024-05-17T21:14:50Z","title":"YORI: Autonomous Cooking System Utilizing a Modular Robotic Kitchen and\n  a Dual-Arm Proprioceptive Manipulator","summary":"  This paper presents Yummy Operations Robot Initiative (YORI), a\nproprioceptive dual-arm robotic system that demonstrates autonomous multi-dish\ncooking for scalable food service applications. YORI integrates a dual-arm\nmanipulator equipped with proprioceptive actuators, custom-designed tools,\nappliances, and a structured kitchen environment to address the complexities of\ncooking tasks. The proprioceptive actuators enable fast, precise,\nforce-controlled movements while mitigating the risks associated with\ncooking-related impacts. The system's modular kitchen design and flexible\ntool-changing mechanism support simultaneous multi-dish preparation through\ntorque control and optimization-based motion planning and scheduling. A\ncomprehensive scheduling framework with dynamic rescheduling ensures reliable\nadaptation to new orders and delays. The system was publicly validated through\nlive demonstrations, reliably preparing steak-frites across multiple convention\nsessions. This paper details YORI's design and explores future directions in\nkitchen optimization, task planning, and food quality control, demonstrating\nits potential as a scalable robotic cooking solution. A system introduction and\ncooking videos are available online\n","authors":["Donghun Noh","Hyunwoo Nam","Kyle Gillespie","Yeting Liu","Dennis Hong"],"pdf_url":"https://arxiv.org/pdf/2405.11094v2.pdf","comment":"This work has been submitted to IEEE Robotics & Automation Magazine\n  for possible publication"},{"id":"http://arxiv.org/abs/2509.00741v1","updated":"2025-08-31T08:22:29Z","published":"2025-08-31T08:22:29Z","title":"DyPho-SLAM : Real-time Photorealistic SLAM in Dynamic Environments","summary":"  Visual SLAM algorithms have been enhanced through the exploration of Gaussian\nSplatting representations, particularly in generating high-fidelity dense maps.\nWhile existing methods perform reliably in static environments, they often\nencounter camera tracking drift and fuzzy mapping when dealing with the\ndisturbances caused by moving objects. This paper presents DyPho-SLAM, a\nreal-time, resource-efficient visual SLAM system designed to address the\nchallenges of localization and photorealistic mapping in environments with\ndynamic objects. Specifically, the proposed system integrates prior image\ninformation to generate refined masks, effectively minimizing noise from mask\nmisjudgment. Additionally, to enhance constraints for optimization after\nremoving dynamic obstacles, we devise adaptive feature extraction strategies\nsignificantly improving the system's resilience. Experiments conducted on\npublicly dynamic RGB-D datasets demonstrate that the proposed system achieves\nstate-of-the-art performance in camera pose estimation and dense map\nreconstruction, while operating in real-time in dynamic scenes.\n","authors":["Yi Liu","Keyu Fan","Bin Lan","Houde Liu"],"pdf_url":"https://arxiv.org/pdf/2509.00741v1.pdf","comment":"Accepted by ICME 2025(Oral)"},{"id":"http://arxiv.org/abs/2405.14093v5","updated":"2025-08-31T06:51:08Z","published":"2024-05-23T01:43:54Z","title":"A Survey on Vision-Language-Action Models for Embodied AI","summary":"  Embodied AI is widely recognized as a key element of artificial general\nintelligence because it involves controlling embodied agents to perform tasks\nin the physical world. Building on the success of large language models and\nvision-language models, a new category of multimodal models -- referred to as\nvision-language-action models (VLAs) -- has emerged to address\nlanguage-conditioned robotic tasks in embodied AI by leveraging their distinct\nability to generate actions. In recent years, a myriad of VLAs have been\ndeveloped, making it imperative to capture the rapidly evolving landscape\nthrough a comprehensive survey. To this end, we present the first survey on\nVLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized\ninto three major lines of research. The first line focuses on individual\ncomponents of VLAs. The second line is dedicated to developing control policies\nadept at predicting low-level actions. The third line comprises high-level task\nplanners capable of decomposing long-horizon tasks into a sequence of subtasks,\nthereby guiding VLAs to follow more general user instructions. Furthermore, we\nprovide an extensive summary of relevant resources, including datasets,\nsimulators, and benchmarks. Finally, we discuss the challenges faced by VLAs\nand outline promising future directions in embodied AI. We have created a\nproject associated with this survey, which is available at\nhttps://github.com/yueen-ma/Awesome-VLA.\n","authors":["Yueen Ma","Zixing Song","Yuzheng Zhuang","Jianye Hao","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2405.14093v5.pdf","comment":"Project page: https://github.com/yueen-ma/Awesome-VLA"},{"id":"http://arxiv.org/abs/2508.20085v3","updated":"2025-08-31T06:47:56Z","published":"2025-08-27T17:53:46Z","title":"HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data\n  for Mobile Dexterous Manipulation","summary":"  Leveraging human motion data to impart robots with versatile manipulation\nskills has emerged as a promising paradigm in robotic manipulation.\nNevertheless, translating multi-source human hand motions into feasible robot\nbehaviors remains challenging, particularly for robots equipped with\nmulti-fingered dexterous hands characterized by complex, high-dimensional\naction spaces. Moreover, existing approaches often struggle to produce policies\ncapable of adapting to diverse environmental conditions. In this paper, we\nintroduce HERMES, a human-to-robot learning framework for mobile bimanual\ndexterous manipulation. First, HERMES formulates a unified reinforcement\nlearning approach capable of seamlessly transforming heterogeneous human hand\nmotions from multiple sources into physically plausible robotic behaviors.\nSubsequently, to mitigate the sim2real gap, we devise an end-to-end, depth\nimage-based sim2real transfer method for improved generalization to real-world\nscenarios. Furthermore, to enable autonomous operation in varied and\nunstructured environments, we augment the navigation foundation model with a\nclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise\nalignment of visual goals and effectively bridging autonomous navigation and\ndexterous manipulation. Extensive experimental results demonstrate that HERMES\nconsistently exhibits generalizable behaviors across diverse, in-the-wild\nscenarios, successfully performing numerous complex mobile bimanual dexterous\nmanipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.\n","authors":["Zhecheng Yuan","Tianming Wei","Langzhe Gu","Pu Hua","Tianhai Liang","Yuanpei Chen","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2508.20085v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00665v1","updated":"2025-08-31T02:24:00Z","published":"2025-08-31T02:24:00Z","title":"ER-LoRA: Effective-Rank Guided Adaptation for Weather-Generalized Depth\n  Estimation","summary":"  Monocular depth estimation under adverse weather conditions (e.g.\\ rain, fog,\nsnow, and nighttime) remains highly challenging due to the lack of reliable\nground truth and the difficulty of learning from unlabeled real-world data.\nExisting methods often rely on synthetic adverse data with pseudo-labels, which\nsuffer from domain gaps, or employ self-supervised learning, which violates\nphotometric assumptions in adverse scenarios. In this work, we propose to\nachieve weather-generalized depth estimation by Parameter-Efficient Fine-Tuning\n(PEFT) of Vision Foundation Models (VFMs), using only a small amount of\nhigh-visibility (normal) data. While PEFT has shown strong performance in\nsemantic tasks such as segmentation, it remains underexplored for geometry --\ncentric tasks like depth estimation -- especially in terms of balancing\neffective adaptation with the preservation of pretrained knowledge. To this\nend, we introduce the Selecting-Tuning-Maintaining (STM) strategy, which\nstructurally decomposes the pretrained weights of VFMs based on two kinds of\neffective ranks (entropy-rank and stable-rank). In the tuning phase, we\nadaptively select the proper rank number as well as the task-aware singular\ndirections for initialization, based on the entropy-rank and full-tuned weight;\nwhile in the maintaining stage, we enforce a principal direction regularization\nbased on the stable-rank. This design guarantees flexible task adaptation while\npreserving the strong generalization capability of the pretrained VFM.\nExtensive experiments on four real-world benchmarks across diverse weather\nconditions demonstrate that STM not only outperforms existing PEFT methods and\nfull fine-tuning but also surpasses methods trained with adverse synthetic\ndata, and even the depth foundation model\n","authors":["Weilong Yan","Xin Zhang","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2509.00665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00660v1","updated":"2025-08-31T02:11:18Z","published":"2025-08-31T02:11:18Z","title":"CARIS: A Context-Adaptable Robot Interface System for Personalized and\n  Scalable Human-Robot Interaction","summary":"  The human-robot interaction (HRI) field has traditionally used Wizard-of-Oz\n(WoZ) controlled robots to explore navigation, conversational dynamics,\nhuman-in-the-loop interactions, and more to explore appropriate robot behaviors\nin everyday settings. However, existing WoZ tools are often limited to one\ncontext, making them less adaptable across different settings, users, and\nrobotic platforms. To mitigate these issues, we introduce a Context-Adaptable\nRobot Interface System (CARIS) that combines advanced robotic capabilities such\nteleoperation, human perception, human-robot dialogue, and multimodal data\nrecording. Through pilot studies, we demonstrate the potential of CARIS to WoZ\ncontrol a robot in two contexts: 1) mental health companion and as a 2) tour\nguide. Furthermore, we identified areas of improvement for CARIS, including\nsmoother integration between movement and communication, clearer functionality\nseparation, recommended prompts, and one-click communication options to enhance\nthe usability wizard control of CARIS. This project offers a publicly\navailable, context-adaptable tool for the HRI community, enabling researchers\nto streamline data-driven approaches to intelligent robot behavior.\n","authors":["Felipe Arias-Russi","Yuanchen Bai","Angelique Taylor"],"pdf_url":"https://arxiv.org/pdf/2509.00660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00649v1","updated":"2025-08-31T00:57:41Z","published":"2025-08-31T00:57:41Z","title":"MV-SSM: Multi-View State Space Modeling for 3D Human Pose Estimation","summary":"  While significant progress has been made in single-view 3D human pose\nestimation, multi-view 3D human pose estimation remains challenging,\nparticularly in terms of generalizing to new camera configurations. Existing\nattention-based transformers often struggle to accurately model the spatial\narrangement of keypoints, especially in occluded scenarios. Additionally, they\ntend to overfit specific camera arrangements and visual scenes from training\ndata, resulting in substantial performance drops in new settings. In this\nstudy, we introduce a novel Multi-View State Space Modeling framework, named\nMV-SSM, for robustly estimating 3D human keypoints. We explicitly model the\njoint spatial sequence at two distinct levels: the feature level from\nmulti-view images and the person keypoint level. We propose a Projective State\nSpace (PSS) block to learn a generalized representation of joint spatial\narrangements using state space modeling. Moreover, we modify Mamba's\ntraditional scanning into an effective Grid Token-guided Bidirectional Scanning\n(GTBS), which is integral to the PSS block. Multiple experiments demonstrate\nthat MV-SSM achieves strong generalization, outperforming state-of-the-art\nmethods: +10.8 on AP25 (+24%) on the challenging three-camera setting in CMU\nPanoptic, +7.0 on AP25 (+13%) on varying camera arrangements, and +15.3 PCP\n(+38%) on Campus A1 in cross-dataset evaluations. Project Website:\nhttps://aviralchharia.github.io/MV-SSM\n","authors":["Aviral Chharia","Wenbo Gou","Haoye Dong"],"pdf_url":"https://arxiv.org/pdf/2509.00649v1.pdf","comment":"CVPR 2025; Project Website: https://aviralchharia.github.io/MV-SSM"},{"id":"http://arxiv.org/abs/2410.12124v2","updated":"2025-08-31T00:47:29Z","published":"2024-10-15T23:57:35Z","title":"Learning from 10 Demos: Generalisable and Sample-Efficient Policy\n  Learning with Oriented Affordance Frames","summary":"  Imitation learning has unlocked the potential for robots to exhibit highly\ndexterous behaviours. However, it still struggles with long-horizon,\nmulti-object tasks due to poor sample efficiency and limited generalisation.\nExisting methods require a substantial number of demonstrations to cover\npossible task variations, making them costly and often impractical for\nreal-world deployment. We address this challenge by introducing oriented\naffordance frames, a structured representation for state and action spaces that\nimproves spatial and intra-category generalisation and enables policies to be\nlearned efficiently from only 10 demonstrations. More importantly, we show how\nthis abstraction allows for compositional generalisation of independently\ntrained sub-policies to solve long-horizon, multi-object tasks. To seamlessly\ntransition between sub-policies, we introduce the notion of self-progress\nprediction, which we directly derive from the duration of the training\ndemonstrations. We validate our method across three real-world tasks, each\nrequiring multi-step, multi-object interactions. Despite the small dataset, our\npolicies generalise robustly to unseen object appearances, geometries, and\nspatial arrangements, achieving high success rates without reliance on\nexhaustive training data. Video demonstration can be found on our project page:\nhttps://affordance-policy.github.io/.\n","authors":["Krishan Rana","Jad Abou-Chakra","Sourav Garg","Robert Lee","Ian Reid","Niko Suenderhauf"],"pdf_url":"https://arxiv.org/pdf/2410.12124v2.pdf","comment":"Accepted at the Conference on Robot Learning (CoRL), 2025. Videos can\n  be found on our project website: https://affordance-policy.github.io"},{"id":"http://arxiv.org/abs/2509.00643v1","updated":"2025-08-31T00:35:55Z","published":"2025-08-31T00:35:55Z","title":"A Risk-aware Spatial-temporal Trajectory Planning Framework for\n  Autonomous Vehicles Using QP-MPC and Dynamic Hazard Fields","summary":"  Trajectory planning is a critical component in ensuring the safety,\nstability, and efficiency of autonomous vehicles. While existing trajectory\nplanning methods have achieved progress, they often suffer from high\ncomputational costs, unstable performance in dynamic environments, and limited\nvalidation across diverse scenarios. To overcome these challenges, we propose\nan enhanced QP-MPC-based framework that incorporates three key innovations: (i)\na novel cost function designed with a dynamic hazard field, which explicitly\nbalances safety, efficiency, and comfort; (ii) seamless integration of this\ncost function into the QP-MPC formulation, enabling direct optimization of\ndesired driving behaviors; and (iii) extensive validation of the proposed\nframework across complex tasks. The spatial safe planning is guided by a\ndynamic hazard field (DHF) for risk assessment, while temporal safe planning is\nbased on a space-time graph. Besides, the quintic polynomial sampling and\nsub-reward of comforts are used to ensure comforts during lane-changing. The\nsub-reward of efficiency is used to maintain driving efficiency. Finally, the\nproposed DHF-enhanced objective function integrates multiple objectives,\nproviding a proper optimization tasks for QP-MPC. Extensive simulations\ndemonstrate that the proposed framework outperforms benchmark optimization\nmethods in terms of efficiency, stability, and comfort across a variety of\nscenarios likes lane-changing, overtaking, and crossing intersections.\n","authors":["Zhen Tian","Zhihao Lin","Dezong Zhao","Christos Anagnostopoulos","Qiyuan Wang","Wenjing Zhao","Xiaodan Wang","Chongfeng Wei"],"pdf_url":"https://arxiv.org/pdf/2509.00643v1.pdf","comment":null}]},"2025-08-30T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2509.00624v1","updated":"2025-08-30T22:43:14Z","published":"2025-08-30T22:43:14Z","title":"Vehicle-in-Virtual-Environment (VVE) Method for Developing and\n  Evaluating VRU Safety of Connected and Autonomous Driving with Focus on\n  Bicyclist Safety","summary":"  Extensive research has already been conducted in the autonomous driving field\nto help vehicles navigate safely and efficiently. At the same time, plenty of\ncurrent research on vulnerable road user (VRU) safety is performed which\nlargely concentrates on perception, localization, or trajectory prediction of\nVRUs. However, existing research still exhibits several gaps, including the\nlack of a unified planning and collision avoidance system for autonomous\nvehicles, limited investigation into delay tolerant control strategies, and the\nabsence of an efficient and standardized testing methodology. Ensuring VRU\nsafety remains one of the most pressing challenges in autonomous driving,\nparticularly in dynamic and unpredictable environments. In this two year\nproject, we focused on applying the Vehicle in Virtual Environment (VVE) method\nto develop, evaluate, and demonstrate safety functions for Vulnerable Road\nUsers (VRUs) using automated steering and braking of ADS. In this current\nsecond year project report, our primary focus was on enhancing the previous\nyear results while also considering bicyclist safety.\n","authors":["Haochong Chen","Xincheng Cao","Bilin Aksun-Guvenc","Levent Guvenc"],"pdf_url":"https://arxiv.org/pdf/2509.00624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23175v2","updated":"2025-08-30T21:47:24Z","published":"2025-05-29T07:12:50Z","title":"LocoTouch: Learning Dynamic Quadrupedal Transport with Tactile Sensing","summary":"  Quadrupedal robots have demonstrated remarkable agility and robustness in\ntraversing complex terrains. However, they struggle with dynamic object\ninteractions, where contact must be precisely sensed and controlled. To bridge\nthis gap, we present LocoTouch, a system that equips quadrupedal robots with\ntactile sensing to address a particularly challenging task in this category:\nlong-distance transport of unsecured cylindrical objects, which typically\nrequires custom mounting or fastening mechanisms to maintain stability. For\nefficient large-area tactile sensing, we design a high-density distributed\ntactile sensor that covers the entire back of the robot. To effectively\nleverage tactile feedback for robot control, we develop a simulation\nenvironment with high-fidelity tactile signals, and train tactile-aware\ntransport policies using a two-stage learning pipeline. Furthermore, we design\na novel reward function to promote robust, symmetric, and frequency-adaptive\nlocomotion gaits. After training in simulation, LocoTouch transfers zero-shot\nto the real world, reliably transporting a wide range of unsecured cylindrical\nobjects with diverse sizes, weights, and surface properties. Moreover, it\nremains robust over long distances, on uneven terrain, and under severe\nperturbations.\n","authors":["Changyi Lin","Yuxin Ray Song","Boda Huo","Mingyang Yu","Yikai Wang","Shiqi Liu","Yuxiang Yang","Wenhao Yu","Tingnan Zhang","Jie Tan","Yiyue Luo","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2505.23175v2.pdf","comment":"Project page: https://linchangyi1.github.io/LocoTouch"},{"id":"http://arxiv.org/abs/2502.07380v2","updated":"2025-08-30T21:44:04Z","published":"2025-02-11T08:57:41Z","title":"Wheeled Lab: Modern Sim2Real for Low-cost, Open-source Wheeled Robotics","summary":"  Reinforcement Learning (RL) has been pivotal in recent robotics milestones\nand is poised to play a prominent role in the future. However, these advances\ncan rely on proprietary simulators, expensive hardware, and a daunting range of\ntools and skills. As a result, broader communities are disconnecting from the\nstate-of-the-art; education curricula are poorly equipped to teach\nindispensable modern robotics skills involving hardware, deployment, and\niterative development. To address this gap between the broader and scientific\ncommunities, we contribute Wheeled Lab, an ecosystem which integrates\naccessible, open-source wheeled robots with Isaac Lab, an open-source robot\nlearning and simulation framework, that is widely adopted in the\nstate-of-the-art. To kickstart research and education, this work demonstrates\nthree state-of-the-art zero-shot policies for small-scale RC cars developed\nthrough Wheeled Lab: controlled drifting, elevation traversal, and visual\nnavigation. The full stack, from hardware to software, is low-cost and\nopen-source. Videos and additional materials can be found at:\nhttps://uwrobotlearning.github.io/WheeledLab/\n","authors":["Tyler Han","Preet Shah","Sidharth Rajagopal","Yanda Bao","Sanghun Jung","Sidharth Talia","Gabriel Guo","Bryan Xu","Bhaumik Mehta","Emma Romig","Rosario Scalise","Byron Boots"],"pdf_url":"https://arxiv.org/pdf/2502.07380v2.pdf","comment":"To appear at Conference on Robot Learning, 2025"},{"id":"http://arxiv.org/abs/2505.09698v2","updated":"2025-08-30T18:59:30Z","published":"2025-05-14T18:01:00Z","title":"ManipBench: Benchmarking Vision-Language Models for Low-Level Robot\n  Manipulation","summary":"  Vision-Language Models (VLMs) have revolutionized artificial intelligence and\nrobotics due to their commonsense reasoning capabilities. In robotic\nmanipulation, VLMs are used primarily as high-level planners, but recent work\nhas also studied their lower-level reasoning ability, which refers to making\ndecisions about precise robot movements. However, the community currently lacks\na clear and common benchmark that can evaluate how well VLMs can aid low-level\nreasoning in robotics. Consequently, we propose a novel benchmark, ManipBench,\nto evaluate the low-level robot manipulation reasoning capabilities of VLMs\nacross various dimensions, including how well they understand object-object\ninteractions and deformable object manipulation. We extensively test 33\nrepresentative VLMs across 10 model families on our benchmark, including\nvariants to test different model sizes. Our evaluation shows that the\nperformance of VLMs significantly varies across tasks, and there is a strong\ncorrelation between this performance and trends in our real-world manipulation\ntasks. It also shows that there remains a significant gap between these models\nand human-level understanding. See our website at:\nhttps://manipbench.github.io.\n","authors":["Enyu Zhao","Vedant Raval","Hejia Zhang","Jiageng Mao","Zeyu Shangguan","Stefanos Nikolaidis","Yue Wang","Daniel Seita"],"pdf_url":"https://arxiv.org/pdf/2505.09698v2.pdf","comment":"Conference on Robot Learning (CoRL) 2025. 50 pages and 30 figures. v2\n  is the camera-ready and includes a few more new experiments compared to v1"},{"id":"http://arxiv.org/abs/2509.00582v1","updated":"2025-08-30T18:31:29Z","published":"2025-08-30T18:31:29Z","title":"Safe and Efficient Lane-Changing for Autonomous Vehicles: An Improved\n  Double Quintic Polynomial Approach with Time-to-Collision Evaluation","summary":"  Autonomous driving technology has made significant advancements in recent\nyears, yet challenges remain in ensuring safe and comfortable interactions with\nhuman-driven vehicles (HDVs), particularly during lane-changing maneuvers. This\npaper proposes an improved double quintic polynomial approach for safe and\nefficient lane-changing in mixed traffic environments. The proposed method\nintegrates a time-to-collision (TTC) based evaluation mechanism directly into\nthe trajectory optimization process, ensuring that the ego vehicle proactively\nmaintains a safe gap from surrounding HDVs throughout the maneuver. The\nframework comprises state estimation for both the autonomous vehicle (AV) and\nHDVs, trajectory generation using double quintic polynomials, real-time TTC\ncomputation, and adaptive trajectory evaluation. To the best of our knowledge,\nthis is the first work to embed an analytic TTC penalty directly into the\nclosed-form double-quintic polynomial solver, enabling real-time safety-aware\ntrajectory generation without post-hoc validation. Extensive simulations\nconducted under diverse traffic scenarios demonstrate the safety, efficiency,\nand comfort of the proposed approach compared to conventional methods such as\nquintic polynomials, Bezier curves, and B-splines. The results highlight that\nthe improved method not only avoids collisions but also ensures smooth\ntransitions and adaptive decision-making in dynamic environments. This work\nbridges the gap between model-based and adaptive trajectory planning\napproaches, offering a stable solution for real-world autonomous driving\napplications.\n","authors":["Rui Bai","Rui Xu","Teng Rui","Jiale Liu","Qi Wei Oung","Hoi Leong Lee","Zhen Tian","Fujiang Yuan"],"pdf_url":"https://arxiv.org/pdf/2509.00582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00576v1","updated":"2025-08-30T18:04:19Z","published":"2025-08-30T18:04:19Z","title":"Galaxea Open-World Dataset and G0 Dual-System VLA Model","summary":"  We present Galaxea Open-World Dataset, a large-scale, diverse collection of\nrobot behaviors recorded in authentic human living and working environments.\nAll demonstrations are gathered using a consistent robotic embodiment, paired\nwith precise subtask-level language annotations to facilitate both training and\nevaluation. Building on this dataset, we introduce G0, a dual-system framework\nthat couples a Vision-Language Model (VLM) for multimodal planning with a\nVision-Language-Action (VLA) model for fine-grained execution. G0 is trained\nusing a three-stage curriculum: cross-embodiment pre-training,\nsingle-embodiment pre-training, and task-specific post-training. A\ncomprehensive benchmark spanning tabletop manipulation, few-shot learning, and\nlong-horizon mobile manipulation, demonstrates the effectiveness of our\napproach. In particular, we find that the single-embodiment pre-training stage,\ntogether with the Galaxea Open-World Dataset, plays a critical role in\nachieving strong performance.\n","authors":["Tao Jiang","Tianyuan Yuan","Yicheng Liu","Chenhao Lu","Jianning Cui","Xiao Liu","Shuiqi Cheng","Jiyang Gao","Huazhe Xu","Hang Zhao"],"pdf_url":"https://arxiv.org/pdf/2509.00576v1.pdf","comment":"https://opengalaxea.github.io/G0/"},{"id":"http://arxiv.org/abs/2509.00574v1","updated":"2025-08-30T17:54:48Z","published":"2025-08-30T17:54:48Z","title":"Learning Dolly-In Filming From Demonstration Using a Ground-Based Robot","summary":"  Cinematic camera control demands a balance of precision and artistry -\nqualities that are difficult to encode through handcrafted reward functions.\nWhile reinforcement learning (RL) has been applied to robotic filmmaking, its\nreliance on bespoke rewards and extensive tuning limits creative usability. We\npropose a Learning from Demonstration (LfD) approach using Generative\nAdversarial Imitation Learning (GAIL) to automate dolly-in shots with a\nfree-roaming, ground-based filming robot. Expert trajectories are collected via\njoystick teleoperation in simulation, capturing smooth, expressive motion\nwithout explicit objective design.\n  Trained exclusively on these demonstrations, our GAIL policy outperforms a\nPPO baseline in simulation, achieving higher rewards, faster convergence, and\nlower variance. Crucially, it transfers directly to a real-world robot without\nfine-tuning, achieving more consistent framing and subject alignment than a\nprior TD3-based method. These results show that LfD offers a robust,\nreward-free alternative to RL in cinematic domains, enabling real-time\ndeployment with minimal technical effort. Our pipeline brings intuitive,\nstylized camera control within reach of creative professionals, bridging the\ngap between artistic intent and robotic autonomy.\n","authors":["Philip Lorimer","Alan Hunter","Wenbin Li"],"pdf_url":"https://arxiv.org/pdf/2509.00574v1.pdf","comment":"Preprint; under double-anonymous review. 6 pages"},{"id":"http://arxiv.org/abs/2509.00571v1","updated":"2025-08-30T17:38:17Z","published":"2025-08-30T17:38:17Z","title":"Gray-Box Computed Torque Control for Differential-Drive Mobile Robot\n  Tracking","summary":"  This study presents a learning-based nonlinear algorithm for tracking control\nof differential-drive mobile robots. The Computed Torque Method (CTM) suffers\nfrom inaccurate knowledge of system parameters, while Deep Reinforcement\nLearning (DRL) algorithms are known for sample inefficiency and weak stability\nguarantees. The proposed method replaces the black-box policy network of a DRL\nagent with a gray-box Computed Torque Controller (CTC) to improve sample\nefficiency and ensure closed-loop stability. This approach enables finding an\noptimal set of controller parameters for an arbitrary reward function using\nonly a few short learning episodes. The Twin-Delayed Deep Deterministic Policy\nGradient (TD3) algorithm is used for this purpose. Additionally, some\ncontroller parameters are constrained to lie within known value ranges,\nensuring the RL agent learns physically plausible values. A technique is also\napplied to enforce a critically damped closed-loop time response. The\ncontroller's performance is evaluated on a differential-drive mobile robot\nsimulated in the MuJoCo physics engine and compared against the raw CTC and a\nconventional kinematic controller.\n","authors":["Arman Javan Sekhavat Pishkhani"],"pdf_url":"https://arxiv.org/pdf/2509.00571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00570v1","updated":"2025-08-30T17:31:52Z","published":"2025-08-30T17:31:52Z","title":"ConceptBot: Enhancing Robot's Autonomy through Task Decomposition with\n  Large Language Models and Knowledge Graph","summary":"  ConceptBot is a modular robotic planning framework that combines Large\nLanguage Models and Knowledge Graphs to generate feasible and risk-aware plans\ndespite ambiguities in natural language instructions and correctly analyzing\nthe objects present in the environment - challenges that typically arise from a\nlack of commonsense reasoning. To do that, ConceptBot integrates (i) an Object\nProperty Extraction (OPE) module that enriches scene understanding with\nsemantic concepts from ConceptNet, (ii) a User Request Processing (URP) module\nthat disambiguates and structures instructions, and (iii) a Planner that\ngenerates context-aware, feasible pick-and-place policies. In comparative\nevaluations against Google SayCan, ConceptBot achieved 100% success on explicit\ntasks, maintained 87% accuracy on implicit tasks (versus 31% for SayCan),\nreached 76% on risk-aware tasks (versus 15%), and outperformed SayCan in\napplication-specific scenarios, including material classification (70% vs. 20%)\nand toxicity detection (86% vs. 36%). On SafeAgentBench, ConceptBot achieved an\noverall score of 80% (versus 46% for the next-best baseline). These results,\nvalidated in both simulation and laboratory experiments, demonstrate\nConceptBot's ability to generalize without domain-specific training and to\nsignificantly improve the reliability of robotic policies in unstructured\nenvironments. Website: https://sites.google.com/view/conceptbot\n","authors":["Alessandro Leanza","Angelo Moroncelli","Giuseppe Vizzari","Francesco Braghin","Loris Roveda","Blerina Spahiu"],"pdf_url":"https://arxiv.org/pdf/2509.00570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00564v1","updated":"2025-08-30T17:14:11Z","published":"2025-08-30T17:14:11Z","title":"Reinforcement Learning of Dolly-In Filming Using a Ground-Based Robot","summary":"  Free-roaming dollies enhance filmmaking with dynamic movement, but challenges\nin automated camera control remain unresolved. Our study advances this field by\napplying Reinforcement Learning (RL) to automate dolly-in shots using\nfree-roaming ground-based filming robots, overcoming traditional control\nhurdles. We demonstrate the effectiveness of combined control for precise film\ntasks by comparing it to independent control strategies. Our robust RL pipeline\nsurpasses traditional Proportional-Derivative controller performance in\nsimulation and proves its efficacy in real-world tests on a modified ROSBot 2.0\nplatform equipped with a camera turret. This validates our approach's\npracticality and sets the stage for further research in complex filming\nscenarios, contributing significantly to the fusion of technology with\ncinematic creativity. This work presents a leap forward in the field and opens\nnew avenues for research and development, effectively bridging the gap between\ntechnological advancement and creative filmmaking.\n","authors":["Philip Lorimer","Jack Saunders","Alan Hunter","Wenbin Li"],"pdf_url":"https://arxiv.org/pdf/2509.00564v1.pdf","comment":"Authors' accepted manuscript (IROS 2024, Abu Dhabi, Oct 14-18, 2024).\n  Please cite the version of record: DOI 10.1109/IROS58592.2024.10802717. 8\n  pages"},{"id":"http://arxiv.org/abs/2509.00530v1","updated":"2025-08-30T15:24:47Z","published":"2025-08-30T15:24:47Z","title":"Needle Biopsy And Fiber-Optic Compatible Robotic Insertion Platform","summary":"  Tissue biopsy is the gold standard for diagnosing many diseases, involving\nthe extraction of diseased tissue for histopathology analysis by expert\npathologists. However, this procedure has two main limitations: 1) Manual\nsampling through tissue biopsy is prone to inaccuracies; 2) The extraction\nprocess is followed by a time-consuming pathology test. To address these\nlimitations, we present a compact, accurate, and maneuverable robotic insertion\nplatform to overcome the limitations in traditional histopathology. Our\nplatform is capable of steering a variety of tools with different sizes,\nincluding needle for tissue extraction and optical fibers for vibrational\nspectroscopy applications. This system facilitates the guidance of end-effector\nto the tissue and assists surgeons in navigating to the biopsy target area for\nmulti-modal diagnosis. In this paper, we outline the general concept of our\ndevice, followed by a detailed description of its mechanical design and control\nscheme. We conclude with the validation of the system through a series of\ntests, including positioning accuracy, admittance performance, and tool\ninsertion efficacy.\n","authors":["Fanxin Wang","Yikun Cheng","Chuyuan Tao","Rohit Bhargava","Thenkurussi Kesavadas"],"pdf_url":"https://arxiv.org/pdf/2509.00530v1.pdf","comment":"Presented in EMBC 2025"},{"id":"http://arxiv.org/abs/2505.03238v2","updated":"2025-08-30T14:28:36Z","published":"2025-05-06T07:07:28Z","title":"RobotxR1: Enabling Embodied Robotic Intelligence on Large Language\n  Models through Closed-Loop Reinforcement Learning","summary":"  Future robotic systems operating in real-world environments will require\non-board embodied intelligence without continuous cloud connection, balancing\ncapabilities with constraints on computational power and memory. This work\npresents an extension of the R1-zero approach, which enables the usage of low\nparameter-count Large Language Models (LLMs) in the robotic domain. The R1-Zero\napproach was originally developed to enable mathematical reasoning in LLMs\nusing static datasets. We extend it to the robotics domain through integration\nin a closed-loop Reinforcement Learning (RL) framework. This extension enhances\nreasoning in Embodied Artificial Intelligence (Embodied AI) settings without\nrelying solely on distillation of large models through Supervised Fine-Tuning\n(SFT). We show that small-scale LLMs can achieve effective reasoning\nperformance by learning through closed-loop interaction with their environment,\nwhich enables tasks that previously required significantly larger models. In an\nautonomous driving setting, a performance gain of 20.2%-points over the\nSFT-based baseline is observed with a Qwen2.5-1.5B model. Using the proposed\ntraining procedure, Qwen2.5-3B achieves a 63.3% control adaptability score,\nsurpassing the 58.5% obtained by the much larger, cloud-bound GPT-4o. These\nresults highlight that practical, on-board deployment of small LLMs is not only\nfeasible but can outperform larger models if trained through environmental\nfeedback, underscoring the importance of an interactive learning framework for\nrobotic Embodied AI, one grounded in practical experience rather than static\nsupervision.\n","authors":["Liam Boyle","Nicolas Baumann","Paviththiren Sivasothilingam","Michele Magno","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2505.03238v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00499v1","updated":"2025-08-30T13:43:11Z","published":"2025-08-30T13:43:11Z","title":"NeuralSVCD for Efficient Swept Volume Collision Detection","summary":"  Robot manipulation in unstructured environments requires efficient and\nreliable Swept Volume Collision Detection (SVCD) for safe motion planning.\nTraditional discrete methods potentially miss collisions between these points,\nwhereas SVCD continuously checks for collisions along the entire trajectory.\nExisting SVCD methods typically face a trade-off between efficiency and\naccuracy, limiting practical use. In this paper, we introduce NeuralSVCD, a\nnovel neural encoder-decoder architecture tailored to overcome this trade-off.\nOur approach leverages shape locality and temporal locality through distributed\ngeometric representations and temporal optimization. This enhances\ncomputational efficiency without sacrificing accuracy. Comprehensive\nexperiments show that NeuralSVCD consistently outperforms existing\nstate-of-the-art SVCD methods in terms of both collision detection accuracy and\ncomputational efficiency, demonstrating its robust applicability across diverse\nrobotic manipulation scenarios. Code and videos are available at\nhttps://neuralsvcd.github.io/.\n","authors":["Dongwon Son","Hojin Jung","Beomjoon Kim"],"pdf_url":"https://arxiv.org/pdf/2509.00499v1.pdf","comment":"CoRL 2025"},{"id":"http://arxiv.org/abs/2509.00497v1","updated":"2025-08-30T13:38:42Z","published":"2025-08-30T13:38:42Z","title":"FLUID: A Fine-Grained Lightweight Urban Signalized-Intersection Dataset\n  of Dense Conflict Trajectories","summary":"  The trajectory data of traffic participants (TPs) is a fundamental resource\nfor evaluating traffic conditions and optimizing policies, especially at urban\nintersections. Although data acquisition using drones is efficient, existing\ndatasets still have limitations in scene representativeness, information\nrichness, and data fidelity. This study introduces FLUID, comprising a\nfine-grained trajectory dataset that captures dense conflicts at typical urban\nsignalized intersections, and a lightweight, full-pipeline framework for\ndrone-based trajectory processing. FLUID covers three distinct intersection\ntypes, with approximately 5 hours of recording time and featuring over 20,000\nTPs across 8 categories. Notably, the dataset averages two vehicle conflicts\nper minute, involving roughly 25% of all motor vehicles. FLUID provides\ncomprehensive data, including trajectories, traffic signals, maps, and raw\nvideos. Comparison with the DataFromSky platform and ground-truth measurements\nvalidates its high spatio-temporal accuracy. Through a detailed classification\nof motor vehicle conflicts and violations, FLUID reveals a diversity of\ninteractive behaviors, demonstrating its value for human preference mining,\ntraffic behavior modeling, and autonomous driving research.\n","authors":["Yiyang Chen","Zhigang Wu","Guohong Zheng","Xuesong Wu","Liwen Xu","Haoyuan Tang","Zhaocheng He","Haipeng Zeng"],"pdf_url":"https://arxiv.org/pdf/2509.00497v1.pdf","comment":"26 pages, 14 figures"},{"id":"http://arxiv.org/abs/2509.00491v1","updated":"2025-08-30T13:24:42Z","published":"2025-08-30T13:24:42Z","title":"Extended Diffeomorphism for Real-Time Motion Replication in Workspaces\n  with Different Spatial Arrangements","summary":"  This paper presents two types of extended diffeomorphism designs to\ncompensate for spatial placement differences between robot workspaces.\nTeleoperation of multiple robots is attracting attention to expand the\nutilization of the robot embodiment. Real-time reproduction of robot motion\nwould facilitate the efficient execution of similar tasks by multiple robots. A\nchallenge in the motion reproduction is compensating for the spatial\narrangement errors of target keypoints in robot workspaces. This paper proposes\na methodology for smooth mappings that transform primary robot poses into\nfollower robot poses based on the predefined key points in each workspace.\nThrough a picking task experiment using a dual-arm UR5 robot, this study\ndemonstrates that the proposed mapping generation method can balance lower\nmapping errors for precise operation and lower mapping gradients for smooth\nreplicated movement.\n","authors":["Masaki Saito","Shunki Itadera","Toshiyuki Murakami"],"pdf_url":"https://arxiv.org/pdf/2509.00491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00465v1","updated":"2025-08-30T11:42:26Z","published":"2025-08-30T11:42:26Z","title":"Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial\n  Reasoning","summary":"  This thesis introduces \"Embodied Spatial Intelligence\" to address the\nchallenge of creating robots that can perceive and act in the real world based\non natural language instructions. To bridge the gap between Large Language\nModels (LLMs) and physical embodiment, we present contributions on two fronts:\nscene representation and spatial reasoning. For perception, we develop robust,\nscalable, and accurate scene representations using implicit neural models, with\ncontributions in self-supervised camera calibration, high-fidelity depth field\ngeneration, and large-scale reconstruction. For spatial reasoning, we enhance\nthe spatial capabilities of LLMs by introducing a novel navigation benchmark, a\nmethod for grounding language in 3D, and a state-feedback mechanism to improve\nlong-horizon decision-making. This work lays a foundation for robots that can\nrobustly perceive their surroundings and intelligently act upon complex,\nlanguage-based commands.\n","authors":["Jiading Fang"],"pdf_url":"https://arxiv.org/pdf/2509.00465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.02161v2","updated":"2025-08-30T10:15:58Z","published":"2023-07-05T10:03:08Z","title":"Multi Object Tracking for Predictive Collision Avoidance","summary":"  The safe and efficient operation of Autonomous Mobile Robots (AMRs) in\ncomplex environments, such as manufacturing, logistics, and agriculture,\nnecessitates accurate multi-object tracking and predictive collision avoidance.\nThis paper presents algorithms and techniques for addressing these challenges\nusing Lidar sensor data, emphasizing ensemble Kalman filter. The developed\npredictive collision avoidance algorithm employs the data provided by lidar\nsensors to track multiple objects and predict their velocities and future\npositions, enabling the AMR to navigate safely and effectively. A modification\nto the dynamic windowing approach is introduced to enhance the performance of\nthe collision avoidance system. The overall system architecture encompasses\nobject detection, multi-object tracking, and predictive collision avoidance\ncontrol. The experimental results, obtained from both simulation and real-world\ndata, demonstrate the effectiveness of the proposed methods in various\nscenarios, which lays the foundation for future research on global planners,\nother controllers, and the integration of additional sensors. This thesis\ncontributes to the ongoing development of safe and efficient autonomous systems\nin complex and dynamic environments.\n","authors":["Bruk Gebregziabher","Hadush Hailu"],"pdf_url":"https://arxiv.org/pdf/2307.02161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00433v1","updated":"2025-08-30T09:29:35Z","published":"2025-08-30T09:29:35Z","title":"AGS: Accelerating 3D Gaussian Splatting SLAM via CODEC-Assisted Frame\n  Covisibility Detection","summary":"  Simultaneous Localization and Mapping (SLAM) is a critical task that enables\nautonomous vehicles to construct maps and localize themselves in unknown\nenvironments. Recent breakthroughs combine SLAM with 3D Gaussian Splatting\n(3DGS) to achieve exceptional reconstruction fidelity. However, existing\n3DGS-SLAM systems provide insufficient throughput due to the need for multiple\ntraining iterations per frame and the vast number of Gaussians.\n  In this paper, we propose AGS, an algorithm-hardware co-design framework to\nboost the efficiency of 3DGS-SLAM based on the intuition that SLAM systems\nprocess frames in a streaming manner, where adjacent frames exhibit high\nsimilarity that can be utilized for acceleration. On the software level: 1) We\npropose a coarse-then-fine-grained pose tracking method with respect to the\nrobot's movement. 2) We avoid redundant computations of Gaussians by sharing\ntheir contribution information across frames. On the hardware level, we propose\na frame covisibility detection engine to extract intermediate data from the\nvideo CODEC. We also implement a pose tracking engine and a mapping engine with\nworkload schedulers to efficiently deploy the AGS algorithm. Our evaluation\nshows that AGS achieves up to $17.12\\times$, $6.71\\times$, and $5.41\\times$\nspeedups against the mobile and high-end GPUs, and a state-of-the-art 3DGS\naccelerator, GSCore.\n","authors":["Houshu He","Naifeng Jing","Li Jiang","Xiaoyao Liang","Zhuoran Song"],"pdf_url":"https://arxiv.org/pdf/2509.00433v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2509.00379v1","updated":"2025-08-30T06:34:39Z","published":"2025-08-30T06:34:39Z","title":"Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D\n  Semantic Segmentation","summary":"  Semantic segmentation of 3D LiDAR data plays a pivotal role in autonomous\ndriving. Traditional approaches rely on extensive annotated data for point\ncloud analysis, incurring high costs and time investments. In contrast,\nrealworld image datasets offer abundant availability and substantial scale. To\nmitigate the burden of annotating 3D LiDAR point clouds, we propose two\ncrossmodal knowledge distillation methods: Unsupervised Domain Adaptation\nKnowledge Distillation (UDAKD) and Feature and Semantic-based Knowledge\nDistillation (FSKD). Leveraging readily available spatio-temporally\nsynchronized data from cameras and LiDARs in autonomous driving scenarios, we\ndirectly apply a pretrained 2D image model to unlabeled 2D data. Through\ncrossmodal knowledge distillation with known 2D-3D correspondence, we actively\nalign the output of the 3D network with the corresponding points of the 2D\nnetwork, thereby obviating the necessity for 3D annotations. Our focus is on\npreserving modality-general information while filtering out modality-specific\ndetails during crossmodal distillation. To achieve this, we deploy\nself-calibrated convolution on 3D point clouds as the foundation of our domain\nadaptation module. Rigorous experimentation validates the effectiveness of our\nproposed methods, consistently surpassing the performance of state-of-the-art\napproaches in the field.\n","authors":["Jialiang Kang","Jiawen Wang","Dingsheng Luo"],"pdf_url":"https://arxiv.org/pdf/2509.00379v1.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2508.20547v2","updated":"2025-08-30T05:51:05Z","published":"2025-08-28T08:38:50Z","title":"SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in Dynamic Scenes","summary":"  Real-time interactive grasp synthesis for dynamic objects remains challenging\nas existing methods fail to achieve low-latency inference while maintaining\npromptability. To bridge this gap, we propose SPGrasp (spatiotemporal\nprompt-driven dynamic grasp synthesis), a novel framework extending segment\nanything model v2 (SAMv2) for video stream grasp estimation. Our core\ninnovation integrates user prompts with spatiotemporal context, enabling\nreal-time interaction with end-to-end latency as low as 59 ms while ensuring\ntemporal consistency for dynamic objects. In benchmark evaluations, SPGrasp\nachieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on\nJacquard. On the challenging GraspNet-1Billion dataset under continuous\ntracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency,\nrepresenting a 58.5% reduction compared to the prior state-of-the-art\npromptable method RoG-SAM while maintaining competitive accuracy. Real-world\nexperiments involving 13 moving objects demonstrate a 94.8% success rate in\ninteractive grasping scenarios. These results confirm SPGrasp effectively\nresolves the latency-interactivity trade-off in dynamic grasp synthesis.\n","authors":["Yunpeng Mei","Hongjie Cao","Yinqiu Xia","Wei Xiao","Zhaohan Feng","Gang Wang","Jie Chen"],"pdf_url":"https://arxiv.org/pdf/2508.20547v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00361v1","updated":"2025-08-30T04:53:32Z","published":"2025-08-30T04:53:32Z","title":"Generative Visual Foresight Meets Task-Agnostic Pose Estimation in\n  Robotic Table-Top Manipulation","summary":"  Robotic manipulation in unstructured environments requires systems that can\ngeneralize across diverse tasks while maintaining robust and reliable\nperformance. We introduce {GVF-TAPE}, a closed-loop framework that combines\ngenerative visual foresight with task-agnostic pose estimation to enable\nscalable robotic manipulation. GVF-TAPE employs a generative video model to\npredict future RGB-D frames from a single side-view RGB image and a task\ndescription, offering visual plans that guide robot actions. A decoupled pose\nestimation model then extracts end-effector poses from the predicted frames,\ntranslating them into executable commands via low-level controllers. By\niteratively integrating video foresight and pose estimation in a closed loop,\nGVF-TAPE achieves real-time, adaptive manipulation across a broad range of\ntasks. Extensive experiments in both simulation and real-world settings\ndemonstrate that our approach reduces reliance on task-specific action data and\ngeneralizes effectively, providing a practical and scalable solution for\nintelligent robotic systems.\n","authors":["Chuye Zhang","Xiaoxiong Zhang","Wei Pan","Linfang Zheng","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2509.00361v1.pdf","comment":"9th Conference on Robot Learning (CoRL 2025), Seoul, Korea"},{"id":"http://arxiv.org/abs/2509.00339v1","updated":"2025-08-30T03:44:11Z","published":"2025-08-30T03:44:11Z","title":"Autonomous Aggregate Sorting in Construction and Mining via Computer\n  Vision-Aided Robotic Arm Systems","summary":"  Traditional aggregate sorting methods, whether manual or mechanical, often\nsuffer from low precision, limited flexibility, and poor adaptability to\ndiverse material properties such as size, shape, and lithology. To address\nthese limitations, this study presents a computer vision-aided robotic arm\nsystem designed for autonomous aggregate sorting in construction and mining\napplications. The system integrates a six-degree-of-freedom robotic arm, a\nbinocular stereo camera for 3D perception, and a ROS-based control framework.\nCore techniques include an attention-augmented YOLOv8 model for aggregate\ndetection, stereo matching for 3D localization, Denavit-Hartenberg kinematic\nmodeling for arm motion control, minimum enclosing rectangle analysis for size\nestimation, and hand-eye calibration for precise coordinate alignment.\nExperimental validation with four aggregate types achieved an average grasping\nand sorting success rate of 97.5%, with comparable classification accuracy.\nRemaining challenges include the reliable handling of small aggregates and\ntexture-based misclassification. Overall, the proposed system demonstrates\nsignificant potential to enhance productivity, reduce operational costs, and\nimprove safety in aggregate handling, while providing a scalable framework for\nadvancing smart automation in construction, mining, and recycling industries.\n","authors":["Md. Taherul Islam Shawon","Yuan Li","Yincai Cai","Junjie Niu","Ting Peng"],"pdf_url":"https://arxiv.org/pdf/2509.00339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00329v1","updated":"2025-08-30T03:04:35Z","published":"2025-08-30T03:04:35Z","title":"Jacobian Exploratory Dual-Phase Reinforcement Learning for Dynamic\n  Endoluminal Navigation of Deformable Continuum Robots","summary":"  Deformable continuum robots (DCRs) present unique planning challenges due to\nnonlinear deformation mechanics and partial state observability, violating the\nMarkov assumptions of conventional reinforcement learning (RL) methods. While\nJacobian-based approaches offer theoretical foundations for rigid manipulators,\ntheir direct application to DCRs remains limited by time-varying kinematics and\nunderactuated deformation dynamics. This paper proposes Jacobian Exploratory\nDual-Phase RL (JEDP-RL), a framework that decomposes planning into phased\nJacobian estimation and policy execution. During each training step, we first\nperform small-scale local exploratory actions to estimate the deformation\nJacobian matrix, then augment the state representation with Jacobian features\nto restore approximate Markovianity. Extensive SOFA surgical dynamic\nsimulations demonstrate JEDP-RL's three key advantages over proximal policy\noptimization (PPO) baselines: 1) Convergence speed: 3.2x faster policy\nconvergence, 2) Navigation efficiency: requires 25% fewer steps to reach the\ntarget, and 3) Generalization ability: achieve 92% success rate under material\nproperty variations and achieve 83% (33% higher than PPO) success rate in the\nunseen tissue environment.\n","authors":["Yu Tian","Chi Kit Ng","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2509.00329v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00328v1","updated":"2025-08-30T03:01:57Z","published":"2025-08-30T03:01:57Z","title":"Mechanistic interpretability for steering vision-language-action models","summary":"  Vision-Language-Action (VLA) models are a promising path to realizing\ngeneralist embodied agents that can quickly adapt to new tasks, modalities, and\nenvironments. However, methods for interpreting and steering VLAs fall far\nshort of classical robotics pipelines, which are grounded in explicit models of\nkinematics, dynamics, and control. This lack of mechanistic insight is a\ncentral challenge for deploying learned policies in real-world robotics, where\nrobustness and explainability are critical. Motivated by advances in\nmechanistic interpretability for large language models, we introduce the first\nframework for interpreting and steering VLAs via their internal\nrepresentations, enabling direct intervention in model behavior at inference\ntime. We project feedforward activations within transformer layers onto the\ntoken embedding basis, identifying sparse semantic directions - such as speed\nand direction - that are causally linked to action selection. Leveraging these\nfindings, we introduce a general-purpose activation steering method that\nmodulates behavior in real time, without fine-tuning, reward signals, or\nenvironment interaction. We evaluate this method on two recent open-source\nVLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in\nsimulation (LIBERO) and on a physical robot (UR5). This work demonstrates that\ninterpretable components of embodied VLAs can be systematically harnessed for\ncontrol - establishing a new paradigm for transparent and steerable foundation\nmodels in robotics.\n","authors":["Bear Häon","Kaylene Stocking","Ian Chuang","Claire Tomlin"],"pdf_url":"https://arxiv.org/pdf/2509.00328v1.pdf","comment":"CoRL 2025. Project website: https://vla-mech-interp.github.io/"},{"id":"http://arxiv.org/abs/2506.08931v2","updated":"2025-08-30T02:53:56Z","published":"2025-06-10T15:58:29Z","title":"CLONE: Closed-Loop Whole-Body Humanoid Teleoperation for Long-Horizon\n  Tasks","summary":"  Humanoid teleoperation plays a vital role in demonstrating and collecting\ndata for complex humanoid-scene interactions. However, current teleoperation\nsystems face critical limitations: they decouple upper- and lower-body control\nto maintain stability, restricting natural coordination, and operate open-loop\nwithout real-time position feedback, leading to accumulated drift. The\nfundamental challenge is achieving precise, coordinated whole-body\nteleoperation over extended durations while maintaining accurate global\npositioning. Here we show that an MoE-based teleoperation system, CLONE, with\nclosed-loop error correction enables unprecedented whole-body teleoperation\nfidelity, maintaining minimal positional drift over long-range trajectories\nusing only head and hand tracking from an MR headset. Unlike previous methods\nthat either sacrifice coordination for stability or suffer from unbounded\ndrift, CLONE learns diverse motion skills while preventing tracking error\naccumulation through real-time feedback, enabling complex coordinated movements\nsuch as ``picking up objects from the ground.'' These results establish a new\nmilestone for whole-body humanoid teleoperation for long-horizon humanoid-scene\ninteraction tasks.\n","authors":["Yixuan Li","Yutang Lin","Jieming Cui","Tengyu Liu","Wei Liang","Yixin Zhu","Siyuan Huang"],"pdf_url":"https://arxiv.org/pdf/2506.08931v2.pdf","comment":"18 pages, 13 figures"},{"id":"http://arxiv.org/abs/2509.00319v1","updated":"2025-08-30T02:42:06Z","published":"2025-08-30T02:42:06Z","title":"Contact-Aided Navigation of Flexible Robotic Endoscope Using Deep\n  Reinforcement Learning in Dynamic Stomach","summary":"  Navigating a flexible robotic endoscope (FRE) through the gastrointestinal\ntract is critical for surgical diagnosis and treatment. However, navigation in\nthe dynamic stomach is particularly challenging because the FRE must learn to\neffectively use contact with the deformable stomach walls to reach target\nlocations. To address this, we introduce a deep reinforcement learning (DRL)\nbased Contact-Aided Navigation (CAN) strategy for FREs, leveraging contact\nforce feedback to enhance motion stability and navigation precision. The\ntraining environment is established using a physics-based finite element method\n(FEM) simulation of a deformable stomach. Trained with the Proximal Policy\nOptimization (PPO) algorithm, our approach achieves high navigation success\nrates (within 3 mm error between the FRE's end-effector and target) and\nsignificantly outperforms baseline policies. In both static and dynamic stomach\nenvironments, the CAN agent achieved a 100% success rate with 1.6 mm average\nerror, and it maintained an 85% success rate in challenging unseen scenarios\nwith stronger external disturbances. These results validate that the DRL-based\nCAN strategy substantially enhances FRE navigation performance over prior\nmethods.\n","authors":["Chi Kit Ng","Huxin Gao","Tian-Ao Ren","Jiewen Lai","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2509.00319v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00317v1","updated":"2025-08-30T02:28:25Z","published":"2025-08-30T02:28:25Z","title":"A Framework for Task and Motion Planning based on Expanding AND/OR\n  Graphs","summary":"  Robot autonomy in space environments presents unique challenges, including\nhigh perception and motion uncertainty, strict kinematic constraints, and\nlimited opportunities for human intervention. Therefore, Task and Motion\nPlanning (TMP) may be critical for autonomous servicing, surface operations, or\neven in-orbit missions, just to name a few, as it models tasks as discrete\naction sequencing integrated with continuous motion feasibility assessments. In\nthis paper, we introduce a TMP framework based on expanding AND/OR graphs,\nreferred to as TMP-EAOG, and demonstrate its adaptability to different\nscenarios. TMP-EAOG encodes task-level abstractions within an AND/OR graph,\nwhich expands iteratively as the plan is executed, and performs in-the-loop\nmotion planning assessments to ascertain their feasibility. As a consequence,\nTMP-EAOG is characterised by the desirable properties of (i) robustness to a\ncertain degree of uncertainty, because AND/OR graph expansion can accommodate\nfor unpredictable information about the robot environment, (ii) controlled\nautonomy, since an AND/OR graph can be validated by human experts, and (iii)\nbounded flexibility, in that unexpected events, including the assessment of\nunfeasible motions, can lead to different courses of action as alternative\npaths in the AND/OR graph. We evaluate TMP-EAOG on two benchmark domains. We\nuse a simulated mobile manipulator as a proxy for space-grade autonomous\nrobots. Our evaluation shows that TMP-EAOG can deal with a wide range of\nchallenges in the benchmarks.\n","authors":["Fulvio Mastrogiovanni","Antony Thomas"],"pdf_url":"https://arxiv.org/pdf/2509.00317v1.pdf","comment":"Accepted for an oral presentation at ASTRA Conference, 2025"},{"id":"http://arxiv.org/abs/2505.13834v2","updated":"2025-08-30T02:08:59Z","published":"2025-05-20T02:20:54Z","title":"Toward Real-World Cooperative and Competitive Soccer with Quadrupedal\n  Robot Teams","summary":"  Achieving coordinated teamwork among legged robots requires both fine-grained\nlocomotion control and long-horizon strategic decision-making. Robot soccer\noffers a compelling testbed for this challenge, combining dynamic, competitive,\nand multi-agent interactions. In this work, we present a hierarchical\nmulti-agent reinforcement learning (MARL) framework that enables fully\nautonomous and decentralized quadruped robot soccer. First, a set of highly\ndynamic low-level skills is trained for legged locomotion and ball\nmanipulation, such as walking, dribbling, and kicking. On top of these, a\nhigh-level strategic planning policy is trained with Multi-Agent Proximal\nPolicy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning\nframework allows agents to adapt to diverse opponent strategies and gives rise\nto sophisticated team behaviors, including coordinated passing, interception,\nand dynamic role allocation. With an extensive ablation study, the proposed\nlearning method shows significant advantages in the cooperative and competitive\nmulti-agent soccer game. We deploy the learned policies to real quadruped\nrobots relying solely on onboard proprioception and decentralized localization,\nwith the resulting system supporting autonomous robot-robot and robot-human\nsoccer matches on indoor and outdoor soccer courts.\n","authors":["Zhi Su","Yuman Gao","Emily Lukas","Yunfei Li","Jiaze Cai","Faris Tulbah","Fei Gao","Chao Yu","Zhongyu Li","Yi Wu","Koushil Sreenath"],"pdf_url":"https://arxiv.org/pdf/2505.13834v2.pdf","comment":"11 pages, 12 figures, CoRL 2025"},{"id":"http://arxiv.org/abs/2509.00310v1","updated":"2025-08-30T01:54:28Z","published":"2025-08-30T01:54:28Z","title":"TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for\n  One-Shot Skill Generalization","summary":"  Robots often struggle to generalize from a single demonstration due to the\nlack of a transferable and interpretable spatial representation. In this work,\nwe introduce TReF-6, a method that infers a simplified, abstracted 6DoF\nTask-Relevant Frame from a single trajectory. Our approach identifies an\ninfluence point purely from the trajectory geometry to define the origin for a\nlocal frame, which serves as a reference for parameterizing a Dynamic Movement\nPrimitive (DMP). This influence point captures the task's spatial structure,\nextending the standard DMP formulation beyond start-goal imitation. The\ninferred frame is semantically grounded via a vision-language model and\nlocalized in novel scenes by Grounded-SAM, enabling functionally consistent\nskill generalization. We validate TReF-6 in simulation and demonstrate\nrobustness to trajectory noise. We further deploy an end-to-end pipeline on\nreal-world manipulation tasks, showing that TReF-6 supports one-shot imitation\nlearning that preserves task intent across diverse object configurations.\n","authors":["Yuxuan Ding","Shuangge Wang","Tesca Fitzgerald"],"pdf_url":"https://arxiv.org/pdf/2509.00310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2504.06661v2","updated":"2025-08-30T01:39:03Z","published":"2025-04-09T07:51:46Z","title":"Domain-Conditioned Scene Graphs for State-Grounded Task Planning","summary":"  Recent robotic task planning frameworks have integrated large multimodal\nmodels (LMMs) such as GPT-4o. To address grounding issues of such models, it\nhas been suggested to split the pipeline into perceptional state grounding and\nsubsequent state-based planning. As we show in this work, the state grounding\nability of LMM-based approaches is still limited by weaknesses in granular,\nstructured, domain-specific scene understanding. To address this shortcoming,\nwe develop a more structured state grounding framework that features a\ndomain-conditioned scene graph as its scene representation. We show that such\nrepresentation is actionable in nature as it is directly mappable to a symbolic\nstate in planning languages such as the Planning Domain Definition Language\n(PDDL). We provide an instantiation of our state grounding framework where the\ndomain-conditioned scene graph generation is implemented with a lightweight\nvision-language approach that classifies domain-specific predicates on top of\ndomain-relevant object detections. Evaluated across three domains, our approach\nachieves significantly higher state rounding accuracy and task planning success\nrates compared to LMM-based approaches.\n","authors":["Jonas Herzog","Jiangpin Liu","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2504.06661v2.pdf","comment":"Accepted for IROS 2025"},{"id":"http://arxiv.org/abs/2509.00294v1","updated":"2025-08-30T01:09:51Z","published":"2025-08-30T01:09:51Z","title":"A Layered Control Perspective on Legged Locomotion: Embedding Reduced\n  Order Models via Hybrid Zero Dynamics","summary":"  Reduced-order models (ROMs) provide a powerful means of synthesizing dynamic\nwalking gaits on legged robots. Yet this approach lacks the formal guarantees\nenjoyed by methods that utilize the full-order model (FOM) for gait synthesis,\ne.g., hybrid zero dynamics. This paper aims to unify these approaches through a\nlayered control perspective. In particular, we establish conditions on when a\nROM of locomotion yields stable walking on the full-order hybrid dynamics. To\nachieve this result, given an ROM we synthesize a zero dynamics manifold\nencoding the behavior of the ROM -- controllers can be synthesized that drive\nthe FOM to this surface, yielding hybrid zero dynamics. We prove that a stable\nperiodic orbit in the ROM implies an input-to-state stable periodic orbit of\nthe FOM's hybrid zero dynamics, and hence the FOM dynamics. This result is\ndemonstrated in simulation on a linear inverted pendulum ROM and a 5-link\nplanar walking FOM.\n","authors":["Sergio A. Esteban","Max H. Cohen","Adrian B. Ghansah","Aaron D. Ames"],"pdf_url":"https://arxiv.org/pdf/2509.00294v1.pdf","comment":null}]},"2025-08-29T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2503.11999v2","updated":"2025-08-29T22:59:16Z","published":"2025-03-15T05:34:26Z","title":"Diffusion Dynamics Models with Generative State Estimation for Cloth\n  Manipulation","summary":"  Cloth manipulation is challenging due to its highly complex dynamics,\nnear-infinite degrees of freedom, and frequent self-occlusions, which\ncomplicate both state estimation and dynamics modeling. Inspired by recent\nadvances in generative models, we hypothesize that these expressive models can\neffectively capture intricate cloth configurations and deformation patterns\nfrom data. Therefore, we propose a diffusion-based generative approach for both\nperception and dynamics modeling. Specifically, we formulate state estimation\nas reconstructing full cloth states from partial observations and dynamics\nmodeling as predicting future states given the current state and robot actions.\nLeveraging a transformer-based diffusion model, our method achieves accurate\nstate reconstruction and reduces long-horizon dynamics prediction errors by an\norder of magnitude compared to prior approaches. We integrate our dynamics\nmodels with model predictive control and show that our framework enables\neffective cloth folding on real robotic systems, demonstrating the potential of\ngenerative models for deformable object manipulation under partial\nobservability and complex dynamics.\n","authors":["Tongxuan Tian","Haoyang Li","Bo Ai","Xiaodi Yuan","Zhiao Huang","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2503.11999v2.pdf","comment":"CoRL 2025. Project website: https://uniclothdiff.github.io/"},{"id":"http://arxiv.org/abs/2509.00271v1","updated":"2025-08-29T22:56:32Z","published":"2025-08-29T22:56:32Z","title":"Learn from What We HAVE: History-Aware VErifier that Reasons about Past\n  Interactions Online","summary":"  We introduce a novel History-Aware VErifier (HAVE) to disambiguate uncertain\nscenarios online by leveraging past interactions. Robots frequently encounter\nvisually ambiguous objects whose manipulation outcomes remain uncertain until\nphysically interacted with. While generative models alone could theoretically\nadapt to such ambiguity, in practice they obtain suboptimal performance in\nambiguous cases, even when conditioned on action history. To address this, we\npropose explicitly decoupling action generation from verification: we use an\nunconditional diffusion-based generator to propose multiple candidate actions\nand employ our history-aware verifier to select the most promising action by\nreasoning about past interactions. Through theoretical analysis, we demonstrate\nthat employing a verifier significantly improves expected action quality.\nEmpirical evaluations and analysis across multiple simulated and real-world\nenvironments including articulated objects, multi-modal doors, and uneven\nobject pick-up confirm the effectiveness of our method and improvements over\nbaselines. Our project website is available at:\nhttps://liy1shu.github.io/HAVE_CoRL25/\n","authors":["Yishu Li","Xinyi Mao","Ying Yuan","Kyutae Sim","Ben Eisner","David Held"],"pdf_url":"https://arxiv.org/pdf/2509.00271v1.pdf","comment":"CoRL 2025"},{"id":"http://arxiv.org/abs/2508.07118v2","updated":"2025-08-29T22:14:21Z","published":"2025-08-09T23:38:17Z","title":"DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of\n  Fruit","summary":"  DexFruit is a robotic manipulation framework that enables gentle, autonomous\nhandling of fragile fruit and precise evaluation of damage. Many fruits are\nfragile and prone to bruising, thus requiring humans to manually harvest them\nwith care. In this work, we demonstrate by using optical tactile sensing,\nautonomous manipulation of fruit with minimal damage can be achieved. We show\nthat our tactile informed diffusion policies outperform baselines in both\nreduced bruising and pick-and-place success rate across three fruits:\nstrawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,\na novel technique to represent and quantify visual damage in high-resolution 3D\nrepresentation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring\ndamage lack quantitative rigor or require expensive equipment. With FruitSplat,\nwe distill a 2D strawberry mask as well as a 2D bruise segmentation mask into\nthe 3DGS representation. Furthermore, this representation is modular and\ngeneral, compatible with any relevant 2D model. Overall, we demonstrate a 92%\ngrasping policy success rate, up to a 20% reduction in visual bruising, and up\nto an 31% improvement in grasp success rate on challenging fruit compared to\nour baselines across our three tested fruits. We rigorously evaluate this\nresult with over 630 trials. Please checkout our website at\nhttps://dex-fruit.github.io .\n","authors":["Aiden Swann","Alex Qiu","Matthew Strong","Angelina Zhang","Samuel Morstein","Kai Rayle","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2508.07118v2.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.23228v2","updated":"2025-08-29T21:58:51Z","published":"2025-03-29T21:19:22Z","title":"Energy-Aware Lane Planning for Connected Electric Vehicles in Urban\n  Traffic: Design and Vehicle-in-the-Loop Validation","summary":"  Urban driving with connected and automated vehicles (CAVs) offers potential\nfor energy savings, yet most eco-driving strategies focus solely on\nlongitudinal speed control within a single lane. This neglects the significant\nimpact of lateral decisions, such as lane changes, on overall energy\nefficiency, especially in environments with traffic signals and heterogeneous\ntraffic flow. To address this gap, we propose a novel energy-aware motion\nplanning framework that jointly optimizes longitudinal speed and lateral\nlane-change decisions using vehicle-to-infrastructure (V2I) communication. Our\napproach estimates long-term energy costs using a graph-based approximation and\nsolves short-horizon optimal control problems under traffic constraints. Using\na data-driven energy model calibrated to an actual battery electric vehicle, we\ndemonstrate with vehicle-in-the-loop experiments that our method reduces motion\nenergy consumption by up to 24 percent compared to a human driver, highlighting\nthe potential of connectivity-enabled planning for sustainable urban autonomy.\n","authors":["Hansung Kim","Eric Yongkeun Choi","Eunhyek Joa","Hotae Lee","Linda Lim","Scott Moura","Francesco Borrelli"],"pdf_url":"https://arxiv.org/pdf/2503.23228v2.pdf","comment":"Accepted at 2025 IEEE Conference on Decision and Control (CDC25')"},{"id":"http://arxiv.org/abs/2507.05643v2","updated":"2025-08-29T20:35:19Z","published":"2025-07-08T03:40:27Z","title":"A Physics-Based Continuum Model for Versatile, Scalable, and Fast\n  Terramechanics Simulation","summary":"  This paper discusses Chrono's Continuous Representation Model (called herein\nChrono::CRM), a general-purpose, scalable, and efficient simulation solution\nfor terramechanics problems. Built on Chrono's Smoothed Particle Hydrodynamics\n(SPH) framework, Chrono::CRM moves beyond semi-empirical terramechanics\napproaches, e.g., Bekker-Wong/Janosi-Hanamoto, to provide a physics-based model\nable to address complex tasks such as digging, grading, as well as interaction\nwith deformable wheels and complex grouser/lug patterns. The terramechanics\nmodel is versatile in that it allows the terrain to interact with both rigid\nand flexible implements simulated via the Chrono dynamics engine. We validate\nChrono::CRM against experimental data from three physical tests, including one\ninvolving NASA's MGRU3 rover. In addition, the simulator is benchmarked against\na high-fidelity Discrete Element Method (DEM) simulation of a digging scenario\ninvolving the Regolith Advanced Surface Systems Operations Robot (RASSOR).\nBeing GPU-accelerated, Chrono::CRM achieves computational efficiency comparable\nto that of semi-empirical simulation approaches for terramechanics problems.\nThrough an ``active domains'' implementation, Chrono::CRM can handle terrain\nstretches up to 10 km long with 100 million SPH particles at near interactive\nrates, making high-fidelity off-road simulations at large scales feasible. As a\ncomponent of the Chrono package, the CRM model is open source and released\nunder a BSD-3 license. All models and simulations used in this contribution are\navailable in a public GitHub repository for reproducibility studies and further\nresearch.\n","authors":["Huzaifa Unjhawala","Luning Bakke","Harry Zhang","Michael Taylor","Ganesh Arivoli","Radu Serban","Dan Negrut"],"pdf_url":"https://arxiv.org/pdf/2507.05643v2.pdf","comment":"32 pages, 21 figures, Submitted to Journal of Terramechanics"},{"id":"http://arxiv.org/abs/2509.00218v1","updated":"2025-08-29T20:01:56Z","published":"2025-08-29T20:01:56Z","title":"Embodied AI in Social Spaces: Responsible and Adaptive Robots in Complex\n  Setting -- UKAIRS 2025 (Copy)","summary":"  This paper introduces and overviews a multidisciplinary project aimed at\ndeveloping responsible and adaptive multi-human multi-robot (MHMR) systems for\ncomplex, dynamic settings. The project integrates co-design, ethical\nframeworks, and multimodal sensing to create AI-driven robots that are\nemotionally responsive, context-aware, and aligned with the needs of diverse\nusers. We outline the project's vision, methodology, and early outcomes,\ndemonstrating how embodied AI can support sustainable, ethical, and\nhuman-centred futures.\n","authors":["Aleksandra Landowska","Aislinn D Gomez Bergin","Ayodeji O. Abioye","Jayati Deshmukh","Andriana Bouadouki","Maria Wheadon","Athina Georgara","Dominic Price","Tuyen Nguyen","Shuang Ao","Lokesh Singh","Yi Long","Raffaele Miele","Joel E. Fischer","Sarvapali D. Ramchurn"],"pdf_url":"https://arxiv.org/pdf/2509.00218v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2509.00178v1","updated":"2025-08-29T18:26:05Z","published":"2025-08-29T18:26:05Z","title":"Poke and Strike: Learning Task-Informed Exploration Policies","summary":"  In many dynamic robotic tasks, such as striking pucks into a goal outside the\nreachable workspace, the robot must first identify the relevant physical\nproperties of the object for successful task execution, as it is unable to\nrecover from failure or retry without human intervention. To address this\nchallenge, we propose a task-informed exploration approach, based on\nreinforcement learning, that trains an exploration policy using rewards\nautomatically generated from the sensitivity of a privileged task policy to\nerrors in estimated properties. We also introduce an uncertainty-based\nmechanism to determine when to transition from exploration to task execution,\nensuring sufficient property estimation accuracy with minimal exploration time.\nOur method achieves a 90% success rate on the striking task with an average\nexploration time under 1.2 seconds, significantly outperforming baselines that\nachieve at most 40% success or require inefficient querying and retraining in a\nsimulator at test time. Additionally, we demonstrate that our task-informed\nrewards capture the relative importance of physical properties in both the\nstriking task and the classical CartPole example. Finally, we validate our\napproach by demonstrating its ability to identify object properties and adjust\ntask execution in a physical setup using the KUKA iiwa robot arm.\n","authors":["Marina Y. Aoyama","Joao Moura","Juan Del Aguila Ferrandis","Sethu Vijayakumar"],"pdf_url":"https://arxiv.org/pdf/2509.00178v1.pdf","comment":"8 pages (main paper), 27 pages (including references and appendices),\n  6 figures (main paper), 21 figures (including appendices), Conference of\n  Robot Learning 2025, For videos and the project website, see\n  https://marina-aoyama.github.io/poke-and-strike/"},{"id":"http://arxiv.org/abs/2508.21800v1","updated":"2025-08-29T17:27:44Z","published":"2025-08-29T17:27:44Z","title":"Tree-Guided Diffusion Planner","summary":"  Planning with pretrained diffusion models has emerged as a promising approach\nfor solving test-time guided control problems. However, standard gradient\nguidance typically performs optimally under convex and differentiable reward\nlandscapes, showing substantially reduced effectiveness in real-world scenarios\ninvolving non-convex objectives, non-differentiable constraints, and\nmulti-reward structures. Furthermore, recent supervised planning approaches\nrequire task-specific training or value estimators, which limits test-time\nflexibility and zero-shot generalization. We propose a Tree-guided Diffusion\nPlanner (TDP), a zero-shot test-time planning framework that balances\nexploration and exploitation through structured trajectory generation. We frame\ntest-time planning as a tree search problem using a bi-level sampling process:\n(1) diverse parent trajectories are produced via training-free particle\nguidance to encourage broad exploration, and (2) sub-trajectories are refined\nthrough fast conditional denoising guided by task objectives. TDP addresses the\nlimitations of gradient guidance by exploring diverse trajectory regions and\nharnessing gradient information across this expanded solution space using only\npretrained models and test-time reward signals. We evaluate TDP on three\ndiverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze\nmulti-goal exploration. TDP consistently outperforms state-of-the-art\napproaches on all tasks. The project page can be found at:\ntree-diffusion-planner.github.io.\n","authors":["Hyeonseong Jeon","Cheolhong Min","Jaesik Park"],"pdf_url":"https://arxiv.org/pdf/2508.21800v1.pdf","comment":"20 pages, 11 figures, 14 tables (main paper + appendix) / under\n  review / project page will be available after the paper becomes public in\n  arxiv"},{"id":"http://arxiv.org/abs/2403.14488v4","updated":"2025-08-29T17:17:40Z","published":"2024-03-21T15:36:26Z","title":"COBRA-PPM: A Causal Bayesian Reasoning Architecture Using Probabilistic\n  Programming for Robot Manipulation Under Uncertainty","summary":"  Manipulation tasks require robots to reason about cause and effect when\ninteracting with objects. Yet, many data-driven approaches lack causal\nsemantics and thus only consider correlations. We introduce COBRA-PPM, a novel\ncausal Bayesian reasoning architecture that combines causal Bayesian networks\nand probabilistic programming to perform interventional inference for robot\nmanipulation under uncertainty. We demonstrate its capabilities through\nhigh-fidelity Gazebo-based experiments on an exemplar block stacking task,\nwhere it predicts manipulation outcomes with high accuracy (Pred Acc: 88.6%)\nand performs greedy next-best action selection with a 94.2% task success rate.\nWe further demonstrate sim2real transfer on a domestic robot, showing\neffectiveness in handling real-world uncertainty from sensor noise and\nstochastic actions. Our generalised and extensible framework supports a wide\nrange of manipulation scenarios and lays a foundation for future work at the\nintersection of robotics and causality.\n","authors":["Ricardo Cannizzaro","Michael Groom","Jonathan Routley","Robert Osazuwa Ness","Lars Kunze"],"pdf_url":"https://arxiv.org/pdf/2403.14488v4.pdf","comment":"8 pages, 7 figures, accepted to the 2025 IEEE European Conference on\n  Mobile Robots (ECMR 2025)"},{"id":"http://arxiv.org/abs/2408.06553v4","updated":"2025-08-29T15:17:15Z","published":"2024-08-13T01:33:19Z","title":"Centralization vs. decentralization in multi-robot sweep coverage with\n  ground robots and UAVs","summary":"  In swarm robotics, decentralized control is often proposed as a more scalable\nand fault-tolerant alternative to centralized control. However, centralized\nbehaviors are often faster and more efficient than their decentralized\ncounterparts. In any given application, the goals and constraints of the task\nbeing solved should guide the choice to use centralized control, decentralized\ncontrol, or a combination of the two. Currently, the exact trade-offs that\nexist between centralization and decentralization are not well defined. In this\npaper, we compare the performance of centralization and decentralization in the\nexample task of sweep coverage, across five different types of multi-robot\ncontrol structures: random walk, decentralized with beacons, hybrid formation\ncontrol using self-organizing hierarchy, centralized formation control, and\npredetermined. In all five approaches, the coverage task is completed by a\ngroup of ground robots. In each approach, except for the random walk, the\nground robots are assisted by UAVs, acting as supervisors or beacons. We\ncompare the approaches in terms of three performance metrics for which\ncentralized approaches are expected to have an advantage -- coverage\ncompleteness, coverage uniformity, and sweep completion time -- and two metrics\nfor which decentralized approaches are expected to have an advantage --\nscalability (4, 8, or 16 ground robots) and fault tolerance (0%, 25%, 50%, or\n75% ground robot failure).\n","authors":["Aryo Jamshidpey","Mostafa Wahby","Michael Allwright","Weixu Zhu","Marco Dorigo","Mary Katherine Heinrich"],"pdf_url":"https://arxiv.org/pdf/2408.06553v4.pdf","comment":"IRIDIA, Universite Libre de Bruxelles, Brussels, Belgium, 2021"},{"id":"http://arxiv.org/abs/2508.21690v1","updated":"2025-08-29T14:56:48Z","published":"2025-08-29T14:56:48Z","title":"Can a mobile robot learn from a pedestrian model to prevent the sidewalk\n  salsa?","summary":"  Pedestrians approaching each other on a sidewalk sometimes end up in an\nawkward interaction known as the \"sidewalk salsa\": they both (repeatedly)\ndeviate to the same side to avoid a collision. This provides an interesting use\ncase to study interactions between pedestrians and mobile robots because, in\nthe vast majority of cases, this phenomenon is avoided through a negotiation\nbased on implicit communication. Understanding how it goes wrong and how\npedestrians end up in the sidewalk salsa will therefore provide insight into\nthe implicit communication. This understanding can be used to design safe and\nacceptable robotic behaviour. In a previous attempt to gain this understanding,\na model of pedestrian behaviour based on the Communication-Enabled Interaction\n(CEI) framework was developed that can replicate the sidewalk salsa. However,\nit is unclear how to leverage this model in robotic planning and\ndecision-making since it violates the assumptions of game theory, a much-used\nframework in planning and decision-making. Here, we present a proof-of-concept\nfor an approach where a Reinforcement Learning (RL) agent leverages the model\nto learn how to interact with pedestrians. The results show that a basic RL\nagent successfully learned to interact with the CEI model. Furthermore, a\nrisk-averse RL agent that had access to the perceived risk of the CEI model\nlearned how to effectively communicate its intention through its motion and\nthereby substantially lowered the perceived risk, and displayed effort by the\nmodelled pedestrian. These results show this is a promising approach and\nencourage further exploration.\n","authors":["Olger Siebinga","David Abbink"],"pdf_url":"https://arxiv.org/pdf/2508.21690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.04619v2","updated":"2025-08-29T14:54:30Z","published":"2025-05-07T17:59:28Z","title":"Merging and Disentangling Views in Visual Reinforcement Learning for\n  Robotic Manipulation","summary":"  Vision is well-known for its use in manipulation, especially using visual\nservoing. Due to the 3D nature of the world, using multiple camera views and\nmerging them creates better representations for Q-learning and in turn, trains\nmore sample efficient policies. Nevertheless, these multi-view policies are\nsensitive to failing cameras and can be burdensome to deploy. To mitigate these\nissues, we introduce a Merge And Disentanglement (MAD) algorithm that\nefficiently merges views to increase sample efficiency while simultaneously\ndisentangling views by augmenting multi-view feature inputs with single-view\nfeatures. This produces robust policies and allows lightweight deployment. We\ndemonstrate the efficiency and robustness of our approach using Meta-World and\nManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad\n","authors":["Abdulaziz Almuzairee","Rohan Patil","Dwait Bhatt","Henrik I. Christensen"],"pdf_url":"https://arxiv.org/pdf/2505.04619v2.pdf","comment":"Accepted at CoRL 2025. For project website and code, see\n  https://aalmuzairee.github.io/mad"},{"id":"http://arxiv.org/abs/2508.21677v1","updated":"2025-08-29T14:45:54Z","published":"2025-08-29T14:45:54Z","title":"Robust Convex Model Predictive Control with collision avoidance\n  guarantees for robot manipulators","summary":"  Industrial manipulators are normally operated in cluttered environments,\nmaking safe motion planning important. Furthermore, the presence of\nmodel-uncertainties make safe motion planning more difficult. Therefore, in\npractice the speed is limited in order to reduce the effect of disturbances.\nThere is a need for control methods that can guarantee safe motions that can be\nexecuted fast. We address this need by suggesting a novel model predictive\ncontrol (MPC) solution for manipulators, where our two main components are a\nrobust tube MPC and a corridor planning algorithm to obtain collision-free\nmotion. Our solution results in a convex MPC, which we can solve fast, making\nour method practically useful. We demonstrate the efficacy of our method in a\nsimulated environment with a 6 DOF industrial robot operating in cluttered\nenvironments with uncertainties in model parameters. We outperform benchmark\nmethods, both in terms of being able to work under higher levels of model\nuncertainties, while also yielding faster motion.\n","authors":["Bernhard Wullt","Johannes Köhler","Per Mattsson","Mikeal Norrlöf","Thomas B. Schön"],"pdf_url":"https://arxiv.org/pdf/2508.21677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2506.19277v2","updated":"2025-08-29T14:30:41Z","published":"2025-06-24T03:14:51Z","title":"Ontology Neural Network and ORTSF: A Framework for Topological Reasoning\n  and Delay-Robust Control","summary":"  The advancement of autonomous robotic systems has led to impressive\ncapabilities in perception, localization, mapping, and control. Yet, a\nfundamental gap remains: existing frameworks excel at geometric reasoning and\ndynamic stability but fall short in representing and preserving relational\nsemantics, contextual reasoning, and cognitive transparency essential for\ncollaboration in dynamic, human-centric environments. This paper introduces a\nunified architecture comprising the Ontology Neural Network (ONN) and the\nOntological Real-Time Semantic Fabric (ORTSF) to address this gap. The ONN\nformalizes relational semantic reasoning as a dynamic topological process. By\nembedding Forman-Ricci curvature, persistent homology, and semantic tensor\nstructures within a unified loss formulation, ONN ensures that relational\nintegrity and topological coherence are preserved as scenes evolve over time.\nThe ORTSF transforms reasoning traces into actionable control commands while\ncompensating for system delays. It integrates predictive and delay-aware\noperators that ensure phase margin preservation and continuity of control\nsignals, even under significant latency conditions. Empirical studies\ndemonstrate the ONN + ORTSF framework's ability to unify semantic cognition and\nrobust control, providing a mathematically principled and practically viable\nsolution for cognitive robotics.\n","authors":["Jaehong Oh"],"pdf_url":"https://arxiv.org/pdf/2506.19277v2.pdf","comment":"12 pages, 5 figures, includes theoretical proofs and simulation\n  results"},{"id":"http://arxiv.org/abs/2508.21637v1","updated":"2025-08-29T14:00:45Z","published":"2025-08-29T14:00:45Z","title":"A-MHA*: Anytime Multi-Heuristic A*","summary":"  Designing good heuristic functions for graph search requires adequate domain\nknowledge. It is often easy to design heuristics that perform well and\ncorrelate with the underlying true cost-to-go values in certain parts of the\nsearch space but these may not be admissible throughout the domain thereby\naffecting the optimality guarantees of the search. Bounded suboptimal search\nusing several such partially good but inadmissible heuristics was developed in\nMulti-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible\nheuristics to potentially generate a faster suboptimal solution, the original\nversion does not improve the solution over time. It is a one shot algorithm\nthat requires careful setting of inflation factors to obtain a desired one time\nsolution. In this work, we tackle this issue by extending MHA* to an anytime\nversion that finds a feasible suboptimal solution quickly and continually\nimproves it until time runs out. Our work is inspired from the Anytime\nRepairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*\nconcepts in the MHA* framework preserves the original suboptimal and\ncompleteness guarantees and enhances MHA* to perform in an anytime fashion.\nFurthermore, we report the performance of A-MHA* in 3-D path planning domain\nand sliding tiles puzzle and compare against MHA* and other anytime algorithms.\n","authors":["Ramkumar Natarajan","Muhammad Suhail Saleem","William Xiao","Sandip Aine","Howie Choset","Maxim Likhachev"],"pdf_url":"https://arxiv.org/pdf/2508.21637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21635v1","updated":"2025-08-29T13:58:55Z","published":"2025-08-29T13:58:55Z","title":"The Rosario Dataset v2: Multimodal Dataset for Agricultural Robotics","summary":"  We present a multi-modal dataset collected in a soybean crop field,\ncomprising over two hours of recorded data from sensors such as stereo infrared\ncamera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single\nPoint Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel\nodometry. This dataset captures key challenges inherent to robotics in\nagricultural environments, including variations in natural lighting, motion\nblur, rough terrain, and long, perceptually aliased sequences. By addressing\nthese complexities, the dataset aims to support the development and\nbenchmarking of advanced algorithms for localization, mapping, perception, and\nnavigation in agricultural robotics. The platform and data collection system is\ndesigned to meet the key requirements for evaluating multi-modal SLAM systems,\nincluding hardware synchronization of sensors, 6-DOF ground truth and loops on\nlong trajectories.\n  We run multimodal state-of-the art SLAM methods on the dataset, showcasing\nthe existing limitations in their application on agricultural settings. The\ndataset and utilities to work with it are released on\nhttps://cifasis.github.io/rosariov2/.\n","authors":["Nicolas Soncini","Javier Cremona","Erica Vidal","Maximiliano García","Gastón Castro","Taihú Pire"],"pdf_url":"https://arxiv.org/pdf/2508.21635v1.pdf","comment":"First published on The International Journal of Robotics Research:\n  https://journals.sagepub.com/doi/10.1177/02783649251368909"},{"id":"http://arxiv.org/abs/2508.21592v1","updated":"2025-08-29T12:48:39Z","published":"2025-08-29T12:48:39Z","title":"Learning Agile Gate Traversal via Analytical Optimal Policy Gradient","summary":"  Traversing narrow gates presents a significant challenge and has become a\nstandard benchmark for evaluating agile and precise quadrotor flight.\nTraditional modularized autonomous flight stacks require extensive design and\nparameter tuning, while end-to-end reinforcement learning (RL) methods often\nsuffer from low sample efficiency and limited interpretability. In this work,\nwe present a novel hybrid framework that adaptively fine-tunes model predictive\ncontrol (MPC) parameters online using outputs from a neural network (NN)\ntrained offline. The NN jointly predicts a reference pose and cost-function\nweights, conditioned on the coordinates of the gate corners and the current\ndrone state. To achieve efficient training, we derive analytical policy\ngradients not only for the MPC module but also for an optimization-based gate\ntraversal detection module. Furthermore, we introduce a new formulation of the\nattitude tracking error that admits a simplified representation, facilitating\neffective learning with bounded gradients. Hardware experiments demonstrate\nthat our method enables fast and accurate quadrotor traversal through narrow\ngates in confined environments. It achieves several orders of magnitude\nimprovement in sample efficiency compared to naive end-to-end RL approaches.\n","authors":["Tianchen Sun","Bingheng Wang","Longbin Tang","Yichao Gao","Lin Zhao"],"pdf_url":"https://arxiv.org/pdf/2508.21592v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.18309v2","updated":"2025-08-29T12:39:22Z","published":"2025-01-30T12:36:36Z","title":"Knowledge in multi-robot systems: an interplay of dynamics, computation\n  and communication","summary":"  In this paper, we provide a framework integrating distributed multi-robot\nsystems and temporal epistemic logic. We show that continuous-discrete hybrid\nsystems are compatible with logical models of knowledge already used in\ndistributed computing, and demonstrate its usefulness by deriving sufficient\nepistemic conditions for exploration and gathering robot tasks to be solvable.\nWe provide a separation of the physical and computational aspects of a robotic\nsystem, allowing us to decouple the problems related to each and directly use\nmethods from control theory and distributed computing, fields that are\ntraditionally distant in the literature. Finally, we demonstrate a novel\napproach for reasoning about the knowledge in multi-robot systems through a\nprincipled method of converting a switched hybrid dynamical system into a\ntemporal-epistemic logic model, passing through an abstract state machine\nrepresentation. This creates space for methods and results to be exchanged\nacross the fields of control theory, distributed computing and\ntemporal-epistemic logic, while reasoning about multi-robot systems.\n","authors":["Giorgio Cignarale","Stephan Felber","Eric Goubault","Bernardo Hummes Flores","Hugo Rincon Galeana"],"pdf_url":"https://arxiv.org/pdf/2501.18309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.20537v2","updated":"2025-08-29T12:33:58Z","published":"2025-05-26T21:48:34Z","title":"CoRI: Communication of Robot Intent for Physical Human-Robot Interaction","summary":"  Clear communication of robot intent fosters transparency and interpretability\nin physical human-robot interaction (pHRI), particularly during assistive tasks\ninvolving direct human-robot contact. We introduce CoRI, a pipeline that\nautomatically generates natural language communication of a robot's upcoming\nactions directly from its motion plan and visual perception. Our pipeline first\nprocesses the robot's image view to identify human poses and key environmental\nfeatures. It then encodes the planned 3D spatial trajectory (including velocity\nand force) onto this view, visually grounding the path and its dynamics. CoRI\nqueries a vision-language model with this visual representation to interpret\nthe planned action within the visual context before generating concise,\nuser-directed statements, without relying on task-specific information. Results\nfrom a user study involving robot-assisted feeding, bathing, and shaving tasks\nacross two different robots indicate that CoRI leads to statistically\nsignificant difference in communication clarity compared to a baseline\ncommunication strategy. Specifically, CoRI effectively conveys not only the\nrobot's high-level intentions but also crucial details about its motion and any\ncollaborative user action needed. Video and code of our project can be found on\nour project website: https://cori-phri.github.io/\n","authors":["Junxiang Wang","Emek Barış Küçüktabak","Rana Soltani Zarrin","Zackory Erickson"],"pdf_url":"https://arxiv.org/pdf/2505.20537v2.pdf","comment":"To be published in Proceedings of the 9th Conference on Robot\n  Learning (CoRL). 34 pages, 10 figures"},{"id":"http://arxiv.org/abs/2508.21549v1","updated":"2025-08-29T12:05:01Z","published":"2025-08-29T12:05:01Z","title":"Estimated Informed Anytime Search for Sampling-Based Planning via\n  Adaptive Sampler","summary":"  Path planning in robotics often involves solving continuously valued,\nhigh-dimensional problems. Popular informed approaches include graph-based\nsearches, such as A*, and sampling-based methods, such as Informed RRT*, which\nutilize informed set and anytime strategies to expedite path optimization\nincrementally. Informed sampling-based planners define informed sets as subsets\nof the problem domain based on the current best solution cost. However, when no\nsolution is found, these planners re-sample and explore the entire\nconfiguration space, which is time-consuming and computationally expensive.\nThis article introduces Multi-Informed Trees (MIT*), a novel planner that\nconstructs estimated informed sets based on prior admissible solution costs\nbefore finding the initial solution, thereby accelerating the initial\nconvergence rate. Moreover, MIT* employs an adaptive sampler that dynamically\nadjusts the sampling strategy based on the exploration process. Furthermore,\nMIT* utilizes length-related adaptive sparse collision checks to guide lazy\nreverse search. These features enhance path cost efficiency and computation\ntimes while ensuring high success rates in confined scenarios. Through a series\nof simulations and real-world experiments, it is confirmed that MIT*\noutperforms existing single-query, sampling-based planners for problems in R^4\nto R^16 and has been successfully applied to real-world robot manipulation\ntasks. A video showcasing our experimental results is available at:\nhttps://youtu.be/30RsBIdexTU\n","authors":["Liding Zhang","Kuanqi Cai","Yu Zhang","Zhenshan Bing","Chaoqun Wang","Fan Wu","Sami Haddadin","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2508.21549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21542v1","updated":"2025-08-29T11:55:47Z","published":"2025-08-29T11:55:47Z","title":"Complete Gaussian Splats from a Single Image with Denoising Diffusion\n  Models","summary":"  Gaussian splatting typically requires dense observations of the scene and can\nfail to reconstruct occluded and unobserved areas. We propose a latent\ndiffusion model to reconstruct a complete 3D scene with Gaussian splats,\nincluding the occluded parts, from only a single image during inference.\nCompleting the unobserved surfaces of a scene is challenging due to the\nambiguity of the plausible surfaces. Conventional methods use a\nregression-based formulation to predict a single \"mode\" for occluded and\nout-of-frustum surfaces, leading to blurriness, implausibility, and failure to\ncapture multiple possible explanations. Thus, they often address this problem\npartially, focusing either on objects isolated from the background,\nreconstructing only visible surfaces, or failing to extrapolate far from the\ninput views. In contrast, we propose a generative formulation to learn a\ndistribution of 3D representations of Gaussian splats conditioned on a single\ninput image. To address the lack of ground-truth training data, we propose a\nVariational AutoReconstructor to learn a latent space only from 2D images in a\nself-supervised manner, over which a diffusion model is trained. Our method\ngenerates faithful reconstructions and diverse samples with the ability to\ncomplete the occluded surfaces for high-quality 360-degree renderings.\n","authors":["Ziwei Liao","Mohamed Sayed","Steven L. Waslander","Sara Vicente","Daniyar Turmukhambetov","Michael Firman"],"pdf_url":"https://arxiv.org/pdf/2508.21542v1.pdf","comment":"Main paper: 11 pages; Supplementary materials: 7 pages"},{"id":"http://arxiv.org/abs/2508.20661v2","updated":"2025-08-29T11:00:18Z","published":"2025-08-28T11:09:19Z","title":"Traversing the Narrow Path: A Two-Stage Reinforcement Learning Framework\n  for Humanoid Beam Walking","summary":"  Traversing narrow beams is challenging for humanoids due to sparse,\nsafety-critical contacts and the fragility of purely learned policies. We\npropose a physically grounded, two-stage framework that couples an XCoM/LIPM\nfootstep template with a lightweight residual planner and a simple low-level\ntracker. Stage-1 is trained on flat ground: the tracker learns to robustly\nfollow footstep targets by adding small random perturbations to heuristic\nfootsteps, without any hand-crafted centerline locking, so it acquires stable\ncontact scheduling and strong target-tracking robustness. Stage-2 is trained in\nsimulation on a beam: a high-level planner predicts a body-frame residual\n(Delta x, Delta y, Delta psi) for the swing foot only, refining the template\nstep to prioritize safe, precise placement under narrow support while\npreserving interpretability. To ease deployment, sensing is kept minimal and\nconsistent between simulation and hardware: the planner consumes compact,\nforward-facing elevation cues together with onboard IMU and joint signals. On a\nUnitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across\nsimulation and real-world studies, residual refinement consistently outperforms\ntemplate-only and monolithic baselines in success rate, centerline adherence,\nand safety margins, while the structured footstep interface enables transparent\nanalysis and low-friction sim-to-real transfer.\n","authors":["TianChen Huang","Wei Gao","Runchen Xu","Shiwu Zhang"],"pdf_url":"https://arxiv.org/pdf/2508.20661v2.pdf","comment":"Project website:\n  https://huangtc233.github.io/Traversing-the-Narrow-Path/"},{"id":"http://arxiv.org/abs/2508.21501v1","updated":"2025-08-29T10:30:58Z","published":"2025-08-29T10:30:58Z","title":"Few-Shot Neuro-Symbolic Imitation Learning for Long-Horizon Planning and\n  Acting","summary":"  Imitation learning enables intelligent systems to acquire complex behaviors\nwith minimal supervision. However, existing methods often focus on\nshort-horizon skills, require large datasets, and struggle to solve\nlong-horizon tasks or generalize across task variations and distribution\nshifts. We propose a novel neuro-symbolic framework that jointly learns\ncontinuous control policies and symbolic domain abstractions from a few skill\ndemonstrations. Our method abstracts high-level task structures into a graph,\ndiscovers symbolic rules via an Answer Set Programming solver, and trains\nlow-level controllers using diffusion policy imitation learning. A high-level\noracle filters task-relevant information to focus each controller on a minimal\nobservation and action space. Our graph-based neuro-symbolic framework enables\ncapturing complex state transitions, including non-spatial and temporal\nrelations, that data-driven learning or clustering techniques often fail to\ndiscover in limited demonstration datasets. We validate our approach in six\ndomains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers\nof Hanoi environments, and a distinct Automated Forklift domain with two\nenvironments. The results demonstrate high data efficiency with as few as five\nskill demonstrations, strong zero- and few-shot generalizations, and\ninterpretable decision making.\n","authors":["Pierrick Lorang","Hong Lu","Johannes Huemer","Patrik Zips","Matthias Scheutz"],"pdf_url":"https://arxiv.org/pdf/2508.21501v1.pdf","comment":"Accepted at CoRL 2025; to appear in PMLR"},{"id":"http://arxiv.org/abs/2411.14290v2","updated":"2025-08-29T09:54:44Z","published":"2024-11-21T16:36:38Z","title":"Soft Manipulation Surface With Reduced Actuator Density For\n  Heterogeneous Object Manipulation","summary":"  Object manipulation in robotics faces challenges due to diverse object\nshapes, sizes, and fragility. Gripper-based methods offer precision and low\ndegrees of freedom (DOF) but the gripper limits the kind of objects to grasp.\nOn the other hand, surface-based approaches provide flexibility for handling\nfragile and heterogeneous objects but require numerous actuators, increasing\ncomplexity. We propose new manipulation hardware that utilizes equally spaced\nlinear actuators placed vertically and connected by a soft surface. In this\nsetup, object manipulation occurs on the soft surface through coordinated\nmovements of the surrounding actuators. This approach requires fewer actuators\nto cover a large manipulation area, offering a cost-effective solution with a\nlower DOF compared to dense actuator arrays. It also effectively handles\nheterogeneous objects of varying shapes and weights, even when they are\nsignificantly smaller than the distance between actuators. This method is\nparticularly suitable for managing highly fragile objects in the food industry.\n","authors":["Pratik Ingle","Kasper Støy","Andres Faiña"],"pdf_url":"https://arxiv.org/pdf/2411.14290v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21455v1","updated":"2025-08-29T09:38:21Z","published":"2025-08-29T09:38:21Z","title":"Assessing Human Cooperation for Enhancing Social Robot Navigation","summary":"  Socially aware robot navigation is a planning paradigm where the robot\nnavigates in human environments and tries to adhere to social constraints while\ninteracting with the humans in the scene. These navigation strategies were\nfurther improved using human prediction models, where the robot takes the\npotential future trajectory of humans while computing its own. Though these\nstrategies significantly improve the robot's behavior, it faces difficulties\nfrom time to time when the human behaves in an unexpected manner. This happens\nas the robot fails to understand human intentions and cooperativeness, and the\nhuman does not have a clear idea of what the robot is planning to do. In this\npaper, we aim to address this gap through effective communication at an\nappropriate time based on a geometric analysis of the context and human\ncooperativeness in head-on crossing scenarios. We provide an assessment\nmethodology and propose some evaluation metrics that could distinguish a\ncooperative human from a non-cooperative one. Further, we also show how\ngeometric reasoning can be used to generate appropriate verbal responses or\nrobot actions.\n","authors":["Hariharan Arunachalam","Phani Teja Singamaneni","Rachid Alami"],"pdf_url":"https://arxiv.org/pdf/2508.21455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2505.23197v2","updated":"2025-08-29T08:59:50Z","published":"2025-05-29T07:34:56Z","title":"Unified Path Planner with Adaptive Safety and Optimality","summary":"  Path planning for autonomous robots presents a fundamental trade-off between\noptimality and safety. While conventional algorithms typically prioritize one\nof these objectives, we introduce the Unified Path Planner (UPP), a unified\nframework that simultaneously addresses both. UPP is a graph-search-based\nalgorithm that employs a modified heuristic function incorporating a dynamic\nsafety cost, enabling an adaptive balance between path length and obstacle\nclearance. We establish theoretical sub-optimality bounds for the planner and\ndemonstrate that its safety-to-optimality ratio can be tuned via adjustable\nparameters, with a trade-off in computational complexity. Extensive simulations\nshow that UPP achieves a high success rate, generating near-optimal paths with\nonly a negligible increase in cost over traditional A*, while ensuring safety\nmargins that closely approach those of the classical Voronoi planner. Finally,\nthe practical efficacy of UPP is validated through a hardware implementation on\na TurtleBot, confirming its ability to navigate cluttered environments by\ngenerating safe, sub-optimal paths.\n","authors":["Jatin Kumar Arora","Soutrik Bandyopadhyay","Shubhendu Bhasin"],"pdf_url":"https://arxiv.org/pdf/2505.23197v2.pdf","comment":"6 pages,4 figures"},{"id":"http://arxiv.org/abs/2508.21378v1","updated":"2025-08-29T07:47:17Z","published":"2025-08-29T07:47:17Z","title":"RoboInspector: Unveiling the Unreliability of Policy Code for\n  LLM-enabled Robotic Manipulation","summary":"  Large language models (LLMs) demonstrate remarkable capabilities in reasoning\nand code generation, enabling robotic manipulation to be initiated with just a\nsingle instruction. The LLM carries out various tasks by generating policy code\nrequired to control the robot. Despite advances in LLMs, achieving reliable\npolicy code generation remains a significant challenge due to the diverse\nrequirements of real-world tasks and the inherent complexity of user\ninstructions. In practice, different users may provide distinct instructions to\ndrive the robot for the same task, which may cause the unreliability of policy\ncode generation. To bridge this gap, we design RoboInspector, a pipeline to\nunveil and characterize the unreliability of the policy code for LLM-enabled\nrobotic manipulation from two perspectives: the complexity of the manipulation\ntask and the granularity of the instruction. We perform comprehensive\nexperiments with 168 distinct combinations of tasks, instructions, and LLMs in\ntwo prominent frameworks. The RoboInspector identifies four main unreliable\nbehaviors that lead to manipulation failure. We provide a detailed\ncharacterization of these behaviors and their underlying causes, giving insight\nfor practical development to reduce unreliability. Furthermore, we introduce a\nrefinement approach guided by failure policy code feedback that improves the\nreliability of policy code generation by up to 35% in LLM-enabled robotic\nmanipulation, evaluated in both simulation and real-world environments.\n","authors":["Chenduo Ying","Linkang Du","Peng Cheng","Yuanchao Shu"],"pdf_url":"https://arxiv.org/pdf/2508.21378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21375v1","updated":"2025-08-29T07:40:00Z","published":"2025-08-29T07:40:00Z","title":"Dynamics-Compliant Trajectory Diffusion for Super-Nominal Payload\n  Manipulation","summary":"  Nominal payload ratings for articulated robots are typically derived from\nworst-case configurations, resulting in uniform payload constraints across the\nentire workspace. This conservative approach severely underutilizes the robot's\ninherent capabilities -- our analysis demonstrates that manipulators can safely\nhandle payloads well above nominal capacity across broad regions of their\nworkspace while staying within joint angle, velocity, acceleration, and torque\nlimits. To address this gap between assumed and actual capability, we propose a\nnovel trajectory generation approach using denoising diffusion models that\nexplicitly incorporates payload constraints into the planning process. Unlike\ntraditional sampling-based methods that rely on inefficient trial-and-error,\noptimization-based methods that are prohibitively slow, or kinodynamic planners\nthat struggle with problem dimensionality, our approach generates dynamically\nfeasible joint-space trajectories in constant time that can be directly\nexecuted on physical hardware without post-processing. Experimental validation\non a 7 DoF Franka Emika Panda robot demonstrates that up to 67.6% of the\nworkspace remains accessible even with payloads exceeding 3 times the nominal\ncapacity. This expanded operational envelope highlights the importance of a\nmore nuanced consideration of payload dynamics in motion planning algorithms.\n","authors":["Anuj Pasricha","Joewie Koh","Jay Vakil","Alessandro Roncone"],"pdf_url":"https://arxiv.org/pdf/2508.21375v1.pdf","comment":"Accepted to 2025 Conference on Robot Learning [CoRL]"},{"id":"http://arxiv.org/abs/2505.03077v2","updated":"2025-08-29T07:38:19Z","published":"2025-05-06T00:09:09Z","title":"Latent Adaptive Planner for Dynamic Manipulation","summary":"  We present the Latent Adaptive Planner (LAP), a trajectory-level\nlatent-variable policy for dynamic nonprehensile manipulation (e.g., box\ncatching) that formulates planning as inference in a low-dimensional latent\nspace and is learned effectively from human demonstration videos. During\nexecution, LAP achieves real-time adaptation by maintaining a posterior over\nthe latent plan and performing variational replanning as new observations\narrive. To bridge the embodiment gap between humans and robots, we introduce a\nmodel-based proportional mapping that regenerates accurate kinematic-dynamic\njoint states and object positions from human demonstrations. Through\nchallenging box catching experiments with varying object properties, LAP\ndemonstrates superior success rates, trajectory smoothness, and energy\nefficiency by learning human-like compliant motions and adaptive behaviors.\nOverall, LAP enables dynamic manipulation with real-time adaptation and\nsuccessfully transfer across heterogeneous robot platforms using the same human\ndemonstration videos.\n","authors":["Donghun Noh","Deqian Kong","Minglu Zhao","Andrew Lizarraga","Jianwen Xie","Ying Nian Wu","Dennis Hong"],"pdf_url":"https://arxiv.org/pdf/2505.03077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21364v1","updated":"2025-08-29T07:13:17Z","published":"2025-08-29T07:13:17Z","title":"Multi-Modal Model Predictive Path Integral Control for Collision\n  Avoidance","summary":"  This paper proposes a novel approach to motion planning and decision-making\nfor automated vehicles, using a multi-modal Model Predictive Path Integral\ncontrol algorithm. The method samples with Sobol sequences around the prior\ninput and incorporates analytical solutions for collision avoidance. By\nleveraging multiple modes, the multi-modal control algorithm explores diverse\ntrajectories, such as manoeuvring around obstacles or stopping safely before\nthem, mitigating the risk of sub-optimal solutions. A non-linear single-track\nvehicle model with a Fiala tyre serves as the prediction model, and tyre force\nconstraints within the friction circle are enforced to ensure vehicle stability\nduring evasive manoeuvres. The optimised steering angle and longitudinal\nacceleration are computed to generate a collision-free trajectory and to\ncontrol the vehicle. In a high-fidelity simulation environment, we demonstrate\nthat the proposed algorithm can successfully avoid obstacles, keeping the\nvehicle stable while driving a double lane change manoeuvre on high and\nlow-friction road surfaces and occlusion scenarios with moving obstacles,\noutperforming a standard Model Predictive Path Integral approach.\n","authors":["Alberto Bertipaglia","Dariu M. Gavrila","Barys Shyrokau"],"pdf_url":"https://arxiv.org/pdf/2508.21364v1.pdf","comment":"Accepted as an oral presentation at the 29th IAVSD. August 18-22,\n  2025. Shanghai, China"},{"id":"http://arxiv.org/abs/2508.18606v2","updated":"2025-08-29T05:59:19Z","published":"2025-08-26T02:24:04Z","title":"SignLoc: Robust Localization using Navigation Signs and Public Maps","summary":"  Navigation signs and maps, such as floor plans and street maps, are widely\navailable and serve as ubiquitous aids for way-finding in human environments.\nYet, they are rarely used by robot systems. This paper presents SignLoc, a\nglobal localization method that leverages navigation signs to localize the\nrobot on publicly available maps -- specifically floor plans and OpenStreetMap\n(OSM) graphs -- without prior sensor-based mapping. SignLoc first extracts a\nnavigation graph from the input map. It then employs a probabilistic\nobservation model to match directional and locational cues from the detected\nsigns to the graph, enabling robust topo-semantic localization within a Monte\nCarlo framework. We evaluated SignLoc in diverse large-scale environments: part\nof a university campus, a shopping mall, and a hospital complex. Experimental\nresults show that SignLoc reliably localizes the robot after observing only one\nto two signs.\n","authors":["Nicky Zimmerman","Joel Loo","Ayush Agrawal","David Hsu"],"pdf_url":"https://arxiv.org/pdf/2508.18606v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2505.03729v5","updated":"2025-08-29T05:22:52Z","published":"2025-05-06T17:57:12Z","title":"Visual Imitation Enables Contextual Humanoid Control","summary":"  How can we teach humanoids to climb staircases and sit on chairs using the\nsurrounding environment context? Arguably, the simplest way is to just show\nthem-casually capture a human motion video and feed it to humanoids. We\nintroduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday\nvideos, jointly reconstructs the humans and the environment, and produces\nwhole-body control policies for humanoid robots that perform the corresponding\nskills. We demonstrate the results of our pipeline on real humanoid robots,\nshowing robust, repeatable contextual control such as staircase ascents and\ndescents, sitting and standing from chairs and benches, as well as other\ndynamic whole-body skills-all from a single policy, conditioned on the\nenvironment and global root commands. VIDEOMIMIC offers a scalable path towards\nteaching humanoids to operate in diverse real-world environments.\n","authors":["Arthur Allshire","Hongsuk Choi","Junyi Zhang","David McAllister","Anthony Zhang","Chung Min Kim","Trevor Darrell","Pieter Abbeel","Jitendra Malik","Angjoo Kanazawa"],"pdf_url":"https://arxiv.org/pdf/2505.03729v5.pdf","comment":"Project website: https://www.videomimic.net/"},{"id":"http://arxiv.org/abs/2508.02512v2","updated":"2025-08-29T04:31:22Z","published":"2025-08-04T15:18:01Z","title":"QuaDreamer: Controllable Panoramic Video Generation for Quadruped Robots","summary":"  Panoramic cameras, capturing comprehensive 360-degree environmental data, are\nsuitable for quadruped robots in surrounding perception and interaction with\ncomplex environments. However, the scarcity of high-quality panoramic training\ndata-caused by inherent kinematic constraints and complex sensor calibration\nchallenges-fundamentally limits the development of robust perception systems\ntailored to these embodied platforms. To address this issue, we propose\nQuaDreamer-the first panoramic data generation engine specifically designed for\nquadruped robots. QuaDreamer focuses on mimicking the motion paradigm of\nquadruped robots to generate highly controllable, realistic panoramic videos,\nproviding a data source for downstream tasks. Specifically, to effectively\ncapture the unique vertical vibration characteristics exhibited during\nquadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts\ncontrollable vertical signals through frequency-domain feature filtering and\nprovides high-quality prompts. To facilitate high-quality panoramic video\ngeneration under jitter signal control, we propose a Scene-Object Controller\n(SOC) that effectively manages object motion and boosts background jitter\ncontrol through the attention mechanism. To address panoramic distortions in\nwide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream\narchitecture that synergizes frequency-texture refinement for local detail\nenhancement with spatial-structure correction for global geometric consistency.\nWe further demonstrate that the generated video sequences can serve as training\ndata for the quadruped robot's panoramic visual perception model, enhancing the\nperformance of multi-object tracking in 360-degree scenes. The source code and\nmodel weights will be publicly available at\nhttps://github.com/losehu/QuaDreamer.\n","authors":["Sheng Wu","Fei Teng","Hao Shi","Qi Jiang","Kai Luo","Kaiwei Wang","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2508.02512v2.pdf","comment":"Accepted to CoRL 2025. The source code and model weights will be\n  publicly available at \\url{https://github.com/losehu/QuaDreamer"},{"id":"http://arxiv.org/abs/2508.21322v1","updated":"2025-08-29T04:24:49Z","published":"2025-08-29T04:24:49Z","title":"Robust Real-Time Coordination of CAVs: A Distributed Optimization\n  Framework under Uncertainty","summary":"  Achieving both safety guarantees and real-time performance in cooperative\nvehicle coordination remains a fundamental challenge, particularly in dynamic\nand uncertain environments. This paper presents a novel coordination framework\nthat resolves this challenge through three key innovations: 1) direct control\nof vehicles' trajectory distributions during coordination, formulated as a\nrobust cooperative planning problem with adaptive enhanced safety constraints,\nensuring a specified level of safety regarding the uncertainty of the\ninteractive trajectory, 2) a fully parallel ADMM-based distributed trajectory\nnegotiation (ADMM-DTN) algorithm that efficiently solves the optimization\nproblem while allowing configurable negotiation rounds to balance solution\nquality and computational resources, and 3) an interactive attention mechanism\nthat selectively focuses on critical interactive participants to further\nenhance computational efficiency. Both simulation results and practical\nexperiments demonstrate that our framework achieves significant advantages in\nsafety (reducing collision rates by up to 40.79\\% in various scenarios) and\nreal-time performance compared to state-of-the-art methods, while maintaining\nstrong scalability with increasing vehicle numbers. The proposed interactive\nattention mechanism further reduces the computational demand by 14.1\\%. The\nframework's effectiveness is further validated through real-world experiments\nwith unexpected dynamic obstacles, demonstrating robust coordination in complex\nenvironments. The experiment demo could be found at\nhttps://youtu.be/4PZwBnCsb6Q.\n","authors":["Haojie Bai","Yang Wang","Cong Guo","Xiongwei Zhao","Hai Zhu"],"pdf_url":"https://arxiv.org/pdf/2508.21322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21316v1","updated":"2025-08-29T02:48:41Z","published":"2025-08-29T02:48:41Z","title":"Cooperative Sensing Enhanced UAV Path-Following and Obstacle Avoidance\n  with Variable Formation","summary":"  The high mobility of unmanned aerial vehicles (UAVs) enables them to be used\nin various civilian fields, such as rescue and cargo transport. Path-following\nis a crucial way to perform these tasks while sensing and collision avoidance\nare essential for safe flight. In this paper, we investigate how to efficiently\nand accurately achieve path-following, obstacle sensing and avoidance subtasks,\nas well as their conflict-free fusion scheduling. Firstly, a high precision\ndeep reinforcement learning (DRL)-based UAV formation path-following model is\ndeveloped, and the reward function with adaptive weights is designed from the\nperspective of distance and velocity errors. Then, we use integrated sensing\nand communication (ISAC) signals to detect the obstacle and derive the\nCramer-Rao lower bound (CRLB) for obstacle sensing by information-level fusion,\nbased on which we propose the variable formation enhanced obstacle position\nestimation (VFEO) algorithm. In addition, an online obstacle avoidance scheme\nwithout pretraining is designed to solve the sparse reward. Finally, with the\naid of null space based (NSB) behavioral method, we present a hierarchical\nsubtasks fusion strategy. Simulation results demonstrate the effectiveness and\nsuperiority of the subtask algorithms and the hierarchical fusion strategy.\n","authors":["Changheng Wang","Zhiqing Wei","Wangjun Jiang","Haoyue Jiang","Zhiyong Feng"],"pdf_url":"https://arxiv.org/pdf/2508.21316v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.20982v2","updated":"2025-08-29T02:26:05Z","published":"2025-08-28T16:37:00Z","title":"UltraTac: Integrated Ultrasound-Augmented Visuotactile Sensor for\n  Enhanced Robotic Perception","summary":"  Visuotactile sensors provide high-resolution tactile information but are\nincapable of perceiving the material features of objects. We present UltraTac,\nan integrated sensor that combines visuotactile imaging with ultrasound sensing\nthrough a coaxial optoacoustic architecture. The design shares structural\ncomponents and achieves consistent sensing regions for both modalities.\nAdditionally, we incorporate acoustic matching into the traditional\nvisuotactile sensor structure, enabling integration of the ultrasound sensing\nmodality without compromising visuotactile performance. Through tactile\nfeedback, we dynamically adjust the operating state of the ultrasound module to\nachieve flexible functional coordination. Systematic experiments demonstrate\nthree key capabilities: proximity sensing in the 3-8 cm range ($R^2=0.90$),\nmaterial classification (average accuracy: 99.20%), and texture-material\ndual-mode object recognition achieving 92.11% accuracy on a 15-class task.\nFinally, we integrate the sensor into a robotic manipulation system to\nconcurrently detect container surface patterns and internal content, which\nverifies its potential for advanced human-machine interaction and precise\nrobotic manipulation.\n","authors":["Junhao Gong","Kit-Wa Sou","Shoujie Li","Changqing Guo","Yan Huang","Chuqiao Lyu","Ziwu Song","Wenbo Ding"],"pdf_url":"https://arxiv.org/pdf/2508.20982v2.pdf","comment":"Accepted to IROS 2025"},{"id":"http://arxiv.org/abs/2508.21309v1","updated":"2025-08-29T02:22:29Z","published":"2025-08-29T02:22:29Z","title":"Observability-driven Assignment of Heterogeneous Sensors for\n  Multi-Target Tracking","summary":"  This paper addresses the challenge of assigning heterogeneous sensors (i.e.,\nrobots with varying sensing capabilities) for multi-target tracking. We\nclassify robots into two categories: (1) sufficient sensing robots, equipped\nwith range and bearing sensors, capable of independently tracking targets, and\n(2) limited sensing robots, which are equipped with only range or bearing\nsensors and need to at least form a pair to collaboratively track a target. Our\nobjective is to optimize tracking quality by minimizing uncertainty in target\nstate estimation through efficient robot-to-target assignment. By leveraging\nmatroid theory, we propose a greedy assignment algorithm that dynamically\nallocates robots to targets to maximize tracking quality. The algorithm\nguarantees constant-factor approximation bounds of 1/3 for arbitrary tracking\nquality functions and 1/2 for submodular functions, while maintaining\npolynomial-time complexity. Extensive simulations demonstrate the algorithm's\neffectiveness in accurately estimating and tracking targets over extended\nperiods. Furthermore, numerical results confirm that the algorithm's\nperformance is close to that of the optimal assignment, highlighting its\nrobustness and practical applicability.\n","authors":["Seyed Ali Rakhshan","Mehdi Golestani","He Kong"],"pdf_url":"https://arxiv.org/pdf/2508.21309v1.pdf","comment":"This paper has been accepted to the 2025 IEEE/RSJ IROS"},{"id":"http://arxiv.org/abs/2505.05753v2","updated":"2025-08-29T01:37:51Z","published":"2025-05-09T03:25:43Z","title":"Towards Embodiment Scaling Laws in Robot Locomotion","summary":"  Cross-embodiment generalization underpins the vision of building generalist\nembodied agents for any robot, yet its enabling factors remain poorly\nunderstood. We investigate embodiment scaling laws, the hypothesis that\nincreasing the number of training embodiments improves generalization to unseen\nones, using robot locomotion as a test bed. We procedurally generate ~1,000\nembodiments with topological, geometric, and joint-level kinematic variations,\nand train policies on random subsets. We observe positive scaling trends\nsupporting the hypothesis, and find that embodiment scaling enables\nsubstantially broader generalization than data scaling on fixed embodiments.\nOur best policy, trained on the full dataset, transfers zero-shot to novel\nembodiments in simulation and the real world, including the Unitree Go2 and H1.\nThese results represent a step toward general embodied intelligence, with\nrelevance to adaptive control for configurable robots, morphology co-design,\nand beyond.\n","authors":["Bo Ai","Liu Dai","Nico Bohlinger","Dichen Li","Tongzhou Mu","Zhanxin Wu","K. Fay","Henrik I. Christensen","Jan Peters","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2505.05753v2.pdf","comment":"Conference on Robot Learning (CoRL), 2025. Project website:\n  https://embodiment-scaling-laws.github.io/"},{"id":"http://arxiv.org/abs/2505.16084v2","updated":"2025-08-29T00:57:31Z","published":"2025-05-21T23:56:11Z","title":"Motion Priors Reimagined: Adapting Flat-Terrain Skills for Complex\n  Quadruped Mobility","summary":"  Reinforcement learning (RL)-based motion imitation methods trained on\ndemonstration data can effectively learn natural and expressive motions with\nminimal reward engineering but often struggle to generalize to novel\nenvironments. We address this by proposing a hierarchical RL framework in which\na low-level policy is first pre-trained to imitate animal motions on flat\nground, thereby establishing motion priors. A subsequent high-level,\ngoal-conditioned policy then builds on these priors, learning residual\ncorrections that enable perceptive locomotion, local obstacle avoidance, and\ngoal-directed navigation across diverse and rugged terrains. Simulation\nexperiments illustrate the effectiveness of learned residuals in adapting to\nprogressively challenging uneven terrains while still preserving the locomotion\ncharacteristics provided by the motion priors. Furthermore, our results\ndemonstrate improvements in motion regularization over baseline models trained\nwithout motion priors under similar reward setups. Real-world experiments with\nan ANYmal-D quadruped robot confirm our policy's capability to generalize\nanimal-like locomotion skills to complex terrains, demonstrating smooth and\nefficient locomotion and local navigation performance amidst challenging\nterrains with obstacles.\n","authors":["Zewei Zhang","Chenhao Li","Takahiro Miki","Marco Hutter"],"pdf_url":"https://arxiv.org/pdf/2505.16084v2.pdf","comment":"Conference on Robot Learning (CoRL)"},{"id":"http://arxiv.org/abs/2508.21278v1","updated":"2025-08-29T00:47:28Z","published":"2025-08-29T00:47:28Z","title":"Detecting Domain Shifts in Myoelectric Activations: Challenges and\n  Opportunities in Stream Learning","summary":"  Detecting domain shifts in myoelectric activations poses a significant\nchallenge due to the inherent non-stationarity of electromyography (EMG)\nsignals. This paper explores the detection of domain shifts using data stream\n(DS) learning techniques, focusing on the DB6 dataset from the Ninapro\ndatabase. We define domains as distinct time-series segments based on different\nsubjects and recording sessions, applying Kernel Principal Component Analysis\n(KPCA) with a cosine kernel to pre-process and highlight these shifts. By\nevaluating multiple drift detection methods such as CUSUM, Page-Hinckley, and\nADWIN, we reveal the limitations of current techniques in achieving high\nperformance for real-time domain shift detection in EMG signals. Our results\nunderscore the potential of streaming-based approaches for maintaining stable\nEMG decoding models, while highlighting areas for further research to enhance\nrobustness and accuracy in real-world scenarios.\n","authors":["Yibin Sun","Nick Lim","Guilherme Weigert Cassales","Heitor Murilo Gomes","Bernhard Pfahringer","Albert Bifet","Anany Dwivedi"],"pdf_url":"https://arxiv.org/pdf/2508.21278v1.pdf","comment":"16 pages, 5 figures, 1 table, PRICAI25"},{"id":"http://arxiv.org/abs/2508.21272v1","updated":"2025-08-29T00:27:03Z","published":"2025-08-29T00:27:03Z","title":"Learning to Assemble the Soma Cube with Legal-Action Masked DQN and Safe\n  ZYZ Regrasp on a Doosan M0609","summary":"  This paper presents the first comprehensive application of legal-action\nmasked Deep Q-Networks with safe ZYZ regrasp strategies to an underactuated\ngripper-equipped 6-DOF collaborative robot for autonomous Soma cube assembly\nlearning. Our approach represents the first systematic integration of\nconstraint-aware reinforcement learning with singularity-safe motion planning\non a Doosan M0609 collaborative robot. We address critical challenges in\nrobotic manipulation: combinatorial action space explosion, unsafe motion\nplanning, and systematic assembly strategy learning. Our system integrates a\nlegal-action masked DQN with hierarchical architecture that decomposes\nQ-function estimation into orientation and position components, reducing\ncomputational complexity from $O(3,132)$ to $O(116) + O(27)$ while maintaining\nsolution completeness. The robot-friendly reward function encourages\nground-first, vertically accessible assembly sequences aligned with\nmanipulation constraints. Curriculum learning across three progressive\ndifficulty levels (2-piece, 3-piece, 7-piece) achieves remarkable training\nefficiency: 100\\% success rate for Level 1 within 500 episodes, 92.9\\% for\nLevel 2, and 39.9\\% for Level 3 over 105,300 total training episodes.\n","authors":["Jaehong Oh","Seungjun Jung","Sawoong Kim"],"pdf_url":"https://arxiv.org/pdf/2508.21272v1.pdf","comment":"13 figures, 17 pages"},{"id":"http://arxiv.org/abs/2508.21271v1","updated":"2025-08-29T00:21:34Z","published":"2025-08-29T00:21:34Z","title":"Mini Autonomous Car Driving based on 3D Convolutional Neural Networks","summary":"  Autonomous driving applications have become increasingly relevant in the\nautomotive industry due to their potential to enhance vehicle safety,\nefficiency, and user experience, thereby meeting the growing demand for\nsophisticated driving assistance features. However, the development of reliable\nand trustworthy autonomous systems poses challenges such as high complexity,\nprolonged training periods, and intrinsic levels of uncertainty. Mini\nAutonomous Cars (MACs) are used as a practical testbed, enabling validation of\nautonomous control methodologies on small-scale setups. This simplified and\ncost-effective environment facilitates rapid evaluation and comparison of\nmachine learning models, which is particularly useful for algorithms requiring\nonline training. To address these challenges, this work presents a methodology\nbased on RGB-D information and three-dimensional convolutional neural networks\n(3D CNNs) for MAC autonomous driving in simulated environments. We evaluate the\nproposed approach against recurrent neural networks (RNNs), with architectures\ntrained and tested on two simulated tracks with distinct environmental\nfeatures. Performance was assessed using task completion success, lap-time\nmetrics, and driving consistency. Results highlight how architectural\nmodifications and track complexity influence the models' generalization\ncapability and vehicle control performance. The proposed 3D CNN demonstrated\npromising results when compared with RNNs.\n","authors":["Pablo Moraes","Monica Rodriguez","Kristofer S. Kappel","Hiago Sodre","Santiago Fernandez","Igor Nunes","Bruna Guterres","Ricardo Grando"],"pdf_url":"https://arxiv.org/pdf/2508.21271v1.pdf","comment":null}]},"2025-08-28T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2508.21260v1","updated":"2025-08-28T23:17:42Z","published":"2025-08-28T23:17:42Z","title":"Remarks on stochastic cloning and delayed-state filtering","summary":"  Many estimation problems in robotics and navigation involve measurements that\ndepend on prior states. A prominent example is odometry, which measures the\nrelative change between states over time. Accurately handling these\ndelayed-state measurements requires capturing their correlations with prior\nstate estimates, and a widely used approach is stochastic cloning (SC), which\naugments the state vector to account for these correlations.\n  This work revisits a long-established but often overlooked alternative--the\ndelayed-state Kalman filter--and demonstrates that a properly derived filter\nyields exactly the same state and covariance update as SC, without requiring\nstate augmentation. Moreover, the generalized Kalman filter formulation\nprovides computational advantages, while also reducing memory requirements for\nhigher-dimensional states.\n  Our findings clarify a common misconception that Kalman filter variants are\ninherently unable to handle correlated delayed-state measurements,\ndemonstrating that an alternative formulation achieves the same results more\nefficiently.\n","authors":["Tara Mina","Lindsey Marinello","John Christian"],"pdf_url":"https://arxiv.org/pdf/2508.21260v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21221v1","updated":"2025-08-28T21:24:04Z","published":"2025-08-28T21:24:04Z","title":"Uncertainty-Aware Ankle Exoskeleton Control","summary":"  Lower limb exoskeletons show promise to assist human movement, but their\nutility is limited by controllers designed for discrete, predefined actions in\ncontrolled environments, restricting their real-world applicability. We present\nan uncertainty-aware control framework that enables ankle exoskeletons to\noperate safely across diverse scenarios by automatically disengaging when\nencountering unfamiliar movements. Our approach uses an uncertainty estimator\nto classify movements as similar (in-distribution) or different\n(out-of-distribution) relative to actions in the training set. We evaluated\nthree architectures (model ensembles, autoencoders, and generative adversarial\nnetworks) on an offline dataset and tested the strongest performing\narchitecture (ensemble of gait phase estimators) online. The online test\ndemonstrated the ability of our uncertainty estimator to turn assistance on and\noff as the user transitioned between in-distribution and out-of-distribution\ntasks (F1: 89.2). This new framework provides a path for exoskeletons to safely\nand autonomously support human movement in unstructured, everyday environments.\n","authors":["Fatima Mumtaza Tourk","Bishoy Galoaa","Sanat Shajan","Aaron J. Young","Michael Everett","Max K. Shepherd"],"pdf_url":"https://arxiv.org/pdf/2508.21221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.11849v2","updated":"2025-08-28T21:09:41Z","published":"2025-08-16T00:13:24Z","title":"LocoMamba: Vision-Driven Locomotion via End-to-End Deep Reinforcement\n  Learning with Mamba","summary":"  We introduce LocoMamba, a vision-driven cross-modal DRL framework built on\nselective state-space models, specifically leveraging Mamba, that achieves\nnear-linear-time sequence modeling, effectively captures long-range\ndependencies, and enables efficient training with longer sequences. First, we\nembed proprioceptive states with a multilayer perceptron and patchify depth\nimages with a lightweight convolutional neural network, producing compact\ntokens that improve state representation. Second, stacked Mamba layers fuse\nthese tokens via near-linear-time selective scanning, reducing latency and\nmemory footprint, remaining robust to token length and image resolution, and\nproviding an inductive bias that mitigates overfitting. Third, we train the\npolicy end-to-end with Proximal Policy Optimization under terrain and\nappearance randomization and an obstacle-density curriculum, using a compact\nstate-centric reward that balances progress, smoothness, and safety. We\nevaluate our method in challenging simulated environments with static and\nmoving obstacles as well as uneven terrain. Compared with state-of-the-art\nbaselines, our method achieves higher returns and success rates with fewer\ncollisions, exhibits stronger generalization to unseen terrains and obstacle\ndensities, and improves training efficiency by converging in fewer updates\nunder the same compute budget.\n","authors":["Yinuo Wang","Gavin Tao"],"pdf_url":"https://arxiv.org/pdf/2508.11849v2.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2508.21205v1","updated":"2025-08-28T20:47:33Z","published":"2025-08-28T20:47:33Z","title":"Multi-robot Path Planning and Scheduling via Model Predictive Optimal\n  Transport (MPC-OT)","summary":"  In this paper, we propose a novel methodology for path planning and\nscheduling for multi-robot navigation that is based on optimal transport theory\nand model predictive control. We consider a setup where $N$ robots are tasked\nto navigate to $M$ targets in a common space with obstacles. Mapping robots to\ntargets first and then planning paths can result in overlapping paths that lead\nto deadlocks. We derive a strategy based on optimal transport that not only\nprovides minimum cost paths from robots to targets but also guarantees\nnon-overlapping trajectories. We achieve this by discretizing the space of\ninterest into $K$ cells and by imposing a ${K\\times K}$ cost structure that\ndescribes the cost of transitioning from one cell to another. Optimal transport\nthen provides \\textit{optimal and non-overlapping} cell transitions for the\nrobots to reach the targets that can be readily deployed without any scheduling\nconsiderations. The proposed solution requires $\\unicode{x1D4AA}(K^3\\log K)$\ncomputations in the worst-case and $\\unicode{x1D4AA}(K^2\\log K)$ for\nwell-behaved problems. To further accommodate potentially overlapping\ntrajectories (unavoidable in certain situations) as well as robot dynamics, we\nshow that a temporal structure can be integrated into optimal transport with\nthe help of \\textit{replans} and \\textit{model predictive control}.\n","authors":["Usman A. Khan","Mouhacine Benosman","Wenliang Liu","Federico Pecora","Joseph W. Durham"],"pdf_url":"https://arxiv.org/pdf/2508.21205v1.pdf","comment":"2025 IEEE Conference on Decision and Control"},{"id":"http://arxiv.org/abs/2503.20957v2","updated":"2025-08-28T20:27:16Z","published":"2025-03-26T19:45:18Z","title":"Pellet-based 3D Printing of Soft Thermoplastic Elastomeric Membranes for\n  Soft Robotic Applications","summary":"  Additive Manufacturing (AM) is a promising solution for handling the\ncomplexity of fabricating soft robots. However, the AM of hyperelastic\nmaterials is still challenging with a limited material range. Within this work,\npellet-based 3D printing of very soft thermoplastic elastomers (TPEs) was\nexplored (down to Shore Hardness 00-30). Our results show that TPEs can have\nsimilar engineering stress and maximum elongation as Ecoflex OO-10. In\naddition, we 3D-printed airtight thin membranes (0.2-1.2 mm), which could\ninflate up to a stretch of 1320%. Combining the membrane's large expansion and\nsoftness with the 3D printing of hollow structures simplified the design of a\nbending actuator that can bend 180 degrees and reach a blocked force of 238\ntimes its weight. In addition, by 3D printing TPE pellets and rigid filaments,\nthe soft membrane could grasp objects by enveloping an object or as a\nsensorized sucker, which relied on the TPE's softness to conform to the object\nor act as a seal. In addition, the membrane of the sucker acted as a tactile\nsensor to detect an object before adhesion. These results suggest the\nfeasibility of AM of soft robots using soft TPEs and membranes as a promising\nclass of materials and sensorized actuators, respectively.\n","authors":["Nick Willemstein","Mohammad Ebrahim Imanian","Herman van der Kooij","Ali Sadeghi"],"pdf_url":"https://arxiv.org/pdf/2503.20957v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.12395v2","updated":"2025-08-28T19:33:11Z","published":"2025-08-17T15:15:16Z","title":"PUB: A Plasma-Propelled Ultra-Quiet Blimp with Two-DOF Vector Thrusting","summary":"  This study presents the design and control of a Plasma-propelled\nUltra-silence Blimp (PUB), a novel aerial robot employing plasma vector\npropulsion for ultra-quiet flight without mechanical propellers. The system\nutilizes a helium-lift platform for extended endurance and a four-layer ring\nasymmetric capacitor to generate ionic wind thrust. The modular propulsion\nunits allow flexible configuration to meet mission-specific requirements, while\na two-degree-of-freedom (DOF) head enables thrust vector control. A closed-loop\nslip control scheme is implemented for stable maneuvering. Flight experiments\ndemonstrate full-envelope capability, including take-off, climb, hover,\ndescent, and smooth landing, confirming the feasibility of plasma vector\npropulsion, the effectiveness of DOF vector control, and the stability of the\ncontrol system. Owing to its low acoustic signature, structural simplicity, and\nhigh maneuverability, PUB is well suited for noise-sensitive, enclosed, and\nnear-space applications.\n","authors":["Zihan Wang"],"pdf_url":"https://arxiv.org/pdf/2508.12395v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2507.08656v2","updated":"2025-08-28T18:57:31Z","published":"2025-07-11T14:59:59Z","title":"Multi-critic Learning for Whole-body End-effector Twist Tracking","summary":"  Learning whole-body control for locomotion and arm motions in a single policy\nhas challenges, as the two tasks have conflicting goals. For instance,\nefficient locomotion typically favors a horizontal base orientation, while\nend-effector tracking may benefit from base tilting to extend reachability.\nAdditionally, current Reinforcement Learning (RL) approaches using a pose-based\ntask specification lack the ability to directly control the end-effector\nvelocity, making smoothly executing trajectories very challenging. To address\nthese limitations, we propose an RL-based framework that allows for dynamic,\nvelocity-aware whole-body end-effector control. Our method introduces a\nmulti-critic actor architecture that decouples the reward signals for\nlocomotion and manipulation, simplifying reward tuning and allowing the policy\nto resolve task conflicts more effectively. Furthermore, we design a\ntwist-based end-effector task formulation that can track both discrete poses\nand motion trajectories. We validate our approach through a set of simulation\nand hardware experiments using a quadruped robot equipped with a robotic arm.\nThe resulting controller can simultaneously walk and move its end-effector and\nshows emergent whole-body behaviors, where the base assists the arm in\nextending the workspace, despite a lack of explicit formulations. Videos and\nsupplementary material can be found at multi-critic-locomanipulation.github.io.\n","authors":["Aravind Elanjimattathil Vijayan","Andrei Cramariuc","Mattia Risiglione","Christian Gehring","Marco Hutter"],"pdf_url":"https://arxiv.org/pdf/2507.08656v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21163v1","updated":"2025-08-28T18:55:16Z","published":"2025-08-28T18:55:16Z","title":"Observer Design for Optical Flow-Based Visual-Inertial Odometry with\n  Almost-Global Convergence","summary":"  This paper presents a novel cascaded observer architecture that combines\noptical flow and IMU measurements to perform continuous monocular\nvisual-inertial odometry (VIO). The proposed solution estimates body-frame\nvelocity and gravity direction simultaneously by fusing velocity direction\ninformation from optical flow measurements with gyro and accelerometer data.\nThis fusion is achieved using a globally exponentially stable Riccati observer,\nwhich operates under persistently exciting translational motion conditions. The\nestimated gravity direction in the body frame is then employed, along with an\noptional magnetometer measurement, to design a complementary observer on\n$\\mathbf{SO}(3)$ for attitude estimation. The resulting interconnected observer\narchitecture is shown to be almost globally asymptotically stable. To extract\nthe velocity direction from sparse optical flow data, a gradient descent\nalgorithm is developed to solve a constrained minimization problem on the unit\nsphere. The effectiveness of the proposed algorithms is validated through\nsimulation results.\n","authors":["Tarek Bouazza","Soulaimane Berkane","Minh-Duc Hua","Tarek Hamel"],"pdf_url":"https://arxiv.org/pdf/2508.21163v1.pdf","comment":"8 pages, 6 figures. To appear in IEEE CDC 2025"},{"id":"http://arxiv.org/abs/2509.00119v1","updated":"2025-08-28T18:45:29Z","published":"2025-08-28T18:45:29Z","title":"A Comparative Study of Spline-Based Trajectory Reconstruction Methods\n  Across Varying Automatic Vehicle Location Data Densities","summary":"  Automatic vehicle location (AVL) data offers insights into transit dynamics,\nbut its effectiveness is often hampered by inconsistent update frequencies,\nnecessitating trajectory reconstruction. This research evaluates 13 trajectory\nreconstruction methods, including several novel approaches, using\nhigh-resolution AVL data from Austin, Texas. We examine the interplay of four\ncritical factors -- velocity, position, smoothing, and data density -- on\nreconstruction performance. A key contribution of this study is evaluation of\nthese methods across sparse and dense datasets, providing insights into the\ntrade-off between accuracy and resource allocation. Our evaluation framework\ncombines traditional mathematical error metrics for positional and velocity\nwith practical considerations, such as physical realism (e.g., aligning\nvelocity and acceleration with stopped states, deceleration rates, and speed\nvariability). In addition, we provide insight into the relative value of each\nmethod in calculating realistic metrics for infrastructure evaluations. Our\nfindings indicate that velocity-aware methods consistently outperform\nposition-only approaches. Interestingly, we discovered that smoothing-based\nmethods can degrade overall performance in complex, congested urban\nenvironments, although enforcing monotonicity remains critical. The velocity\nconstrained Hermite interpolation with monotonicity enforcement (VCHIP-ME)\nyields optimal results, offering a balance between high accuracy and\ncomputational efficiency. Its minimal overhead makes it suitable for both\nhistorical analysis and real-time applications, providing significant\npredictive power when combined with dense datasets. These findings offer\npractical guidance for researchers and practitioners implementing trajectory\nreconstruction systems and emphasize the importance of investing in\nhigher-frequency AVL data collection for improved analysis.\n","authors":["Jake Robbennolt","Sirajum Munira","Stephen D. Boyles"],"pdf_url":"https://arxiv.org/pdf/2509.00119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.19953v2","updated":"2025-08-28T18:28:29Z","published":"2025-08-27T15:05:49Z","title":"Divide, Discover, Deploy: Factorized Skill Learning with Symmetry and\n  Style Priors","summary":"  Unsupervised Skill Discovery (USD) allows agents to autonomously learn\ndiverse behaviors without task-specific rewards. While recent USD methods have\nshown promise, their application to real-world robotics remains underexplored.\nIn this paper, we propose a modular USD framework to address the challenges in\nthe safety, interpretability, and deployability of the learned skills. Our\napproach employs user-defined factorization of the state space to learn\ndisentangled skill representations. It assigns different skill discovery\nalgorithms to each factor based on the desired intrinsic reward function. To\nencourage structured morphology-aware skills, we introduce symmetry-based\ninductive biases tailored to individual factors. We also incorporate a style\nfactor and regularization penalties to promote safe and robust behaviors. We\nevaluate our framework in simulation using a quadrupedal robot and demonstrate\nzero-shot transfer of the learned skills to real hardware. Our results show\nthat factorization and symmetry lead to the discovery of structured\nhuman-interpretable behaviors, while the style factor and penalties enhance\nsafety and diversity. Additionally, we show that the learned skills can be used\nfor downstream tasks and perform on par with oracle policies trained with\nhand-crafted rewards.\n","authors":["Rafael Cathomen","Mayank Mittal","Marin Vlastelica","Marco Hutter"],"pdf_url":"https://arxiv.org/pdf/2508.19953v2.pdf","comment":"Accepted to CoRL 2025. For code and videos, please check:\n  https://leggedrobotics.github.io/d3-skill-discovery/"},{"id":"http://arxiv.org/abs/2508.21065v1","updated":"2025-08-28T17:59:34Z","published":"2025-08-28T17:59:34Z","title":"Learning on the Fly: Rapid Policy Adaptation via Differentiable\n  Simulation","summary":"  Learning control policies in simulation enables rapid, safe, and\ncost-effective development of advanced robotic capabilities. However,\ntransferring these policies to the real world remains difficult due to the\nsim-to-real gap, where unmodeled dynamics and environmental disturbances can\ndegrade policy performance. Existing approaches, such as domain randomization\nand Real2Sim2Real pipelines, can improve policy robustness, but either struggle\nunder out-of-distribution conditions or require costly offline retraining. In\nthis work, we approach these problems from a different perspective. Instead of\nrelying on diverse training conditions before deployment, we focus on rapidly\nadapting the learned policy in the real world in an online fashion. To achieve\nthis, we propose a novel online adaptive learning framework that unifies\nresidual dynamics learning with real-time policy adaptation inside a\ndifferentiable simulation. Starting from a simple dynamics model, our framework\nrefines the model continuously with real-world data to capture unmodeled\neffects and disturbances such as payload changes and wind. The refined dynamics\nmodel is embedded in a differentiable simulation framework, enabling gradient\nbackpropagation through the dynamics and thus rapid, sample-efficient policy\nupdates beyond the reach of classical RL methods like PPO. All components of\nour system are designed for rapid adaptation, enabling the policy to adjust to\nunseen disturbances within 5 seconds of training. We validate the approach on\nagile quadrotor control under various disturbances in both simulation and the\nreal world. Our framework reduces hovering error by up to 81% compared to\nL1-MPC and 55% compared to DATT, while also demonstrating robustness in\nvision-based control without explicit state estimation.\n","authors":["Jiahe Pan","Jiaxu Xing","Rudolf Reiter","Yifan Zhai","Elie Aljalbout","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2508.21065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.21063v1","updated":"2025-08-28T17:59:05Z","published":"2025-08-28T17:59:05Z","title":"Prompt-to-Product: Generative Assembly via Bimanual Manipulation","summary":"  Creating assembly products demands significant manual effort and expert\nknowledge in 1) designing the assembly and 2) constructing the product. This\npaper introduces Prompt-to-Product, an automated pipeline that generates\nreal-world assembly products from natural language prompts. Specifically, we\nleverage LEGO bricks as the assembly platform and automate the process of\ncreating brick assembly structures. Given the user design requirements,\nPrompt-to-Product generates physically buildable brick designs, and then\nleverages a bimanual robotic system to construct the real assembly products,\nbringing user imaginations into the real world. We conduct a comprehensive user\nstudy, and the results demonstrate that Prompt-to-Product significantly lowers\nthe barrier and reduces manual effort in creating assembly products from\nimaginative ideas.\n","authors":["Ruixuan Liu","Philip Huang","Ava Pun","Kangle Deng","Shobhit Aggarwal","Kevin Tang","Michelle Liu","Deva Ramanan","Jun-Yan Zhu","Jiaoyang Li","Changliu Liu"],"pdf_url":"https://arxiv.org/pdf/2508.21063v1.pdf","comment":"12 pages, 10 figures, 2 tables"},{"id":"http://arxiv.org/abs/2508.21046v1","updated":"2025-08-28T17:50:58Z","published":"2025-08-28T17:50:58Z","title":"CogVLA: Cognition-Aligned Vision-Language-Action Model via\n  Instruction-Driven Routing & Sparsification","summary":"  Recent Vision-Language-Action (VLA) models built on pre-trained\nVision-Language Models (VLMs) require extensive post-training, resulting in\nhigh computational overhead that limits scalability and deployment.We propose\nCogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages\ninstruction-driven routing and sparsification to improve both efficiency and\nperformance. CogVLA draws inspiration from human multimodal coordination and\nintroduces a 3-stage progressive architecture. 1) Encoder-FiLM based\nAggregation Routing (EFA-Routing) injects instruction information into the\nvision encoder to selectively aggregate and compress dual-stream visual tokens,\nforming a instruction-aware latent representation. 2) Building upon this\ncompact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)\nintroduces action intent into the language model by pruning\ninstruction-irrelevant visually grounded tokens, thereby achieving token-level\nsparsity. 3) To ensure that compressed perception inputs can still support\naccurate and coherent action generation, we introduce V-L-A Coupled Attention\n(CAtten), which combines causal vision-language attention with bidirectional\naction parallel decoding. Extensive experiments on the LIBERO benchmark and\nreal-world robotic tasks demonstrate that CogVLA achieves state-of-the-art\nperformance with success rates of 97.4% and 70.0%, respectively, while reducing\ntraining costs by 2.5-fold and decreasing inference latency by 2.8-fold\ncompared to OpenVLA. CogVLA is open-sourced and publicly available at\nhttps://github.com/JiuTian-VL/CogVLA.\n","authors":["Wei Li","Renshan Zhang","Rui Shao","Jie He","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2508.21046v1.pdf","comment":"23 pages, 8 figures, Project Page:\n  https://jiutian-vl.github.io/CogVLA-page"},{"id":"http://arxiv.org/abs/2508.21007v1","updated":"2025-08-28T17:09:05Z","published":"2025-08-28T17:09:05Z","title":"Rapid Mismatch Estimation via Neural Network Informed Variational\n  Inference","summary":"  With robots increasingly operating in human-centric environments, ensuring\nsoft and safe physical interactions, whether with humans, surroundings, or\nother machines, is essential. While compliant hardware can facilitate such\ninteractions, this work focuses on impedance controllers that allow\ntorque-controlled robots to safely and passively respond to contact while\naccurately executing tasks. From inverse dynamics to quadratic\nprogramming-based controllers, the effectiveness of these methods relies on\naccurate dynamics models of the robot and the object it manipulates. Any model\nmismatch results in task failures and unsafe behaviors. Thus, we introduce\nRapid Mismatch Estimation (RME), an adaptive, controller-agnostic,\nprobabilistic framework that estimates end-effector dynamics mismatches online,\nwithout relying on external force-torque sensors. From the robot's\nproprioceptive feedback, a Neural Network Model Mismatch Estimator generates a\nprior for a Variational Inference solver, which rapidly converges to the\nunknown parameters while quantifying uncertainty. With a real 7-DoF manipulator\ndriven by a state-of-the-art passive impedance controller, RME adapts to sudden\nchanges in mass and center of mass at the end-effector in $\\sim400$ ms, in\nstatic and dynamic settings. We demonstrate RME in a collaborative scenario\nwhere a human attaches an unknown basket to the robot's end-effector and\ndynamically adds/removes heavy items, showcasing fast and safe adaptation to\nchanging dynamics during physical interaction without any external sensory\nsystem.\n","authors":["Mateusz Jaszczuk","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2508.21007v1.pdf","comment":"Accepted at 9th Annual Conference on Robot Learning. Project Website\n  - https://mateusz-jaszczuk.github.io/rme/"},{"id":"http://arxiv.org/abs/2508.21001v1","updated":"2025-08-28T17:04:00Z","published":"2025-08-28T17:04:00Z","title":"Train-Once Plan-Anywhere Kinodynamic Motion Planning via Diffusion Trees","summary":"  Kinodynamic motion planning is concerned with computing collision-free\ntrajectories while abiding by the robot's dynamic constraints. This critical\nproblem is often tackled using sampling-based planners (SBPs) that explore the\nrobot's high-dimensional state space by constructing a search tree via action\npropagations. Although SBPs can offer global guarantees on completeness and\nsolution quality, their performance is often hindered by slow exploration due\nto uninformed action sampling. Learning-based approaches can yield\nsignificantly faster runtimes, yet they fail to generalize to\nout-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,\nthus limiting their deployment on physical robots. We present Diffusion Tree\n(DiTree): a \\emph{provably-generalizable} framework leveraging diffusion\npolicies (DPs) as informed samplers to efficiently guide state-space search\nwithin SBPs. DiTree combines DP's ability to model complex distributions of\nexpert trajectories, conditioned on local observations, with the completeness\nof SBPs to yield \\emph{provably-safe} solutions within a few action propagation\niterations for complex dynamical systems. We demonstrate DiTree's power with an\nimplementation combining the popular RRT planner with a DP action sampler\ntrained on a \\emph{single environment}. In comprehensive evaluations on OOD\nscenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than\nclassical SBPs), while improving the average success rate over DP and SBPs.\nDiTree is on average 3x faster than classical SBPs, and outperforms all other\napproaches by achieving roughly 30\\% higher success rate. Project webpage:\nhttps://sites.google.com/view/ditree.\n","authors":["Yaniv Hassidof","Tom Jurgenson","Kiril Solovey"],"pdf_url":"https://arxiv.org/pdf/2508.21001v1.pdf","comment":"Accepted to CoRL 2025. Project page:\n  https://sites.google.com/view/ditree"},{"id":"http://arxiv.org/abs/2508.20981v1","updated":"2025-08-28T16:36:02Z","published":"2025-08-28T16:36:02Z","title":"ActLoc: Learning to Localize on the Move via Active Viewpoint Selection","summary":"  Reliable localization is critical for robot navigation, yet most existing\nsystems implicitly assume that all viewing directions at a location are equally\ninformative. In practice, localization becomes unreliable when the robot\nobserves unmapped, ambiguous, or uninformative regions. To address this, we\npresent ActLoc, an active viewpoint-aware planning framework for enhancing\nlocalization accuracy for general robot navigation tasks. At its core, ActLoc\nemploys a largescale trained attention-based model for viewpoint selection. The\nmodel encodes a metric map and the camera poses used during map construction,\nand predicts localization accuracy across yaw and pitch directions at arbitrary\n3D locations. These per-point accuracy distributions are incorporated into a\npath planner, enabling the robot to actively select camera orientations that\nmaximize localization robustness while respecting task and motion constraints.\nActLoc achieves stateof-the-art results on single-viewpoint selection and\ngeneralizes effectively to fulltrajectory planning. Its modular design makes it\nreadily applicable to diverse robot navigation and inspection tasks.\n","authors":["Jiajie Li","Boyang Sun","Luca Di Giammarino","Hermann Blum","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2508.20981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.20959v1","updated":"2025-08-28T16:19:16Z","published":"2025-08-28T16:19:16Z","title":"Scaling Fabric-Based Piezoresistive Sensor Arrays for Whole-Body Tactile\n  Sensing","summary":"  Scaling tactile sensing for robust whole-body manipulation is a significant\nchallenge, often limited by wiring complexity, data throughput, and system\nreliability. This paper presents a complete architecture designed to overcome\nthese barriers. Our approach pairs open-source, fabric-based sensors with\ncustom readout electronics that reduce signal crosstalk to less than 3.3%\nthrough hardware-based mitigation. Critically, we introduce a novel,\ndaisy-chained SPI bus topology that avoids the practical limitations of common\nwireless protocols and the prohibitive wiring complexity of USB hub-based\nsystems. This architecture streams synchronized data from over 8,000 taxels\nacross 1 square meter of sensing area at update rates exceeding 50 FPS,\nconfirming its suitability for real-time control. We validate the system's\nefficacy in a whole-body grasping task where, without feedback, the robot's\nopen-loop trajectory results in an uncontrolled application of force that\nslowly crushes a deformable cardboard box. With real-time tactile feedback, the\nrobot transforms this motion into a gentle, stable grasp, successfully\nmanipulating the object without causing structural damage. This work provides a\nrobust and well-characterized platform to enable future research in advanced\nwhole-body control and physical human-robot interaction.\n","authors":["Curtis C. Johnson","Daniel Webb","David Hill","Marc D. Killpack"],"pdf_url":"https://arxiv.org/pdf/2508.20959v1.pdf","comment":"In submission to IEEE Sensors"},{"id":"http://arxiv.org/abs/2508.20926v1","updated":"2025-08-28T15:55:36Z","published":"2025-08-28T15:55:36Z","title":"PLUME: Procedural Layer Underground Modeling Engine","summary":"  As space exploration advances, underground environments are becoming\nincreasingly attractive due to their potential to provide shelter, easier\naccess to resources, and enhanced scientific opportunities. Although such\nenvironments exist on Earth, they are often not easily accessible and do not\naccurately represent the diversity of underground environments found throughout\nthe solar system. This paper presents PLUME, a procedural generation framework\naimed at easily creating 3D underground environments. Its flexible structure\nallows for the continuous enhancement of various underground features, aligning\nwith our expanding understanding of the solar system. The environments\ngenerated using PLUME can be used for AI training, evaluating robotics\nalgorithms, 3D rendering, and facilitating rapid iteration on developed\nexploration algorithms. In this paper, it is demonstrated that PLUME has been\nused along with a robotic simulator. PLUME is open source and has been released\non Github. https://github.com/Gabryss/P.L.U.M.E\n","authors":["Gabriel Manuel Garcia","Antoine Richard","Miguel Olivares-Mendez"],"pdf_url":"https://arxiv.org/pdf/2508.20926v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2508.20920v1","updated":"2025-08-28T15:50:29Z","published":"2025-08-28T15:50:29Z","title":"COMETH: Convex Optimization for Multiview Estimation and Tracking of\n  Humans","summary":"  In the era of Industry 5.0, monitoring human activity is essential for\nensuring both ergonomic safety and overall well-being. While multi-camera\ncentralized setups improve pose estimation accuracy, they often suffer from\nhigh computational costs and bandwidth requirements, limiting scalability and\nreal-time applicability. Distributing processing across edge devices can reduce\nnetwork bandwidth and computational load. On the other hand, the constrained\nresources of edge devices lead to accuracy degradation, and the distribution of\ncomputation leads to temporal and spatial inconsistencies. We address this\nchallenge by proposing COMETH (Convex Optimization for Multiview Estimation and\nTracking of Humans), a lightweight algorithm for real-time multi-view human\npose fusion that relies on three concepts: it integrates kinematic and\nbiomechanical constraints to increase the joint positioning accuracy; it\nemploys convex optimization-based inverse kinematics for spatial fusion; and it\nimplements a state observer to improve temporal consistency. We evaluate COMETH\non both public and industrial datasets, where it outperforms state-of-the-art\nmethods in localization, detection, and tracking accuracy. The proposed fusion\npipeline enables accurate and scalable human motion tracking, making it\nwell-suited for industrial and safety-critical applications. The code is\npublicly available at https://github.com/PARCO-LAB/COMETH.\n","authors":["Enrico Martini","Ho Jin Choi","Nadia Figueroa","Nicola Bombieri"],"pdf_url":"https://arxiv.org/pdf/2508.20920v1.pdf","comment":"Submitted to Information Fusion"},{"id":"http://arxiv.org/abs/2508.20899v1","updated":"2025-08-28T15:27:35Z","published":"2025-08-28T15:27:35Z","title":"Language-Enhanced Mobile Manipulation for Efficient Object Search in\n  Indoor Environments","summary":"  Enabling robots to efficiently search for and identify objects in complex,\nunstructured environments is critical for diverse applications ranging from\nhousehold assistance to industrial automation. However, traditional scene\nrepresentations typically capture only static semantics and lack interpretable\ncontextual reasoning, limiting their ability to guide object search in\ncompletely unfamiliar settings. To address this challenge, we propose a\nlanguage-enhanced hierarchical navigation framework that tightly integrates\nsemantic perception and spatial reasoning. Our method, Goal-Oriented\nDynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large\nlanguage models (LLMs) to infer scene semantics and guide the search process\nthrough a multi-level decision hierarchy. Reliability in reasoning is achieved\nthrough the use of structured prompts and logical constraints applied at each\nstage of the hierarchy. For the specific challenges of mobile manipulation, we\nintroduce a heuristic-based motion planner that combines polar angle sorting\nwith distance prioritization to efficiently generate exploration paths.\nComprehensive evaluations in Isaac Sim demonstrate the feasibility of our\nframework, showing that GODHS can locate target objects with higher search\nefficiency compared to conventional, non-semantic search strategies. Website\nand Video are available at: https://drapandiger.github.io/GODHS\n","authors":["Liding Zhang","Zeqi Li","Kuanqi Cai","Qian Huang","Zhenshan Bing","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2508.20899v1.pdf","comment":null}]}}