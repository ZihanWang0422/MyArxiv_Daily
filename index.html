<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-09-04T00:00:00Z">2025-09-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">36</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMMA: Scaling Mobile <span class="highlight-title">Manipulation</span> via Egocentric Human Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lawrence Y. Zhu, Pranav Kuppili, Ryan Punamiya, Patcharapong Aphiwetsa, Dhruv Patel, Simar Kareer, Sehoon Ha, Danfei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling mobile manipulation imitation learning is bottlenecked by expensive
mobile robot teleoperation. We present Egocentric Mobile MAnipulation (EMMA),
an end-to-end framework training mobile manipulation policies from human mobile
manipulation data with static robot data, sidestepping mobile teleoperation. To
accomplish this, we co-train human full-body motion data with static robot
data. In our experiments across three real-world tasks, EMMA demonstrates
comparable performance to baselines trained on teleoperated mobile robot data
(Mobile ALOHA), achieving higher or equivalent task performance in full task
success. We find that EMMA is able to generalize to new spatial configurations
and scenes, and we observe positive performance scaling as we increase the
hours of human data, opening new avenues for scalable robotic learning in
real-world environments. Details of this project can be found at
https://ego-moma.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEXOP: A Device for <span class="highlight-title">Robot</span>ic Transfer of Dexterous Human <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Shu Fang, Branden Romero, Yichen Xie, Arthur Hu, Bo-Ruei Huang, Juan Alvarez, Matthew Kim, Gabriel Margolis, Kavya Anbarasu, Masayoshi Tomizuka, Edward Adelson, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce perioperation, a paradigm for robotic data collection that
sensorizes and records human manipulation while maximizing the transferability
of the data to real robots. We implement this paradigm in DEXOP, a passive hand
exoskeleton designed to maximize human ability to collect rich sensory (vision
+ tactile) data for diverse dexterous manipulation tasks in natural
environments. DEXOP mechanically connects human fingers to robot fingers,
providing users with direct contact feedback (via proprioception) and mirrors
the human hand pose to the passive robot hand to maximize the transfer of
demonstrated skills to the robot. The force feedback and pose mirroring make
task demonstrations more natural for humans compared to teleoperation,
increasing both speed and accuracy. We evaluate DEXOP across a range of
dexterous, contact-rich tasks, demonstrating its ability to collect
high-quality demonstration data at scale. Policies learned with DEXOP data
significantly improve task performance per unit time of data collection
compared to teleoperation, making DEXOP a powerful tool for advancing robot
dexterity. Our project page is at https://dex-op.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://dex-op.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFE--MA--RRT: Multi-Agent Motion <span class="highlight-title">Planning</span> with Data-Driven Safety
  Certificates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04413v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04413v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Babak Esmaeili, Hamidreza Modares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a fully data-driven motion-planning framework for
homogeneous linear multi-agent systems that operate in shared, obstacle-filled
workspaces without access to explicit system models. Each agent independently
learns its closed-loop behavior from experimental data by solving convex
semidefinite programs that generate locally invariant ellipsoids and
corresponding state-feedback gains. These ellipsoids, centered along grid-based
waypoints, certify the dynamic feasibility of short-range transitions and
define safe regions of operation. A sampling-based planner constructs a tree of
such waypoints, where transitions are allowed only when adjacent ellipsoids
overlap, ensuring invariant-to-invariant transitions and continuous safety. All
agents expand their trees simultaneously and are coordinated through a
space-time reservation table that guarantees inter-agent safety by preventing
simultaneous occupancy and head-on collisions. Each successful edge in the tree
is equipped with its own local controller, enabling execution without
re-solving optimization problems at runtime. The resulting trajectories are not
only dynamically feasible but also provably safe with respect to both
environmental constraints and inter-agent collisions. Simulation results
demonstrate the effectiveness of the approach in synthesizing synchronized,
safe trajectories for multiple agents under shared dynamics and constraints,
using only data and convex optimization tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Automation Science and Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Equivariances and Symmetries in the Control Barrier Function
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Wiltz, Dimos V. Dimarogonas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The synthesis of Control Barrier Functions (CBFs) often involves demanding
computations or a meticulous construction. However, structural properties of
the system dynamics and constraints have the potential to mitigate these
challenges. In this paper, we explore how equivariances in the dynamics,
loosely speaking a form of symmetry, can be leveraged in the CBF synthesis.
Although CBFs are generally not inherently symmetric, we show how equivariances
in the dynamics and symmetries in the constraints induce symmetries in CBFs
derived through reachability analysis. This insight allows us to infer their
CBF values across the entire domain from their values on a subset, leading to
significant computational savings. Interestingly, equivariances can be even
leveraged to the CBF synthesis for non-symmetric constraints. Specifically, we
show how a partially known CBF can be leveraged together with equivariances to
construct a CBF for various new constraints. Throughout the paper, we provide
examples illustrating the theoretical findings. Furthermore, a numerical study
investigates the computational gains from invoking equivariances into the CBF
synthesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy Perceptions in <span class="highlight-title">Robot</span>-Assisted Well-Being Coaching: Examining the
  Roles of Information Transparency, User Control, and Proactivity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atikkhan Faridkhan Nilgar, Manuel Dietrich, Kristof Van Laerhoven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social robots are increasingly recognized as valuable supporters in the field
of well-being coaching. They can function as independent coaches or provide
support alongside human coaches, and healthcare professionals. In coaching
interactions, these robots often handle sensitive information shared by users,
making privacy a relevant issue. Despite this, little is known about the
factors that shape users' privacy perceptions. This research aims to examine
three key factors systematically: (1) the transparency about information usage,
(2) the level of specific user control over how the robot uses their
information, and (3) the robot's behavioral approach - whether it acts
proactively or only responds on demand. Our results from an online study (N =
200) show that even when users grant the robot general access to personal data,
they additionally expect the ability to explicitly control how that information
is interpreted and shared during sessions. Experimental conditions that
provided such control received significantly higher ratings for perceived
privacy appropriateness and trust. Compared to user control, the effects of
transparency and proactivity on privacy appropriateness perception were low,
and we found no significant impact. The results suggest that merely informing
users or proactive sharing is insufficient without accompanying user control.
These insights underscore the need for further research on mechanisms that
allow users to manage robots' information processing and sharing, especially
when social robots take on more proactive roles alongside humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social <span class="highlight-title">Robot</span>ic
  Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04356v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04356v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atikkhan Faridkhan Nilgar, Kristof Van Laerhoven, Ayub Kinoti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SRWToolkit, an open-source Wizard of Oz toolkit designed to
facilitate the rapid prototyping of social robotic avatars powered by local
large language models (LLMs). Our web-based toolkit enables multimodal
interaction through text input, button-activated speech, and wake-word command.
The toolkit offers real-time configuration of avatar appearance, behavior,
language, and voice via an intuitive control panel. In contrast to prior works
that rely on cloud-based LLM services, SRWToolkit emphasizes modularity and
ensures on-device functionality through local LLM inference. In our small-scale
user study ($n=11$), participants created and interacted with diverse robotic
roles (hospital receptionist, mathematics teacher, and driving assistant),
which demonstrated positive outcomes in the toolkit's usability, trust, and
user experience. The toolkit enables rapid and efficient development of robot
characters customized to researchers' needs, supporting scalable research in
human-robot interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OVGrasp: <span class="highlight-title">Open-Vocabulary</span> Grasping Assistance via Multimodal Intent
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Hu, Shan Luo, Letizia Gionfrida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grasping assistance is essential for restoring autonomy in individuals with
motor impairments, particularly in unstructured environments where object
categories and user intentions are diverse and unpredictable. We present
OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp
assistance that integrates RGB-D vision, open-vocabulary prompts, and voice
commands to enable robust multimodal interaction. To enhance generalization in
open environments, OVGrasp incorporates a vision-language foundation model with
an open-vocabulary mechanism, allowing zero-shot detection of previously unseen
objects without retraining. A multimodal decision-maker further fuses spatial
and linguistic cues to infer user intent, such as grasp or release, in
multi-object scenarios. We deploy the complete framework on a custom
egocentric-view wearable exoskeleton and conduct systematic evaluations on 15
objects across three grasp types. Experimental results with ten participants
demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,
outperforming state-of-the-art baselines and achieving improved kinematic
alignment with natural hand motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compatibility of Multiple Control Barrier Functions for Constrained
  Nonlinear Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max H. Cohen, Eugene Lavretsky, Aaron D. Ames
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control barrier functions (CBFs) are a powerful tool for the constrained
control of nonlinear systems; however, the majority of results in the
literature focus on systems subject to a single CBF constraint, making it
challenging to synthesize provably safe controllers that handle multiple state
constraints. This paper presents a framework for constrained control of
nonlinear systems subject to box constraints on the systems' vector-valued
outputs using multiple CBFs. Our results illustrate that when the output has a
vector relative degree, the CBF constraints encoding these box constraints are
compatible, and the resulting optimization-based controller is locally
Lipschitz continuous and admits a closed-form expression. Additional results
are presented to characterize the degradation of nominal tracking objectives in
the presence of safety constraints. Simulations of a planar quadrotor are
presented to demonstrate the efficacy of the proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at IEEE CDC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind
  Turbine Components 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serhii Svystun, Pavlo Radiuk, Oleksandr Melnychenko, Oleg Savenko, Anatoliy Sachenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up
new opportunities for monitoring wind power plants, including blades, towers,
and other critical components. However, reliable defect detection requires
high-resolution data and efficient methods to process multispectral imagery. In
this research, we aim to enhance defect detection accuracy through the
development of an ensemble of YOLO-based deep learning models that integrate
both visible and thermal channels. We propose an ensemble approach that
integrates a general-purpose YOLOv8 model with a specialized thermal model,
using a sophisticated bounding box fusion algorithm to combine their
predictions. Our experiments show this approach achieves a mean Average
Precision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone
YOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that
combining multiple YOLO architectures with fused multispectral data provides a
more reliable solution, improving the detection of both visual and thermal
defects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 13th IEEE International Conference on Intelligent Data
  Acquisition and Advanced Computing Systems: Technology and Applications, 4-6
  September, 2025, Gliwice, Poland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Kinematic and Static Modeling of Cable-Driven Continuum
  <span class="highlight-title">Robots</span> via Actuation-Space Energy Formulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wu, Yuhao Wang, Kevin Henry, Cesare Stefanini, Gang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continuum robots, inspired by octopus arms and elephant trunks, combine
dexterity with intrinsic compliance, making them well suited for unstructured
and confined environments. Yet their continuously deformable morphology poses
challenges for motion planning and control, calling for accurate but
lightweight models. We propose the Lightweight Actuation Space Energy Modeling
(LASEM) framework for cable driven continuum robots, which formulates actuation
potential energy directly in actuation space. LASEM yields an analytical
forward model derived from geometrically nonlinear beam and rod theories via
Hamilton's principle, while avoiding explicit modeling of cable backbone
contact. It accepts both force and displacement inputs, thereby unifying
kinematic and static formulations. Assuming the friction is neglected, the
framework generalizes to nonuniform geometries, arbitrary cable routings,
distributed loading and axial extensibility, while remaining computationally
efficient for real-time use. Numerical simulations validate its accuracy, and a
semi-analytical iterative scheme is developed for inverse kinematics. To
address discretization in practical robots, LASEM further reformulates the
functional minimization as a numerical optimization, which also naturally
incorporates cable potential energy without explicit contact modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cloud-Assisted Remote Control for Aerial <span class="highlight-title">Robots</span>: From Theory to
  Proof-of-Concept Implementation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achilleas Santi Seisa, Viswa Narayanan Sankaranarayanan, Gerasimos Damigos, Sumeet Gajanan Satpute, George Nikolakopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloud robotics has emerged as a promising technology for robotics
applications due to its advantages of offloading computationally intensive
tasks, facilitating data sharing, and enhancing robot coordination. However,
integrating cloud computing with robotics remains a complex challenge due to
network latency, security concerns, and the need for efficient resource
management. In this work, we present a scalable and intuitive framework for
testing cloud and edge robotic systems. The framework consists of two main
components enabled by containerized technology: (a) a containerized cloud
cluster and (b) the containerized robot simulation environment. The system
incorporates two endpoints of a User Datagram Protocol (UDP) tunnel, enabling
bidirectional communication between the cloud cluster container and the robot
simulation environment, while simulating realistic network conditions. To
achieve this, we consider the use case of cloud-assisted remote control for
aerial robots, while utilizing Linux-based traffic control to introduce
artificial delay and jitter, replicating variable network conditions
encountered in practical cloud-robot deployments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, CCGridW 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-Reconstruction-Aware <span class="highlight-title">Whole-body Control</span> of Mobile Manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatih Dursun, Bruno Vilhena Adorno, Simon Watson, Wei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object reconstruction and inspection tasks play a crucial role in various
robotics applications. Identifying paths that reveal the most unknown areas of
the object becomes paramount in this context, as it directly affects
efficiency, and this problem is known as the view path planning problem.
Current methods often use sampling-based path planning techniques, evaluating
potential views along the path to enhance reconstruction performance. However,
these methods are computationally expensive as they require evaluating several
candidate views on the path. To this end, we propose a computationally
efficient solution that relies on calculating a focus point in the most
informative (unknown) region and having the robot maintain this point in the
camera field of view along the path. We incorporated this strategy into the
whole-body control of a mobile manipulator employing a visibility constraint
without the need for an additional path planner. We conducted comprehensive and
realistic simulations using a large dataset of 114 diverse objects of varying
sizes from 57 categories to compare our method with a sampling-based planning
strategy using Bayesian data analysis. Furthermore, we performed real-world
experiments with an 8-DoF mobile manipulator to demonstrate the proposed
method's performance in practice. Our results suggest that there is no
significant difference in object coverage and entropy. In contrast, our method
is approximately nine times faster than the baseline sampling-based method in
terms of the average time the robot spends between views.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 13 figures, 3 tables. Under Review for the IEEE
  Transactions on Robotics (T-RO)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keypoint-based Diffusion for <span class="highlight-title">Robot</span>ic Motion <span class="highlight-title">Planning</span> on the NICOL <span class="highlight-title">Robot</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Clasmeier, Jan-Gerrit Habekost, Connor Gäde, Philipp Allgeuer, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel diffusion-based action model for robotic motion planning.
Commonly, established numerical planning approaches are used to solve general
motion planning problems, but have significant runtime requirements. By
leveraging the power of deep learning, we are able to achieve good results in a
much smaller runtime by learning from a dataset generated by these planners.
While our initial model uses point cloud embeddings in the input to predict
keypoint-based joint sequences in its output, we observed in our ablation study
that it remained challenging to condition the network on the point cloud
embeddings. We identified some biases in our dataset and refined it, which
improved the model's performance. Our model, even without the use of the point
cloud encodings, outperforms numerical models by an order of magnitude
regarding the runtime, while reaching a success rate of up to 90% of collision
free solutions on the test set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICANN 20255 Special Session on Neural Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving <span class="highlight-title">Robot</span>ics Tasks with Prior Demonstration via
  Exploration-Efficient Deep Reinforcement <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04069v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04069v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyandan Shen, Christoffer Sloth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes an exploration-efficient Deep Reinforcement Learning with
Reference policy (DRLR) framework for learning robotics tasks that incorporates
demonstrations. The DRLR framework is developed based on an algorithm called
Imitation Bootstrapped Reinforcement Learning (IBRL). We propose to improve
IBRL by modifying the action selection module. The proposed action selection
module provides a calibrated Q-value, which mitigates the bootstrapping error
that otherwise leads to inefficient exploration. Furthermore, to prevent the RL
policy from converging to a sub-optimal policy, SAC is used as the RL policy
instead of TD3. The effectiveness of our method in mitigating bootstrapping
error and preventing overfitting is empirically validated by learning two
robotics tasks: bucket loading and open drawer, which require extensive
interactions with the environment. Simulation results also demonstrate the
robustness of the DRLR framework across tasks with both low and high
state-action dimensions, and varying demonstration qualities. To evaluate the
developed framework on a real-world industrial robotics task, the bucket
loading task is deployed on a real wheel loader. The sim2real results validate
the successful deployment of the DRLR framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing Signal and Variance: Adaptive Offline RL Post-Training for <span class="highlight-title">VLA</span>
  Flow Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyin Zhang, Shiyuan Zhang, Junxi Jin, Qixin Zeng, Yifan Qiao, Hongchao Lu, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models based on flow matching have shown
excellent performance in general-purpose robotic manipulation tasks. However,
the action accuracy of these models on complex downstream tasks is
unsatisfactory. One important reason is that these models rely solely on the
post-training paradigm of imitation learning, which makes it difficult to have
a deeper understanding of the distribution properties of data quality, which is
exactly what Reinforcement Learning (RL) excels at. In this paper, we
theoretically propose an offline RL post-training objective for VLA flow models
and induce an efficient and feasible offline RL fine-tuning algorithm --
Adaptive Reinforced Flow Matching (ARFM). By introducing an adaptively adjusted
scaling factor in the VLA flow model loss, we construct a principled
bias-variance trade-off objective function to optimally control the impact of
RL signal on flow loss. ARFM adaptively balances RL advantage preservation and
flow loss gradient variance control, resulting in a more stable and efficient
fine-tuning process. Extensive simulation and real-world experimental results
show that ARFM exhibits excellent generalization, robustness, few-shot
learning, and continuous learning performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrated Wheel Sensor Communication using ESP32 -- A Contribution
  towards a Digital Twin of the Road System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ventseslav Yordanov, Simon Schäfer, Alexander Mann, Stefan Kowalewski, Bassam Alrifaee, Lutz Eckstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While current onboard state estimation methods are adequate for most driving
and safety-related applications, they do not provide insights into the
interaction between tires and road surfaces. This paper explores a novel
communication concept for efficiently transmitting integrated wheel sensor data
from an ESP32 microcontroller. Our proposed approach utilizes a
publish-subscribe system, surpassing comparable solutions in the literature
regarding data transmission volume. We tested this approach on a drum tire test
rig with our prototype sensors system utilizing a diverse selection of sample
frequencies between 1 Hz and 32 000 Hz to demonstrate the efficacy of our
communication concept. The implemented prototype sensor showcases minimal data
loss, approximately 0.1 % of the sampled data, validating the reliability of
our developed communication system. This work contributes to advancing
real-time data acquisition, providing insights into optimizing integrated wheel
sensor communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures, this work was submitted to and accepted by IEEE
  International Conference on Intelligent Transportation Systems (ITSC) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FPC-<span class="highlight-title">VLA</span>: A <span class="highlight-title">Vision-Language-Action</span> Framework with a Supervisor for
  Failure Prediction and Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yang, Zhixiang Duan, Tianshi Xie, Fuyu Cao, Pinxi Shen, Peili Song, Piaopiao Jin, Guokang Sun, Shaoqing Xu, Yangwei You, Jingtai Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation is a fundamental component of automation. However,
traditional perception-planning pipelines often fall short in open-ended tasks
due to limited flexibility, while the architecture of a single end-to-end
Vision-Language-Action (VLA) offers promising capabilities but lacks crucial
mechanisms for anticipating and recovering from failure. To address these
challenges, we propose FPC-VLA, a dual-model framework that integrates VLA with
a supervisor for failure prediction and correction. The supervisor evaluates
action viability through vision-language queries and generates corrective
strategies when risks arise, trained efficiently without manual labeling. A
similarity-guided fusion module further refines actions by leveraging past
predictions. Evaluation results on multiple simulation platforms (SIMPLER and
LIBERO) and robot embodiments (WidowX, Google Robot, Franka) show that FPC-VLA
outperforms state-of-the-art models in both zero-shot and fine-tuned settings.
By activating the supervisor only at keyframes, our approach significantly
increases task success rates with minimal impact on execution time. Successful
real-world deployments on diverse, long-horizon tasks confirm FPC-VLA's strong
generalization and practical utility for building more reliable autonomous
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Odometry Calibration and Pose <span class="highlight-title">Estimation</span> of a 4WIS4WID Mobile Wall
  Climbing <span class="highlight-title">Robot</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Branimir Ćaran, Vladimir Milić, Marko Švaco, Bojan Jerbić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the design of a pose estimator for a four wheel
independent steer four wheel independent drive (4WIS4WID) wall climbing mobile
robot, based on the fusion of multimodal measurements, including wheel
odometry, visual odometry, and an inertial measurement unit (IMU) data using
Extended Kalman Filter (EKF) and Unscented Kalman Filter (UKF). The pose
estimator is a critical component of wall climbing mobile robots, as their
operational environment involves carrying precise measurement equipment and
maintenance tools in construction, requiring information about pose on the
building at the time of measurement. Due to the complex geometry and material
properties of building facades, the use of traditional localization sensors
such as laser, ultrasonic, or radar is often infeasible for wall-climbing
robots. Moreover, GPS-based localization is generally unreliable in these
environments because of signal degradation caused by reinforced concrete and
electromagnetic interference. Consequently, robot odometry remains the primary
source of velocity and position information, despite being susceptible to drift
caused by both systematic and non-systematic errors. The calibrations of the
robot's systematic parameters were conducted using nonlinear optimization and
Levenberg-Marquardt methods as Newton-Gauss and gradient-based model fitting
methods, while Genetic algorithm and Particle swarm were used as
stochastic-based methods for kinematic parameter calibration. Performance and
results of the calibration methods and pose estimators were validated in detail
with experiments on the experimental mobile wall climbing robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACCEPTED FOR IEEE EUROPEAN CONFERENCE ON MOBILE ROBOTS 2025. PREPRINT
  VERSION. ACCEPTED JUNE, 2025 AND PRESENTED SEPTEMBER, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reactive In-Air Clothing <span class="highlight-title">Manipulation</span> with Confidence-Aware Dense
  Correspondence and Visuotactile Affordance <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neha Sunil, Megha Tippur, Arnau Saumell, Edward Adelson, Alberto Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulating clothing is challenging due to complex configurations, variable
material dynamics, and frequent self-occlusion. Prior systems often flatten
garments or assume visibility of key features. We present a dual-arm
visuotactile framework that combines confidence-aware dense visual
correspondence and tactile-supervised grasp affordance to operate directly on
crumpled and suspended garments. The correspondence model is trained on a
custom, high-fidelity simulated dataset using a distributional loss that
captures cloth symmetries and generates correspondence confidence estimates.
These estimates guide a reactive state machine that adapts folding strategies
based on perceptual uncertainty. In parallel, a visuotactile grasp affordance
network, self-supervised using high-resolution tactile feedback, determines
which regions are physically graspable. The same tactile classifier is used
during execution for real-time grasp validation. By deferring action in
low-confidence states, the system handles highly occluded table-top and in-air
configurations. We demonstrate our task-agnostic grasp selection module in
folding and hanging tasks. Moreover, our dense descriptors provide a reusable
intermediate representation for other planning modalities, such as extracting
grasp targets from human video demonstrations, paving the way for more
generalizable and scalable garment manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CoRL 2025. Project website:
  https://mhtippur.github.io/inairclothmanipulation/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Learning</span> Multi-Stage Pick-and-Place with a <span class="highlight-title">Legged</span> Mobile Manipulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Zhang, Haonan Yu, Le Zhao, Andrew Choi, Qinxun Bai, Yiqing Yang, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadruped-based mobile manipulation presents significant challenges in
robotics due to the diversity of required skills, the extended task horizon,
and partial observability. After presenting a multi-stage pick-and-place task
as a succinct yet sufficiently rich setup that captures key desiderata for
quadruped-based mobile manipulation, we propose an approach that can train a
visuo-motor policy entirely in simulation, and achieve nearly 80\% success in
the real world. The policy efficiently performs search, approach, grasp,
transport, and drop into actions, with emerged behaviors such as re-grasping
and task chaining. We conduct an extensive set of real-world experiments with
ablation studies highlighting key techniques for efficient training and
effective sim-to-real transfer. Additional experiments demonstrate deployment
across a variety of indoor and outdoor environments. Demo videos and additional
resources are available on the project page:
https://horizonrobotics.github.io/gail/SLIM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project: https://horizonrobotics.github.io/gail/SLIM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INGRID: Intelligent <span class="highlight-title">Generative</span> <span class="highlight-title">Robot</span>ic Design Using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanglu Jia, Ceng Zhang, Gregory S. Chirikjian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of large language models (LLMs) into robotic systems has
accelerated progress in embodied artificial intelligence, yet current
approaches remain constrained by existing robotic architectures, particularly
serial mechanisms. This hardware dependency fundamentally limits the scope of
robotic intelligence. Here, we present INGRID (Intelligent Generative Robotic
Design), a framework that enables the automated design of parallel robotic
mechanisms through deep integration with reciprocal screw theory and kinematic
synthesis methods. We decompose the design challenge into four progressive
tasks: constraint analysis, kinematic joint generation, chain construction, and
complete mechanism design. INGRID demonstrates the ability to generate novel
parallel mechanisms with both fixed and variable mobility, discovering
kinematic configurations not previously documented in the literature. We
validate our approach through three case studies demonstrating how INGRID
assists users in designing task-specific parallel robots based on desired
mobility requirements. By bridging the gap between mechanism theory and machine
learning, INGRID enables researchers without specialized robotics training to
create custom parallel mechanisms, thereby decoupling advances in robotic
intelligence from hardware constraints. This work establishes a foundation for
mechanism intelligence, where AI systems actively design robotic hardware,
potentially transforming the development of embodied AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Buoyancy <span class="highlight-title">Estimation</span> for AUV Simulations Using Convex
  Hull-Based Submerged Volume Calculation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ad-Deen Mahbub, Md Ragib Shaharear
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate real-time buoyancy modeling is essential for high-fidelity
Autonomous Underwater Vehicle (AUV) simulations, yet NVIDIA Isaac Sim lacks a
native buoyancy system, requiring external solutions for precise underwater
physics. This paper presents a novel convex hull-based approach to dynamically
compute the submerged volume of an AUV in real time. By extracting mesh
geometry from the simulation environment and calculating the hull portion
intersecting the water level along the z-axis, our method enhances accuracy
over traditional geometric approximations. A cross-sectional area extension
reduces computational overhead, enabling efficient buoyant force updates that
adapt to orientation, depth, and sinusoidal wave fluctuations (+-0.3 m). Tested
on a custom AUV design for SAUVC 2025, this approach delivers real-time
performance and scalability, improving simulation fidelity for underwater
robotics research without precomputed hydrodynamic models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taming High-Dimensional <span class="highlight-title">Dynamic</span>s: <span class="highlight-title">Learning</span> Optimal Projections onto
  Spectral Submanifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.03157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.03157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Buurmeijer, Luis A. Pabon, John Irvin Alora, Roshan S. Kaundinya, George Haller, Marco Pavone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-dimensional nonlinear systems pose considerable challenges for modeling
and control across many domains, from fluid mechanics to advanced robotics.
Such systems are typically approximated with reduced-order models, which often
rely on orthogonal projections, a simplification that may lead to large
prediction errors. In this work, we derive optimality of fiber-aligned
projections onto spectral submanifolds, preserving the nonlinear geometric
structure and minimizing long-term prediction error. We propose a data-driven
procedure to learn these projections from trajectories and demonstrate its
effectiveness through a 180-dimensional robotic system. Our reduced-order
models achieve up to fivefold improvement in trajectory tracking accuracy under
model predictive control compared to the state of the art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robot</span>ic <span class="highlight-title">Manipulation</span> via Imitation <span class="highlight-title">Learning</span>: Taxonomy, Evolution,
  Benchmark, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.17449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.17449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zezeng Li, Alexandre Chapin, Enda Xiang, Rui Yang, Bruno Machado, Na Lei, Emmanuel Dellandrea, Di Huang, Liming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic Manipulation (RM) is central to the advancement of autonomous robots,
enabling them to interact with and manipulate objects in real-world
environments. This survey focuses on RM methodologies that leverage imitation
learning, a powerful technique that allows robots to learn complex manipulation
skills by mimicking human demonstrations. We identify and analyze the most
influential studies in this domain, selected based on community impact and
intrinsic quality. For each paper, we provide a structured summary, covering
the research purpose, technical implementation, hierarchical classification,
input formats, key priors, strengths and limitations, and citation metrics.
Additionally, we trace the chronological development of imitation learning
techniques within RM policy (RMP), offering a timeline of key technological
advancements. Where available, we report benchmark results and perform
quantitative evaluations to compare existing methods. By synthesizing these
insights, this review provides a comprehensive resource for researchers and
practitioners, highlighting both the state of the art and the challenges that
lie ahead in the field of robotic manipulation through imitation learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Spatially-Enhanced Recurrent Memory for Long-Range Mapless <span class="highlight-title">Navigation</span>
  via End-to-End Reinforcement <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yang, Per Frivik, David Hoeller, Chen Wang, Cesar Cadena, <span class="highlight-author">Marco Hutter</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in robot navigation, particularly with end-to-end
learning approaches such as reinforcement learning (RL), have demonstrated
strong performance. However, successful navigation still depends on two key
capabilities: mapping and planning (explicitly or implicitly). Classical
approaches rely on explicit mapping pipelines to register egocentric
observations into a coherent map. In contrast, end-to-end learning often
achieves this implicitly -- through recurrent neural networks (RNNs) that fuse
current and historical observations into a latent space for planning. While
existing architectures, such as LSTM and GRU, can capture temporal
dependencies, our findings reveal a critical limitation: their inability to
effectively perform spatial memorization. This capability is essential for
integrating sequential observations from varying perspectives to build spatial
representations that support planning. To address this, we propose
Spatially-Enhanced Recurrent Units (SRUs) -- a simple yet effective
modification to existing RNNs -- that enhance spatial memorization. We further
introduce an attention-based network architecture integrated with SRUs,
enabling long-range mapless navigation using a single forward-facing stereo
camera. We also employ regularization techniques to facilitate robust
end-to-end recurrent training via RL. Experimental results show 23.5% overall
improvement in long-range navigation compared to existing RNNs. With SRU
memory, our method outperforms RL baselines -- one relying on explicit mapping
and the other on stacked historical observations -- by 29.6% and 105.0%,
respectively, across diverse environments requiring long-horizon mapping and
memorization. Finally, we address the sim-to-real gap by leveraging large-scale
pretraining on synthetic depth data, enabling zero-shot transfer for deployment
across diverse and complex real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via
  Flow Variational Inference <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15161v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15161v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Feng, Jianxiang Feng, Zhaopeng Chen, Rudolph Triebel, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from
partial observations remains a critical challenge in robot learning. Prior
generative methods struggle to model the intricate grasp distribution of
dexterous hands and often fail to reason about shape uncertainty inherent in
partial point clouds, leading to unreliable or overly conservative grasps. We
propose FFHFlow, a flow-based variational framework that generates diverse,
robust multi-finger grasps while explicitly quantifying perceptual uncertainty
in the partial point clouds. Our approach leverages a normalizing flow-based
deep latent variable model to learn a hierarchical grasp manifold, overcoming
the mode collapse and rigid prior limitations of conditional Variational
Autoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of
flows, FFHFlow introspects shape uncertainty in partial observations and
identifies novel object structures, enabling risk-aware grasp synthesis. To
further enhance reliability, we integrate a discriminative grasp evaluator with
the flow likelihoods, formulating an uncertainty-aware ranking strategy that
prioritizes grasps robust to shape ambiguity. Extensive experiments in
simulation and real-world setups demonstrate that FFHFlow outperforms
state-of-the-art baselines (including diffusion models) in grasp diversity and
success rate, while achieving run-time efficient sampling. We also showcase its
practical value in cluttered and confined environments, where diversity-driven
sampling excels by mitigating collisions (Project Page:
https://sites.google.com/view/ffhflow/home/).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally, whose ordering decided via
  coin-tossing. Accepted for CoRL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HITTER: A <span class="highlight-title">HumanoId</span> Table TEnnis <span class="highlight-title">Robot</span> via Hierarchical <span class="highlight-title">Planning</span> and
  <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Su, Bike Zhang, Nima Rahmanian, Yuman Gao, Qiayuan Liao, Caitlin Regan, Koushil Sreenath, S. Shankar Sastry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid robots have recently achieved impressive progress in locomotion and
whole-body control, yet they remain constrained in tasks that demand rapid
interaction with dynamic environments through manipulation. Table tennis
exemplifies such a challenge: with ball speeds exceeding 5 m/s, players must
perceive, predict, and act within sub-second reaction times, requiring both
agility and precision. To address this, we present a hierarchical framework for
humanoid table tennis that integrates a model-based planner for ball trajectory
prediction and racket target planning with a reinforcement learning-based
whole-body controller. The planner determines striking position, velocity and
timing, while the controller generates coordinated arm and leg motions that
mimic human strikes and maintain stability and agility across consecutive
rallies. Moreover, to encourage natural movements, human motion references are
incorporated during training. We validate our system on a general-purpose
humanoid robot, achieving up to 106 consecutive shots with a human opponent and
sustained exchanges against another humanoid. These results demonstrate
real-world humanoid table tennis with sub-second reactive control, marking a
step toward agile and interactive humanoid behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>add more references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ First Order Model-Based RL through Decoupled Backpropagation <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Amigo, Rooholla Khorrambakht, Elliot Chane-Sane, Nicolas Mansard, Ludovic Righetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is growing interest in reinforcement learning (RL) methods that
leverage the simulator's derivatives to improve learning efficiency. While
early gradient-based approaches have demonstrated superior performance compared
to derivative-free methods, accessing simulator gradients is often impractical
due to their implementation cost or unavailability. Model-based RL (MBRL) can
approximate these gradients via learned dynamics models, but the solver
efficiency suffers from compounding prediction errors during training rollouts,
which can degrade policy performance. We propose an approach that decouples
trajectory generation from gradient computation: trajectories are unrolled
using a simulator, while gradients are computed via backpropagation through a
learned differentiable model of the simulator. This hybrid design enables
efficient and consistent first-order policy optimization, even when simulator
gradients are unavailable, as well as learning a critic from simulation
rollouts, which is more accurate. Our method achieves the sample efficiency and
speed of specialized optimizers such as SHAC, while maintaining the generality
of standard approaches like PPO and avoiding ill behaviors observed in other
first-order MBRL methods. We empirically validate our algorithm on benchmark
control tasks and demonstrate its effectiveness on a real Go2 quadruped robot,
across both quadrupedal and bipedal locomotion tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoRL 2025. Project website: https://machines-in-motion.github.io/DMO/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification of <span class="highlight-title">Vision</span>-Based Tactile Sensors: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Li, Yijiong Lin, Chenghua Lu, Max Yang, Efi Psomopoulou, Nathan F Lepora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based tactile sensors (VBTS) have gained widespread application in
robotic hands, grippers and prosthetics due to their high spatial resolution,
low manufacturing costs, and ease of customization. While VBTSs have common
design features, such as a camera module, they can differ in a rich diversity
of sensing principles, material compositions, multimodal approaches, and data
interpretation methods. Here, we propose a novel classification of VBTS that
categorizes the technology into two primary sensing principles based on the
underlying transduction of contact into a tactile image: the Marker-Based
Transduction Principle and the Intensity-Based Transduction Principle.
Marker-Based Transduction interprets tactile information by detecting marker
displacement and changes in marker density. In contrast, Intensity-Based
Transduction maps external disturbances with variations in pixel values.
Depending on the design of the contact module, Marker-Based Transduction can be
further divided into two subtypes: Simple Marker-Based (SMB) and Morphological
Marker-Based (MMB) mechanisms. Similarly, the Intensity-Based Transduction
Principle encompasses the Reflective Layer-based (RLB) and Transparent
Layer-Based (TLB) mechanisms. This paper provides a comparative study of the
hardware characteristics of these four types of sensors including various
combination types, and discusses the commonly used methods for interpreting
tactile information. This~comparison reveals some current challenges faced by
VBTS technology and directions for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deliberate <span class="highlight-title">Planning</span> of 3D Bin Packing on Packing Configuration Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04421v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04421v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Zhao, Juzhan Xu, Kexiong Yu, Ruizhen Hu, Chenyang Zhu, Bo Du, Kai Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online 3D Bin Packing Problem (3D-BPP) has widespread applications in
industrial automation. Existing methods usually solve the problem with limited
resolution of spatial discretization, and/or cannot deal with complex practical
constraints well. We propose to enhance the practical applicability of online
3D-BPP via learning on a novel hierarchical representation, packing
configuration tree (PCT). PCT is a full-fledged description of the state and
action space of bin packing which can support packing policy learning based on
deep reinforcement learning (DRL). The size of the packing action space is
proportional to the number of leaf nodes, making the DRL model easy to train
and well-performing even with continuous solution space. We further discover
the potential of PCT as tree-based planners in deliberately solving packing
problems of industrial significance, including large-scale packing and
different variations of BPP setting. A recursive packing method is proposed to
decompose large-scale packing into smaller sub-trees while a spatial ensemble
mechanism integrates local solutions into global. For different BPP variations
with additional decision variables, such as lookahead, buffering, and offline
packing, we propose a unified planning framework enabling out-of-the-box
problem solving. Extensive evaluations demonstrate that our method outperforms
existing online BPP baselines and is versatile in incorporating various
practical constraints. The planning process excels across large-scale problems
and diverse problem variations. We develop a real-world packing robot for
industrial warehousing, with careful designs accounting for constrained
placement and transportation stability. Our packing robot operates reliably and
efficiently on unprotected pallets at 10 seconds per box. It achieves averagely
19 boxes per pallet with 57.4% space utilization for relatively large-size
boxes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Journal of Robotics Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segmented Trajectory <span class="highlight-title">Optimization</span> for Autonomous Parking in Unstructured
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05041v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05041v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Yu, Renjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a Segmented Trajectory Optimization (STO) method for
autonomous parking, which refines an initial trajectory into a dynamically
feasible and collision-free one using an iterative SQP-based approach. STO
maintains the maneuver strategy of the high-level global planner while allowing
curvature discontinuities at switching points to improve maneuver efficiency.
To ensure safety, a convex corridor is constructed via GJK-accelerated ellipse
shrinking and expansion, serving as safety constraints in each iteration.
Numerical simulations in perpendicular and reverse-angled parking scenarios
demonstrate that STO enhances maneuver efficiency while ensuring safety.
Moreover, computational performance confirms its practicality for real-world
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Vision</span>-based <span class="highlight-title">Manipulation</span> from Single Human Video with Open-World Object
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifeng Zhu, Arisrei Lim, Peter Stone, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents an object-centric approach to learning vision-based
manipulation skills from human videos. We investigate the problem of robot
manipulation via imitation in the open-world setting, where a robot learns to
manipulate novel objects from a single video demonstration. We introduce ORION,
an algorithm that tackles the problem by extracting an object-centric
manipulation plan from a single RGB or RGB-D video and deriving a policy that
conditions on the extracted plan. Our method enables the robot to learn from
videos captured by daily mobile devices and to generalize the policies to
deployment environments with varying visual backgrounds, camera angles, spatial
layouts, and novel object instances. We systematically evaluate our method on
both short-horizon and long-horizon tasks, using RGB-D and RGB-only
demonstration videos. Across varied tasks and demonstration types (RGB-D /
RGB), we observe an average success rate of 74.4%, demonstrating the efficacy
of ORION in learning from a single human video in the open world. Additional
materials can be found on our project website:
https://ut-austin-rpl.github.io/ORION-release.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of paper adding results with RGB-only demonstration
  videos uploaded on 09/04/2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Digital Twin for <span class="highlight-title">Robot</span>ic Post Mortem Tissue Sampling using Virtual
  Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02760v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02760v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Neidhardt, Ludwig Bosse, Vidas Raudonis, Kristina Allgoewer, Axel Heinemann, Benjamin Ondruschka, Alexander Schlaefer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studying tissue samples obtained during autopsies is the gold standard when
diagnosing the cause of death and for understanding disease pathophysiology.
Recently, the interest in post mortem minimally invasive biopsies has grown
which is a less destructive approach in comparison to an open autopsy and
reduces the risk of infection. While manual biopsies under ultrasound guidance
are more widely performed, robotic post mortem biopsies have been recently
proposed. This approach can further reduce the risk of infection for
physicians. However, planning of the procedure and control of the robot need to
be efficient and usable. We explore a virtual reality setup with a digital twin
to realize fully remote planning and control of robotic post mortem biopsies.
The setup is evaluated with forensic pathologists in a usability study for
three interaction methods. Furthermore, we evaluate clinical feasibility and
evaluate the system with three human cadavers. Overall, 132 needle insertions
were performed with an off-axis needle placement error of 5.30+-3.25 mm. Tissue
samples were successfully biopsied and histopathologically verified. Users
reported a very intuitive needle placement approach, indicating that the system
is a promising, precise, and low-risk alternative to conventional approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClutterDexGrasp: A <span class="highlight-title">Sim-to-Real</span> System for General Dexterous Grasping in
  Cluttered Scenes <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14317v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14317v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyuan Chen, Qiyang Yan, Yuanpei Chen, Tianhao Wu, Jiyao Zhang, Zihan Ding, Jinzhou Li, Yaodong Yang, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dexterous grasping in cluttered scenes presents significant challenges due to
diverse object geometries, occlusions, and potential collisions. Existing
methods primarily focus on single-object grasping or grasp-pose prediction
without interaction, which are insufficient for complex, cluttered scenes.
Recent vision-language-action models offer a potential solution but require
extensive real-world demonstrations, making them costly and difficult to scale.
To address these limitations, we revisit the sim-to-real transfer pipeline and
develop key techniques that enable zero-shot deployment in reality while
maintaining robust generalization. We propose ClutterDexGrasp, a two-stage
teacher-student framework for closed-loop target-oriented dexterous grasping in
cluttered scenes. The framework features a teacher policy trained in simulation
using clutter density curriculum learning, incorporating both a geometry and
spatially-embedded scene representation and a novel comprehensive safety
curriculum, enabling general, dynamic, and safe grasping behaviors. Through
imitation learning, we distill the teacher's knowledge into a student 3D
diffusion policy (DP3) that operates on partial point cloud observations. To
the best of our knowledge, this represents the first zero-shot sim-to-real
closed-loop system for target-oriented dexterous grasping in cluttered scenes,
demonstrating robust performance across diverse objects and layouts. More
details and videos are available at https://clutterdexgrasp.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CoRL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMT: General Motion Tracking for <span class="highlight-title">Humanoid</span> <span class="highlight-title">Whole-Body Control</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixuan Chen, Mazeyu Ji, Xuxin Cheng, Xuanbin Peng, Xue Bin Peng, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to track general whole-body motions in the real world is a useful
way to build general-purpose humanoid robots. However, achieving this can be
challenging due to the temporal and kinematic diversity of the motions, the
policy's capability, and the difficulty of coordination of the upper and lower
bodies. To address these issues, we propose GMT, a general and scalable
motion-tracking framework that trains a single unified policy to enable
humanoid robots to track diverse motions in the real world. GMT is built upon
two core components: an Adaptive Sampling strategy and a Motion
Mixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically
balances easy and difficult motions during training. The MoE ensures better
specialization of different regions of the motion manifold. We show through
extensive experiments in both simulation and the real world the effectiveness
of GMT, achieving state-of-the-art performance across a broad spectrum of
motions using a unified general policy. Videos and additional information can
be found at https://gmt-humanoid.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span> Offline Imitation <span class="highlight-title">Learning</span> Through State-level Trajectory
  Stitching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.22524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.22524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuze Wang, Yunpeng Mei, Hongjie Cao, Yetian Yuan, Gang Wang, Jian Sun, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning (IL) has proven effective for enabling robots to acquire
visuomotor skills through expert demonstrations. However, traditional IL
methods are limited by their reliance on high-quality, often scarce, expert
data, and suffer from covariate shift. To address these challenges, recent
advances in offline IL have incorporated suboptimal, unlabeled datasets into
the training. In this paper, we propose a novel approach to enhance policy
learning from mixed-quality offline datasets by leveraging task-relevant
trajectory fragments and rich environmental dynamics. Specifically, we
introduce a state-based search framework that stitches state-action pairs from
imperfect demonstrations, generating more diverse and informative training
trajectories. Experimental results on standard IL benchmarks and real-world
robotic tasks showcase that our proposed method significantly improves both
generalization and performance.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">122</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Virtual Fitting Room: Generating Arbitrarily Long Videos of Virtual
  Try-On from a Single Image -- Technical P<span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Kun Chen, Aayush Bansal, Minh Phuoc Vo, Yu-Xiong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Virtual Fitting Room (VFR), a novel video generative model
that produces arbitrarily long virtual try-on videos. Our VFR models long video
generation tasks as an auto-regressive, segment-by-segment generation process,
eliminating the need for resource-intensive generation and lengthy video data,
while providing the flexibility to generate videos of arbitrary length. The key
challenges of this task are twofold: ensuring local smoothness between adjacent
segments and maintaining global temporal consistency across different segments.
To address these challenges, we propose our VFR framework, which ensures
smoothness through a prefix video condition and enforces consistency with the
anchor video -- a 360-degree video that comprehensively captures the human's
wholebody appearance. Our VFR generates minute-scale virtual try-on videos with
both local smoothness and global temporal consistency under various motions,
making it a pioneering work in long virtual try-on video generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://immortalco.github.io/VirtualFittingRoom/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TRUST-VL: An Explainable News Assistant for General Multimodal
  Misinformation Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehong Yan, Peng Qi, Wynne Hsu, Mong Li Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal misinformation, encompassing textual, visual, and cross-modal
distortions, poses an increasing societal threat that is amplified by
generative AI. Existing methods typically focus on a single type of distortion
and struggle to generalize to unseen scenarios. In this work, we observe that
different distortion types share common reasoning capabilities while also
requiring task-specific skills. We hypothesize that joint training across
distortion types facilitates knowledge sharing and enhances the model's ability
to generalize. To this end, we introduce TRUST-VL, a unified and explainable
vision-language model for general multimodal misinformation detection. TRUST-VL
incorporates a novel Question-Aware Visual Amplifier module, designed to
extract task-specific visual features. To support training, we also construct
TRUST-Instruct, a large-scale instruction dataset containing 198K samples
featuring structured reasoning chains aligned with human fact-checking
workflows. Extensive experiments on both in-domain and zero-shot benchmarks
demonstrate that TRUST-VL achieves state-of-the-art performance, while also
offering strong generalization and interpretability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025; Project Homepage: https://yanzehong.github.io/trust-vl/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Plot'n Polish: Zero-shot Story <span class="highlight-title">Visual</span>ization and Disentangled Editing
  with Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiymet Akdemir, Jing Shi, Kushal Kafle, Brian Price, Pinar Yanardag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have demonstrated significant capabilities to
generate diverse and detailed visuals in various domains, and story
visualization is emerging as a particularly promising application. However, as
their use in real-world creative domains increases, the need for providing
enhanced control, refinement, and the ability to modify images post-generation
in a consistent manner becomes an important challenge. Existing methods often
lack the flexibility to apply fine or coarse edits while maintaining visual and
narrative consistency across multiple frames, preventing creators from
seamlessly crafting and refining their visual stories. To address these
challenges, we introduce Plot'n Polish, a zero-shot framework that enables
consistent story generation and provides fine-grained control over story
visualizations at various levels of detail.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One Flight Over the Gap: A <span class="highlight-title">Survey</span> from Perspective to Panoramic <span class="highlight-title">Vision</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Lin, Xian Ge, Dizhe Zhang, Zhaoliang Wan, Xianshun Wang, Xiangtai Li, Wenjie Jiang, Bo Du, Dacheng Tao, Ming-Hsuan Yang, Lu Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by the demand for spatial intelligence and holistic scene perception,
omnidirectional images (ODIs), which provide a complete 360\textdegree{} field
of view, are receiving growing attention across diverse applications such as
virtual reality, autonomous driving, and embodied robotics. Despite their
unique characteristics, ODIs exhibit remarkable differences from perspective
images in geometric projection, spatial distribution, and boundary continuity,
making it challenging for direct domain adaption from perspective methods. This
survey reviews recent panoramic vision techniques with a particular emphasis on
the perspective-to-panorama adaptation. We first revisit the panoramic imaging
pipeline and projection methods to build the prior knowledge required for
analyzing the structural disparities. Then, we summarize three challenges of
domain adaptation: severe geometric distortions near the poles, non-uniform
sampling in Equirectangular Projection (ERP), and periodic boundary continuity.
Building on this, we cover 20+ representative tasks drawn from more than 300
research papers in two dimensions. On one hand, we present a cross-method
analysis of representative strategies for addressing panoramic specific
challenges across different tasks. On the other hand, we conduct a cross-task
comparison and classify panoramic vision into four major categories: visual
quality enhancement and assessment, visual understanding, multimodal
understanding, and visual generation. In addition, we discuss open challenges
and future directions in data, models, and applications that will drive the
advancement of panoramic vision research. We hope that our work can provide new
insight and forward looking perspectives to advance the development of
panoramic vision technologies. Our project page is
https://insta360-research-team.github.io/Survey-of-Panorama
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEXOP: A Device for <span class="highlight-title">Robot</span>ic Transfer of Dexterous Human <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Shu Fang, Branden Romero, Yichen Xie, Arthur Hu, Bo-Ruei Huang, Juan Alvarez, Matthew Kim, Gabriel Margolis, Kavya Anbarasu, Masayoshi Tomizuka, Edward Adelson, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce perioperation, a paradigm for robotic data collection that
sensorizes and records human manipulation while maximizing the transferability
of the data to real robots. We implement this paradigm in DEXOP, a passive hand
exoskeleton designed to maximize human ability to collect rich sensory (vision
+ tactile) data for diverse dexterous manipulation tasks in natural
environments. DEXOP mechanically connects human fingers to robot fingers,
providing users with direct contact feedback (via proprioception) and mirrors
the human hand pose to the passive robot hand to maximize the transfer of
demonstrated skills to the robot. The force feedback and pose mirroring make
task demonstrations more natural for humans compared to teleoperation,
increasing both speed and accuracy. We evaluate DEXOP across a range of
dexterous, contact-rich tasks, demonstrating its ability to collect
high-quality demonstration data at scale. Policies learned with DEXOP data
significantly improve task performance per unit time of data collection
compared to teleoperation, making DEXOP a powerful tool for advancing robot
dexterity. Our project page is at https://dex-op.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://dex-op.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Telephone Game: Evaluating <span class="highlight-title">Semantic</span> Drift in Unified Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabbir Mollah, Rohit Gupta, Sirnam Swetha, Qingyang Liu, Ahnaf Munir, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Employing a single, unified model (UM) for both visual understanding
(image-to-text: I2T) and and visual generation (text-to-image: T2I) has opened
a new direction in Visual Language Model (VLM) research. While UMs can also
support broader unimodal tasks (e.g., text-to-text, image-to-image), we focus
on the core cross-modal pair T2I and I2T, as consistency between understanding
and generation is critical for downstream use. Existing evaluations consider
these capabilities in isolation: FID and GenEval for T2I, and benchmarks such
as MME, MMBench for I2T. These single-pass metrics do not reveal whether a
model that understands a concept can also render it, nor whether meaning is
preserved when cycling between image and text modalities. To address this, we
introduce the Unified Consistency Framework for Unified Models (UCF-UM), a
cyclic evaluation protocol that alternates I2T and T2I over multiple
generations to quantify semantic drift. UCF formulates 3 metrics: (i) Mean
Cumulative Drift (MCD), an embedding-based measure of overall semantic loss;
(ii) Semantic Drift Rate (SDR), that summarizes semantic decay rate; and (iii)
Multi-Generation GenEval (MGG), an object-level compliance score extending
GenEval. To assess generalization beyond COCO, which is widely used in
training; we create a new benchmark ND400, sampled from NoCaps and DOCCI and
evaluate on seven recent models. UCF-UM reveals substantial variation in
cross-modal stability: some models like BAGEL maintain semantics over many
alternations, whereas others like Vila-u drift quickly despite strong
single-pass scores. Our results highlight cyclic consistency as a necessary
complement to standard I2T and T2I evaluations, and provide practical metrics
to consistently assess unified model's cross-modal stability and strength of
their shared representations. Code:
https://github.com/mollahsabbir/Semantic-Drift-in-Unified-Models
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Lines to Shapes: Geometric-Constrained Segmentation of X-Ray
  Collimators via Hough Transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin El-Zein, Dominik Eckert, Andreas Fieselmann, Christopher Syben, Ludwig Ritschl, Steffen Kappler, Sebastian Stober
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collimation in X-ray imaging restricts exposure to the region-of-interest
(ROI) and minimizes the radiation dose applied to the patient. The detection of
collimator shadows is an essential image-based preprocessing step in digital
radiography posing a challenge when edges get obscured by scattered X-ray
radiation. Regardless, the prior knowledge that collimation forms
polygonal-shaped shadows is evident. For this reason, we introduce a deep
learning-based segmentation that is inherently constrained to its geometry. We
achieve this by incorporating a differentiable Hough transform-based network to
detect the collimation borders and enhance its capability to extract the
information about the ROI center. During inference, we combine the information
of both tasks to enable the generation of refined, line-constrained
segmentation masks. We demonstrate robust reconstruction of collimated regions
achieving median Hausdorff distances of 4.3-5.0mm on diverse test sets of real
Xray images. While this application involves at most four shadow borders, our
method is not fundamentally limited by a specific number of edges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Durian: Dual Reference-guided Portrait Animation with Attribute Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunsoo Cha, Byungjun Kim, Hanbyul Joo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Durian, the first method for generating portrait animation videos
with facial attribute transfer from a given reference image to a target
portrait in a zero-shot manner. To enable high-fidelity and spatially
consistent attribute transfer across frames, we introduce dual reference
networks that inject spatial features from both the portrait and attribute
images into the denoising process of a diffusion model. We train the model
using a self-reconstruction formulation, where two frames are sampled from the
same portrait video: one is treated as the attribute reference and the other as
the target portrait, and the remaining frames are reconstructed conditioned on
these inputs and their corresponding masks. To support the transfer of
attributes with varying spatial extent, we propose a mask expansion strategy
using keypoint-conditioned image generation for training. In addition, we
further augment the attribute and portrait images with spatial and
appearance-level transformations to improve robustness to positional
misalignment between them. These strategies allow the model to effectively
generalize across diverse attributes and in-the-wild reference combinations,
despite being trained without explicit triplet supervision. Durian achieves
state-of-the-art performance on portrait animation with attribute transfer, and
notably, its dual reference design enables multi-attribute composition in a
single generation pass without additional training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://hyunsoocha.github.io/durian</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-step Flow for 3D Generation via Marginal-Data Transport Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zanwei Zhou, Taoran Yi, Jiemin Fang, Chen Yang, Lingxi Xie, Xinggang Wang, Wei Shen, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flow-based 3D generation models typically require dozens of sampling steps
during inference. Though few-step distillation methods, particularly
Consistency Models (CMs), have achieved substantial advancements in
accelerating 2D diffusion models, they remain under-explored for more complex
3D generation tasks. In this study, we propose a novel framework, MDT-dist, for
few-step 3D flow distillation. Our approach is built upon a primary objective:
distilling the pretrained model to learn the Marginal-Data Transport. Directly
learning this objective needs to integrate the velocity fields, while this
integral is intractable to be implemented. Therefore, we propose two
optimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),
to equivalently convert the optimization target from the transport level to the
velocity and the distribution level respectively. Velocity Matching (VM) learns
to stably match the velocity fields between the student and the teacher, but
inevitably provides biased gradient estimates. Velocity Distillation (VD)
further enhances the optimization process by leveraging the learned velocity
fields to perform probability density distillation. When evaluated on the
pioneer 3D generation framework TRELLIS, our method reduces sampling steps of
each flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s
(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high
visual and geometric fidelity. Extensive experiments demonstrate that our
method significantly outperforms existing CM distillation methods, and enables
TRELLIS to achieve superior performance in few-step 3D generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/Zanue/MDT-dist</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-adaptive <span class="highlight-title">Dataset</span> Construction for Real-World Multimodal Safety
  Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingen Qu, Lijun Li, Bo Zhang, Yichen Yan, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) are rapidly evolving, presenting
increasingly complex safety challenges. However, current dataset construction
methods, which are risk-oriented, fail to cover the growing complexity of
real-world multimodal safety scenarios (RMS). And due to the lack of a unified
evaluation metric, their overall effectiveness remains unproven. This paper
introduces a novel image-oriented self-adaptive dataset construction method for
RMS, which starts with images and end constructing paired text and guidance
responses. Using the image-oriented method, we automatically generate an RMS
dataset comprising 35k image-text pairs with guidance responses. Additionally,
we introduce a standardized safety dataset evaluation metric: fine-tuning a
safety judge model and evaluating its capabilities on other safety
datasets.Extensive experiments on various tasks demonstrate the effectiveness
of the proposed image-oriented pipeline. The results confirm the scalability
and effectiveness of the image-oriented approach, offering a new perspective
for the construction of real-world multimodal safety datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Learning</span> neural representations for X-ray ptychography reconstruction
  with unknown probes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyou Li, Zixin Xu, Zirui Gao, Hanfei Yan, Xiaojing Huang, Jizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  X-ray ptychography provides exceptional nanoscale resolution and is widely
applied in materials science, biology, and nanotechnology. However, its full
potential is constrained by the critical challenge of accurately reconstructing
images when the illuminating probe is unknown. Conventional iterative methods
and deep learning approaches are often suboptimal, particularly under the
low-signal conditions inherent to low-dose and high-speed experiments. These
limitations compromise reconstruction fidelity and restrict the broader
adoption of the technique. In this work, we introduce the Ptychographic
Implicit Neural Representation (PtyINR), a self-supervised framework that
simultaneously addresses the object and probe recovery problem. By
parameterizing both as continuous neural representations, PtyINR performs
end-to-end reconstruction directly from raw diffraction patterns without
requiring any pre-characterization of the probe. Extensive evaluations
demonstrate that PtyINR achieves superior reconstruction quality on both
simulated and experimental data, with remarkable robustness under challenging
low-signal conditions. Furthermore, PtyINR offers a generalizable,
physics-informed framework for addressing probe-dependent inverse problems,
making it applicable to a wide range of computational microscopy problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transition Models: Rethinking the <span class="highlight-title">Generative</span> <span class="highlight-title">Learning</span> Objective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue, Yangguang Li, Wanli Ouyang, Lei Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental dilemma in generative modeling persists: iterative diffusion
models achieve outstanding fidelity, but at a significant computational cost,
while efficient few-step alternatives are constrained by a hard quality
ceiling. This conflict between generation steps and output quality arises from
restrictive training objectives that focus exclusively on either infinitesimal
dynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by
introducing an exact, continuous-time dynamics equation that analytically
defines state transitions across any finite time interval. This leads to a
novel generative paradigm, Transition Models (TiM), which adapt to
arbitrary-step transitions, seamlessly traversing the generative trajectory
from single leaps to fine-grained refinement with more steps. Despite having
only 865M parameters, TiM achieves state-of-the-art performance, surpassing
leading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across
all evaluated step counts. Importantly, unlike previous few-step generators,
TiM demonstrates monotonic quality improvement as the sampling budget
increases. Additionally, when employing our native-resolution strategy, TiM
delivers exceptional fidelity at resolutions up to 4096x4096.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is released at https://github.com/WZDTHU/TiM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSGaussian: <span class="highlight-title">Semantic</span>-Aware and Structure-Preserving 3D Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimin Xu, Bosheng Qin, Tao Jin, Zhou Zhao, Zhenhui Ye, Jun Yu, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in neural representations, such as Neural Radiance Fields
and 3D Gaussian Splatting, have increased interest in applying style transfer
to 3D scenes. While existing methods can transfer style patterns onto
3D-consistent neural representations, they struggle to effectively extract and
transfer high-level style semantics from the reference style image.
Additionally, the stylized results often lack structural clarity and
separation, making it difficult to distinguish between different instances or
objects within the 3D scene. To address these limitations, we propose a novel
3D style transfer pipeline that effectively integrates prior knowledge from
pretrained 2D diffusion models. Our pipeline consists of two key stages: First,
we leverage diffusion priors to generate stylized renderings of key viewpoints.
Then, we transfer the stylized key views onto the 3D representation. This
process incorporates two innovative designs. The first is cross-view style
alignment, which inserts cross-view attention into the last upsampling block of
the UNet, allowing feature interactions across multiple key views. This ensures
that the diffusion model generates stylized key views that maintain both style
fidelity and instance-level consistency. The second is instance-level style
transfer, which effectively leverages instance-level consistency across
stylized key views and transfers it onto the 3D representation. This results in
a more structured, visually coherent, and artistically enriched stylization.
Extensive qualitative and quantitative experiments demonstrate that our 3D
style transfer pipeline significantly outperforms state-of-the-art methods
across a wide range of scenes, from forward-facing to challenging 360-degree
environments. Visit our project page https://jm-xu.github.io/SSGaussian for
immersive visualization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aesthetic Image Captioning with Saliency Enhanced MLLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilin Tao, Jiashui Huang, Huaze Xu, Ling Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aesthetic Image Captioning (AIC) aims to generate textual descriptions of
image aesthetics, becoming a key research direction in the field of
computational aesthetics. In recent years, pretrained Multimodal Large Language
Models (MLLMs) have advanced rapidly, leading to a significant increase in
image aesthetics research that integrates both visual and textual modalities.
However, most existing studies on image aesthetics primarily focus on
predicting aesthetic ratings and have shown limited application in AIC.
Existing AIC works leveraging MLLMs predominantly rely on fine-tuning methods
without specifically adapting MLLMs to focus on target aesthetic content. To
address this limitation, we propose the Aesthetic Saliency Enhanced Multimodal
Large Language Model (ASE-MLLM), an end-to-end framework that explicitly
incorporates aesthetic saliency into MLLMs. Within this framework, we introduce
the Image Aesthetic Saliency Module (IASM), which efficiently and effectively
extracts aesthetic saliency features from images. Additionally, we design
IAS-ViT as the image encoder for MLLMs, this module fuses aesthetic saliency
features with original image features via a cross-attention mechanism. To the
best of our knowledge, ASE-MLLM is the first framework to integrate image
aesthetic saliency into MLLMs specifically for AIC tasks. Extensive experiments
demonstrated that our approach significantly outperformed traditional methods
and generic MLLMs on current mainstream AIC benchmarks, achieving
state-of-the-art (SOTA) performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AnomalyLMM: Bridging <span class="highlight-title">Generative</span> Knowledge and Discriminative Retrieval
  for Text-Based Person Anomaly Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ju, Hu Zhang, Zhedong Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With growing public safety demands, text-based person anomaly search has
emerged as a critical task, aiming to retrieve individuals with abnormal
behaviors via natural language descriptions. Unlike conventional person search,
this task presents two unique challenges: (1) fine-grained cross-modal
alignment between textual anomalies and visual behaviors, and (2) anomaly
recognition under sparse real-world samples. While Large Multi-modal Models
(LMMs) excel in multi-modal understanding, their potential for fine-grained
anomaly retrieval remains underexplored, hindered by: (1) a domain gap between
generative knowledge and discriminative retrieval, and (2) the absence of
efficient adaptation strategies for deployment. In this work, we propose
AnomalyLMM, the first framework that harnesses LMMs for text-based person
anomaly search. Our key contributions are: (1) A novel coarse-to-fine pipeline
integrating LMMs to bridge generative world knowledge with retrieval-centric
anomaly detection; (2) A training-free adaptation cookbook featuring masked
cross-modal prompting, behavioral saliency prediction, and knowledge-aware
re-ranking, enabling zero-shot focus on subtle anomaly cues. As the first study
to explore LMMs for this task, we conduct a rigorous evaluation on the PAB
dataset, the only publicly available benchmark for text-based person anomaly
search, with its curated real-world anomalies covering diverse scenarios (e.g.,
falling, collision, and being hit). Experiments show the effectiveness of the
proposed method, surpassing the competitive baseline by +0.96% Recall@1
accuracy. Notably, our method reveals interpretable alignment between textual
anomalies and visual behaviors, validated via qualitative analysis. Our code
and models will be released for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stitching the Story: Creating Panoramic Incident Summaries from
  Body-Worn Footage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04370v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04370v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dor Cohen, Inga Efrosman, Yehudit Aperstein, Alexander Apartsin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  First responders widely adopt body-worn cameras to document incident scenes
and support post-event analysis. However, reviewing lengthy video footage is
impractical in time-critical situations. Effective situational awareness
demands a concise visual summary that can be quickly interpreted. This work
presents a computer vision pipeline that transforms body-camera footage into
informative panoramic images summarizing the incident scene. Our method
leverages monocular Simultaneous Localization and Mapping (SLAM) to estimate
camera trajectories and reconstruct the spatial layout of the environment. Key
viewpoints are identified by clustering camera poses along the trajectory, and
representative frames from each cluster are selected. These frames are fused
into spatially coherent panoramic images using multi-frame stitching
techniques. The resulting summaries enable rapid understanding of complex
environments and facilitate efficient decision-making and incident review.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global-to-Local or Local-to-Global? Enhancing Image Retrieval with
  Efficient Local Search and Effective Global Re-ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dror Aiger, Bingyi Cao, Kaifeng Chen, Andre Araujo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dominant paradigm in image retrieval systems today is to search large
databases using global image features, and re-rank those initial results with
local image feature matching techniques. This design, dubbed global-to-local,
stems from the computational cost of local matching approaches, which can only
be afforded for a small number of retrieved images. However, emerging efficient
local feature search approaches have opened up new possibilities, in particular
enabling detailed retrieval at large scale, to find partial matches which are
often missed by global feature search. In parallel, global feature-based
re-ranking has shown promising results with high computational efficiency. In
this work, we leverage these building blocks to introduce a local-to-global
retrieval paradigm, where efficient local feature search meets effective global
feature re-ranking. Critically, we propose a re-ranking method where global
features are computed on-the-fly, based on the local feature retrieval
similarities. Such re-ranking-only global features leverage multidimensional
scaling techniques to create embeddings which respect the local similarities
obtained during search, enabling a significant re-ranking boost.
Experimentally, we demonstrate solid retrieval performance, setting new
state-of-the-art results on the Revisited Oxford and Paris datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MICACL: Multi-Instance Category-Aware Contrastive <span class="highlight-title">Learning</span> for
  Long-Tailed <span class="highlight-title">Dynamic</span> Facial Expression Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng-Qi Cui, Zhen Lin, Xinlong Rao, Anyang Tong, Shiyao Li, Fei Wang, Changlin Chen, Bin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic facial expression recognition (DFER) faces significant challenges due
to long-tailed category distributions and complexity of spatio-temporal feature
modeling. While existing deep learning-based methods have improved DFER
performance, they often fail to address these issues, resulting in severe model
induction bias. To overcome these limitations, we propose a novel
multi-instance learning framework called MICACL, which integrates
spatio-temporal dependency modeling and long-tailed contrastive learning
optimization. Specifically, we design the Graph-Enhanced Instance Interaction
Module (GEIIM) to capture intricate spatio-temporal between adjacent instances
relationships through adaptive adjacency matrices and multiscale convolutions.
To enhance instance-level feature aggregation, we develop the Weighted Instance
Aggregation Network (WIAN), which dynamically assigns weights based on instance
importance. Furthermore, we introduce a Multiscale Category-aware Contrastive
Learning (MCCL) strategy to balance training between major and minor
categories. Extensive experiments on in-the-wild datasets (i.e., DFEW and
FERV39k) demonstrate that MICACL achieves state-of-the-art performance with
superior robustness and generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE ISPA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Editor to Dense Geometry Estimator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JiYuan Wang, Chunyu Lin, Lei Sun, Rongying Liu, Lang Nie, Mingxing Li, Kang Liao, Xiangxiang Chu, Yao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging visual priors from pre-trained text-to-image (T2I) generative
models has shown success in dense prediction. However, dense prediction is
inherently an image-to-image task, suggesting that image editing models, rather
than T2I generative models, may be a more suitable foundation for fine-tuning.
  Motivated by this, we conduct a systematic analysis of the fine-tuning
behaviors of both editors and generators for dense geometry estimation. Our
findings show that editing models possess inherent structural priors, which
enable them to converge more stably by ``refining" their innate features, and
ultimately achieve higher performance than their generative counterparts.
  Based on these findings, we introduce \textbf{FE2E}, a framework that
pioneeringly adapts an advanced editing model based on Diffusion Transformer
(DiT) architecture for dense geometry prediction. Specifically, to tailor the
editor for this deterministic task, we reformulate the editor's original flow
matching loss into the ``consistent velocity" training objective. And we use
logarithmic quantization to resolve the precision conflict between the editor's
native BFloat16 format and the high precision demand of our tasks.
Additionally, we leverage the DiT's global attention for a cost-free joint
estimation of depth and normals in a single forward pass, enabling their
supervisory signals to mutually enhance each other.
  Without scaling up the training data, FE2E achieves impressive performance
improvements in zero-shot monocular depth and normal estimation across multiple
datasets. Notably, it achieves over 35\% performance gains on the ETH3D dataset
and outperforms the DepthAnything series, which is trained on 100$\times$ data.
The project page can be accessed \href{https://amap-ml.github.io/FE2E/}{here}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoArena: An Open Platform for Benchmarking Large <span class="highlight-title">Vision</span>-language Models
  on WorldWide Image Geo<span class="highlight-title">localization</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengyue Jia, Yingyi Zhang, Xiangyu Zhao, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image geolocalization aims to predict the geographic location of images
captured anywhere on Earth, but its global nature presents significant
challenges. Current evaluation methodologies suffer from two major limitations.
First, data leakage: advanced approaches often rely on large vision-language
models (LVLMs) to predict image locations, yet these models are frequently
pretrained on the test datasets, compromising the accuracy of evaluating a
model's actual geolocalization capability. Second, existing metrics primarily
rely on exact geographic coordinates to assess predictions, which not only
neglects the reasoning process but also raises privacy concerns when user-level
location data is required. To address these issues, we propose GeoArena, a
first open platform for evaluating LVLMs on worldwide image geolocalization
tasks, offering true in-the-wild and human-centered benchmarking. GeoArena
enables users to upload in-the-wild images for a more diverse evaluation
corpus, and it leverages pairwise human judgments to determine which model
output better aligns with human expectations. Our platform has been deployed
online for two months, during which we collected over thousands voting records.
Based on this data, we conduct a detailed analysis and establish a leaderboard
of different LVLMs on the image geolocalization task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Odd-One-Out Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvio Chito, Paolo Rabino, Tatiana Tommasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently introduced odd-one-out anomaly detection task involves
identifying the odd-looking instances within a multi-object scene. This problem
presents several challenges for modern deep learning models, demanding spatial
reasoning across multiple views and relational reasoning to understand context
and generalize across varying object categories and layouts. We argue that
these challenges must be addressed with efficiency in mind. To this end, we
propose a DINO-based model that reduces the number of parameters by one third
and shortens training time by a factor of three compared to the current
state-of-the-art, while maintaining competitive performance. Our experimental
evaluation also introduces a Multimodal Large Language Model baseline,
providing insights into its current limitations in structured visual reasoning
tasks. The project page can be found at
https://silviochito.github.io/EfficientOddOneOut/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICIAP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OVGrasp: <span class="highlight-title">Open-Vocabulary</span> Grasping Assistance via Multimodal Intent
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Hu, Shan Luo, Letizia Gionfrida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grasping assistance is essential for restoring autonomy in individuals with
motor impairments, particularly in unstructured environments where object
categories and user intentions are diverse and unpredictable. We present
OVGrasp, a hierarchical control framework for soft exoskeleton-based grasp
assistance that integrates RGB-D vision, open-vocabulary prompts, and voice
commands to enable robust multimodal interaction. To enhance generalization in
open environments, OVGrasp incorporates a vision-language foundation model with
an open-vocabulary mechanism, allowing zero-shot detection of previously unseen
objects without retraining. A multimodal decision-maker further fuses spatial
and linguistic cues to infer user intent, such as grasp or release, in
multi-object scenarios. We deploy the complete framework on a custom
egocentric-view wearable exoskeleton and conduct systematic evaluations on 15
objects across three grasp types. Experimental results with ten participants
demonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,
outperforming state-of-the-art baselines and achieving improved kinematic
alignment with natural hand motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noisy Label Refinement with <span class="highlight-title">Semantic</span>ally Reliable Synthetic Images <span class="chip">ICIP2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingxuan Li, Jiafeng Mao, Yusuke Matsui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic noise in image classification datasets, where visually similar
categories are frequently mislabeled, poses a significant challenge to
conventional supervised learning approaches. In this paper, we explore the
potential of using synthetic images generated by advanced text-to-image models
to address this issue. Although these high-quality synthetic images come with
reliable labels, their direct application in training is limited by domain gaps
and diversity constraints. Unlike conventional approaches, we propose a novel
method that leverages synthetic images as reliable reference points to identify
and correct mislabeled samples in noisy datasets. Extensive experiments across
multiple benchmark datasets show that our approach significantly improves
classification accuracy under various noise conditions, especially in
challenging scenarios with semantic label noise. Additionally, since our method
is orthogonal to existing noise-robust learning techniques, when combined with
state-of-the-art noise-robust training methods, it achieves superior
performance, improving accuracy by 30% on CIFAR-10 and by 11% on CIFAR-100
under 70% semantic noise, and by 24% on ImageNet-100 under real-world noise
conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICIP2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAOLI: Pose-free Articulated Object <span class="highlight-title">Learning</span> from Sparse-view Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianning Deng, Kartic Subr, Hakan Bilen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel self-supervised framework for learning articulated object
representations from sparse-view, unposed images. Unlike prior methods that
require dense multi-view observations and ground-truth camera poses, our
approach operates with as few as four views per articulation and no camera
supervision. To address the inherent challenges, we first reconstruct each
articulation independently using recent advances in sparse-view 3D
reconstruction, then learn a deformation field that establishes dense
correspondences across poses. A progressive disentanglement strategy further
separates static from moving parts, enabling robust separation of camera and
object motion. Finally, we jointly optimize geometry, appearance, and
kinematics with a self-supervised loss that enforces cross-view and cross-pose
consistency. Experiments on the standard benchmark and real-world examples
demonstrate that our method produces accurate and detailed articulated object
representations under significantly weaker input assumptions than existing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Scale Volume Priors with Wasserstein-Based Consistency for
  Semi-Supervised Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junying Meng, Gangxuan Zhou, Jun Liu, Weihong Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite signi cant progress in semi-supervised medical image segmentation,
most existing segmentation networks overlook e ective methodological guidance
for feature extraction and important prior information from
  datasets. In this paper, we develop a semi-supervised medical image
segmentation framework that e ectively integrates spatial regularization
methods and volume priors. Speci cally, our approach integrates a strong
explicit volume prior at the image scale and Threshold Dynamics spatial
regularization, both derived from variational models, into the backbone
segmentation network. The target region volumes for each unlabeled image are
estimated by a regression network, which e ectively regularizes the backbone
segmentation network through an image-scale Wasserstein distance constraint,
ensuring that the class ratios in the segmentation results for each unlabeled
image match those predicted by the regression network. Additionally, we design
a dataset-scale Wasserstein distance loss function based on a weak implicit
volume prior, which enforces that the volume distribution predicted for the
unlabeled dataset is similar to that of labeled dataset. Experimental results
on the 2017 ACDC dataset, PROMISE12 dataset, and thigh muscle MR image dataset
show the superiority of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TauGenNet: Plasma-Driven Tau PET Image Synthesis via Text-Guided 3D
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Gong, Se-in Jang, Wei Shao, Yi Su, Kuang Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate quantification of tau pathology via tau positron emission tomography
(PET) scan is crucial for diagnosing and monitoring Alzheimer's disease (AD).
However, the high cost and limited availability of tau PET restrict its
widespread use. In contrast, structural magnetic resonance imaging (MRI) and
plasma-based biomarkers provide non-invasive and widely available complementary
information related to brain anatomy and disease progression. In this work, we
propose a text-guided 3D diffusion model for 3D tau PET image synthesis,
leveraging multimodal conditions from both structural MRI and plasma
measurement. Specifically, the textual prompt is from the plasma p-tau217
measurement, which is a key indicator of AD progression, while MRI provides
anatomical structure constraints. The proposed framework is trained and
evaluated using clinical AV1451 tau PET data from the Alzheimer's Disease
Neuroimaging Initiative (ADNI) database. Experimental results demonstrate that
our approach can generate realistic, clinically meaningful 3D tau PET across a
range of disease stages. The proposed framework can help perform tau PET data
augmentation under different settings, provide a non-invasive, cost-effective
alternative for visualizing tau pathology, and support the simulation of
disease progression under varying plasma biomarker levels and cognitive
conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, submitted to IEEE Transactions on Radiation and
  Plasma Medical Sciences</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differential Morphological Profile Neural Networks for <span class="highlight-title">Semantic</span>
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Huangal, J. Alex Hurt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of overhead remote sensing imagery enables applications
in mapping, urban planning, and disaster response. State-of-the-art
segmentation networks are typically developed and tuned on ground-perspective
photographs and do not directly address remote sensing challenges such as
extreme scale variation, foreground-background imbalance, and large image
sizes. We explore the incorporation of the differential morphological profile
(DMP), a multi-scale shape extraction method based on grayscale morphology,
into modern segmentation networks. Prior studies have shown that the DMP can
provide critical shape information to Deep Neural Networks to enable superior
detection and classification performance in overhead imagery. In this work, we
extend prior DMPNet work beyond classification and object detection by
integrating DMP features into three state-of-the-art convolutional and
transformer semantic segmentation architectures. We utilize both direct input,
which adapts the input stem of feature extraction architectures to accept DMP
channels, and hybrid architectures, a dual-stream design that fuses RGB and DMP
encoders. Using the iSAID benchmark dataset, we evaluate a variety of DMP
differentials and structuring element shapes to more effectively provide shape
information to the model. Our results show that while non-DMP models generally
outperform the direct-input variants, hybrid DMP consistently outperforms
direct-input and is capable of surpassing a non-DMP model on mIoU, F1, and
Recall.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Learning</span> Active Perception via Self-Evolving Preference <span class="highlight-title">Optimization</span> for
  GUI Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanfu Wang, Qipeng Huang, Guangquan Xue, Xiaobo Liang, Juntao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have recently achieved significant progress in
bridging visual perception and linguistic reasoning. Recently, OpenAI o3 model
introduced a zoom-in search strategy that effectively elicits active perception
capabilities in VLMs, improving downstream task performance. However, enabling
VLMs to reason effectively over appropriate image regions remains a core
challenge in GUI grounding, particularly under high-resolution inputs and
complex multi-element visual interactions. In this work, we propose LASER, a
self-evolving framework that progressively endows VLMs with multi-step
perception capabilities, enabling precise coordinate prediction. Specifically,
our approach integrate Monte Carlo quality estimation with
Intersection-over-Union (IoU)-based region quality evaluation to jointly
encourage both accuracy and diversity in constructing high-quality preference
data. This combination explicitly guides the model to focus on
instruction-relevant key regions while adaptively allocating reasoning steps
based on task complexity. Comprehensive experiments on the ScreenSpot Pro and
ScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating
the effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER
achieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new
state-of-the-art (SoTA) among 7B-scale models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DUDE: Diffusion-Based Unsupervised Cross-Domain Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04193v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04193v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruohong Yang, Peng Hu, Yunfan Li, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised cross-domain image retrieval (UCIR) aims to retrieve images of
the same category across diverse domains without relying on annotations.
Existing UCIR methods, which align cross-domain features for the entire image,
often struggle with the domain gap, as the object features critical for
retrieval are frequently entangled with domain-specific styles. To address this
challenge, we propose DUDE, a novel UCIR method building upon feature
disentanglement. In brief, DUDE leverages a text-to-image generative model to
disentangle object features from domain-specific styles, thus facilitating
semantical image retrieval. To further achieve reliable alignment of the
disentangled object features, DUDE aligns mutual neighbors from within domains
to across domains in a progressive manner. Extensive experiments demonstrate
that DUDE achieves state-of-the-art performance across three benchmark datasets
over 13 domains. The code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer
  <span class="highlight-title">Vision</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Safouane El Ghazouali, Umberto Michelucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI models rely on annotated data to learn pattern and perform prediction.
Annotation is usually a labor-intensive step that require associating labels
ranging from a simple classification label to more complex tasks such as object
detection, oriented bounding box estimation, and instance segmentation.
Traditional tools often require extensive manual input, limiting scalability
for large datasets. To address this, we introduce VisioFirm, an open-source web
application designed to streamline image labeling through AI-assisted
automation. VisioFirm integrates state-of-the-art foundation models into an
interface with a filtering pipeline to reduce human-in-the-loop efforts. This
hybrid approach employs CLIP combined with pre-trained detectors like
Ultralytics models for common classes and zero-shot models such as Grounding
DINO for custom labels, generating initial annotations with low-confidence
thresholding to maximize recall. Through this framework, when tested on
COCO-type of classes, initial prediction have been proven to be mostly correct
though the users can refine these via interactive tools supporting bounding
boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has
on-the-fly segmentation powered by Segment Anything accelerated through WebGPU
for browser-side efficiency. The tool supports multiple export formats (YOLO,
COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing
accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort
through benchmarks on diverse datasets, while maintaining high annotation
accuracy via clustering of connected CLIP-based disambiguate components and
IoU-graph for redundant detection suppression. VisioFirm can be accessed from
\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind
  Turbine Components 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serhii Svystun, Pavlo Radiuk, Oleksandr Melnychenko, Oleg Savenko, Anatoliy Sachenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up
new opportunities for monitoring wind power plants, including blades, towers,
and other critical components. However, reliable defect detection requires
high-resolution data and efficient methods to process multispectral imagery. In
this research, we aim to enhance defect detection accuracy through the
development of an ensemble of YOLO-based deep learning models that integrate
both visible and thermal channels. We propose an ensemble approach that
integrates a general-purpose YOLOv8 model with a specialized thermal model,
using a sophisticated bounding box fusion algorithm to combine their
predictions. Our experiments show this approach achieves a mean Average
Precision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone
YOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that
combining multiple YOLO architectures with fused multispectral data provides a
more reliable solution, improving the detection of both visual and thermal
defects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 13th IEEE International Conference on Intelligent Data
  Acquisition and Advanced Computing Systems: Technology and Applications, 4-6
  September, 2025, Gliwice, Poland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Simple Baselines for In-The-Wild Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Orlando Castaneda, Kevin So-Tang, Kshitij Gurung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread adoption of synthetic media demands accessible deepfake
detectors and realistic benchmarks. While most existing research evaluates
deepfake detectors on highly controlled datasets, we focus on the recently
released "in-the-wild" benchmark, Deepfake-Eval-2024. Initial reporting on
Deepfake-Eval-2024 showed that three finetuned open-source models achieve
accuracies between 61% and 69%, significantly lagging behind the leading
commercial deepfake detector with 82% accuracy. Our work revisits one of these
baseline approaches, originally introduced by Ojha et al., which adapts
standard pretrained vision backbones to produce generalizable deepfake
detectors. We demonstrate that with better-tuned hyperparameters, this simple
approach actually yields much higher performance -- 81% accuracy on
Deepfake-Eval-2024 -- surpassing the previously reported accuracy of this
baseline approach by 18% and competing with commercial deepfake detectors. We
discuss tradeoffs in accuracy, computational costs, and interpretability,
focusing on how practical these deepfake detectors might be when deployed in
real-world settings. Our code can be found at
https://github.com/Deepfake-Detection-KKO/deepfake-detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyper Diffusion Avatars: <span class="highlight-title">Dynamic</span> Human Avatar Generation using Network
  Weight Space Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongliang Cao, Guoxing Sun, Marc Habermann, Florian Bernard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating human avatars is a highly desirable yet challenging task. Recent
advancements in radiance field rendering have achieved unprecedented
photorealism and real-time performance for personalized dynamic human avatars.
However, these approaches are typically limited to person-specific rendering
models trained on multi-view video data for a single individual, limiting their
ability to generalize across different identities. On the other hand,
generative approaches leveraging prior knowledge from pre-trained 2D diffusion
models can produce cartoonish, static human avatars, which are animated through
simple skeleton-based articulation. Therefore, the avatars generated by these
methods suffer from lower rendering quality compared to person-specific
rendering methods and fail to capture pose-dependent deformations such as cloth
wrinkles. In this paper, we propose a novel approach that unites the strengths
of person-specific rendering and diffusion-based generative modeling to enable
dynamic human avatar generation with both high photorealism and realistic
pose-dependent deformations. Our method follows a two-stage pipeline: first, we
optimize a set of person-specific UNets, with each network representing a
dynamic human avatar that captures intricate pose-dependent deformations. In
the second stage, we train a hyper diffusion model over the optimized network
weights. During inference, our method generates network weights for real-time,
controllable rendering of dynamic human avatars. Using a large-scale,
cross-identity, multi-view video dataset, we demonstrate that our approach
outperforms state-of-the-art human avatar generation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEPG:Multi-Expert <span class="highlight-title">Planning</span> and Generation for Compositionally-Rich Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhao, Liu Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have achieved remarkable image quality, but
they still struggle with complex, multiele ment prompts, and limited stylistic
diversity. To address these limitations, we propose a Multi-Expert Planning and
Gen eration Framework (MEPG) that synergistically integrates position- and
style-aware large language models (LLMs) with spatial-semantic expert modules.
The framework comprises two core components: (1) a Position-Style-Aware (PSA)
module that utilizes a supervised fine-tuned LLM to decom pose input prompts
into precise spatial coordinates and style encoded semantic instructions; and
(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera
tion through dynamic expert routing across both local regions and global areas.
During the generation process for each lo cal region, specialized models (e.g.,
realism experts, styliza tion specialists) are selectively activated for each
spatial par tition via attention-based gating mechanisms. The architec ture
supports lightweight integration and replacement of ex pert models, providing
strong extensibility. Additionally, an interactive interface enables real-time
spatial layout editing and per-region style selection from a portfolio of
experts. Ex periments show that MEPG significantly outperforms base line models
with the same backbone in both image quality
  and style diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TaleDiffusion: Multi-Character Story Generation with Dialogue Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayan Banerjee, Josep Lladós, Umapada Pal, Anjan Dutta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-story visualization is challenging due to the need for consistent
interaction among multiple characters across frames. Existing methods struggle
with character consistency, leading to artifact generation and inaccurate
dialogue rendering, which results in disjointed storytelling. In response, we
introduce TaleDiffusion, a novel framework for generating multi-character
stories with an iterative process, maintaining character consistency, and
accurate dialogue assignment via postprocessing. Given a story, we use a
pre-trained LLM to generate per-frame descriptions, character details, and
dialogues via in-context learning, followed by a bounded attention-based
per-box mask technique to control character interactions and minimize
artifacts. We then apply an identity-consistent self-attention mechanism to
ensure character consistency across frames and region-aware cross-attention for
precise object placement. Dialogues are also rendered as bubbles and assigned
to characters via CLIPSeg. Experimental results demonstrate that TaleDiffusion
outperforms existing methods in consistency, noise reduction, and dialogue
rendering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DVS-PedX: Synthetic-and-Real Event-Based Pedestrian <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mustafa Sakhai, Kaung Sithu, Min Khant Soe Oke, Maciej Wielgosz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event cameras like Dynamic Vision Sensors (DVS) report micro-timed brightness
changes instead of full frames, offering low latency, high dynamic range, and
motion robustness. DVS-PedX (Dynamic Vision Sensor Pedestrian eXploration) is a
neuromorphic dataset designed for pedestrian detection and crossing-intention
analysis in normal and adverse weather conditions across two complementary
sources: (1) synthetic event streams generated in the CARLA simulator for
controlled "approach-cross" scenes under varied weather and lighting; and (2)
real-world JAAD dash-cam videos converted to event streams using the v2e tool,
preserving natural behaviors and backgrounds. Each sequence includes paired RGB
frames, per-frame DVS "event frames" (33 ms accumulations), and frame-level
labels (crossing vs. not crossing). We also provide raw AEDAT 2.0/AEDAT 4.0
event files and AVI DVS video files and metadata for flexible re-processing.
Baseline spiking neural networks (SNNs) using SpikingJelly illustrate dataset
usability and reveal a sim-to-real gap, motivating domain adaptation and
multimodal fusion. DVS-PedX aims to accelerate research in event-based
pedestrian safety, intention prediction, and neuromorphic perception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures, 3 tables; dataset descriptor paper introducing
  DVS-PedX (synthetic-and-real event-based pedestrian dataset with baselines)
  External URL: https://doi.org/10.5281/zenodo.17030898</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FedQuad: Federated Stochastic Quadruplet <span class="highlight-title">Learning</span> to Mitigate Data
  Heterogeneity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ozgu Goksu, Nicolas Pugeault
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) provides decentralised model training, which
effectively tackles problems such as distributed data and privacy preservation.
However, the generalisation of global models frequently faces challenges from
data heterogeneity among clients. This challenge becomes even more pronounced
when datasets are limited in size and class imbalance. To address data
heterogeneity, we propose a novel method, \textit{FedQuad}, that explicitly
optimises smaller intra-class variance and larger inter-class variance across
clients, thereby decreasing the negative impact of model aggregation on the
global model over client representations. Our approach minimises the distance
between similar pairs while maximising the distance between negative pairs,
effectively disentangling client data in the shared feature space. We evaluate
our method on the CIFAR-10 and CIFAR-100 datasets under various data
distributions and with many clients, demonstrating superior performance
compared to existing approaches. Furthermore, we provide a detailed analysis of
metric learning-based strategies within both supervised and federated learning
paradigms, highlighting their efficacy in addressing representational learning
challenges in federated settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 3rd IEEE International Conference on Federated Learning
  Technologies and Applications (FLTA25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TriLiteNet: Lightweight Model for Multi-Task <span class="highlight-title">Visual</span> Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang-Huy Che, Duc-Khai Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient perception models are essential for Advanced Driver Assistance
Systems (ADAS), as these applications require rapid processing and response to
ensure safety and effectiveness in real-world environments. To address the
real-time execution needs of such perception models, this study introduces the
TriLiteNet model. This model can simultaneously manage multiple tasks related
to panoramic driving perception. TriLiteNet is designed to optimize performance
while maintaining low computational costs. Experimental results on the BDD100k
dataset demonstrate that the model achieves competitive performance across
three key tasks: vehicle detection, drivable area segmentation, and lane line
segmentation. Specifically, the TriLiteNet_{base} demonstrated a recall of
85.6% for vehicle detection, a mean Intersection over Union (mIoU) of 92.4% for
drivable area segmentation, and an Acc of 82.3% for lane line segmentation with
only 2.35M parameters and a computational cost of 7.72 GFLOPs. Our proposed
model includes a tiny configuration with just 0.14M parameters, which provides
a multi-task solution with minimal computational demand. Evaluated for latency
and power consumption on embedded devices, TriLiteNet in both configurations
shows low latency and reasonable power during inference. By balancing
performance, computational efficiency, and scalability, TriLiteNet offers a
practical and deployable solution for real-world autonomous driving
applications. Code is available at https://github.com/chequanghuy/TriLiteNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TEn-CATS: Text-Enriched Audio-<span class="highlight-title">Visual</span> Video Parsing with Multi-Scale
  Category-Aware Temporal Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaru Chen, Faegheh Sardari, Peiliang Zhang, Ruohao Guo, Yang Xiang, Zhenbo Li, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio-Visual Video Parsing (AVVP) task aims to identify event categories and
their occurrence times in a given video with weakly supervised labels. Existing
methods typically fall into two categories: (i) designing enhanced
architectures based on attention mechanism for better temporal modeling, and
(ii) generating richer pseudo-labels to compensate for the absence of
frame-level annotations. However, the first type methods treat noisy
segment-level pseudo labels as reliable supervision and the second type methods
let indiscriminate attention spread them across all frames, the initial errors
are repeatedly amplified during training. To address this issue, we propose a
method that combines the Bi-Directional Text Fusion (BiT) module and
Category-Aware Temporal Graph (CATS) module. Specifically, we integrate the
strengths and complementarity of the two previous research directions. We first
perform semantic injection and dynamic calibration on audio and visual modality
features through the BiT module, to locate and purify cleaner and richer
semantic cues. Then, we leverage the CATS module for semantic propagation and
connection to enable precise semantic information dissemination across time.
Experimental results demonstrate that our proposed method achieves
state-of-the-art (SOTA) performance in multiple key indicators on two benchmark
datasets, LLP and UnAV-100.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SMooGPT: Stylized Motion Generation using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zhong, Yi Yang, Changjian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stylized motion generation is actively studied in computer graphics,
especially benefiting from the rapid advances in diffusion models. The goal of
this task is to produce a novel motion respecting both the motion content and
the desired motion style, e.g., ``walking in a loop like a Monkey''. Existing
research attempts to address this problem via motion style transfer or
conditional motion generation. They typically embed the motion style into a
latent space and guide the motion implicitly in a latent space as well. Despite
the progress, their methods suffer from low interpretability and control,
limited generalization to new styles, and fail to produce motions other than
``walking'' due to the strong bias in the public stylization dataset. In this
paper, we propose to solve the stylized motion generation problem from a new
perspective of reasoning-composition-generation, based on our observations: i)
human motion can often be effectively described using natural language in a
body-part centric manner, ii) LLMs exhibit a strong ability to understand and
reason about human motion, and iii) human motion has an inherently
compositional nature, facilitating the new motion content or style generation
via effective recomposing. We thus propose utilizing body-part text space as an
intermediate representation, and present SMooGPT, a fine-tuned LLM, acting as a
reasoner, composer, and generator when generating the desired stylized motion.
Our method executes in the body-part text space with much higher
interpretability, enabling fine-grained motion control, effectively resolving
potential conflicts between motion content and style, and generalizes well to
new styles thanks to the open-vocabulary ability of LLMs. Comprehensive
experiments and evaluations, and a user perceptual study, demonstrate the
effectiveness of our approach, especially under the pure text-driven stylized
motion generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Re-ranking Method using K-nearest Weighted Fusion for Person
  Re-identification <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang-Huy Che, Le-Chuong Nguyen, Gia-Nghia Tran, Dinh-Duy Phan, Vinh-Tiep Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In person re-identification, re-ranking is a crucial step to enhance the
overall accuracy by refining the initial ranking of retrieved results. Previous
studies have mainly focused on features from single-view images, which can
cause view bias and issues like pose variation, viewpoint changes, and
occlusions. Using multi-view features to present a person can help reduce view
bias. In this work, we present an efficient re-ranking method that generates
multi-view features by aggregating neighbors' features using K-nearest Weighted
Fusion (KWF) method. Specifically, we hypothesize that features extracted from
re-identification models are highly similar when representing the same
identity. Thus, we select K neighboring features in an unsupervised manner to
generate multi-view features. Additionally, this study explores the weight
selection strategies during feature aggregation, allowing us to identify an
effective strategy. Our re-ranking approach does not require model fine-tuning
or extra annotations, making it applicable to large-scale datasets. We evaluate
our method on the person re-identification datasets Market1501, MSMT17, and
Occluded-DukeMTMC. The results show that our method significantly improves
Rank@1 and mAP when re-ranking the top M candidates from the initial ranking
results. Specifically, compared to the initial results, our re-ranking method
achieves improvements of 9.8%/22.0% in Rank@1 on the challenging datasets:
MSMT17 and Occluded-DukeMTMC, respectively. Furthermore, our approach
demonstrates substantial enhancements in computational efficiency compared to
other re-ranking methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICPRAM 2025, ISBN 978-989-758-730-6, ISSN 2184-4313</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TensoIS: A Step Towards Feed-Forward Tensorial Inverse Subsurface
  Scattering for Perlin Distributed Heterogeneous Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Tiwari, Satyam Bhardwaj, Yash Bachwana, Parag Sarvoday Sahu, T. M. Feroz Ali, Bhargava Chintalapati, Shanmuganathan Raman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating scattering parameters of heterogeneous media from images is a
severely under-constrained and challenging problem. Most of the existing
approaches model BSSRDF either through an analysis-by-synthesis approach,
approximating complex path integrals, or using differentiable volume rendering
techniques to account for heterogeneity. However, only a few studies have
applied learning-based methods to estimate subsurface scattering parameters,
but they assume homogeneous media. Interestingly, no specific distribution is
known to us that can explicitly model the heterogeneous scattering parameters
in the real world. Notably, procedural noise models such as Perlin and Fractal
Perlin noise have been effective in representing intricate heterogeneities of
natural, organic, and inorganic surfaces. Leveraging this, we first create
HeteroSynth, a synthetic dataset comprising photorealistic images of
heterogeneous media whose scattering parameters are modeled using Fractal
Perlin noise. Furthermore, we propose Tensorial Inverse Scattering (TensoIS), a
learning-based feed-forward framework to estimate these Perlin-distributed
heterogeneous scattering parameters from sparse multi-view image observations.
Instead of directly predicting the 3D scattering parameter volume, TensoIS uses
learnable low-rank tensor components to represent the scattering volume. We
evaluate TensoIS on unseen heterogeneous variations over shapes from the
HeteroSynth test set, smoke and cloud geometries obtained from open-source
realistic volumetric simulations, and some real-world samples to establish its
effectiveness for inverse scattering. Overall, this study is an attempt to
explore Perlin noise distribution, given the lack of any such well-defined
distribution in literature, to potentially model real-world heterogeneous
scattering in a feed-forward manner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Pacific Graphics 2025 (CGF Journal Track), Project page:
  https://yashbachwana.github.io/TensoIS/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Millisecond-Response Tracking and Gazing System for UAVs: A Domestic
  Solution Based on "Phytium + Cambricon" 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchen Zhu, Longxiang Yin, Kai Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the frontier research and application of current video surveillance
technology, traditional camera systems exhibit significant limitations of
response delay exceeding 200 ms in dynamic scenarios due to the insufficient
deep feature extraction capability of automatic recognition algorithms and the
efficiency bottleneck of computing architectures, failing to meet the real-time
requirements in complex scenes. To address this issue, this study proposes a
heterogeneous computing architecture based on Phytium processors and Cambricon
accelerator cards, constructing a UAV tracking and gazing system with
millisecond-level response capability. At the hardware level, the system adopts
a collaborative computing architecture of Phytium FT-2000/4 processors and
MLU220 accelerator cards, enhancing computing power through multi-card
parallelism. At the software level, it innovatively integrates a lightweight
YOLOv5s detection network with a DeepSORT cascaded tracking algorithm, forming
a closed-loop control chain of "detection-tracking-feedback". Experimental
results demonstrate that the system achieves a stable single-frame
comprehensive processing delay of 50-100 ms in 1920*1080 resolution video
stream processing, with a multi-scale target recognition accuracy of over
98.5%, featuring both low latency and high precision. This study provides an
innovative solution for UAV monitoring and the application of domestic chips.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages,17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Learning</span> from Majority Label: A Novel Problem in Multi-class
  Multiple-Instance <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiku Kaito, Shinnosuke Matsuo, Daiki Suehiro, Ryoma Bise
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper proposes a novel multi-class Multiple-Instance Learning (MIL)
problem called Learning from Majority Label (LML). In LML, the majority class
of instances in a bag is assigned as the bag-level label. The goal of LML is to
train a classification model that estimates the class of each instance using
the majority label. This problem is valuable in a variety of applications,
including pathology image segmentation, political voting prediction, customer
sentiment analysis, and environmental monitoring. To solve LML, we propose a
Counting Network trained to produce bag-level majority labels, estimated by
counting the number of instances in each class. Furthermore, analysis
experiments on the characteristics of LML revealed that bags with a high
proportion of the majority class facilitate learning. Based on this result, we
developed a Majority Proportion Enhancement Module (MPEM) that increases the
proportion of the majority class by removing minority class instances within
the bags. Experiments demonstrate the superiority of the proposed method on
four datasets compared to conventional MIL methods. Moreover, ablation studies
confirmed the effectiveness of each module. The code is available at
\href{https://github.com/Shiku-Kaito/Learning-from-Majority-Label-A-Novel-Problem-in-Multi-class-Multiple-Instance-Learning}{here}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 9 figures, Accepted in Pattern recognition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Regional Spurious Correlations in <span class="highlight-title">Vision</span> Transformers via
  Token Discarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Solha Kang, Esla Timothy Anzaku, Wesley De Neve, Arnout Van Messem, Joris Vankerschaver, Francois Rameau, Utku Ozbulak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to their powerful feature association capabilities, neural network-based
computer vision models have the ability to detect and exploit unintended
patterns within the data, potentially leading to correct predictions based on
incorrect or unintended but statistically relevant signals. These clues may
vary from simple color aberrations to small texts within the image. In
situations where these unintended signals align with the predictive task,
models can mistakenly link these features with the task and rely on them for
making predictions. This phenomenon is referred to as spurious correlations,
where patterns appear to be associated with the task but are actually
coincidental. As a result, detection and mitigation of spurious correlations
have become crucial tasks for building trustworthy, reliable, and generalizable
machine learning models. In this work, we present a novel method to detect
spurious correlations in vision transformers, a type of neural network
architecture that gained significant popularity in recent years. Using both
supervised and self-supervised trained models, we present large-scale
experiments on the ImageNet dataset demonstrating the ability of the proposed
method to identify spurious correlations. We also find that, even if the same
architecture is used, the training methodology has a significant impact on the
model's reliance on spurious correlations. Furthermore, we show that certain
classes in the ImageNet dataset contain spurious signals that are easily
detected by the models and discuss the underlying reasons for those spurious
signals. In light of our findings, we provide an exhaustive list of the
aforementioned images and call for caution in their use in future research
efforts. Lastly, we present a case study investigating spurious signals in
invasive breast mass classification, grounding our work in real-world
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SliceSemOcc: Vertical Slice Based Multimodal 3D <span class="highlight-title">Semantic</span> Occupancy
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03999v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03999v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Huang, Han Sun, Ningzhong Liu, Huiyu Zhou, Jiaquan Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by autonomous driving's demands for precise 3D perception, 3D semantic
occupancy prediction has become a pivotal research topic. Unlike
bird's-eye-view (BEV) methods, which restrict scene representation to a 2D
plane, occupancy prediction leverages a complete 3D voxel grid to model spatial
structures in all dimensions, thereby capturing semantic variations along the
vertical axis. However, most existing approaches overlook height-axis
information when processing voxel features. And conventional SENet-style
channel attention assigns uniform weight across all height layers, limiting
their ability to emphasize features at different heights. To address these
limitations, we propose SliceSemOcc, a novel vertical slice based multimodal
framework for 3D semantic occupancy representation. Specifically, we extract
voxel features along the height-axis using both global and local vertical
slices. Then, a global local fusion module adaptively reconciles fine-grained
spatial details with holistic contextual information. Furthermore, we propose
the SEAttention3D module, which preserves height-wise resolution through
average pooling and assigns dynamic channel attention weights to each height
layer. Extensive experiments on nuScenes-SurroundOcc and nuScenes-OpenOccupancy
datasets verify that our method significantly enhances mean IoU, achieving
especially pronounced gains on most small-object categories. Detailed ablation
studies further validate the effectiveness of the proposed SliceSemOcc
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, accepted by PRCV2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Promptception: How Sensitive Are Large Multimodal Models to Prompts? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Insaf Ismithdeen, Muhammad Uzair Khattak, Salman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of Large Multimodal Models (LMMs) in recent years, prompt
design for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly
understood. We show that even minor variations in prompt phrasing and structure
can lead to accuracy deviations of up to 15% for certain prompts and models.
This variability poses a challenge for transparent and fair LMM evaluation, as
models often report their best-case performance using carefully selected
prompts. To address this, we introduce Promptception, a systematic framework
for evaluating prompt sensitivity in LMMs. It consists of 61 prompt types,
spanning 15 categories and 6 supercategories, each targeting specific aspects
of prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight
open-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks:
MMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit
greater sensitivity to prompt phrasing, reflecting tighter alignment with
instruction semantics, while open-source models are steadier but struggle with
nuanced and complex phrasing. Based on this analysis, we propose Prompting
Principles tailored to proprietary and open-source LMMs, enabling more robust
and fair model evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Vessel Segmentation with Multi-Task <span class="highlight-title">Learning</span> and Auxiliary
  Data Available Only During Model Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Sobotka, Alexander Herold, Matthias Perkonigg, Lucian Beer, Nina Bastati, Alina Sablatnig, Ahmed Ba-Ssalamah, Georg Langs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Liver vessel segmentation in magnetic resonance imaging data is important for
the computational analysis of vascular remodelling, associated with a wide
spectrum of diffuse liver diseases. Existing approaches rely on contrast
enhanced imaging data, but the necessary dedicated imaging sequences are not
uniformly acquired. Images without contrast enhancement are acquired more
frequently, but vessel segmentation is challenging, and requires large-scale
annotated data. We propose a multi-task learning framework to segment vessels
in liver MRI without contrast. It exploits auxiliary contrast enhanced MRI data
available only during training to reduce the need for annotated training
examples. Our approach draws on paired native and contrast enhanced data with
and without vessel annotations for model training. Results show that auxiliary
data improves the accuracy of vessel segmentation, even if they are not
available during inference. The advantage is most pronounced if only few
annotations are available for training, since the feature representation
benefits from the shared task structure. A validation of this approach to
augment a model for brain tumor segmentation confirms its benefits across
different domains. An auxiliary informative imaging modality can augment expert
annotations even if it is only available during training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAC-MIL: Spatial-Aware Correlated Multiple Instance <span class="highlight-title">Learning</span> for
  Histopathology Whole Slide Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Bai, Zitong Yu, Haowen Tian, Xijing Wang, Shuo Yan, Lin Wang, Honglin Li, Xitong Ling, Bo Zhang, Zheng Zhang, Wufan Wang, Hui Gao, Xiangyang Gong, Wendong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for
performing WSI classification. SAC-MIL consists of a positional encoding module
to encode position information and a SAC block to perform full instance
correlations. The positional encoding module utilizes the instance coordinates
within the slide to encode the spatial relationships instead of the instance
index in the input WSI sequence. The positional encoding module can also handle
the length extrapolation issue where the training and testing sequences have
different lengths. The SAC block is an MLP-based method that performs full
instance correlation in linear time complexity with respect to the sequence
length. Due to the simple structure of MLP, it is easy to deploy since it does
not require custom CUDA kernels, compared to Transformer-based methods for WSI
classification. SAC-MIL has achieved state-of-the-art performance on the
CAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon
acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Feature Fusion Network with Text Difference Enhancement for
  Remote Sensing Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Zhou, Yikui Zhai, Zilu Ying, Tingfeng Xian, Wenlve Zhou, Zhiheng Zhou, Xiaolin Tian, Xudong Jia, Hongsheng Zhang, C. L. Philip Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although deep learning has advanced remote sensing change detection (RSCD),
most methods rely solely on image modality, limiting feature representation,
change pattern modeling, and generalization especially under illumination and
noise disturbances. To address this, we propose MMChange, a multimodal RSCD
method that combines image and text modalities to enhance accuracy and
robustness. An Image Feature Refinement (IFR) module is introduced to highlight
key regions and suppress environmental noise. To overcome the semantic
limitations of image features, we employ a vision language model (VLM) to
generate semantic descriptions of bitemporal images. A Textual Difference
Enhancement (TDE) module then captures fine grained semantic shifts, guiding
the model toward meaningful changes. To bridge the heterogeneity between
modalities, we design an Image Text Feature Fusion (ITFF) module that enables
deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and
SYSUCD demonstrate that MMChange consistently surpasses state of the art
methods across multiple metrics, validating its effectiveness for multimodal
RSCD. Code is available at: https://github.com/yikuizhai/MMChange.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ANTS: Shaping the Adaptive Negative Textual Space by MLLM for OOD
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhu Wenjie, Zhang Yabin, Xin Jin, Wenjun Zeng, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of negative labels (NLs) has proven effective in enhancing
Out-of-Distribution (OOD) detection. However, existing methods often lack an
understanding of OOD images, making it difficult to construct an accurate
negative space. In addition, the presence of false negative labels
significantly degrades their near-OOD performance. To address these issues, we
propose shaping an Adaptive Negative Textual Space (ANTS) by leveraging the
understanding and reasoning capabilities of multimodal large language models
(MLLMs). Specifically, we identify images likely to be OOD samples as negative
images and prompt the MLLM to describe these images, generating expressive
negative sentences that precisely characterize the OOD distribution and enhance
far-OOD detection. For the near-OOD setting, where OOD samples resemble the
in-distribution (ID) subset, we first identify the subset of ID classes that
are visually similar to negative images and then leverage the reasoning
capability of MLLMs to generate visually similar negative labels tailored to
this subset, effectively reducing false negatives and improving near-OOD
detection. To balance these two types of negative textual spaces, we design an
adaptive weighted score that enables the method to handle different OOD task
settings (near-OOD and far-OOD) without relying on task-specific prior
knowledge, making it highly adaptable in open environments. On the ImageNet
benchmark, our ANTS significantly reduces the FPR95 by 4.2\%, establishing a
new state-of-the-art. Furthermore, our method is training-free and zero-shot,
enabling high scalability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer
  <span class="highlight-title">Learning</span> in a U-Net Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alvaro Aranibar Roque, Helga Sebastian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pneumothorax, the abnormal accumulation of air in the pleural space, can be
life-threatening if undetected. Chest X-rays are the first-line diagnostic
tool, but small cases may be subtle. We propose an automated deep-learning
pipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax
regions. Trained on the SIIM-ACR dataset with data augmentation and a combined
binary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and
Dice score of 0.8241 on the independent PTX-498 dataset. These results
demonstrate that the model can accurately localize pneumothoraces and support
radiologists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 page, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TopoSculpt: Betti-Steered Topological Sculpting of 3D Fine-grained
  Tubular Shapes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghui Zhang, Yaoyu Liu, Junyang Wu, Xin You, Hanxiao Zhang, Junjun He, Yun Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical tubular anatomical structures are inherently three-dimensional
conduits with lumens, enclosing walls, and complex branching topologies.
Accurate reconstruction of their geometry and topology is crucial for
applications such as bronchoscopic navigation and cerebral arterial
connectivity assessment. Existing methods often rely on voxel-wise overlap
measures, which fail to capture topological correctness and completeness.
Although topology-aware losses and persistent homology constraints have shown
promise, they are usually applied patch-wise and cannot guarantee global
preservation or correct geometric errors at inference. To address these
limitations, we propose a novel TopoSculpt, a framework for topological
refinement of 3D fine-grained tubular structures. TopoSculpt (i) adopts a
holistic whole-region modeling strategy to capture full spatial context, (ii)
first introduces a Topological Integrity Betti (TIB) constraint that jointly
enforces Betti number priors and global integrity, and (iii) employs a
curriculum refinement scheme with persistent homology to progressively correct
errors from coarse to fine scales. Extensive experiments on challenging
pulmonary airway and Circle of Willis datasets demonstrate substantial
improvements in both geometry and topology. For instance, $\beta_{0}$ errors
are reduced from 69.00 to 3.40 on the airway dataset and from 1.65 to 0.30 on
the CoW dataset, with Tree length detected and branch detected rates improving
by nearly 10\%. These results highlight the effectiveness of TopoSculpt in
correcting critical topological errors and advancing the high-fidelity modeling
of complex 3D tubular anatomy. The project homepage is available at:
https://github.com/Puzzled-Hui/TopoSculpt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LMVC: An End-to-End Learned Multiview Video Coding Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xihua Sheng, Yingwen Zhang, Long Xu, Shiqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiview video is a key data source for volumetric video, enabling immersive
3D scene reconstruction but posing significant challenges in storage and
transmission due to its massive data volume. Recently, deep learning-based
end-to-end video coding has achieved great success, yet most focus on
single-view or stereo videos, leaving general multiview scenarios
underexplored. This paper proposes an end-to-end learned multiview video coding
(LMVC) framework that ensures random access and backward compatibility while
enhancing compression efficiency. Our key innovation lies in effectively
leveraging independent-view motion and content information to enhance
dependent-view compression. Specifically, to exploit the inter-view motion
correlation, we propose a feature-based inter-view motion vector prediction
method that conditions dependent-view motion encoding on decoded
independent-view motion features, along with an inter-view motion entropy model
that learns inter-view motion priors. To exploit the inter-view content
correlation, we propose a disparity-free inter-view context prediction module
that predicts inter-view contexts from decoded independent-view content
features, combined with an inter-view contextual entropy model that captures
inter-view context priors. Experimental results show that our proposed LMVC
framework outperforms the reference software of the traditional MV-HEVC
standard by a large margin, establishing a strong baseline for future research
in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Generative</span> Foundation Model for Chest Radiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanfeng Ji, Dan Lin, Xiyue Wang, Lu Zhang, Wenhui Zhou, Chongjian Ge, Ruihang Chu, Xiaoli Yang, Junhan Zhao, Junsong Chen, Xiangde Luo, Sen Yang, Jin Fang, Ping Luo, Ruijiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scarcity of well-annotated diverse medical images is a major hurdle for
developing reliable AI models in healthcare. Substantial technical advances
have been made in generative foundation models for natural images. Here we
develop `ChexGen', a generative vision-language foundation model that
introduces a unified framework for text-, mask-, and bounding box-guided
synthesis of chest radiographs. Built upon the latent diffusion transformer
architecture, ChexGen was pretrained on the largest curated chest X-ray dataset
to date, consisting of 960,000 radiograph-report pairs. ChexGen achieves
accurate synthesis of radiographs through expert evaluations and quantitative
metrics. We demonstrate the utility of ChexGen for training data augmentation
and supervised pretraining, which led to performance improvements across
disease classification, detection, and segmentation tasks using a small
fraction of training data. Further, our model enables the creation of diverse
patient cohorts that enhance model fairness by detecting and mitigating
demographic biases. Our study supports the transformative role of generative
foundation models in building more accurate, data-efficient, and equitable
medical AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPECS: Specificity-Enhanced CLIP-Score for Long Image Caption Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofu Chen, Israfel Salazar, Yova Kementchedjhieva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As interest grows in generating long, detailed image captions, standard
evaluation metrics become increasingly unreliable. N-gram-based metrics though
efficient, fail to capture semantic correctness. Representational Similarity
(RS) metrics, designed to address this, initially saw limited use due to high
computational costs, while today, despite advances in hardware, they remain
unpopular due to low correlation to human judgments. Meanwhile, metrics based
on large language models (LLMs) show strong correlation with human judgments,
but remain too expensive for iterative use during model development.
  We introduce SPECS (Specificity-Enhanced CLIPScore), a reference-free RS
metric tailored to long image captioning. SPECS modifies CLIP with a new
objective that emphasizes specificity: rewarding correct details and penalizing
incorrect ones. We show that SPECS matches the performance of open-source
LLM-based metrics in correlation to human judgments, while being far more
efficient. This makes it a practical alternative for iterative checkpoint
evaluation during image captioning model development.Our code can be found at
https://github.com/mbzuai-nlp/SPECS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attn-Adapter: Attention Is All You Need for Online Few-shot Learner of
  <span class="highlight-title">Vision</span>-Language Model <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuoc-Nguyen Bui, Khanh-Binh Nguyen, Hyunseung Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive vision-language models excel in zero-shot image recognition but
face challenges in few-shot scenarios due to computationally intensive offline
fine-tuning using prompt learning, which risks overfitting. To overcome these
limitations, we propose Attn-Adapter, a novel online few-shot learning
framework that enhances CLIP's adaptability via a dual attention mechanism. Our
design incorporates dataset-specific information through two components: the
Memory Attn-Adapter, which refines category embeddings using support examples,
and the Local-Global Attn-Adapter, which enriches image embeddings by
integrating local and global features. This architecture enables dynamic
adaptation from a few labeled samples without retraining the base model.
Attn-Adapter outperforms state-of-the-art methods in cross-category and
cross-dataset generalization, maintaining efficient inference and scaling
across CLIP backbones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 - LIMIT Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly-Supervised <span class="highlight-title">Learning</span> of Dense Functional Correspondences <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03893v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03893v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Stojanov, Linan Zhao, Yunzhi Zhang, Daniel L. K. Yamins, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Establishing dense correspondences across image pairs is essential for tasks
such as shape reconstruction and robot manipulation. In the challenging setting
of matching across different categories, the function of an object, i.e., the
effect that an object can cause on other objects, can guide how correspondences
should be established. This is because object parts that enable specific
functions often share similarities in shape and appearance. We derive the
definition of dense functional correspondence based on this observation and
propose a weakly-supervised learning paradigm to tackle the prediction task.
The main insight behind our approach is that we can leverage vision-language
models to pseudo-label multi-view images to obtain functional parts. We then
integrate this with dense contrastive learning from pixel correspondences to
distill both functional and spatial knowledge into a new model that can
establish dense functional correspondence. Further, we curate synthetic and
real evaluation datasets as task benchmarks. Our results demonstrate the
advantages of our approach over baseline solutions consisting of off-the-shelf
self-supervised image representations and grounded vision language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV 2025. Project website:
  https://dense-functional-correspondence.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gowen Loo, Chang Liu, Qinghong Yin, Xiang Chen, Jiawei Chen, Jingyuan Zhang, Yu Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smartphones have become indispensable in people's daily lives, permeating
nearly every aspect of modern society. With the continuous advancement of large
language models (LLMs), numerous LLM-based mobile agents have emerged. These
agents are capable of accurately parsing diverse user queries and automatically
assisting users in completing complex or repetitive operations. However,
current agents 1) heavily rely on the comprehension ability of LLMs, which can
lead to errors caused by misoperations or omitted steps during tasks, 2) lack
interaction with the external environment, often terminating tasks when an app
cannot fulfill user queries, and 3) lack memory capabilities, requiring each
instruction to reconstruct the interface and being unable to learn from and
correct previous mistakes. To alleviate the above issues, we propose MobileRAG,
a mobile agents framework enhanced by Retrieval-Augmented Generation (RAG),
which includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly
and accurately identify user queries and accomplish complex and long-sequence
mobile tasks. Additionally, to more comprehensively assess the performance of
MobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark
characterized by numerous complex, real-world mobile tasks that require
external knowledge assistance. Extensive experimental results on MobileRAG-Eval
demonstrate that MobileRAG can easily handle real-world mobile tasks, achieving
10.3\% improvement over state-of-the-art methods with fewer operational steps.
Our code is publicly available at:
https://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OccTENS: 3D Occupancy World Model via Temporal Next-Scale Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bu Jin, Songen Gu, Xiaotao Hu, Yupeng Zheng, Xiaoyang Guo, Qian Zhang, Xiaoxiao Long, Wei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose OccTENS, a generative occupancy world model that
enables controllable, high-fidelity long-term occupancy generation while
maintaining computational efficiency. Different from visual generation, the
occupancy world model must capture the fine-grained 3D geometry and dynamic
evolution of the 3D scenes, posing great challenges for the generative models.
Recent approaches based on autoregression (AR) have demonstrated the potential
to predict vehicle movement and future occupancy scenes simultaneously from
historical observations, but they typically suffer from \textbf{inefficiency},
\textbf{temporal degradation} in long-term generation and \textbf{lack of
controllability}. To holistically address these issues, we reformulate the
occupancy world model as a temporal next-scale prediction (TENS) task, which
decomposes the temporal sequence modeling problem into the modeling of spatial
scale-by-scale generation and temporal scene-by-scene prediction. With a
\textbf{TensFormer}, OccTENS can effectively manage the temporal causality and
spatial relationships of occupancy sequences in a flexible and scalable way. To
enhance the pose controllability, we further propose a holistic pose
aggregation strategy, which features a unified sequence modeling for occupancy
and ego-motion. Experiments show that OccTENS outperforms the state-of-the-art
method with both higher occupancy quality and faster inference time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Motion Video Generation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwei Xue, Xiangyang Luo, Zhanghao Hu, Xin Zhang, Xunzhi Xiang, Yuqin Dai, Jianzhuang Liu, Zhensong Zhang, Minglei Li, Jian Yang, Fei Ma, Zhiyong Wu, Changpeng Yang, Zonghong Dai, Fei Richard Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human motion video generation has garnered significant research interest due
to its broad applications, enabling innovations such as photorealistic singing
heads or dynamic avatars that seamlessly dance to music. However, existing
surveys in this field focus on individual methods, lacking a comprehensive
overview of the entire generative process. This paper addresses this gap by
providing an in-depth survey of human motion video generation, encompassing
over ten sub-tasks, and detailing the five key phases of the generation
process: input, motion planning, motion video generation, refinement, and
output. Notably, this is the first survey that discusses the potential of large
language models in enhancing human motion video generation. Our survey reviews
the latest developments and technological trends in human motion video
generation across three primary modalities: vision, text, and audio. By
covering over two hundred papers, we offer a thorough overview of the field and
highlight milestone works that have driven significant technological
breakthroughs. Our goal for this survey is to unveil the prospects of human
motion video generation and serve as a valuable resource for advancing the
comprehensive applications of digital humans. A complete list of the models
examined in this survey is available in Our Repository
https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TPAMI. Github Repo:
  https://github.com/Winn1y/Awesome-Human-Motion-Video-Generation IEEE Access:
  https://ieeexplore.ieee.org/document/11106267</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Song, Xiaoou Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Food recognition has gained significant attention, but the rapid emergence of
new dishes requires methods for recognizing unseen food categories, motivating
Zero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot
Food Recognition (CZSFR), where cuisines and ingredients naturally align with
attributes and objects in Compositional Zero-Shot learning (CZSL). However,
CZSFR faces three challenges: (1) Redundant background information distracts
models from learning meaningful food features, (2) Role confusion between
staple and side dishes leads to misclassification, and (3) Semantic bias in a
single attribute can lead to confusion of understanding. Therefore, we propose
SalientFusion, a context-aware CZSFR method with two components: SalientFormer,
which removes background redundancy and uses depth features to resolve role
confusion; DebiasAT, which reduces the semantic bias by aligning prompts with
visual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we
show that SalientFusion achieves state-of-the-art results on these benchmarks
and the most popular general datasets for the general CZSL. The code is
avaliable at https://github.com/Jiajun-RUC/SalientFusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34th International Conference on Artificial Neural Networks - ICANN
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Focus Through Motion: RGB-Event Collaborative Token Sparsification for
  Efficient Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Yang, Yang Wang, Zhanwen Liu, Yuchao Dai, Yang Liu, Xiangmo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing RGB-Event detection methods process the low-information regions of
both modalities (background in images and non-event regions in event data)
uniformly during feature extraction and fusion, resulting in high computational
costs and suboptimal performance. To mitigate the computational redundancy
during feature extraction, researchers have respectively proposed token
sparsification methods for the image and event modalities. However, these
methods employ a fixed number or threshold for token selection, hindering the
retention of informative tokens for samples with varying complexity. To achieve
a better balance between accuracy and efficiency, we propose FocusMamba, which
performs adaptive collaborative sparsification of multimodal features and
efficiently integrates complementary information. Specifically, an Event-Guided
Multimodal Sparsification (EGMS) strategy is designed to identify and
adaptively discard low-information regions within each modality by leveraging
scene content changes perceived by the event camera. Based on the
sparsification results, a Cross-Modality Focus Fusion (CMFF) module is proposed
to effectively capture and integrate complementary features from both
modalities. Experiments on the DSEC-Det and PKU-DAVIS-SOD datasets demonstrate
that the proposed method achieves superior performance in both accuracy and
efficiency compared to existing methods. The code will be available at
https://github.com/Zizzzzzzz/FocusMamba.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Augmented Quantization-Aware Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03850v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03850v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Kur, Kaiqi Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization-aware training (QAT) and Knowledge Distillation (KD) are
combined to achieve competitive performance in creating low-bit deep learning
models. Existing KD and QAT works focus on improving the accuracy of quantized
models from the network output perspective by designing better KD loss
functions or optimizing QAT's forward and backward propagation. However,
limited attention has been given to understanding the impact of input
transformations, such as data augmentation (DA). The relationship between
quantization-aware KD and DA remains unexplored. In this paper, we address the
question: how to select a good DA in quantization-aware KD, especially for the
models with low precisions? We propose a novel metric which evaluates DAs
according to their capacity to maximize the Contextual Mutual Information--the
information not directly related to an image's label--while also ensuring the
predictions for each class are close to the ground truth labels on average. The
proposed method automatically ranks and selects DAs, requiring minimal training
overhead, and it is compatible with any KD or QAT algorithm. Extensive
evaluations demonstrate that selecting DA strategies using our metric
significantly improves state-of-the-art QAT and KD works across various model
architectures and datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multidimensional AI-powered Framework for Analyzing Tourist Perception
  in Historic Urban Quarters: A Case Study in Shanghai 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizhen Tan, Yufan Wu, Yuxuan Liu, Haoran Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Historic urban quarters play a vital role in preserving cultural heritage
while serving as vibrant spaces for tourism and everyday life. Understanding
how tourists perceive these environments is essential for sustainable,
human-centered urban planning. This study proposes a multidimensional
AI-powered framework for analyzing tourist perception in historic urban
quarters using multimodal data from social media. Applied to twelve historic
quarters in central Shanghai, the framework integrates focal point extraction,
color theme analysis, and sentiment mining. Visual focus areas are identified
from tourist-shared photos using a fine-tuned semantic segmentation model. To
assess aesthetic preferences, dominant colors are extracted using a clustering
method, and their spatial distribution across quarters is analyzed. Color
themes are further compared between social media photos and real-world street
views, revealing notable shifts. This divergence highlights potential gaps
between visual expectations and the built environment, reflecting both
stylistic preferences and perceptual bias. Tourist reviews are evaluated
through a hybrid sentiment analysis approach combining a rule-based method and
a multi-task BERT model. Satisfaction is assessed across four dimensions:
tourist activities, built environment, service facilities, and business
formats. The results reveal spatial variations in aesthetic appeal and
emotional response. Rather than focusing on a single technical innovation, this
framework offers an integrated, data-driven approach to decoding tourist
perception and contributes to informed decision-making in tourism, heritage
conservation, and the design of aesthetically engaging public spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EGTM: Event-guided Efficient Turbulence Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanan Li, Rui Fan, Juntao Guan, Weidong Hao, Lai Rui, Tong Wu, Yikai Wang, Lin Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Turbulence mitigation (TM) aims to remove the stochastic distortions and
blurs introduced by atmospheric turbulence into frame cameras. Existing
state-of-the-art deep-learning TM methods extract turbulence cues from multiple
degraded frames to find the so-called "lucky'', not distorted patch, for "lucky
fusion''. However, it requires high-capacity network to learn from
coarse-grained turbulence dynamics between synchronous frames with limited
frame-rate, thus fall short in computational and storage efficiency. Event
cameras, with microsecond-level temporal resolution, have the potential to
fundamentally address this bottleneck with efficient sparse and asynchronous
imaging mechanism. In light of this, we (i) present the fundamental
\textbf{``event-lucky insight''} to reveal the correlation between turbulence
distortions and inverse spatiotemporal distribution of event streams. Then,
build upon this insight, we (ii) propose a novel EGTM framework that extracts
pixel-level reliable turbulence-free guidance from the explicit but noisy
turbulent events for temporal lucky fusion. Moreover, we (iii) build the first
turbulence data acquisition system to contribute the first real-world
event-driven TM dataset. Extensive experimental results demonstrate that our
approach significantly surpass the existing SOTA TM method by 710 times, 214
times and 224 times in model size, inference latency and model complexity
respectively, while achieving the state-of-the-art in restoration quality
(+0.94 PSNR and +0.08 SSIM) on our real-world EGTM dataset. This demonstrating
the great efficiency merit of introducing event modality into TM task. Demo
code and data have been uploaded in supplementary material and will be released
once accepted.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causality-guided Prompt <span class="highlight-title">Learning</span> for <span class="highlight-title">Vision</span>-language Models via <span class="highlight-title">Visual</span>
  Granulation <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengyu Gao, Qiulei Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt learning has recently attracted much attention for adapting
pre-trained vision-language models (e.g., CLIP) to downstream recognition
tasks. However, most of the existing CLIP-based prompt learning methods only
show a limited ability for handling fine-grained datasets. To address this
issue, we propose a causality-guided text prompt learning method via visual
granulation for CLIP, called CaPL, where the explored visual granulation
technique could construct sets of visual granules for the text prompt to
capture subtle discrepancies among different fine-grained classes through
casual inference. The CaPL method contains the following two modules: (1) An
attribute disentanglement module is proposed to decompose visual features into
non-individualized attributes (shared by some classes) and individualized
attributes (specific to single classes) using a Brownian Bridge Diffusion
Model; (2) A granule learning module is proposed to construct visual granules
by integrating the aforementioned attributes for recognition under two causal
inference strategies. Thanks to the learned visual granules, more
discriminative text prompt is expected to be learned. Extensive experimental
results on 15 datasets demonstrate that our CaPL method significantly
outperforms the state-of-the-art prompt learning methods, especially on
fine-grained datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2025 Accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedVista3D: <span class="highlight-title">Vision</span>-Language Modeling for Reducing Diagnostic Errors in
  3D CT Disease Detection, Understanding and Reporting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Li, Yenho Chen, Yuxiang Lai, Jike Zhong, Vanessa Wildman, Xiaofeng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiologic diagnostic errors-under-reading errors, inattentional blindness,
and communication failures-remain prevalent in clinical practice. These issues
often stem from missed localized abnormalities, limited global context, and
variability in report language. These challenges are amplified in 3D imaging,
where clinicians must examine hundreds of slices per scan. Addressing them
requires systems with precise localized detection, global volume-level
reasoning, and semantically consistent natural language reporting. However,
existing 3D vision-language models are unable to meet all three needs jointly,
lacking local-global understanding for spatial reasoning and struggling with
the variability and noise of uncurated radiology reports. We present
MedVista3D, a multi-scale semantic-enriched vision-language pretraining
framework for 3D CT analysis. To enable joint disease detection and holistic
interpretation, MedVista3D performs local and global image-text alignment for
fine-grained representation learning within full-volume context. To address
report variability, we apply language model rewrites and introduce a Radiology
Semantic Matching Bank for semantics-aware alignment. MedVista3D achieves
state-of-the-art performance on zero-shot disease classification, report
retrieval, and medical visual question answering, while transferring well to
organ segmentation and prognosis prediction. Code and datasets will be
released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fitting Image Diffusion Models on Video <span class="highlight-title">Dataset</span>s <span class="chip">ICCV25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juhun Lee, Simon S. Woo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image diffusion models are trained on independently sampled static images.
While this is the bedrock task protocol in generative modeling, capturing the
temporal world through the lens of static snapshots is information-deficient by
design. This limitation leads to slower convergence, limited distributional
coverage, and reduced generalization. In this work, we propose a simple and
effective training strategy that leverages the temporal inductive bias present
in continuous video frames to improve diffusion training. Notably, the proposed
method requires no architectural modification and can be seamlessly integrated
into standard diffusion training pipelines. We evaluate our method on the
HandCo dataset, where hand-object interactions exhibit dense temporal coherence
and subtle variations in finger articulation often result in semantically
distinct motions. Empirically, our method accelerates convergence by over
2$\text{x}$ faster and achieves lower FID on both training and validation
distributions. It also improves generative diversity by encouraging the model
to capture meaningful temporal variations. We further provide an optimization
analysis showing that our regularization reduces the gradient variance, which
contributes to faster convergence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV25 Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SLENet: A Guidance-Enhanced Network for Underwater Camouflaged Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinxin Wang, Han Sun, Ningzhong Liu, Huiyu Zhou, Yinan Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Underwater Camouflaged Object Detection (UCOD) aims to identify objects that
blend seamlessly into underwater environments. This task is critically
important to marine ecology. However, it remains largely underexplored and
accurate identification is severely hindered by optical distortions, water
turbidity, and the complex traits of marine organisms. To address these
challenges, we introduce the UCOD task and present DeepCamo, a benchmark
dataset designed for this domain. We also propose Semantic Localization and
Enhancement Network (SLENet), a novel framework for UCOD. We first benchmark
state-of-the-art COD models on DeepCamo to reveal key issues, upon which SLENet
is built. In particular, we incorporate Gamma-Asymmetric Enhancement (GAE)
module and a Localization Guidance Branch (LGB) to enhance multi-scale feature
representation while generating a location map enriched with global semantic
information. This map guides the Multi-Scale Supervised Decoder (MSSD) to
produce more accurate predictions. Experiments on our DeepCamo dataset and
three benchmark COD datasets confirm SLENet's superior performance over SOTA
methods, and underscore its high generality for the broader COD task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14pages, accepted by PRCV2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Res-MoCoDiff: Residual-guided diffusion models for motion artifact
  correction in brain MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03498v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03498v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mojtaba Safari, Shansong Wang, Qiang Li, Zach Eidex, Richard L. J. Qiu, Chih-Wei Chang, Hui Mao, Xiaofeng Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective. Motion artifacts in brain MRI, mainly from rigid head motion,
degrade image quality and hinder downstream applications. Conventional methods
to mitigate these artifacts, including repeated acquisitions or motion
tracking, impose workflow burdens. This study introduces Res-MoCoDiff, an
efficient denoising diffusion probabilistic model specifically designed for MRI
motion artifact correction.Approach.Res-MoCoDiff exploits a novel residual
error shifting mechanism during the forward diffusion process to incorporate
information from motion-corrupted images. This mechanism allows the model to
simulate the evolution of noise with a probability distribution closely
matching that of the corrupted data, enabling a reverse diffusion process that
requires only four steps. The model employs a U-net backbone, with attention
layers replaced by Swin Transformer blocks, to enhance robustness across
resolutions. Furthermore, the training process integrates a combined l1+l2 loss
function, which promotes image sharpness and reduces pixel-level errors.
Res-MoCoDiff was evaluated on both an in-silico dataset generated using a
realistic motion simulation framework and an in-vivo MR-ART dataset.
Comparative analyses were conducted against established methods, including
CycleGAN, Pix2pix, and a diffusion model with a vision transformer backbone,
using quantitative metrics such as PSNR, SSIM, and NMSE.Main results. The
proposed method demonstrated superior performance in removing motion artifacts
across minor, moderate, and heavy distortion levels. Res-MoCoDiff consistently
achieved the highest SSIM and the lowest NMSE values, with a PSNR of up to
41.91+-2.94 dB for minor distortions. Notably, the average sampling time was
reduced to 0.37 seconds per batch of two image slices, compared with 101.74
seconds for conventional approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sat-DN: Implicit Surface Reconstruction from Multi-View Satellite Images
  with Depth and Normal Super<span class="highlight-title">vision</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08352v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08352v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianle Liu, Shuangming Zhao, Wanshou Jiang, Bingxuan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With advancements in satellite imaging technology, acquiring high-resolution
multi-view satellite imagery has become increasingly accessible, enabling rapid
and location-independent ground model reconstruction. However, traditional
stereo matching methods struggle to capture fine details, and while neural
radiance fields (NeRFs) achieve high-quality reconstructions, their training
time is prohibitively long. Moreover, challenges such as low visibility of
building facades, illumination and style differences between pixels, and weakly
textured regions in satellite imagery further make it hard to reconstruct
reasonable terrain geometry and detailed building facades. To address these
issues, we propose Sat-DN, a novel framework leveraging a progressively trained
multi-resolution hash grid reconstruction architecture with explicit depth
guidance and surface normal consistency constraints to enhance reconstruction
quality. The multi-resolution hash grid accelerates training, while the
progressive strategy incrementally increases the learning frequency, using
coarse low-frequency geometry to guide the reconstruction of fine
high-frequency details. The depth and normal constraints ensure a clear
building outline and correct planar distribution. Extensive experiments on the
DFC2019 dataset demonstrate that Sat-DN outperforms existing methods, achieving
state-of-the-art results in both qualitative and quantitative evaluations. The
code is available at https://github.com/costune/SatDN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demographic-aware fine-grained classification of pediatric wrist
  fractures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12964v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12964v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ammar Ahmed, Ali Shariq Imran, Zenun Kastrati, Sher Muhammad Daudpota
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wrist pathologies are frequently observed, particularly among children who
constitute the majority of fracture cases. Computer vision presents a promising
avenue, contingent upon the availability of extensive datasets, a notable
challenge in medical imaging. Therefore, reliance solely on one modality, such
as images, proves inadequate, especially in an era of diverse and plentiful
data types. This study addresses the problem using a multifaceted approach:
framing it as a fine-grained recognition task, fusing patient metadata with
X-rays, and leveraging weights from a separate fine-grained dataset rather than
from a coarse-grained dataset like ImageNet. Unlike prior work, this is the
first application of metadata integration for wrist pathology recognition. Our
results show that combining fine-grained transformer approach, fine-grained
pre-training, and metadata integration improves diagnostic accuracy by 2% on
small custom curated dataset and over 10% on a larger fracture dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ecological Legacies of Pre-Columbian Settlements Evident in Palm
  Clusters of Neotropical Mountain Forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.06949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.06949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Fajardo, Sina Mohammadi, Jonas Gregorio de Souza, César Ardila, Alan Tapscott Baltar, Shaddai Heidgen, Maria Isabel Mayorga Hernández, Sylvia Mota de Oliveira, Fernando Montejo, Marco Moderato, Vinicius Peripato, Katy Puche, Carlos Reina, Juan Carlos Vargas, Frank W. Takes, Marco Madella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ancient populations markedly transformed Neotropical forests, yet the spatial
extent of their ecological influence remains underexplored at high resolution.
Here we present a deep learning and remote sensing based approach to estimate
areas of pre-Columbian forest modification based on modern vegetation. We apply
this method to high-resolution satellite imagery from the Sierra Nevada de
Santa Marta, Colombia, as a demonstration of a scalable approach, to evaluate
palm tree distributions in relation to archaeological infrastructure. Palms
were significantly more abundant near archaeological sites with large
infrastructure investment. The extent of the largest palm cluster indicates
that ancient human-managed areas linked to major infrastructure sites may be up
to two orders of magnitude bigger than indicated by current archaeological
evidence alone. Our findings suggest that pre-Columbian populations influenced
vegetation, fostering conditions conducive to palm proliferation, leaving a
lasting ecological footprint. This may have lowered the logistical costs of
establishing infrastructure-heavy settlements in less accessible locations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FADE: A <span class="highlight-title">Dataset</span> for Detecting Falling Objects around Buildings in Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhigang Tu, Zitao Gao, Zhengbo Zhang, Chunluan Zhou, Junsong Yuan, Bo Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Falling objects from buildings can cause severe injuries to pedestrians due
to the great impact force they exert. Although surveillance cameras are
installed around some buildings, it is challenging for humans to capture such
events in surveillance videos due to the small size and fast motion of falling
objects, as well as the complex background. Therefore, it is necessary to
develop methods to automatically detect falling objects around buildings in
surveillance videos. To facilitate the investigation of falling object
detection, we propose a large, diverse video dataset called FADE (FAlling
Object DEtection around Buildings) for the first time. FADE contains 1,881
videos from 18 scenes, featuring 8 falling object categories, 4 weather
conditions, and 4 video resolutions. Additionally, we develop a new object
detection method called FADE-Net, which effectively leverages motion
information and produces small-sized but high-quality proposals for detecting
falling objects around buildings. Importantly, our method is extensively
evaluated and analyzed by comparing it with the previous approaches used for
generic object detection, video object detection, and moving object detection
on the FADE dataset. Experimental results show that the proposed FADE-Net
significantly outperforms other methods, providing an effective baseline for
future research. The dataset and code are publicly available at
https://fadedataset.github.io/FADE.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Information Forensics and Security
  (TIFS), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Space Is Rocket Science -- Only Top Reasoning Models Can
  Solve Spatial Understanding Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Hoehing, Mayug Maniparambil, Ellen Rushe, Noel E. O'Connor, Anthony Ventresque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose RocketScience, an open-source contrastive VLM benchmark that tests
for spatial relation understanding. It is comprised of entirely new real-world
image-text pairs covering mostly relative spatial understanding and the order
of objects. The benchmark is designed to be very easy for humans and hard for
the current generation of VLMs, and this is empirically verified. Our results
show a striking lack of spatial relation understanding in open source and
frontier commercial VLMs and a surprisingly high performance of reasoning
models. Additionally, we perform a disentanglement analysis to separate the
contributions of object localization and spatial reasoning in
chain-of-thought-based models and find that the performance on the benchmark is
bottlenecked by spatial reasoning and not object localization capabilities. We
release the dataset with a CC-BY-4.0 license and make the evaluation code
available at: https://github.com/nilshoehing/rocketscience
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DIO: Refining Mutual Information and Causal Chain to Enhance Machine
  Abstract Reasoning Ability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.15387v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.15387v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhuo Song, Beiming Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the outstanding performance of current deep learning models across
various domains, their fundamental bottleneck in abstract reasoning remains
unresolved. To address this challenge, the academic community has introduced
Raven's Progressive Matrices (RPM) problems as an authoritative benchmark for
evaluating the abstract reasoning capabilities of deep learning algorithms,
with a focus on core intelligence dimensions such as abstract reasoning,
pattern recognition, and complex problem-solving. Therefore, this paper centers
on solving RPM problems, aiming to contribute to enhancing the abstract
reasoning abilities of machine intelligence. Firstly, this paper adopts a
``causal chain modeling'' perspective to systematically analyze the complete
causal chain in RPM tasks: image $\rightarrow$ abstract attributes
$\rightarrow$ progressive attribute patterns $\rightarrow$ pattern consistency
$\rightarrow$ correct answer. Based on this analysis, the network architecture
of the baseline model DIO is designed. However, experiments reveal that the
optimization objective formulated for DIO, namely maximizing the variational
lower bound of mutual information between the context and the correct option,
fails to enable the model to genuinely acquire the predefined human reasoning
logic. This is attributed to two main reasons: the tightness of the lower bound
significantly impacts the effectiveness of mutual information maximization, and
mutual information, as a statistical measure, does not capture the causal
relationship between subjects and objects. To overcome these limitations, this
paper progressively proposes three improvement methods:
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 9 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Completing Spatial Transcriptomics Data for Gene Expression Prediction
  Benchmarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.02980v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.02980v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniela Ruiz, Paula Cárdenas, Leonardo Manrique, Daniela Vega, Gabriel M. Mejia, Pablo Arbeláez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial Transcriptomics is a groundbreaking technology that integrates
histology images with spatially resolved gene expression profiles. Among the
various Spatial Transcriptomics techniques available, Visium has emerged as the
most widely adopted. However, its accessibility is limited by high costs, the
need for specialized expertise, and slow clinical integration. Additionally,
gene capture inefficiencies lead to significant dropout, corrupting acquired
data. To address these challenges, the deep learning community has explored the
gene expression prediction task directly from histology images. Yet,
inconsistencies in datasets, preprocessing, and training protocols hinder fair
comparisons between models. To bridge this gap, we introduce SpaRED, a
systematically curated database comprising 26 public datasets, providing a
standardized resource for model evaluation. We further propose SpaCKLE, a
state-of-the-art transformer-based gene expression completion model that
reduces mean squared error by over 82.5% compared to existing approaches.
Finally, we establish the SpaRED benchmark, evaluating eight state-of-the-art
prediction models on both raw and SpaCKLE-completed data, demonstrating SpaCKLE
substantially improves the results across all the gene expression prediction
models. Altogether, our contributions constitute the most comprehensive
benchmark of gene expression prediction from histology images to date and a
stepping stone for future research on Spatial Transcriptomics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2407.13027</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pulling Back the Curtain on ReLU Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22832v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22832v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Satkiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since any ReLU network is piecewise affine, its hidden units can be
characterized by their pullbacks through the active subnetwork, i.e., by their
gradients (up to bias terms). However, gradients of deeper neurons are
notoriously misaligned, which obscures the network's internal representations.
We posit that models do align gradients with data, yet this is concealed by the
intrinsic noise of the ReLU hard gating. We validate this intuition by applying
soft gating in the backward pass only, reducing the local impact of weakly
excited neurons. The resulting modified gradients, which we call "excitation
pullbacks", exhibit striking perceptual alignment on a number of
ImageNet-pretrained architectures, while the rudimentary pixel-space gradient
ascent quickly produces easily interpretable input- and target-specific
features. Inspired by these findings, we formulate the "path stability"
hypothesis, claiming that the binary activation patterns largely stabilize
during training and get encoded in the pre-activation distribution of the final
model. When true, excitation pullbacks become aligned with the gradients of a
kernel machine that mainly determines the network's decision. This provides a
theoretical justification for the apparent faithfulness of the feature
attributions based on these pullbacks, potentially even leading to mechanistic
interpretability of deeper models. Incidentally, we give a possible explanation
for the effectiveness of Batch Normalization and Deep Features, together with a
novel perspective on the network's internal memory and generalization
properties. We release the code and an interactive app for easier exploration
of the excitation pullbacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3-page appendix, 4 figures, preprint; v3 changes: changed
  title, improved abstract, expanded introduction, added section on
  implications of the path stability</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Embedding Sampling Method for Diverse Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sania Waheed, Na Min An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Captioning for state-of-the-art VLMs has significantly improved over
time; however, this comes at the cost of increased computational complexity,
making them less accessible for resource-constrained applications such as
mobile devices and assistive technologies. Alternatively, comparably smaller
VLMs prioritize high-level scene descriptions, overlooking finer details that
contribute to a richer understanding of an image. In this paper, we introduce a
training-free framework that enhances caption diversity and informativeness by
explicitly attending to distinct image regions using a comparably small VLM,
BLIP, as the backbone. Our approach leverages structured segmentation to
produce hierarchical representations that capture both global and localized
semantics. Without requiring additional model training, we demonstrate that our
method allows smaller VLMs to achieve performance comparable to larger models
in terms of image-caption alignment, semantic integrity, and diversity. We
evaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets,
achieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset,
respectively, while maintaining strong image-caption relevancy and semantic
integrity with the human-annotated captions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Style Transfer to Calvin and Hobbes comics using Stable Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asvin Kumar Venkataramanan, Sloke Shrestha, Sundar Sripada Venugopalaswamy Sriraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This project report summarizes our journey to perform stable diffusion
fine-tuning on a dataset containing Calvin and Hobbes comics. The purpose is to
convert any given input image into the comic style of Calvin and Hobbes,
essentially performing style transfer. We train stable-diffusion-v1.5 using Low
Rank Adaptation (LoRA) to efficiently speed up the fine-tuning process. The
diffusion itself is handled by a Variational Autoencoder (VAE), which is a
U-net. Our results were visually appealing for the amount of training time and
the quality of input data that went into training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated authorship</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Straighter Flow Matching via a Diffusion-Based Coupling Prior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.16507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.16507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Xing, Jie Cao, Huaibo Huang, Haichao Shi, Xiao-Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flow matching as a paradigm of generative model achieves notable success
across various domains. However, existing methods use either multi-round
training or knowledge within minibatches, posing challenges in finding a
favorable coupling strategy for straightening trajectories to few-step
generation. To address this issue, we propose a novel approach, Straighter
trajectories of Flow Matching (StraightFM). It straightens trajectories with
the coupling strategy from the entire distribution level. More specifically,
during training, StraightFM creates couplings of images and noise via one
diffusion model as a coupling prior to straighten trajectories for few-step
generation. Our coupling strategy can also integrate with the existing coupling
direction from real data to noise, improving image quality in few-step
generation. Experimental results on pixel space and latent space show that
StraightFM yields attractive samples within 5 steps. Moreover, our
unconditional StraightFM is seamlessly compatible with training-free multimodal
conditional generation, maintaining high-quality image generation in few steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Drawing2CAD: Sequence-to-Sequence <span class="highlight-title">Learning</span> for CAD Generation from
  Vector Drawings <span class="chip">ACM MM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.18733v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.18733v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, Ligang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer-Aided Design (CAD) generative modeling is driving significant
innovations across industrial applications. Recent works have shown remarkable
progress in creating solid models from various inputs such as point clouds,
meshes, and text descriptions. However, these methods fundamentally diverge
from traditional industrial workflows that begin with 2D engineering drawings.
The automatic generation of parametric CAD models from these 2D vector drawings
remains underexplored despite being a critical step in engineering design. To
address this gap, our key insight is to reframe CAD generation as a
sequence-to-sequence learning problem where vector drawing primitives directly
inform the generation of parametric CAD operations, preserving geometric
precision and design intent throughout the transformation process. We propose
Drawing2CAD, a framework with three key technical components: a
network-friendly vector primitive representation that preserves precise
geometric information, a dual-decoder transformer architecture that decouples
command type and parameter generation while maintaining precise correspondence,
and a soft target distribution loss function accommodating inherent flexibility
in CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,
a dataset of paired engineering drawings and parametric CAD models, and conduct
thorough experiments to demonstrate the effectiveness of our method. Code and
dataset are available at https://github.com/lllssc/Drawing2CAD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM MM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoPETIII: The Tracer Frontier. What Frontier? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zacharia Mesbah, Léo Mottay, Romain Modzelewski, Pierre Decazes, Sébastien Hapdey, Su Ruan, Sébastien Thureau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the last three years, the AutoPET competition gathered the medical
imaging community around a hot topic: lesion segmentation on Positron Emitting
Tomography (PET) scans. Each year a different aspect of the problem is
presented; in 2024 the multiplicity of existing and used tracers was at the
core of the challenge. Specifically, this year's edition aims to develop a
fully automatic algorithm capable of performing lesion segmentation on a PET/CT
scan, without knowing the tracer, which can either be a FDG or PSMA-based
tracer. In this paper we describe how we used the nnUNetv2 framework to train
two sets of 6 fold ensembles of models to perform fully automatic PET/CT lesion
segmentation as well as a MIP-CNN to choose which set of models to use for
segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D
  <span class="highlight-title">Visual</span> Grounding based on CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Li, Zanyi Wang, Zeyi Huang, Guang Dai, Jingdong Wang, Mengmeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text
  Pairing <span class="chip">ICCV25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22627v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22627v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Girella, Davide Talon, Ziyue Liu, Zanxi Ruan, Yiming Wang, Marco Cristani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fashion design is a complex creative process that blends visual and textual
expressions. Designers convey ideas through sketches, which define spatial
structure and design elements, and textual descriptions, capturing material,
texture, and stylistic details. In this paper, we present LOcalized Text and
Sketch for fashion image generation (LOTS), an approach for compositional
sketch-text based generation of complete fashion outlooks. LOTS leverages a
global description with paired localized sketch + text information for
conditioning and introduces a novel step-based merging strategy for diffusion
adaptation. First, a Modularized Pair-Centric representation encodes sketches
and text into a shared latent space while preserving independent localized
features; then, a Diffusion Pair Guidance phase integrates both local and
global conditioning via attention-based guidance within the diffusion model's
multi-step denoising process. To validate our method, we build on Fashionpedia
to release Sketchy, the first fashion dataset where multiple text-sketch pairs
are provided per image. Quantitative results show LOTS achieves
state-of-the-art image generation performance on both global and localized
metrics, while qualitative examples and a human evaluation study highlight its
unprecedented level of design customization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV25 (Oral). Project page:
  https://intelligolabs.github.io/lots/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Imitating Radiological Scrolling: A Global-Local Attention Model for 3D
  Chest CT Volumes Multi-Label Anomaly Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20652v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20652v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theo Di Piazza, Carole Lazarus, Olivier Nempont, Loic Boussel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid increase in the number of Computed Tomography (CT) scan
examinations has created an urgent need for automated tools, such as organ
segmentation, anomaly classification, and report generation, to assist
radiologists with their growing workload. Multi-label classification of
Three-Dimensional (3D) CT scans is a challenging task due to the volumetric
nature of the data and the variety of anomalies to be detected. Existing deep
learning methods based on Convolutional Neural Networks (CNNs) struggle to
capture long-range dependencies effectively, while Vision Transformers require
extensive pre-training, posing challenges for practical use. Additionally,
these existing methods do not explicitly model the radiologist's navigational
behavior while scrolling through CT scan slices, which requires both global
context understanding and local detail awareness. In this study, we present
CT-Scroll, a novel global-local attention model specifically designed to
emulate the scrolling behavior of radiologists during the analysis of 3D CT
scans. Our approach is evaluated on two public datasets, demonstrating its
efficacy through comprehensive experiments and an ablation study that
highlights the contribution of each model component.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures. Accepted for publication at MIDL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Replication Study and Benchmarking of Real-Time Object Detection Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.06911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.06911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre-Luc Asselin, Vincent Coulombe, William Guimont-Martin, William Larrivée-Hardy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work examines the reproducibility and benchmarking of state-of-the-art
real-time object detection models. As object detection models are often used in
real-world contexts, such as robotics, where inference time is paramount,
simply measuring models' accuracy is not enough to compare them. We thus
compare a large variety of object detection models' accuracy and inference
speed on multiple graphics cards. In addition to this large benchmarking
attempt, we also reproduce the following models from scratch using PyTorch on
the MS COCO 2017 dataset: DETR, RTMDet, ViTDet and YOLOv7. More importantly, we
propose a unified training and evaluation pipeline, based on MMDetection's
features, to better compare models. Our implementation of DETR and ViTDet could
not achieve accuracy or speed performances comparable to what is declared in
the original papers. On the other hand, reproduced RTMDet and YOLOv7 could
match such performances. Studied papers are also found to be generally lacking
for reproducibility purposes. As for MMDetection pretrained models, speed
performances are severely reduced with limited computing resources (larger,
more accurate models even more so). Moreover, results exhibit a strong
trade-off between accuracy and speed, prevailed by anchor-free models - notably
RTMDet or YOLOx models. The code used is this paper and all the experiments is
available in the repository at https://github.com/willGuimont/segdet_mlcr2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Authors are presented in alphabetical order, each having equal
  contribution to the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MUNBa: Machine Un<span class="highlight-title">learning</span> via Nash Bargaining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.15537v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.15537v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Wu, Mehrtash Harandi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Unlearning (MU) aims to selectively erase harmful behaviors from
models while retaining the overall utility of the model. As a multi-task
learning problem, MU involves balancing objectives related to forgetting
specific concepts/data and preserving general performance. A naive integration
of these forgetting and preserving objectives can lead to gradient conflicts
and dominance, impeding MU algorithms from reaching optimal solutions. To
address the gradient conflict and dominance issue, we reformulate MU as a
two-player cooperative game, where the two players, namely, the forgetting
player and the preservation player, contribute via their gradient proposals to
maximize their overall gain and balance their contributions. To this end,
inspired by the Nash bargaining theory, we derive a closed-form solution to
guide the model toward the Pareto stationary point. Our formulation of MU
guarantees an equilibrium solution, where any deviation from the final state
would lead to a reduction in the overall objectives for both players, ensuring
optimality in each objective. We evaluate our algorithm's effectiveness on a
diverse set of tasks across image classification and image generation.
Extensive experiments with ResNet, vision-language model CLIP, and
text-to-image diffusion models demonstrate that our method outperforms
state-of-the-art MU algorithms, achieving a better trade-off between forgetting
and preserving. Our results also highlight improvements in forgetting
precision, preservation of generalization, and robustness against adversarial
attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferable Mask Transformer: Cross-domain <span class="highlight-title">Semantic</span> Segmentation with
  Region-adaptive Transferability <span class="highlight-title">Estimation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhua Liu, Zhengyu Li, Yanru Wu, Jingge Wang, Yang Tan, Ruizhe Zhao, Guan Wang, Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Vision Transformers (ViTs) have set new benchmarks in
semantic segmentation. However, when adapting pretrained ViTs to new target
domains, significant performance degradation often occurs due to distribution
shifts, resulting in suboptimal global attention. Since self-attention
mechanisms are inherently data-driven, they may fail to effectively attend to
key objects when source and target domains exhibit differences in texture,
scale, or object co-occurrence patterns. While global and patch-level domain
adaptation methods provide partial solutions, region-level adaptation with
dynamically shaped regions is crucial due to spatial heterogeneity in
transferability across different image areas. We present Transferable Mask
Transformer (TMT), a novel region-level adaptation framework for semantic
segmentation that aligns cross-domain representations through spatial
transferability analysis. TMT consists of two key components: (1) An Adaptive
Cluster-based Transferability Estimator (ACTE) that dynamically segments images
into structurally and semantically coherent regions for localized
transferability assessment, and (2) A Transferable Masked Attention (TMA)
module that integrates region-specific transferability maps into ViTs'
attention mechanisms, prioritizing adaptation in regions with low
transferability and high semantic uncertainty. Comprehensive evaluations across
20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2%
MIoU improvement over vanilla fine-tuning and a 1.28% increase compared to
state-of-the-art baselines. The source code will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIN: A Knowledge-Intensive <span class="highlight-title">Dataset</span> for Paired and Interleaved Multimodal
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wang, Yuxiang Zhang, Minghao Liu, Yin Zhang, Yatai Ji, Weihao Xuan, Nie Lin, Kang Zhu, Zhiqiang Lin, Yiming Ren, Chunyang Jiang, Yiyao Yu, Zekun Wang, Tiezhen Wang, Wenhao Huang, Jie Fu, Qunshu Liu, Yujiu Yang, Ge Zhang, Ruibin Yuan, Bei Chen, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large multimodal models (LMMs) have leveraged
extensive multimodal datasets to enhance capabilities in complex
knowledge-driven tasks. However, persistent challenges in perceptual and
reasoning errors limit their efficacy, particularly in interpreting intricate
visual data and deducing multimodal relationships. To address these issues, we
introduce PIN (Paired and INterleaved multimodal documents), a novel data
format designed to foster a deeper integration of visual and textual knowledge.
The PIN format uniquely combines semantically rich Markdown files, which
preserve fine-grained textual structures, with holistic overall images that
capture the complete document layout. Following this format, we construct and
release two large-scale, open-source datasets: PIN-200M (~200 million
documents) and PIN-14M (~14 million), compiled from diverse web and scientific
sources in both English and Chinese. To maximize usability, we provide detailed
statistical analyses and equip the datasets with quality signals, enabling
researchers to easily filter and select data for specific tasks. Our work
provides the community with a versatile data format and substantial resources,
offering a foundation for new research in pre-training strategies and the
development of more powerful knowledge-intensive LMMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report v1.0</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPARE: Symmetrized Point-to-Plane Distance for <span class="highlight-title">Robust</span> Non-Rigid
  Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20188v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20188v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Yao, Bailin Deng, Junhui Hou, Juyong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing optimization-based methods for non-rigid registration typically
minimize an alignment error metric based on the point-to-point or
point-to-plane distance between corresponding point pairs on the source surface
and target surface. However, these metrics can result in slow convergence or a
loss of detail. In this paper, we propose SPARE, a novel formulation that
utilizes a symmetrized point-to-plane distance for robust non-rigid
registration. The symmetrized point-to-plane distance relies on both the
positions and normals of the corresponding points, resulting in a more accurate
approximation of the underlying geometry and can achieve higher accuracy than
existing methods. To solve this optimization problem efficiently, we introduce
an as-rigid-as-possible regulation term to estimate the deformed normals and
propose an alternating minimization solver using a majorization-minimization
strategy. Moreover, for effective initialization of the solver, we incorporate
a deformation graph-based coarse alignment that improves registration quality
and efficiency. Extensive experiments show that the proposed method greatly
improves the accuracy of non-rigid registration problems and maintains
relatively high solution efficiency. The code is publicly available at
https://github.com/yaoyx689/spare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Pattern Analysis and Machine
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional Video Generation for High-Efficiency Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangqiu Yi, Jingyu Xu, Jiawei Shao, Chi Zhang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Critical methodology flaws invalidate key results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced <span class="highlight-title">Generative</span> Data Augmentation for <span class="highlight-title">Semantic</span> Segmentation via
  Stronger Guidance <span class="chip">ICPR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06002v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06002v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang-Huy Che, Duc-Tri Le, Bich-Nga Pham, Duc-Khai Lam, Vinh-Tiep Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is crucial for pixel-wise annotation tasks like semantic
segmentation, where labeling requires significant effort and intensive labor.
Traditional methods, involving simple transformations such as rotations and
flips, create new images but often lack diversity along key semantic dimensions
and fail to alter high-level semantic properties. To address this issue,
generative models have emerged as an effective solution for augmenting data by
generating synthetic images. Controllable Generative models offer data
augmentation methods for semantic segmentation tasks by using prompts and
visual references from the original image. However, these models face
challenges in generating synthetic images that accurately reflect the content
and structure of the original image due to difficulties in creating effective
prompts and visual references. In this work, we introduce an effective data
augmentation pipeline for semantic segmentation using Controllable Diffusion
model. Our proposed method includes efficient prompt generation using
Class-Prompt Appending and Visual Prior Blending to enhance attention to
labeled classes in real images, allowing the pipeline to generate a precise
number of augmented images while preserving the structure of
segmentation-labeled classes. In addition, we implement a class balancing
algorithm to ensure a balanced training dataset when merging the synthetic and
original images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates
its effectiveness in generating high-quality synthetic images for semantic
segmentation. Our code is available at
https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICPRAM 2025, ISBN 978-989-758-730-6, ISSN 2184-4313</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep <span class="highlight-title">Learning</span> Advances in <span class="highlight-title">Vision</span>-Based Traffic Accident Anticipation: A
  Comprehensive <span class="highlight-title">Review</span> of Methods, <span class="highlight-title">Dataset</span>s, and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.07611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.07611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruonan Lin, Tao Tang, Yongtai Liu, Wenye Zhou, Xin Yang, Hao Zheng, Jianpu Lin, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic accident prediction and detection are critical for enhancing road
safety, and vision-based traffic accident anticipation (Vision-TAA) has emerged
as a promising approach in the era of deep learning. This paper reviews 147
recent studies, focusing on the application of supervised, unsupervised, and
hybrid deep learning models for accident prediction, alongside the use of
real-world and synthetic datasets. Current methodologies are categorized into
four key approaches: image and video feature-based prediction, spatio-temporal
feature-based prediction, scene understanding, and multi modal data fusion.
While these methods demonstrate significant potential, challenges such as data
scarcity, limited generalization to complex scenarios, and real-time
performance constraints remain prevalent. This review highlights opportunities
for future research, including the integration of multi modal data fusion,
self-supervised learning, and Transformer-based architectures to enhance
prediction accuracy and scalability. By synthesizing existing advancements and
identifying critical gaps, this paper provides a foundational reference for
developing robust and adaptive Vision-TAA systems, contributing to road safety
and traffic management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spatial-aware Transformer-GRU Framework for Enhanced Glaucoma Diagnosis
  from 3D OCT Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.05702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.05702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mona Ashtari-Majlan, David Masip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Glaucoma, a leading cause of irreversible blindness, necessitates early
detection for accurate and timely intervention to prevent irreversible vision
loss. In this study, we present a novel deep learning framework that leverages
the diagnostic value of 3D Optical Coherence Tomography (OCT) imaging for
automated glaucoma detection. In this framework, we integrate a pre-trained
Vision Transformer on retinal data for rich slice-wise feature extraction and a
bidirectional Gated Recurrent Unit for capturing inter-slice spatial
dependencies. This dual-component approach enables comprehensive analysis of
local nuances and global structural integrity, crucial for accurate glaucoma
diagnosis. Experimental results on a large dataset demonstrate the superior
performance of the proposed method over state-of-the-art ones, achieving an
F1-score of 93.01%, Matthews Correlation Coefficient (MCC) of 69.33%, and AUC
of 94.20%. The framework's ability to leverage the valuable information in 3D
OCT data holds significant potential for enhancing clinical decision support
systems and improving patient outcomes in glaucoma management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HLG: Comprehensive 3D Room Construction via Hierarchical Layout
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.17832v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.17832v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiping Wang, Yuxi Wang, Mengqi Zhou, Junsong Fan, Zhaoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realistic 3D indoor scene generation is crucial for virtual reality, interior
design, embodied intelligence, and scene understanding. While existing methods
have made progress in coarse-scale furniture arrangement, they struggle to
capture fine-grained object placements, limiting the realism and utility of
generated environments. This gap hinders immersive virtual experiences and
detailed scene comprehension for embodied AI applications. To address these
issues, we propose Hierarchical Layout Generation (HLG), a novel method for
fine-grained 3D scene generation. HLG is the first to adopt a coarse-to-fine
hierarchical approach, refining scene layouts from large-scale furniture
placement to intricate object arrangements. Specifically, our fine-grained
layout alignment module constructs a hierarchical layout through vertical and
horizontal decoupling, effectively decomposing complex 3D indoor scenes into
multiple levels of granularity. Additionally, our trainable layout optimization
network addresses placement issues, such as incorrect positioning, orientation
errors, and object intersections, ensuring structurally coherent and physically
plausible scene generation. We demonstrate the effectiveness of our approach
through extensive experiments, showing superior performance in generating
realistic indoor scenes compared to existing methods. This work advances the
field of scene generation and opens new possibilities for applications
requiring detailed 3D environments. We will release our code upon publication
to encourage future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accurate and lightweight dehazing via multi-receptive-field non-local
  network and novel contrastive regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16494v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16494v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zewei He, Zixuan Chen, Jinlei Li, Ziqian Lu, Xuecheng Sun, Hao Luo, Zhe-Ming Lu, Evangelos K. Markakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, deep learning-based methods have dominated image dehazing domain. A
multi-receptive-field non-local network (MRFNLN) consisting of the multi-stream
feature attention block (MSFAB) and the cross non-local block (CNLB) is
presented in this paper to further enhance the performance. We start with
extracting richer features for dehazing. Specifically, a multi-stream feature
extraction (MSFE) sub-block, which contains three parallel convolutions with
different receptive fields (i.e., $1\times 1$, $3\times 3$, $5\times 5$), is
designed for extracting multi-scale features. Following MSFE, an attention
sub-block is employed to make the model adaptively focus on important
channels/regions. These two sub-blocks constitute our MSFAB. Then, we design a
cross non-local block (CNLB), which can capture long-range dependencies beyond
the query. Instead of the same input source of query branch, the key and value
branches are enhanced by fusing more preceding features. CNLB is
computation-friendly by leveraging a spatial pyramid down-sampling (SPDS)
strategy to reduce the computation and memory consumption without sacrificing
the performance. Last but not least, a novel detail-focused contrastive
regularization (DFCR) is presented by emphasizing the low-level details and
ignoring the high-level semantic information in a representation space
specially designed for dehazing. Comprehensive experimental results demonstrate
that the proposed MRFNLN model outperforms recent state-of-the-art dehazing
methods with less than 1.5 Million parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to the IEEE Journal for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SWiFT: Soft-Mask Weight Fine-tuning for Bias Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.18826v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.18826v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Yan, Feng Chen, Yuyang Xue, Yuning Du, Konstantinos Vilouras, Sotirios A. Tsaftaris, Steven McDonagh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have shown that Machine Learning (ML) models can exhibit bias
in real-world scenarios, posing significant challenges in ethically sensitive
domains such as healthcare. Such bias can negatively affect model fairness,
model generalization abilities and further risks amplifying social
discrimination. There is a need to remove biases from trained models. Existing
debiasing approaches often necessitate access to original training data and
need extensive model retraining; they also typically exhibit trade-offs between
model fairness and discriminative performance. To address these challenges, we
propose Soft-Mask Weight Fine-Tuning (SWiFT), a debiasing framework that
efficiently improves fairness while preserving discriminative performance with
much less debiasing costs. Notably, SWiFT requires only a small external
dataset and only a few epochs of model fine-tuning. The idea behind SWiFT is to
first find the relative, and yet distinct, contributions of model parameters to
both bias and predictive performance. Then, a two-step fine-tuning process
updates each parameter with different gradient flows defined by its
contribution. Extensive experiments with three bias sensitive attributes
(gender, skin tone, and age) across four dermatological and two chest X-ray
datasets demonstrate that SWiFT can consistently reduce model bias while
achieving competitive or even superior diagnostic accuracy under common
fairness and accuracy metrics, compared to the state-of-the-art. Specifically,
we demonstrate improved model generalization ability as evidenced by superior
performance on several out-of-distribution (OOD) datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the Journal of Machine Learning for
  Biomedical Imaging (MELBA) https://melba-journal.org/2025:015</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BuzzSet v1.0: A <span class="highlight-title">Dataset</span> for Pollinator Detection in Field Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.19762v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.19762v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Emam, Mohamed Elbassiouny, Julius Miller, Patrick Donworth, Sabine Seidel, Ribana Roscher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pollinator insects such as honeybees and bumblebees are vital to global food
production and ecosystem stability, yet their populations are declining due to
anthropogenic and environmental stressors. Scalable, automated monitoring in
agricultural environments remains an open challenge due to the difficulty of
detecting small, fast-moving, and often camouflaged insects. To address this,
we present BuzzSet v1.0, a large-scale dataset of high-resolution pollinator
images collected under real field conditions. BuzzSet contains 7,856 manually
verified images with more than 8,000 annotated instances across three classes:
honeybees, bumblebees, and unidentified insects. Initial annotations were
produced using a YOLOv12 model trained on external data and refined through
human verification with open-source tools. All images were preprocessed into
256 x 256 tiles to improve the detection of small insects. We provide baselines
using the RF-DETR transformer-based object detector. The model achieves strong
classification accuracy with F1 scores of 0.94 and 0.92 for honeybees and
bumblebees, with minimal confusion between these categories. The unidentified
class remains more difficult due to label ambiguity and fewer samples, yet
still contributes insights for robustness evaluation. Overall detection
performance (mAP at 0.50 of 0.559) illustrates the challenging nature of the
dataset and its potential to drive advances in small object detection under
realistic ecological conditions. Future work focuses on expanding the dataset
to version 2.0 with additional annotations and evaluating further detection
strategies. BuzzSet establishes a benchmark for ecological computer vision,
with the primary challenge being reliable detection of insects frequently
camouflaged within natural vegetation, highlighting an open problem for future
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Separate to Collaborate: Dual-Stream Diffusion Model for <span class="highlight-title">Coordinated</span>
  Piano Hand Motion Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.09885v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.09885v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Liu, Mingwen Ou, Zunnan Xu, Jiaqi Huang, Haonan Han, Ronghui Li, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating the synthesis of coordinated bimanual piano performances poses
significant challenges, particularly in capturing the intricate choreography
between the hands while preserving their distinct kinematic signatures. In this
paper, we propose a dual-stream neural framework designed to generate
synchronized hand gestures for piano playing from audio input, addressing the
critical challenge of modeling both hand independence and coordination. Our
framework introduces two key innovations: (i) a decoupled diffusion-based
generation framework that independently models each hand's motion via
dual-noise initialization, sampling distinct latent noise for each while
leveraging a shared positional condition, and (ii) a Hand-Coordinated
Asymmetric Attention (HCAA) mechanism suppresses symmetric (common-mode) noise
to highlight asymmetric hand-specific features, while adaptively enhancing
inter-hand coordination during denoising. Comprehensive evaluations demonstrate
that our framework outperforms existing state-of-the-art methods across
multiple metrics. Our project is available at
https://monkek123king.github.io/S2C_page/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, Accepted to ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Vision</span>-based <span class="highlight-title">Manipulation</span> from Single Human Video with Open-World Object
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20321v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20321v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifeng Zhu, Arisrei Lim, Peter Stone, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents an object-centric approach to learning vision-based
manipulation skills from human videos. We investigate the problem of robot
manipulation via imitation in the open-world setting, where a robot learns to
manipulate novel objects from a single video demonstration. We introduce ORION,
an algorithm that tackles the problem by extracting an object-centric
manipulation plan from a single RGB or RGB-D video and deriving a policy that
conditions on the extracted plan. Our method enables the robot to learn from
videos captured by daily mobile devices and to generalize the policies to
deployment environments with varying visual backgrounds, camera angles, spatial
layouts, and novel object instances. We systematically evaluate our method on
both short-horizon and long-horizon tasks, using RGB-D and RGB-only
demonstration videos. Across varied tasks and demonstration types (RGB-D /
RGB), we observe an average success rate of 74.4%, demonstrating the efficacy
of ORION in learning from a single human video in the open world. Additional
materials can be found on our project website:
https://ut-austin-rpl.github.io/ORION-release.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of paper adding results with RGB-only demonstration
  videos uploaded on 09/04/2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Encoder-Only Image Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Chen, Renjiu Hu, Jinwei Zhang, Yuxi Zhang, Xinyao Yue, Min Liu, Yaonan Wang, Hang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based techniques have significantly improved the accuracy and speed
of deformable image registration. However, challenges such as reducing
computational complexity and handling large deformations persist. To address
these challenges, we analyze how convolutional neural networks (ConvNets)
influence registration performance using the Horn-Schunck optical flow
equation. Supported by prior studies and our empirical experiments, we observe
that ConvNets play two key roles in registration: linearizing local intensities
and harmonizing global contrast variations. Based on these insights, we propose
the Encoder-Only Image Registration (EOIR) framework, designed to achieve a
better accuracy-efficiency trade-off. EOIR separates feature learning from flow
estimation, employing only a 3-layer ConvNet for feature extraction and a set
of 3-layer flow estimators to construct a Laplacian feature pyramid,
progressively composing diffeomorphic deformations under a large-deformation
model. Results on five datasets across different modalities and anatomical
regions demonstrate EOIR's effectiveness, achieving superior
accuracy-efficiency and accuracy-smoothness trade-offs. With comparable
accuracy, EOIR provides better efficiency and smoothness, and vice versa. The
source code of EOIR is publicly available on
https://github.com/XiangChen1994/EOIR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DianJin-OCR-R1: Enhancing OCR Capabilities via a Reasoning-and-Tool
  Interleaved <span class="highlight-title">Vision</span>-Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.13238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.13238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Chen, Xianyin Zhang, Lifan Guo, Feng Chen, Chi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large vision-language models (LVLMs) have enabled a new
paradigm of end-to-end document image parsing, excelling in Optical Character
Recognition (OCR) tasks such as text, table, and formula recognition. However,
generative LVLMs, similarly to large language models (LLMs), are prone to
hallucinations--generating words that do not exist in input images.
Furthermore, LVLMs are designed for general purposes and tend to be less
effective on OCR tasks compared to expert models that are trained on
domain-specific datasets. In this paper, we propose DianJin-OCR-R1, a
reasoning-enhanced framework designed to address these limitations through
training reasoning-and-tool interleaved VLMs. Given a recognition instruction,
our DianJin-OCR-R1 model first recognizes the content in the input image by its
own OCR capabilities, and then calls other tools (i.e., other expert models) to
obtain their results as references, finally "looks again" the image and
rethinks about the reasoning process to provide the final recognized content.
Since architectures of expert models are tailored for specific OCR tasks, which
makes them less prone to hallucinations, their results can help VLMs mitigate
hallucinations. We evaluate our model on ReST and OmniDocBench, and
experimental results show that our DianJin-OCR-R1 models consistently
outperform their non-reasoning counterparts and expert OCR models, which proves
the effectiveness of our method. Additionally, the results indicate that
enhancing expert models, which are typically small and easy to iterate, enable
performance improvements for VLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundations and Models in Modern Computer <span class="highlight-title">Vision</span>: Key Building Blocks in
  Landmark Architectures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.23357v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.23357v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Radu-Andrei Bourceanu, Neil De La Fuente, Jan Grimm, Andrei Jardan, Andriy Manucharyan, Cornelius Weiss, Daniel Cremers, Roman Pflugfelder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report analyzes the evolution of key design patterns in computer vision
by examining six influential papers. The analysis begins with foundational
architectures for image recognition. We review ResNet, which introduced
residual connections to overcome the vanishing gradient problem and enable
effective training of significantly deeper convolutional networks.
Subsequently, we examine the Vision Transformer (ViT), which established a new
paradigm by applying the Transformer architecture to sequences of image
patches, demonstrating the efficacy of attention-based models for large-scale
image recognition. Building on these visual representation backbones, we
investigate generative models. Generative Adversarial Networks (GANs) are
analyzed for their novel adversarial training process, which challenges a
generator against a discriminator to learn complex data distributions. Then,
Latent Diffusion Models (LDMs) are covered, which improve upon prior generative
methods by performing a sequential denoising process in a perceptually
compressed latent space. LDMs achieve high-fidelity synthesis with greater
computational efficiency, representing the current state-of-the-art for image
generation. Finally, we explore self-supervised learning techniques that reduce
dependency on labeled data. DINO is a self-distillation framework in which a
student network learns to match the output of a momentum-updated teacher,
yielding features with strong k-NN classification performance. We conclude with
Masked Autoencoders (MAE), which utilize an asymmetric encoder-decoder design
to reconstruct heavily masked inputs, providing a highly scalable and effective
method for pre-training large-scale vision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating Intermediate Layer <span class="highlight-title">Optimization</span> and Projected <span class="highlight-title">Gradient</span>
  Descent for Solving Inverse Problems with Diffusion Models <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20789v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20789v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zheng, Wen Li, Zhaoqiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inverse problems (IPs) involve reconstructing signals from noisy
observations. Recently, diffusion models (DMs) have emerged as a powerful
framework for solving IPs, achieving remarkable reconstruction performance.
However, existing DM-based methods frequently encounter issues such as heavy
computational demands and suboptimal convergence. In this work, building upon
the idea of the recent work DMPlug, we propose two novel methods, DMILO and
DMILO-PGD, to address these challenges. Our first method, DMILO, employs
intermediate layer optimization (ILO) to alleviate the memory burden inherent
in DMPlug. Additionally, by introducing sparse deviations, we expand the range
of DMs, enabling the exploration of underlying signals that may lie outside the
range of the diffusion model. We further propose DMILO-PGD, which integrates
ILO with projected gradient descent (PGD), thereby reducing the risk of
suboptimal convergence. We provide an intuitive theoretical analysis of our
approaches under appropriate conditions and validate their superiority through
extensive experiments on diverse image datasets, encompassing both linear and
nonlinear IPs. Our results demonstrate significant performance gains over
state-of-the-art methods, highlighting the effectiveness of DMILO and DMILO-PGD
in addressing common challenges in DM-based IP solvers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is an Ultra Large Natural Image-Based Foundation Model Superior to a
  Retina-Specific Model for Detecting Ocular and Systemic Diseases? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingshan Hou, Yukun Zhou, Jocelyn Hui Lin Goh, Ke Zou, Samantha Min Er Yew, Sahana Srinivasan, Meng Wang, Thaddaeus Lo, Xiaofeng Lei, Siegfried K. Wagner, Mark A. Chia, Dawei Yang, Hongyang Jiang, An Ran Ran, Rui Santos, Gabor Mark Somfai, Juan Helen Zhou, Haoyu Chen, Qingyu Chen, Carol Y. Cheung, Pearse A. Keane, Yih Chung Tham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of foundation models (FMs) is transforming medical domain. In
ophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4
million natural images and 1.6 million retinal images, has demonstrated high
adaptability across clinical applications. Conversely, DINOv2, a
general-purpose vision FM pre-trained on 142 million natural images, has shown
promise in non-medical domains. However, its applicability to clinical tasks
remains underexplored. To address this, we conducted head-to-head evaluations
by fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular
disease detection and systemic disease prediction tasks, across eight
standardized open-source ocular datasets, as well as the Moorfields AlzEye and
the UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting
diabetic retinopathy (AUROC=0.850-0.952 vs 0.823-0.944, across three datasets,
all P<=0.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In
glaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940,
P<0.001). Conversely, RETFound achieved superior performance over all DINOv2
models in predicting heart failure, myocardial infarction, and ischaemic stroke
(AUROC=0.732-0.796 vs 0.663-0.771, all P<0.001). These trends persisted even
with 10% of the fine-tuning data. These findings showcase the distinct
scenarios where general-purpose and domain-specific FMs excel, highlighting the
importance of aligning FM selection with task-specific requirements to optimise
clinical performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Ophthalmology Science and is currently in press</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hallo4: High-Fidelity <span class="highlight-title">Dynamic</span> Portrait Animation via Direct Preference
  <span class="highlight-title">Optimization</span> and Temporal Motion Modulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Cui, Yan Chen, Mingwang Xu, Hanlin Shang, Yuxuan Chen, Yun Zhan, Zilong Dong, Yao Yao, Jingdong Wang, Siyu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating highly dynamic and photorealistic portrait animations driven by
audio and skeletal motion remains challenging due to the need for precise lip
synchronization, natural facial expressions, and high-fidelity body motion
dynamics. We propose a human-preference-aligned diffusion framework that
addresses these challenges through two key innovations. First, we introduce
direct preference optimization tailored for human-centric animation, leveraging
a curated dataset of human preferences to align generated outputs with
perceptual metrics for portrait motion-video alignment and naturalness of
expression. Second, the proposed temporal motion modulation resolves
spatiotemporal resolution mismatches by reshaping motion conditions into
dimensionally aligned latent features through temporal channel redistribution
and proportional feature expansion, preserving the fidelity of high-frequency
motion details in diffusion-based synthesis. The proposed mechanism is
complementary to existing UNet and DiT-based portrait diffusion approaches, and
experiments demonstrate obvious improvements in lip-audio synchronization,
expression vividness, body motion coherence over baseline methods, alongside
notable gains in human preference metrics. Our model and source code can be
found at: https://github.com/xyz123xyz456/hallo4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hardware-Friendly Diffusion Models with Fixed-Size Reusable Structures
  for On-Device Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanchar Palit, Sathya Veera Reddy Dendi, Mallikarjuna Talluri, Raj Narayana Gadde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers and U-Net architectures have been widely adopted in the
implementation of Diffusion Models. However, each architecture presents
specific challenges while realizing them on-device. Vision Transformers require
positional embedding to maintain correspondence between the tokens processed by
the transformer, although they offer the advantage of using fixed-size,
reusable repetitive blocks following tokenization. The U-Net architecture lacks
these attributes, as it utilizes variable-sized intermediate blocks for
down-convolution and up-convolution in the noise estimation backbone for the
diffusion process. To address these issues, we propose an architecture that
utilizes a fixed-size, reusable transformer block as a core structure, making
it more suitable for hardware implementation. Our architecture is characterized
by low complexity, token-free design, absence of positional embeddings,
uniformity, and scalability, making it highly suitable for deployment on mobile
and resource-constrained devices. The proposed model exhibit competitive and
consistent performance across both unconditional and conditional image
generation tasks. The model achieved a state-of-the-art FID score of 1.6 on
unconditional image generation with the CelebA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>presented at IJCNN 2025 poster track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defending LVLMs Against <span class="highlight-title">Vision</span> Attacks through Partial-Perception
  Super<span class="highlight-title">vision</span> <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Zhou, Tianlin Li, Qing Guo, Dongxia Wang, Yun Lin, Yang Liu, Jin Song Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have raised significant concerns regarding the vulnerability
of Large Vision Language Models (LVLMs) to maliciously injected or perturbed
input images, which can mislead their responses. Existing defense methods show
that such vision attacks are sensitive to image modifications especially
cropping, using majority voting across responses of modified images as
corrected responses. However, these modifications often result in partial
images and distort the semantics, which reduces response quality on clean
images after voting. Instead of directly using responses from partial images
for voting, we investigate using them to supervise the LVLM's responses to the
original images. We propose a black-box, training-free method called DPS
(Defense through Partial-Perception Supervision). In this approach, the model
is prompted using the responses generated by a model that perceives only a
partial image. With DPS, the model can adjust its response based on partial
image understanding when under attack, while confidently maintaining its
original response for clean input. Our findings show that the weak model can
supervise the strong model: when faced with an attacked input, the strong model
becomes less confident and adjusts its response based on the weak model's
partial understanding, effectively defending against the attack. With clean
input, it confidently maintains its original response. Empirical experiments
show our method outperforms the baseline, cutting the average attack success
rate by 76.3% across six datasets on three popular models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Framework for Supervised and Unsupervised Segmentation and
  Classification of Materials Microstructure Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kungang Zhang, Wei Chen, Wing K. Liu, L. Catherine Brinson, Daniel W. Apley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Microstructure of materials is often characterized through image analysis to
understand processing-structure-properties linkages. We propose a largely
automated framework that integrates unsupervised and supervised learning
methods to classify micrographs according to microstructure phase/class and,
for multiphase microstructures, segments them into different homogeneous
regions. With the advance of manufacturing and imaging techniques, the
ultra-high resolution of imaging that reveals the complexity of microstructures
and the rapidly increasing quantity of images (i.e., micrographs) enables and
necessitates a more powerful and automated framework to extract materials
characteristics and knowledge. The framework we propose can be used to
gradually build a database of microstructure classes relevant to a particular
process or group of materials, which can help in analyzing and
discovering/identifying new materials. The framework has three steps: (1)
segmentation of multiphase micrographs through a recently developed score-based
method so that different microstructure homogeneous regions can be identified
in an unsupervised manner; (2) {identification and classification of}
homogeneous regions of micrographs through an uncertainty-aware supervised
classification network trained using the segmented micrographs from Step $1$
with their identified labels verified via the built-in uncertainty
quantification and minimal human inspection; (3) supervised segmentation (more
powerful than the segmentation in Step $1$) of multiphase microstructures
through a segmentation network trained with micrographs and the results from
Steps $1$-$2$ using a form of data augmentation. This framework can iteratively
characterize/segment new homogeneous or multiphase materials while expanding
the database to enhance performance. The framework is demonstrated on various
sets of materials and texture images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARTalk: Speech-Driven 3D Head Animation via Autoregressive Model <span class="chip">SIGGRAPH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20323v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20323v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuangeng Chu, Nabarun Goswami, Ziteng Cui, Hanqin Wang, Tatsuya Harada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-driven 3D facial animation aims to generate realistic lip movements
and facial expressions for 3D head models from arbitrary audio clips. Although
existing diffusion-based methods are capable of producing natural motions,
their slow generation speed limits their application potential. In this paper,
we introduce a novel autoregressive model that achieves real-time generation of
highly synchronized lip movements and realistic head poses and eye blinks by
learning a mapping from speech to a multi-scale motion codebook. Furthermore,
our model can adapt to unseen speaking styles, enabling the creation of 3D
talking avatars with unique personal styles beyond the identities seen during
training. Extensive evaluations and user studies demonstrate that our method
outperforms existing approaches in lip synchronization accuracy and perceived
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGGRAPH Asia 2025, More video demonstrations, code, models and data
  can be found on our project website: http://xg-chu.site/project_artalk/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Short-video Propagation Influence Rating: A New Real-world <span class="highlight-title">Dataset</span> and A
  New Large Graph Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dizhan Xue, Shengsheng Qian, Chuanrui Hu, Changsheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Short-video platforms have gained immense popularity, captivating the
interest of millions, if not billions, of users globally. Recently, researchers
have highlighted the significance of analyzing the propagation of short-videos,
which typically involves discovering commercial values, public opinions, user
behaviors, etc. This paper proposes a new Short-video Propagation Influence
Rating (SPIR) task and aims to promote SPIR from both the dataset and method
perspectives. First, we propose a new Cross-platform Short-Video (XS-Video)
dataset, which aims to provide a large-scale and real-world short-video
propagation network across various platforms to facilitate the research on
short-video propagation. Our XS-Video dataset includes 117,720 videos, 381,926
samples, and 535 topics across 5 biggest Chinese platforms, annotated with the
propagation influence from level 0 to 9. To the best of our knowledge, this is
the first large-scale short-video dataset that contains cross-platform data or
provides all of the views, likes, shares, collects, fans, comments, and comment
content. Second, we propose a Large Graph Model (LGM) named NetGPT, based on a
novel three-stage training mechanism, to bridge heterogeneous graph-structured
data with the powerful reasoning ability and knowledge of Large Language Models
(LLMs). Our NetGPT can comprehend and analyze the short-video propagation
graph, enabling it to predict the long-term propagation influence of
short-videos. Comprehensive experimental results evaluated by both
classification and regression metrics on our XS-Video dataset indicate the
superiority of our method for SPIR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OC4-ReID: Occluded Cloth-Changing Person Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08557v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08557v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Chen, Yiyuan Ge, Yanyan Lv, Ziyang Wang, Mingya Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of Cloth-Changing Person Re-identification (CC-ReID) focuses on
retrieving specific pedestrians when their clothing has changed, typically
under the assumption that the entire pedestrian images are visible. Pedestrian
images in real-world scenarios, however, are often partially obscured by
obstacles, presenting a significant challenge to existing CC-ReID systems. In
this paper, we introduce a more challenging task termed Occluded Cloth-Changing
Person Re-Identification (OC4-ReID), which simultaneously addresses two
challenges of clothing changes and occlusion. Concretely, we construct two new
datasets, Occ-LTCC and Occ-PRCC, based on original CC-ReID datasets to include
random occlusions of key pedestrians components (e.g., head, torso). Moreover,
a novel benchmark is proposed for OC4-ReID incorporating a Train-Test Micro
Granularity Screening (T2MGS) module to mitigate the influence of occlusion and
proposing a Part-Robust Triplet (PRT) loss for partial features learning.
Comprehensive experiments on the proposed datasets, as well as on two CC-ReID
benchmark datasets demonstrate the superior performance of proposed method
against other state-of-the-art methods. The codes and datasets are available
at: https://github.com/1024AILab/OC4-ReID.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BOSC: A Backdoor-based Framework for Open Set Synthetic Image
  Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11491v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11491v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Wang, Benedetta Tondi, Mauro Barni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic image attribution addresses the problem of tracing back the origin
of images produced by generative models. Extensive efforts have been made to
explore unique representations of generative models and use them to attribute a
synthetic image to the model that produced it. Most of the methods classify the
models or the architectures among those in a closed set without considering the
possibility that the system is fed with samples produced by unknown
architectures. With the continuous progress of AI technology, new generative
architectures continuously appear, thus driving the attention of researchers
towards the development of tools capable of working in open-set scenarios. In
this paper, we propose a framework for open set attribution of synthetic
images, named BOSC (Backdoor-based Open Set Classification), that relies on the
concept of backdoor attacks to design a classifier with rejection option. BOSC
works by purposely injecting class-specific triggers inside a portion of the
images in the training set to induce the network to establish a matching
between class features and trigger features. The behavior of the trained model
with respect to triggered samples is then exploited at test time to perform
sample rejection using an ad-hoc score. Experiments show that the proposed
method has good performance, always surpassing the state-of-the-art. Robustness
against image processing is also very good. Although we designed our method for
the task of synthetic image attribution, the proposed framework is a general
one and can be used for other image forensic applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Optimization</span> of Module Transferability in Single Image Super-Resolution:
  Universality Assessment and Cycle Residual Blocks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotong Cheng, Zhiqi Zhang, Hao Li, Xinshang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has substantially advanced the field of Single Image
Super-Resolution (SISR). However, existing research has predominantly focused
on raw performance gains, with little attention paid to quantifying the
transferability of architectural components. In this paper, we introduce the
concept of "Universality" and its associated definitions, which extend the
traditional notion of "Generalization" to encompass the ease of transferability
of modules. We then propose the Universality Assessment Equation (UAE), a
metric that quantifies how readily a given module can be transplanted across
models and reveals the combined influence of multiple existing metrics on
transferability. Guided by the UAE results of standard residual blocks and
other plug-and-play modules, we further design two optimized modules: the Cycle
Residual Block (CRB) and the Depth-Wise Cycle Residual Block (DCRB). Through
comprehensive experiments on natural-scene benchmarks, remote-sensing datasets,
and other low-level tasks, we demonstrate that networks embedded with the
proposed plug-and-play modules outperform several state-of-the-art methods,
achieving a PSNR improvement of up to 0.83 dB or enabling a 71.3% reduction in
parameters with negligible loss in reconstruction fidelity. Similar
optimization approaches could be applied to a broader range of basic modules,
offering a new paradigm for the design of plug-and-play modules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted to IET Image Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POET: Supporting Prompting Creativity and Personalization with Automated
  Expansion of Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.13392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.13392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evans Xu Han, Alice Qian Zhang, Haiyi Zhu, Hong Shen, Paul Pu Liang, Jane Hsieh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art visual generative AI tools hold immense potential to assist
users in the early ideation stages of creative tasks -- offering the ability to
generate (rather than search for) novel and unprecedented (instead of existing)
images of considerable quality that also adhere to boundless combinations of
user specifications. However, many large-scale text-to-image systems are
designed for broad applicability, yielding conventional output that may limit
creative exploration. They also employ interaction methods that may be
difficult for beginners. Given that creative end users often operate in
diverse, context-specific ways that are often unpredictable, more variation and
personalization are necessary. We introduce POET, a real-time interactive tool
that (1) automatically discovers dimensions of homogeneity in text-to-image
generative models, (2) expands these dimensions to diversify the output space
of generated images, and (3) learns from user feedback to personalize
expansions. An evaluation with 28 users spanning four creative task domains
demonstrated POET's ability to generate results with higher perceived diversity
and help users reach satisfaction in fewer prompts during creative tasks,
thereby prompting them to deliberate and reflect more on a wider range of
possible produced results during the co-creative process. Focusing on visual
creativity, POET offers a first glimpse of how interaction techniques of future
text-to-image generation tools may support and align with more pluralistic
values and the needs of end users during the ideation stages of their work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kwai Keye-VL 1.5 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Guowang Zhang, Han Shen, Hao Peng, Haojie Ding, Hao Wang, Haonan Fang, Hengrui Ju, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Muhao Wei, Qiang Wang, Ruitao Wang, Sen Na, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zeyi Lu, Zhenhua Wu, Zhixin Ling, Zhuoran Yang, Ziming Li, Di Xu, Haixuan Gao, Hang Li, Jing Wang, Lejian Ren, Qigen Hu, Qianqian Wang, Shiyao Wang, Xinchen Luo, Yan Li, Yuhang Hu, Zixing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the development of Large Language Models (LLMs) has
significantly advanced, extending their capabilities to multimodal tasks
through Multimodal Large Language Models (MLLMs). However, video understanding
remains a challenging area due to the dynamic and information-dense nature of
videos. Existing models struggle with the trade-off between spatial resolution
and temporal coverage when processing video content. We present Keye-VL-1.5,
which addresses fundamental challenges in video comprehension through three key
innovations. First, we introduce a novel Slow-Fast video encoding strategy that
dynamically allocates computational resources based on inter-frame similarity,
processing key frames with significant visual changes at higher resolution
(Slow pathway) while handling relatively static frames with increased temporal
coverage at lower resolution (Fast pathway). Second, we implement a progressive
four-stage pre-training methodology that systematically extends the model's
context length from 8K to 128K tokens, enabling processing of longer videos and
more complex visual content. Third, we develop a comprehensive post-training
pipeline focusing on reasoning enhancement and human preference alignment,
incorporating a 5-step chain-of-thought data construction process, iterative
GSPO-based reinforcement learning with progressive prompt hinting for difficult
cases, and alignment training. Through extensive evaluation on public
benchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstrates
significant improvements over existing models, particularly excelling in video
understanding tasks while maintaining competitive performance on general
multimodal benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github page: https://github.com/Kwai-Keye/Keye</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoDiff: Conditional Diffusion Model for Collaborative 3D Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14891v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14891v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Huang, Shuo Wang, Yongcai Wang, Lei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative 3D object detection holds significant importance in the field
of autonomous driving, as it greatly enhances the perception capabilities of
each individual agent by facilitating information exchange among multiple
agents. However, in practice, due to pose estimation errors and time delays,
the fusion of information across agents often results in feature
representations with spatial and temporal noise, leading to detection errors.
Diffusion models naturally have the ability to denoise noisy samples to the
ideal data, which motivates us to explore the use of diffusion models to
address the noise problem between multi-agent systems. In this work, we propose
CoDiff, a novel robust collaborative perception framework that leverages the
potential of diffusion models to generate more comprehensive and clearer
feature representations. To the best of our knowledge, this is the first work
to apply diffusion models to multi-agent collaborative perception.
Specifically, we project high-dimensional feature map into the latent space of
a powerful pre-trained autoencoder. Within this space, individual agent
information serves as a condition to guide the diffusion model's sampling. This
process denoises coarse feature maps and progressively refines the fused
features. Experimental study on both simulated and real-world datasets
demonstrates that the proposed framework CoDiff consistently outperforms
existing relevant methods in terms of the collaborative object detection
performance, and exhibits highly desired robustness when the pose and delay
information of agents is with high-level noise. The code is released at
https://github.com/HuangZhe885/CoDiff
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Per-Garment Virtual Try-On with Temporal Consistency for
  Loose-Fitting Garments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12348v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12348v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaiqiang Wu, I-Chao Shen, Takeo Igarashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Per-garment virtual try-on methods collect garment-specific datasets and
train networks tailored to each garment to achieve superior results. However,
these approaches often struggle with loose-fitting garments due to two key
limitations: (1) They rely on human body semantic maps to align garments with
the body, but these maps become unreliable when body contours are obscured by
loose-fitting garments, resulting in degraded outcomes; (2) They train garment
synthesis networks on a per-frame basis without utilizing temporal information,
leading to noticeable jittering artifacts. To address the first limitation, we
propose a two-stage approach for robust semantic map estimation. First, we
extract a garment-invariant representation from the raw input image. This
representation is then passed through an auxiliary network to estimate the
semantic map. This enhances the robustness of semantic map estimation under
loose-fitting garments during garment-specific dataset generation. To address
the second limitation, we introduce a recurrent garment synthesis framework
that incorporates temporal dependencies to improve frame-to-frame coherence
while maintaining real-time performance. We conducted qualitative and
quantitative evaluations to demonstrate that our method outperforms existing
approaches in both image quality and temporal coherence. Ablation studies
further validate the effectiveness of the garment-invariant representation and
the recurrent synthesis framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble of Pathology Foundation Models for MIDOG 2025 Track 2: Atypical
  Mitosis Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02591v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02591v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mieko Ochi, Bae Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitotic figures are classified into typical and atypical variants, with
atypical counts correlating strongly with tumor aggressiveness. Accurate
differentiation is therefore essential for patient prognostication and resource
allocation, yet remains challenging even for expert pathologists. Here, we
leveraged Pathology Foundation Models (PFMs) pre-trained on large
histopathology datasets and applied parameter-efficient fine-tuning via
low-rank adaptation. In addition, we incorporated ConvNeXt V2, a
state-of-the-art convolutional neural network architecture, to complement PFMs.
During training, we employed a fisheye transform to emphasize mitoses and
Fourier Domain Adaptation using ImageNet target images. Finally, we ensembled
multiple PFMs to integrate complementary morphological insights, achieving
competitive balanced accuracy on the Preliminary Evaluation Phase dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Vision</span>-Based Autonomous MM-Wave Reflector Using ArUco-Driven
  Angle-of-Arrival <span class="highlight-title">Estimation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.05195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.05195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josue Marroquin, Nan Inzali, Miles Dillon Lantz, Campbell Freeman, Amod Ashtekar, \\Ajinkya Umesh Mulik, Mohammed E Eltayeb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable millimeter-wave (mmWave) communication in non-line-of-sight (NLoS)
conditions remains a major challenge for both military and civilian operations,
especially in urban or infrastructure-limited environments. This paper presents
a vision-aided autonomous reflector system designed to enhance mmWave link
performance by dynamically steering signal reflections using a motorized
metallic plate. The proposed system leverages a monocular camera to detect
ArUco markers on allied transmitter and receiver nodes, estimate their angles
of arrival, and align the reflector in real time for optimal signal
redirection. This approach enables selective beam coverage by serving only
authenticated targets with visible markers and reduces the risk of unintended
signal exposure. The designed prototype, built on a Raspberry Pi 4 and
low-power hardware, operates autonomously without reliance on external
infrastructure or GPS. Experimental results at 60\,GHz demonstrate a 23\,dB
average gain in received signal strength and an 0.89 probability of maintaining
signal reception above a target threshold of -65 dB in an indoor environment,
far exceeding the static and no-reflector baselines. These results demonstrate
the system's potential for resilient and adaptive mmWave connectivity in
complex and dynamic environments.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChronoGraph: A Real-World Graph-Based Multivariate Time Series <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrian Catalin Lutu, Ioana Pintilie, Elena Burceanu, Andrei Manolache
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present ChronoGraph, a graph-structured multivariate time series
forecasting dataset built from real-world production microservices. Each node
is a service that emits a multivariate stream of system-level performance
metrics, capturing CPU, memory, and network usage patterns, while directed
edges encode dependencies between services. The primary task is forecasting
future values of these signals at the service level. In addition, ChronoGraph
provides expert-annotated incident windows as anomaly labels, enabling
evaluation of anomaly detection methods and assessment of forecast robustness
during operational disruptions. Compared to existing benchmarks from industrial
control systems or traffic and air-quality domains, ChronoGraph uniquely
combines (i) multivariate time series, (ii) an explicit, machine-readable
dependency graph, and (iii) anomaly labels aligned with real incidents. We
report baseline results spanning forecasting models, pretrained time-series
foundation models, and standard anomaly detectors. ChronoGraph offers a
realistic benchmark for studying structure-aware forecasting and incident-aware
evaluation in microservice systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Delta Activations: A Representation for Finetuned Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04442v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04442v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiu Xu, Amish Sethi, Mayur Naik, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of powerful open source Large Language Models (LLMs) has enabled
the community to create a vast collection of post-trained models adapted to
specific tasks and domains. However, navigating and understanding these models
remains challenging due to inconsistent metadata and unstructured repositories.
We introduce Delta Activations, a method to represent finetuned models as
vector embeddings by measuring shifts in their internal activations relative to
a base model. This representation allows for effective clustering by domain and
task, revealing structure in the model landscape. Delta Activations also
demonstrate desirable properties: it is robust across finetuning settings and
exhibits an additive property when finetuning datasets are mixed. In addition,
we show that Delta Activations can embed tasks via few-shot finetuning, and
further explore its use for model selection and merging. We hope Delta
Activations can facilitate the practice of reusing publicly available models.
Code is available at https://github.com/OscarXZQ/delta_activations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DEXOP: A Device for <span class="highlight-title">Robot</span>ic Transfer of Dexterous Human <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Shu Fang, Branden Romero, Yichen Xie, Arthur Hu, Bo-Ruei Huang, Juan Alvarez, Matthew Kim, Gabriel Margolis, Kavya Anbarasu, Masayoshi Tomizuka, Edward Adelson, Pulkit Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce perioperation, a paradigm for robotic data collection that
sensorizes and records human manipulation while maximizing the transferability
of the data to real robots. We implement this paradigm in DEXOP, a passive hand
exoskeleton designed to maximize human ability to collect rich sensory (vision
+ tactile) data for diverse dexterous manipulation tasks in natural
environments. DEXOP mechanically connects human fingers to robot fingers,
providing users with direct contact feedback (via proprioception) and mirrors
the human hand pose to the passive robot hand to maximize the transfer of
demonstrated skills to the robot. The force feedback and pose mirroring make
task demonstrations more natural for humans compared to teleoperation,
increasing both speed and accuracy. We evaluate DEXOP across a range of
dexterous, contact-rich tasks, demonstrating its ability to collect
high-quality demonstration data at scale. Policies learned with DEXOP data
significantly improve task performance per unit time of data collection
compared to teleoperation, making DEXOP a powerful tool for advancing robot
dexterity. Our project page is at https://dex-op.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://dex-op.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArcMemo: Abstract Reasoning Composition with Lifelong LLM Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Ho, Chen Si, Zhaoxiang Feng, Fangxu Yu, Zhijian Liu, Zhiting Hu, Lianhui Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While inference-time scaling enables LLMs to carry out increasingly long and
capable reasoning traces, the patterns and insights uncovered during these
traces are immediately discarded once the context window is reset for a new
query. External memory is a natural way to persist these discoveries, and
recent work has shown clear benefits for reasoning-intensive tasks. We see an
opportunity to make such memories more broadly reusable and scalable by moving
beyond instance-based memory entries (e.g. exact query/response pairs, or
summaries tightly coupled with the original problem context) toward
concept-level memory: reusable, modular abstractions distilled from solution
traces and stored in natural language. For future queries, relevant concepts
are selectively retrieved and integrated into the prompt, enabling test-time
continual learning without weight updates. Our design introduces new strategies
for abstracting takeaways from rollouts and retrieving entries for new queries,
promoting reuse and allowing memory to expand with additional experiences. On
the challenging ARC-AGI benchmark, our method yields a 7.5% relative gain over
a strong no-memory baseline with performance continuing to scale with inference
compute. We find abstract concepts to be the most consistent memory design,
outscoring the baseline at all tested inference compute scales. Moreover, we
confirm that dynamically updating memory during test-time outperforms an
otherwise identical fixed memory setting with additional attempts, supporting
the hypothesis that solving more problems and abstracting more patterns to
memory enables further solutions in a form of self-improvement. Code available
at https://github.com/matt-seb-ho/arc_memo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Unified View of Large Language Model Post-Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingtai Lv, Yuxin Zuo, Youbang Sun, Hongyi Liu, Yuntian Wei, Zhekai Chen, Lixuan He, Xuekai Zhu, Kaiyan Zhang, Bingning Wang, Ning Ding, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two major sources of training data exist for post-training modern language
models: online (model-generated rollouts) data, and offline (human or
other-model demonstrations) data. These two types of data are typically used by
approaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT),
respectively. In this paper, we show that these approaches are not in
contradiction, but are instances of a single optimization process. We derive a
Unified Policy Gradient Estimator, and present the calculations of a wide
spectrum of post-training approaches as the gradient of a common objective
under different data distribution assumptions and various bias-variance
tradeoffs. The gradient estimator is constructed with four interchangeable
parts: stabilization mask, reference policy denominator, advantage estimate,
and likelihood gradient. Motivated by our theoretical findings, we propose
Hybrid Post-Training (HPT), an algorithm that dynamically selects different
training signals. HPT is designed to yield both effective exploitation of
demonstration and stable exploration without sacrificing learned reasoning
patterns. We provide extensive experiments and ablation studies to verify the
effectiveness of our unified theoretical framework and HPT. Across six
mathematical reasoning benchmarks and two out-of-distribution suites, HPT
consistently surpasses strong baselines across models of varying scales and
families.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Thoughts Just AI: Biased LLM Recommendations Limit Human Agency in
  Resume Screening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04404v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04404v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyra Wilson, Mattea Sim, Anna-Maria Gueorguieva, Aylin Caliskan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we conduct a resume-screening experiment (N=528) where people
collaborate with simulated AI models exhibiting race-based preferences (bias)
to evaluate candidates for 16 high and low status occupations. Simulated AI
bias approximates factual and counterfactual estimates of racial bias in
real-world AI systems. We investigate people's preferences for White, Black,
Hispanic, and Asian candidates (represented through names and affinity groups
on quality-controlled resumes) across 1,526 scenarios and measure their
unconscious associations between race and status using implicit association
tests (IATs), which predict discriminatory hiring decisions but have not been
investigated in human-AI collaboration. When making decisions without AI or
with AI that exhibits no race-based preferences, people select all candidates
at equal rates. However, when interacting with AI favoring a particular group,
people also favor those candidates up to 90% of the time, indicating a
significant behavioral shift. The likelihood of selecting candidates whose
identities do not align with common race-status stereotypes can increase by 13%
if people complete an IAT before conducting resume screening. Finally, even if
people think AI recommendations are low quality or not important, their
decisions are still vulnerable to AI bias under certain circumstances. This
work has implications for people's autonomy in AI-HITL scenarios, AI and work,
design and evaluation of AI hiring systems, and strategies for mitigating bias
in collaborative decision-making tasks. In particular, organizational and
regulatory policy should acknowledge the complex nature of AI-HITL decision
making when implementing these systems, educating people who use them, and
determining which are subject to oversight.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Proceedings of the 2025 AAAI/ACM Conference on AI,
  Ethics, and Society; code available at
  https://github.com/kyrawilson/No-Thoughts-Just-AI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IPA: An Information-Preserving Input Projection Framework for Efficient
  Foundation Model Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Yin, Shashanka Venkataramanan, Tuan-Hung Vu, Andrei Bursuc, Matthieu Cord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduce
adaptation cost by injecting low-rank updates into pretrained weights. However,
LoRA's down-projection is randomly initialized and data-agnostic, discarding
potentially useful information. Prior analyses show that this projection
changes little during training, while the up-projection carries most of the
adaptation, making the random input compression a performance bottleneck. We
propose IPA, a feature-aware projection framework that explicitly preserves
information in the reduced hidden space. In the linear case, we instantiate IPA
with algorithms approximating top principal components, enabling efficient
projector pretraining with negligible inference overhead. Across language and
vision benchmarks, IPA consistently improves over LoRA and DoRA, achieving on
average 1.5 points higher accuracy on commonsense reasoning and 2.3 points on
VTAB-1k, while matching full LoRA performance with roughly half the trainable
parameters when the projection is frozen.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SSGaussian: <span class="highlight-title">Semantic</span>-Aware and Structure-Preserving 3D Style Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimin Xu, Bosheng Qin, Tao Jin, Zhou Zhao, Zhenhui Ye, Jun Yu, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in neural representations, such as Neural Radiance Fields
and 3D Gaussian Splatting, have increased interest in applying style transfer
to 3D scenes. While existing methods can transfer style patterns onto
3D-consistent neural representations, they struggle to effectively extract and
transfer high-level style semantics from the reference style image.
Additionally, the stylized results often lack structural clarity and
separation, making it difficult to distinguish between different instances or
objects within the 3D scene. To address these limitations, we propose a novel
3D style transfer pipeline that effectively integrates prior knowledge from
pretrained 2D diffusion models. Our pipeline consists of two key stages: First,
we leverage diffusion priors to generate stylized renderings of key viewpoints.
Then, we transfer the stylized key views onto the 3D representation. This
process incorporates two innovative designs. The first is cross-view style
alignment, which inserts cross-view attention into the last upsampling block of
the UNet, allowing feature interactions across multiple key views. This ensures
that the diffusion model generates stylized key views that maintain both style
fidelity and instance-level consistency. The second is instance-level style
transfer, which effectively leverages instance-level consistency across
stylized key views and transfers it onto the 3D representation. This results in
a more structured, visually coherent, and artistically enriched stylization.
Extensive qualitative and quantitative experiments demonstrate that our 3D
style transfer pipeline significantly outperforms state-of-the-art methods
across a wide range of scenes, from forward-facing to challenging 360-degree
environments. Visit our project page https://jm-xu.github.io/SSGaussian for
immersive visualization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parking Availability Prediction via Fusing Multi-Source Data with A
  Self-Supervised <span class="highlight-title">Learning</span> Enhanced Spatio-Temporal Inverted Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yin Huang, Yongqi Dong, Youhua Tang, Li Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of private car ownership has worsened the urban parking
predicament, underscoring the need for accurate and effective parking
availability prediction to support urban planning and management. To address
key limitations in modeling spatio-temporal dependencies and exploiting
multi-source data for parking availability prediction, this study proposes a
novel approach with SST-iTransformer. The methodology leverages K-means
clustering to establish parking cluster zones (PCZs), extracting and
integrating traffic demand characteristics from various transportation modes
(i.e., metro, bus, online ride-hailing, and taxi) associated with the targeted
parking lots. Upgraded on vanilla iTransformer, SST-iTransformer integrates
masking-reconstruction-based pretext tasks for self-supervised spatio-temporal
representation learning, and features an innovative dual-branch attention
mechanism: Series Attention captures long-term temporal dependencies via
patching operations, while Channel Attention models cross-variate interactions
through inverted dimensions. Extensive experiments using real-world data from
Chengdu, China, demonstrate that SST-iTransformer outperforms baseline deep
learning models (including Informer, Autoformer, Crossformer, and
iTransformer), achieving state-of-the-art performance with the lowest mean
squared error (MSE) and competitive mean absolute error (MAE). Comprehensive
ablation studies quantitatively reveal the relative importance of different
data sources: incorporating ride-hailing data provides the largest performance
gains, followed by taxi, whereas fixed-route transit features (bus/metro)
contribute marginally. Spatial correlation analysis further confirms that
excluding historical data from correlated parking lots within PCZs leads to
substantial performance degradation, underscoring the importance of modeling
spatial dependencies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 5 figures, under review for journal publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PARCO: Phoneme-Augmented <span class="highlight-title">Robust</span> Contextual ASR via Contrastive Entity
  Disambiguation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun He, Naoki Sawada, Koichi Miyazaki, Tomoki Toda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic speech recognition (ASR) systems struggle with domain-specific
named entities, especially homophones. Contextual ASR improves recognition but
often fails to capture fine-grained phoneme variations due to limited entity
diversity. Moreover, prior methods treat entities as independent tokens,
leading to incomplete multi-token biasing. To address these issues, we propose
Phoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation
(PARCO), which integrates phoneme-aware encoding, contrastive entity
disambiguation, entity-level supervision, and hierarchical entity filtering.
These components enhance phonetic discrimination, ensure complete entity
retrieval, and reduce false positives under uncertainty. Experiments show that
PARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English
DATA2 under 1,000 distractors, significantly outperforming baselines. PARCO
also demonstrates robust gains on out-of-domain datasets like THCHS-30 and
LibriSpeech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ASRU 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AUDETER: A Large-scale <span class="highlight-title">Dataset</span> for Deepfake Audio Detection in Open
  Worlds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qizhou Wang, Hanxun Huang, Guansong Pang, Sarah Erfani, Christopher Leckie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech generation systems can produce remarkably realistic vocalisations that
are often indistinguishable from human speech, posing significant authenticity
challenges. Although numerous deepfake detection methods have been developed,
their effectiveness in real-world environments remains unrealiable due to the
domain shift between training and test samples arising from diverse human
speech and fast evolving speech synthesis systems. This is not adequately
addressed by current datasets, which lack real-world application challenges
with diverse and up-to-date audios in both real and deep-fake categories. To
fill this gap, we introduce AUDETER (AUdio DEepfake TEst Range), a large-scale,
highly diverse deepfake audio dataset for comprehensive evaluation and robust
development of generalised models for deepfake audio detection. It consists of
over 4,500 hours of synthetic audio generated by 11 recent TTS models and 10
vocoders with a broad range of TTS/vocoder patterns, totalling 3 million audio
clips, making it the largest deepfake audio dataset by scale. Through extensive
experiments with AUDETER, we reveal that i) state-of-the-art (SOTA) methods
trained on existing datasets struggle to generalise to novel deepfake audio
samples and suffer from high false positive rates on unseen human voice,
underscoring the need for a comprehensive dataset; and ii) these methods
trained on AUDETER achieve highly generalised detection performance and
significantly reduce detection error rate by 44.1% to 51.6%, achieving an error
rate of only 4.17% on diverse cross-domain samples in the popular In-the-Wild
dataset, paving the way for training generalist deepfake audio detectors.
AUDETER is available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Psychologically Enhanced AI Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Besta, Shriram Chandran, Robert Gerstenberger, Mathis Lindner, Marcin Chrapek, Sebastian Hermann Martschat, Taraneh Ghandi, Patrick Iff, Hubert Niewiadomski, Piotr Nyczyk, Jürgen Müller, Torsten Hoefler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce MBTI-in-Thoughts, a framework for enhancing the effectiveness of
Large Language Model (LLM) agents through psychologically grounded personality
conditioning. Drawing on the Myers-Briggs Type Indicator (MBTI), our method
primes agents with distinct personality archetypes via prompt engineering,
enabling control over behavior along two foundational axes of human psychology,
cognition and affect. We show that such personality priming yields consistent,
interpretable behavioral biases across diverse tasks: emotionally expressive
agents excel in narrative generation, while analytically primed agents adopt
more stable strategies in game-theoretic settings. Our framework supports
experimenting with structured multi-agent communication protocols and reveals
that self-reflection prior to interaction improves cooperation and reasoning
quality. To ensure trait persistence, we integrate the official 16Personalities
test for automated verification. While our focus is on MBTI, we show that our
approach generalizes seamlessly to other psychological frameworks such as Big
Five, HEXACO, or Enneagram. By bridging psychological theory and LLM behavior
design, we establish a foundation for psychologically enhanced AI agents
without any fine-tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Editor to Dense Geometry Estimator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        JiYuan Wang, Chunyu Lin, Lei Sun, Rongying Liu, Lang Nie, Mingxing Li, Kang Liao, Xiangxiang Chu, Yao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging visual priors from pre-trained text-to-image (T2I) generative
models has shown success in dense prediction. However, dense prediction is
inherently an image-to-image task, suggesting that image editing models, rather
than T2I generative models, may be a more suitable foundation for fine-tuning.
  Motivated by this, we conduct a systematic analysis of the fine-tuning
behaviors of both editors and generators for dense geometry estimation. Our
findings show that editing models possess inherent structural priors, which
enable them to converge more stably by ``refining" their innate features, and
ultimately achieve higher performance than their generative counterparts.
  Based on these findings, we introduce \textbf{FE2E}, a framework that
pioneeringly adapts an advanced editing model based on Diffusion Transformer
(DiT) architecture for dense geometry prediction. Specifically, to tailor the
editor for this deterministic task, we reformulate the editor's original flow
matching loss into the ``consistent velocity" training objective. And we use
logarithmic quantization to resolve the precision conflict between the editor's
native BFloat16 format and the high precision demand of our tasks.
Additionally, we leverage the DiT's global attention for a cost-free joint
estimation of depth and normals in a single forward pass, enabling their
supervisory signals to mutually enhance each other.
  Without scaling up the training data, FE2E achieves impressive performance
improvements in zero-shot monocular depth and normal estimation across multiple
datasets. Notably, it achieves over 35\% performance gains on the ETH3D dataset
and outperforms the DepthAnything series, which is trained on 100$\times$ data.
The project page can be accessed \href{https://amap-ml.github.io/FE2E/}{here}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupled Entity Representation <span class="highlight-title">Learning</span> for Pinterest Ads Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Liu, Yinrui Li, Jiankai Sun, Kungang Li, Han Sun, Sihan Wang, Huasen Wu, Siyuan Gao, Paulo Soares, Nan Li, Zhifang Liu, Haoyang Li, Siping Ji, Ling Leng, Prathibha Deshikachar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel framework following an
upstream-downstream paradigm to construct user and item (Pin) embeddings from
diverse data sources, which are essential for Pinterest to deliver personalized
Pins and ads effectively. Our upstream models are trained on extensive data
sources featuring varied signals, utilizing complex architectures to capture
intricate relationships between users and Pins on Pinterest. To ensure
scalability of the upstream models, entity embeddings are learned, and
regularly refreshed, rather than real-time computation, allowing for
asynchronous interaction between the upstream and downstream models. These
embeddings are then integrated as input features in numerous downstream tasks,
including ad retrieval and ranking models for CTR and CVR predictions. We
demonstrate that our framework achieves notable performance improvements in
both offline and online settings across various downstream tasks. This
framework has been deployed in Pinterest's production ad ranking systems,
resulting in significant gains in online metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving <span class="highlight-title">Robust</span>ness of AlphaZero Algorithms to Test-Time Environment
  Changes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isidoro Tamassia, Wendelin Böhmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The AlphaZero framework provides a standard way of combining Monte Carlo
planning with prior knowledge provided by a previously trained policy-value
neural network. AlphaZero usually assumes that the environment on which the
neural network was trained will not change at test time, which constrains its
applicability. In this paper, we analyze the problem of deploying AlphaZero
agents in potentially changed test environments and demonstrate how the
combination of simple modifications to the standard framework can significantly
boost performance, even in settings with a low planning budget available. The
code is publicly available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EvoEmo: Towards Evolved Emotional Policies for LLM Agents in Multi-Turn
  Negotiation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunbo Long, Liming Xu, Lukas Beckenbauer, Yuhan Liu, Alexandra Brintrup
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research on Chain-of-Thought (CoT) reasoning in Large Language Models
(LLMs) has demonstrated that agents can engage in \textit{complex},
\textit{multi-turn} negotiations, opening new avenues for agentic AI. However,
existing LLM agents largely overlook the functional role of emotions in such
negotiations, instead generating passive, preference-driven emotional responses
that make them vulnerable to manipulation and strategic exploitation by
adversarial counterparts. To address this gap, we present EvoEmo, an
evolutionary reinforcement learning framework that optimizes dynamic emotional
expression in negotiations. EvoEmo models emotional state transitions as a
Markov Decision Process and employs population-based genetic optimization to
evolve high-reward emotion policies across diverse negotiation scenarios. We
further propose an evaluation framework with two baselines -- vanilla
strategies and fixed-emotion strategies -- for benchmarking emotion-aware
negotiation. Extensive experiments and ablation studies show that EvoEmo
consistently outperforms both baselines, achieving higher success rates, higher
efficiency, and increased buyer savings. This findings highlight the importance
of adaptive emotional expression in enabling more effective LLM agents for
multi-turn negotiation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge
  in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juraj Vladika, Mahdi Dhaini, Florian Matthes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing capabilities of Large Language Models (LLMs) show significant
potential to enhance healthcare by assisting medical researchers and
physicians. However, their reliance on static training data is a major risk
when medical recommendations evolve with new research and developments. When
LLMs memorize outdated medical knowledge, they can provide harmful advice or
fail at clinical reasoning tasks. To investigate this problem, we introduce two
novel question-answering (QA) datasets derived from systematic reviews:
MedRevQA (16,501 QA pairs covering general biomedical knowledge) and
MedChangeQA (a subset of 512 QA pairs where medical consensus has changed over
time). Our evaluation of eight prominent LLMs on the datasets reveals
consistent reliance on outdated knowledge across all models. We additionally
analyze the influence of obsolete pre-training data and training strategies to
explain this phenomenon and propose future directions for mitigation, laying
the groundwork for developing more current and reliable medical AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HumAIne-Chatbot: Real-Time Personalized Conversational AI via
  Reinforcement <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Makridis, Georgios Fragiadakis, Jorge Oliveira, Tomaz Saraiva, Philip Mavrepis, Georgios Fatouros, Dimosthenis Kyriazis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current conversational AI systems often provide generic, one-size-fits-all
interactions that overlook individual user characteristics and lack adaptive
dialogue management. To address this gap, we introduce
\textbf{HumAIne-chatbot}, an AI-driven conversational agent that personalizes
responses through a novel user profiling framework. The system is pre-trained
on a diverse set of GPT-generated virtual personas to establish a broad prior
over user types. During live interactions, an online reinforcement learning
agent refines per-user models by combining implicit signals (e.g. typing speed,
sentiment, engagement duration) with explicit feedback (e.g., likes and
dislikes). This profile dynamically informs the chatbot dialogue policy,
enabling real-time adaptation of both content and style. To evaluate the
system, we performed controlled experiments with 50 synthetic personas in
multiple conversation domains. The results showed consistent improvements in
user satisfaction, personalization accuracy, and task achievement when
personalization features were enabled. Statistical analysis confirmed
significant differences between personalized and nonpersonalized conditions,
with large effect sizes across key metrics. These findings highlight the
effectiveness of AI-driven user profiling and provide a strong foundation for
future real-world validation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, IEEE conference format</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement <span class="highlight-title">Learning</span> for <span class="highlight-title">Robust</span> Ageing-Aware Control of Li-ion Battery
  Systems with Data-Driven Formal Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rudi Coppola, Hovsep Touloujian, Pierfrancesco Ombrini, Manuel Mazo Jr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rechargeable lithium-ion (Li-ion) batteries are a ubiquitous element of
modern technology. In the last decades, the production and design of such
batteries and their adjacent embedded charging and safety protocols, denoted by
Battery Management Systems (BMS), has taken central stage. A fundamental
challenge to be addressed is the trade-off between the speed of charging and
the ageing behavior, resulting in the loss of capacity in the battery cell. We
rely on a high-fidelity physics-based battery model and propose an approach to
data-driven charging and safety protocol design. Following a
Counterexample-Guided Inductive Synthesis scheme, we combine Reinforcement
Learning (RL) with recent developments in data-driven formal methods to obtain
a hybrid control strategy: RL is used to synthesise the individual controllers,
and a data-driven abstraction guides their partitioning into a switched
structure, depending on the initial output measurements of the battery. The
resulting discrete selection among RL-based controllers, coupled with the
continuous battery dynamics, realises a hybrid system. When a design meets the
desired criteria, the abstraction provides probabilistic guarantees on the
closed-loop performance of the cell.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Study of Vulnerabilities in Python Packages and Their
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Quan, Junjie Wang, Xinzhe Li, Terry Yue Zhuo, Xiao Chen, Xiaoning Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the rapidly evolving software development landscape, Python stands out for
its simplicity, versatility, and extensive ecosystem. Python packages, as units
of organization, reusability, and distribution, have become a pressing concern,
highlighted by the considerable number of vulnerability reports. As a scripting
language, Python often cooperates with other languages for performance or
interoperability. This adds complexity to the vulnerabilities inherent to
Python packages, and the effectiveness of current vulnerability detection tools
remains underexplored. This paper addresses these gaps by introducing PyVul,
the first comprehensive benchmark suite of Python-package vulnerabilities.
PyVul includes 1,157 publicly reported, developer-verified vulnerabilities,
each linked to its affected packages. To accommodate diverse detection
techniques, it provides annotations at both commit and function levels. An
LLM-assisted data cleansing method is incorporated to improve label accuracy,
achieving 100% commit-level and 94% function-level accuracy, establishing PyVul
as the most precise large-scale Python vulnerability benchmark. We further
carry out a distribution analysis of PyVul, which demonstrates that
vulnerabilities in Python packages involve multiple programming languages and
exhibit a wide variety of types. Moreover, our analysis reveals that
multi-lingual Python packages are potentially more susceptible to
vulnerabilities. Evaluation of state-of-the-art detectors using this benchmark
reveals a significant discrepancy between the capabilities of existing tools
and the demands of effectively identifying real-world security issues in Python
packages. Additionally, we conduct an empirical review of the top-ranked CWEs
observed in Python packages, to diagnose the fine-grained limitations of
current detection tools and highlight the necessity for future advancements in
the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How many patients could we save with LLM priors? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shota Arai, David Selby, Andrew Vargo, Sebastian Vollmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imagine a world where clinical trials need far fewer patients to achieve the
same statistical power, thanks to the knowledge encoded in large language
models (LLMs). We present a novel framework for hierarchical Bayesian modeling
of adverse events in multi-center clinical trials, leveraging LLM-informed
prior distributions. Unlike data augmentation approaches that generate
synthetic data points, our methodology directly obtains parametric priors from
the model. Our approach systematically elicits informative priors for
hyperparameters in hierarchical Bayesian models using a pre-trained LLM,
enabling the incorporation of external clinical expertise directly into
Bayesian safety modeling. Through comprehensive temperature sensitivity
analysis and rigorous cross-validation on real-world clinical trial data, we
demonstrate that LLM-derived priors consistently improve predictive performance
compared to traditional meta-analytical approaches. This methodology paves the
way for more efficient and expert-informed clinical trial design, enabling
substantial reductions in the number of patients required to achieve robust
safety assessment and with the potential to transform drug safety monitoring
and regulatory decision making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Learning</span> Active Perception via Self-Evolving Preference <span class="highlight-title">Optimization</span> for
  GUI Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanfu Wang, Qipeng Huang, Guangquan Xue, Xiaobo Liang, Juntao Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Language Models (VLMs) have recently achieved significant progress in
bridging visual perception and linguistic reasoning. Recently, OpenAI o3 model
introduced a zoom-in search strategy that effectively elicits active perception
capabilities in VLMs, improving downstream task performance. However, enabling
VLMs to reason effectively over appropriate image regions remains a core
challenge in GUI grounding, particularly under high-resolution inputs and
complex multi-element visual interactions. In this work, we propose LASER, a
self-evolving framework that progressively endows VLMs with multi-step
perception capabilities, enabling precise coordinate prediction. Specifically,
our approach integrate Monte Carlo quality estimation with
Intersection-over-Union (IoU)-based region quality evaluation to jointly
encourage both accuracy and diversity in constructing high-quality preference
data. This combination explicitly guides the model to focus on
instruction-relevant key regions while adaptively allocating reasoning steps
based on task complexity. Comprehensive experiments on the ScreenSpot Pro and
ScreenSpot-v2 benchmarks demonstrate consistent performance gains, validating
the effectiveness of our method. Furthermore, when fine-tuned on GTA1-7B, LASER
achieves a score of 55.7 on the ScreenSpot-Pro benchmark, establishing a new
state-of-the-art (SoTA) among 7B-scale models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Quality of Gaming Narratives Co-created with AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arturo Valdivia, Paolo Burelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a structured methodology to evaluate AI-generated game
narratives, leveraging the Delphi study structure with a panel of narrative
design experts. Our approach synthesizes story quality dimensions from
literature and expert insights, mapping them into the Kano model framework to
understand their impact on player satisfaction. The results can inform game
developers on prioritizing quality aspects when co-creating game narratives
with generative AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain size asymptotics for Markov logic networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vera Koponen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A Markov logic network (MLN) determines a probability distribution on the set
of structures, or ``possible worlds'', with an arbitrary finite domain. We
study the properties of such distributions as the domain size tends to
infinity. Three types of concrete examples of MLNs will be considered, and the
properties of random structures with domain sizes tending to infinity will be
studied: (1) Arbitrary quantifier-free MLNs over a language with only one
relation symbol which has arity 1. In this case we give a pretty complete
characterization of the possible limit behaviours of random structures. (2) An
MLN that favours graphs with fewer triangles (or more generally, fewer
k-cliques). As a corollary of the analysis a ``$\delta$-approximate 0-1 law''
for first-order logic is obtained. (3) An MLN that favours graphs with fewer
vertices with degree higher than a fixed (but arbitrary) number. The analysis
shows that depending on which ``soft constraints'' an MLN uses the limit
behaviour of random structures can be quite different, and the weights of the
soft constraints may, or may not, have influence on the limit behaviour. It
will also be demonstrated, using (1), that quantifier-free MLNs and lifted
Bayesian networks (in a broad sense) are asymptotically incomparable, roughly
meaning that there is a sequence of distributions on possible worlds with
increasing domain sizes that can be defined by one of the formalisms but not
even approximated by the other. In a rather general context it is also shown
that on large domains the distribution determined by an MLN concentrates almost
all its probability mass on a totally different part of the space of possible
worlds than the uniform distribution does.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAGneT: <span class="highlight-title">Coordinated</span> Multi-Agent Generation of Synthetic Multi-Turn
  Mental Health Counseling Sessions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishik Mandal, Tanmoy Chakraborty, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing demand for scalable psychological counseling highlights the need
for fine-tuning open-source Large Language Models (LLMs) with high-quality,
privacy-compliant data, yet such data remains scarce. Here we introduce MAGneT,
a novel multi-agent framework for synthetic psychological counseling session
generation that decomposes counselor response generation into coordinated
sub-tasks handled by specialized LLM agents, each modeling a key psychological
technique. Unlike prior single-agent approaches, MAGneT better captures the
structure and nuance of real counseling. In addition, we address
inconsistencies in prior evaluation protocols by proposing a unified evaluation
framework integrating diverse automatic and expert metrics. Furthermore, we
expand the expert evaluations from four aspects of counseling in previous works
to nine aspects, enabling a more thorough and robust assessment of data
quality. Empirical results show that MAGneT significantly outperforms existing
methods in quality, diversity, and therapeutic alignment of the generated
counseling sessions, improving general counseling skills by 3.2% and
CBT-specific skills by 4.3% on average on cognitive therapy rating scale
(CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases
on average across all aspects. Moreover, fine-tuning an open-source model on
MAGneT-generated sessions shows better performance, with improvements of 6.3%
on general counseling skills and 7.3% on CBT-specific skills on average on CTRS
over those fine-tuned with sessions generated by baseline methods. We also make
our code and data public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 29 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer
  <span class="highlight-title">Vision</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Safouane El Ghazouali, Umberto Michelucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI models rely on annotated data to learn pattern and perform prediction.
Annotation is usually a labor-intensive step that require associating labels
ranging from a simple classification label to more complex tasks such as object
detection, oriented bounding box estimation, and instance segmentation.
Traditional tools often require extensive manual input, limiting scalability
for large datasets. To address this, we introduce VisioFirm, an open-source web
application designed to streamline image labeling through AI-assisted
automation. VisioFirm integrates state-of-the-art foundation models into an
interface with a filtering pipeline to reduce human-in-the-loop efforts. This
hybrid approach employs CLIP combined with pre-trained detectors like
Ultralytics models for common classes and zero-shot models such as Grounding
DINO for custom labels, generating initial annotations with low-confidence
thresholding to maximize recall. Through this framework, when tested on
COCO-type of classes, initial prediction have been proven to be mostly correct
though the users can refine these via interactive tools supporting bounding
boxes, oriented bounding boxes, and polygons. Additionally, VisioFirm has
on-the-fly segmentation powered by Segment Anything accelerated through WebGPU
for browser-side efficiency. The tool supports multiple export formats (YOLO,
COCO, Pascal VOC, CSV) and operates offline after model caching, enhancing
accessibility. VisioFirm demonstrates up to 90\% reduction in manual effort
through benchmarks on diverse datasets, while maintaining high annotation
accuracy via clustering of connected CLIP-based disambiguate components and
IoU-graph for redundant detection suppression. VisioFirm can be accessed from
\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Crossing the Species Divide: Transfer <span class="highlight-title">Learning</span> from Speech to Animal
  Sounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jules Cauzinille, Marius Miron, Olivier Pietquin, Masato Hagiwara, Ricard Marxer, Arnaud Rey, Benoit Favre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised speech models have demonstrated impressive performance in
speech processing, but their effectiveness on non-speech data remains
underexplored. We study the transfer learning capabilities of such models on
bioacoustic detection and classification tasks. We show that models such as
HuBERT, WavLM, and XEUS can generate rich latent representations of animal
sounds across taxa. We analyze the models properties with linear probing on
time-averaged representations. We then extend the approach to account for the
effect of time-wise information with other downstream architectures. Finally,
we study the implication of frequency range and noise on performance. Notably,
our results are competitive with fine-tuned bioacoustic pre-trained models and
show the impact of noise-robust pre-training setups. These findings highlight
the potential of speech-based self-supervised learning as an efficient
framework for advancing bioacoustic research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, uses dcase2025.sty, submitted to DCASE 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards an Action-Centric Ontology for Cooking Procedures Using Temporal
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aarush Kumbhakern, Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formalizing cooking procedures remains a challenging task due to their
inherent complexity and ambiguity. We introduce an extensible domain-specific
language for representing recipes as directed action graphs, capturing
processes, transfers, environments, concurrency, and compositional structure.
Our approach enables precise, modular modeling of complex culinary workflows.
Initial manual evaluation on a full English breakfast recipe demonstrates the
DSL's expressiveness and suitability for future automated recipe analysis and
execution. This work represents initial steps towards an action-centric
ontology for cooking, using temporal graphs to enable structured machine
understanding, precise interpretation, and scalable automation of culinary
processes - both in home kitchens and professional culinary settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 1 table, 11 references, ACM International
  Conference on Multimedia 2025 - Multi-modal Food Computing Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOLO Ensemble for UAV-based Multispectral Defect Detection in Wind
  Turbine Components 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Serhii Svystun, Pavlo Radiuk, Oleksandr Melnychenko, Oleg Savenko, Anatoliy Sachenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned aerial vehicles (UAVs) equipped with advanced sensors have opened up
new opportunities for monitoring wind power plants, including blades, towers,
and other critical components. However, reliable defect detection requires
high-resolution data and efficient methods to process multispectral imagery. In
this research, we aim to enhance defect detection accuracy through the
development of an ensemble of YOLO-based deep learning models that integrate
both visible and thermal channels. We propose an ensemble approach that
integrates a general-purpose YOLOv8 model with a specialized thermal model,
using a sophisticated bounding box fusion algorithm to combine their
predictions. Our experiments show this approach achieves a mean Average
Precision (mAP@.5) of 0.93 and an F1-score of 0.90, outperforming a standalone
YOLOv8 model, which scored an mAP@.5 of 0.91. These findings demonstrate that
combining multiple YOLO architectures with fused multispectral data provides a
more reliable solution, improving the detection of both visual and thermal
defects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 13th IEEE International Conference on Intelligent Data
  Acquisition and Advanced Computing Systems: Technology and Applications, 4-6
  September, 2025, Gliwice, Poland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention as an Adaptive Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Racioppo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Adaptive Filter Attention (AFA), a novel attention mechanism
that incorporates a learnable dynamics model directly into the computation of
attention weights. Rather than comparing queries and keys directly, we model
the input sequence as discrete observations of a linear stochastic differential
equation (SDE). By imposing a linear dynamics model with simultaneously
diagonalizable state matrices and noise covariances, we can make use of a
closed-form solution to the differential Lyapunov equation to efficiently
propagate pairwise uncertainties through the dynamics. Attention naturally
arises as the maximum likelihood solution for this linear SDE, with attention
weights corresponding to robust residual-based reweightings of the propagated
pairwise precisions. Imposing an additional constraint on the state matrix's
eigenvalues leads to a simplified variant with the same computational and
memory complexity as standard attention. In the limit of vanishing dynamics and
process noise, and using a small-angle approximation, we recover ordinary
dot-product attention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAGAL: Tabular Data Generation using Agentic LLM Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benoît Ronval, Pierre Dupont, Siegfried Nijssen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generation of data is a common approach to improve the performance of
machine learning tasks, among which is the training of models for
classification. In this paper, we present TAGAL, a collection of methods able
to generate synthetic tabular data using an agentic workflow. The methods
leverage Large Language Models (LLMs) for an automatic and iterative process
that uses feedback to improve the generated data without any further LLM
training. The use of LLMs also allows for the addition of external knowledge in
the generation process. We evaluate TAGAL across diverse datasets and different
aspects of quality for the generated data. We look at the utility of downstream
ML models, both by training classifiers on synthetic data only and by combining
real and synthetic data. Moreover, we compare the similarities between the real
and the generated data. We show that TAGAL is able to perform on par with
state-of-the-art approaches that require LLM training and generally outperforms
other training-free approaches. These findings highlight the potential of
agentic workflow and open new directions for LLM-based data generation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Technical Documents Retrieval for RAG 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songjiang Lai, Tsun-Hin Cheung, Ka-Chun Fung, Kaiwen Xue, Kwan-Ho Lin, Yan-Ming Choi, Vincent Ng, Kin-Man Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Technical-Embeddings, a novel framework designed
to optimize semantic retrieval in technical documentation, with applications in
both hardware and software development. Our approach addresses the challenges
of understanding and retrieving complex technical content by leveraging the
capabilities of Large Language Models (LLMs). First, we enhance user queries by
generating expanded representations that better capture user intent and improve
dataset diversity, thereby enriching the fine-tuning process for embedding
models. Second, we apply summary extraction techniques to encode essential
contextual information, refining the representation of technical documents. To
further enhance retrieval performance, we fine-tune a bi-encoder BERT model
using soft prompting, incorporating separate learning parameters for queries
and document context to capture fine-grained semantic nuances. We evaluate our
approach on two public datasets, RAG-EDA and Rust-Docs-QA, demonstrating that
Technical-Embeddings significantly outperforms baseline models in both
precision and recall. Our findings highlight the effectiveness of integrating
query expansion and contextual summarization to enhance information access and
comprehension in technical domains. This work advances the state of
Retrieval-Augmented Generation (RAG) systems, offering new avenues for
efficient and accurate technical document retrieval in engineering and product
development workflows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The human biological advantage over AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04130v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04130v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Stewart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in AI raise the possibility that AI systems will one day be
able to do anything humans can do, only better. If artificial general
intelligence (AGI) is achieved, AI systems may be able to understand, reason,
problem solve, create, and evolve at a level and speed that humans will
increasingly be unable to match, or even understand. These possibilities raise
a natural question as to whether AI will eventually become superior to humans,
a successor "digital species", with a rightful claim to assume leadership of
the universe. However, a deeper consideration suggests the overlooked
differentiator between human beings and AI is not the brain, but the central
nervous system (CNS), providing us with an immersive integration with physical
reality. It is our CNS that enables us to experience emotion including pain,
joy, suffering, and love, and therefore to fully appreciate the consequences of
our actions on the world around us. And that emotional understanding of the
consequences of our actions is what is required to be able to develop
sustainable ethical systems, and so be fully qualified to be the leaders of the
universe. A CNS cannot be manufactured or simulated; it must be grown as a
biological construct. And so, even the development of consciousness will not be
sufficient to make AI systems superior to humans. AI systems may become more
capable than humans on almost every measure and transform our society. However,
the best foundation for leadership of our universe will always be DNA, not
silicon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simplicity Lies in the Eye of the Beholder: A Strategic Perspective on
  Controllers in Reactive Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mickael Randour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the game-theoretic approach to controller synthesis, we model the
interaction between a system to be controlled and its environment as a game
between these entities, and we seek an appropriate (e.g., winning or optimal)
strategy for the system. This strategy then serves as a formal blueprint for a
real-world controller. A common belief is that simple (e.g., using limited
memory) strategies are better: corresponding controllers are easier to conceive
and understand, and cheaper to produce and maintain.
  This invited contribution focuses on the complexity of strategies in a
variety of synthesis contexts. We discuss recent results concerning memory and
randomness, and take a brief look at what lies beyond our traditional notions
of complexity for strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Invited paper at RP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MEPG:Multi-Expert <span class="highlight-title">Planning</span> and Generation for Compositionally-Rich Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Zhao, Liu Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have achieved remarkable image quality, but
they still struggle with complex, multiele ment prompts, and limited stylistic
diversity. To address these limitations, we propose a Multi-Expert Planning and
Gen eration Framework (MEPG) that synergistically integrates position- and
style-aware large language models (LLMs) with spatial-semantic expert modules.
The framework comprises two core components: (1) a Position-Style-Aware (PSA)
module that utilizes a supervised fine-tuned LLM to decom pose input prompts
into precise spatial coordinates and style encoded semantic instructions; and
(2) a Multi-Expert Dif fusion (MED) module that implements cross-region genera
tion through dynamic expert routing across both local regions and global areas.
During the generation process for each lo cal region, specialized models (e.g.,
realism experts, styliza tion specialists) are selectively activated for each
spatial par tition via attention-based gating mechanisms. The architec ture
supports lightweight integration and replacement of ex pert models, providing
strong extensibility. Additionally, an interactive interface enables real-time
spatial layout editing and per-region style selection from a portfolio of
experts. Ex periments show that MEPG significantly outperforms base line models
with the same backbone in both image quality
  and style diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of Bluffing by DQN and CFR in Leduc Hold'em Poker 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarik Zaciragic, Aske Plaat, K. Joost Batenburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the game of poker, being unpredictable, or bluffing, is an essential
skill. When humans play poker, they bluff. However, most works on
computer-poker focus on performance metrics such as win rates, while bluffing
is overlooked. In this paper we study whether two popular algorithms, DQN
(based on reinforcement learning) and CFR (based on game theory), exhibit
bluffing behavior in Leduc Hold'em, a simplified version of poker. We designed
an experiment where we let the DQN and CFR agent play against each other while
we log their actions. We find that both DQN and CFR exhibit bluffing behavior,
but they do so in different ways. Although both attempt to perform bluffs at
different rates, the percentage of successful bluffs (where the opponent folds)
is roughly the same. This suggests that bluffing is an essential aspect of the
game, not of the algorithm. Future work should look at different bluffing
styles and at the full game of poker. Code at
https://github.com/TarikZ03/Bluffing-by-DQN-and-CFR-in-Leduc-Hold-em-Poker-Codebase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EHVC: Efficient Hierarchical Reference and Quality Structure for Neural
  Video Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junqi Liao, Yaojun Wu, Chaoyi Lin, Zhipin Deng, Li Li, Dong Liu, Xiaoyan Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural video codecs (NVCs), leveraging the power of end-to-end learning, have
demonstrated remarkable coding efficiency improvements over traditional video
codecs. Recent research has begun to pay attention to the quality structures in
NVCs, optimizing them by introducing explicit hierarchical designs. However,
less attention has been paid to the reference structure design, which
fundamentally should be aligned with the hierarchical quality structure. In
addition, there is still significant room for further optimization of the
hierarchical quality structure. To address these challenges in NVCs, we propose
EHVC, an efficient hierarchical neural video codec featuring three key
innovations: (1) a hierarchical multi-reference scheme that draws on
traditional video codec design to align reference and quality structures,
thereby addressing the reference-quality mismatch; (2) a lookahead strategy to
utilize an encoder-side context from future frames to enhance the quality
structure; (3) a layer-wise quality scale with random quality training strategy
to stabilize quality structures during inference. With these improvements, EHVC
achieves significantly superior performance to the state-of-the-art NVCs. Code
will be released in: https://github.com/bytedance/NEVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, Accepted to ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Reinforcement <span class="highlight-title">Learning</span> and Search for Flight Trajectory <span class="highlight-title">Planning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Luise, Michele Lombardi, Florent Teichteil Koenigsbuch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the combination of Reinforcement Learning (RL) and
search-based path planners to speed up the optimization of flight paths for
airliners, where in case of emergency a fast route re-calculation can be
crucial. The fundamental idea is to train an RL Agent to pre-compute
near-optimal paths based on location and atmospheric data and use those at
runtime to constrain the underlying path planning solver and find a solution
within a certain distance from the initial guess. The approach effectively
reduces the size of the solver's search space, significantly speeding up route
optimization. Although global optimality is not guaranteed, empirical results
conducted with Airbus aircraft's performance models show that fuel consumption
remains nearly identical to that of an unconstrained solver, with deviations
typically within 1%. At the same time, computation speed can be improved by up
to 50% as compared to using a conventional solver alone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intermediate Languages Matter: Formal Languages and LLMs affect
  Neurosymbolic Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Beiser, David Penz, Nysret Musliu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) achieve astonishing results on a wide range of
tasks. However, their formal reasoning ability still lags behind. A promising
approach is Neurosymbolic LLM reasoning. It works by using LLMs as translators
from natural to formal languages and symbolic solvers for deriving correct
results. Still, the contributing factors to the success of Neurosymbolic LLM
reasoning remain unclear. This paper demonstrates that one previously
overlooked factor is the choice of the formal language. We introduce the
intermediate language challenge: selecting a suitable formal language for
neurosymbolic reasoning. By comparing four formal languages across three
datasets and seven LLMs, we show that the choice of formal language affects
both syntactic and semantic reasoning capabilities. We also discuss the varying
effects across different LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the proceedings of The Second Workshop on Knowledge
  Graphs and Neurosymbolic AI (KG-NeSy) Co-located with SEMANTiCS 2025
  Conference, Vienna, Austria - September 3rd, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RepoDebug: Repository-Level Multi-Task and Multi-Language Debugging
  Evaluation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Liu, Zeming Liu, Zihao Cheng, Mengliang He, Xiaoming Shi, Yuhang Guo, Xiangrong Zhu, Yuanfang Guo, Yunhong Wang, Haifeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited significant proficiency in code
debugging, especially in automatic program repair, which may substantially
reduce the time consumption of developers and enhance their efficiency.
Significant advancements in debugging datasets have been made to promote the
development of code debugging. However, these datasets primarily focus on
assessing the LLM's function-level code repair capabilities, neglecting the
more complex and realistic repository-level scenarios, which leads to an
incomplete understanding of the LLM's challenges in repository-level debugging.
While several repository-level datasets have been proposed, they often suffer
from limitations such as limited diversity of tasks, languages, and error
types. To mitigate this challenge, this paper introduces RepoDebug, a
multi-task and multi-language repository-level code debugging dataset with 22
subtypes of errors that supports 8 commonly used programming languages and 3
debugging tasks. Furthermore, we conduct evaluation experiments on 10 LLMs,
where Claude 3.5 Sonnect, the best-performing model, still cannot perform well
in repository-level debugging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 12 figures, EMNLP 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keypoint-based Diffusion for <span class="highlight-title">Robot</span>ic Motion <span class="highlight-title">Planning</span> on the NICOL <span class="highlight-title">Robot</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Clasmeier, Jan-Gerrit Habekost, Connor Gäde, Philipp Allgeuer, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel diffusion-based action model for robotic motion planning.
Commonly, established numerical planning approaches are used to solve general
motion planning problems, but have significant runtime requirements. By
leveraging the power of deep learning, we are able to achieve good results in a
much smaller runtime by learning from a dataset generated by these planners.
While our initial model uses point cloud embeddings in the input to predict
keypoint-based joint sequences in its output, we observed in our ablation study
that it remained challenging to condition the network on the point cloud
embeddings. We identified some biases in our dataset and refined it, which
improved the model's performance. Our model, even without the use of the point
cloud encodings, outperforms numerical models by an order of magnitude
regarding the runtime, while reaching a success rate of up to 90% of collision
free solutions on the test set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICANN 20255 Special Session on Neural Robotics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neural Video Compression with In-Loop Contextual Filtering and
  Out-of-Loop Reconstruction Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaojun Wu, Chaoyi Lin, Yiming Wang, Semih Esenlik, Zhaobin Zhang, Kai Zhang, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the application of enhancement filtering techniques in
neural video compression. Specifically, we categorize these techniques into
in-loop contextual filtering and out-of-loop reconstruction enhancement based
on whether the enhanced representation affects the subsequent coding loop.
In-loop contextual filtering refines the temporal context by mitigating error
propagation during frame-by-frame encoding. However, its influence on both the
current and subsequent frames poses challenges in adaptively applying filtering
throughout the sequence. To address this, we introduce an adaptive coding
decision strategy that dynamically determines filtering application during
encoding. Additionally, out-of-loop reconstruction enhancement is employed to
refine the quality of reconstructed frames, providing a simple yet effective
improvement in coding efficiency. To the best of our knowledge, this work
presents the first systematic study of enhancement filtering in the context of
conditional-based neural video compression. Extensive experiments demonstrate a
7.71% reduction in bit rate compared to state-of-the-art neural video codecs,
validating the effectiveness of the proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, Accepted to ACMMM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Oruga: An Avatar of Representational Systems Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Raggi, Gem Stapleton, Mateja Jamnik, Aaron Stockdill, Grecia Garcia Garcia, Peter C-H. Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans use representations flexibly. We draw diagrams, change representations
and exploit creative analogies across different domains. We want to harness
this kind of power and endow machines with it to make them more compatible with
human use. Previously we developed Representational Systems Theory (RST) to
study the structure and transformations of representations. In this paper we
present Oruga (caterpillar in Spanish; a symbol of transformation), an
implementation of various aspects of RST. Oruga consists of a core of data
structures corresponding to concepts in RST, a language for communicating with
the core, and an engine for producing transformations using a method we call
structure transfer. In this paper we present an overview of the core and
language of Oruga, with a brief example of the kind of transformation that
structure transfer can execute.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoT-Space: A Theoretical Framework for Internal Slow-Thinking via
  Reinforcement <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Gan, Hao Yi, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has become a pivotal approach for enhancing the
reasoning capabilities of Large Language Models (LLMs). However, a significant
theoretical gap persists, as traditional token-level RL frameworks fail to
align with the reasoning-level nature of complex, multi-step thought processes
like Chain-of-Thought (CoT). To address this challenge, we introduce CoT-Space,
a novel theoretical framework that recasts LLM reasoning from a discrete
token-prediction task to an optimization process within a continuous,
reasoning-level semantic space. By analyzing this process from both a noise
perspective and a risk perspective, we demonstrate that the convergence to an
optimal CoT length is a natural consequence of the fundamental trade-off
between underfitting and overfitting. Furthermore, extensive experiments
provide strong empirical validation for our theoretical findings. Our framework
not only provides a coherent explanation for empirical phenomena such as
overthinking but also offers a solid theoretical foundation to guide the future
development of more effective and generalizable reasoning agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint Edition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On <span class="highlight-title">Robust</span>ness and Reliability of Benchmark-Based Evaluation of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Lunardi, Vincenzo Della Mea, Stefano Mizzaro, Kevin Roitero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) effectiveness is usually evaluated by means of
benchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in
their original wording, thus in a fixed, standardized format. However,
real-world applications involve linguistic variability, requiring models to
maintain their effectiveness across diverse rewordings of the same question or
query. In this study, we systematically assess the robustness of LLMs to
paraphrased benchmark questions and investigate whether benchmark-based
evaluations provide a reliable measure of model capabilities. We systematically
generate various paraphrases of all the questions across six different common
benchmarks, and measure the resulting variations in effectiveness of 34
state-of-the-art LLMs, of different size and effectiveness. Our findings reveal
that while LLM rankings remain relatively stable across paraphrased inputs,
absolute effectiveness scores change, and decline significantly. This suggests
that LLMs struggle with linguistic variability, raising concerns about their
generalization abilities and evaluation methodologies. Furthermore, the
observed performance drop challenges the reliability of benchmark-based
evaluations, indicating that high benchmark scores may not fully capture a
model's robustness to real-world input variations. We discuss the implications
of these findings for LLM evaluation methodologies, emphasizing the need for
robustness-aware benchmarks that better reflect practical deployment scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ECAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware
  Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Or Shachar, Uri Katz, Yoav Goldberg, Oren Glickman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named
Entity Retrieval, a variant of Named Entity Recognition (NER), where the types
of interest are not provided in advance, and a user-defined type description is
used to retrieve documents mentioning entities of that type. Instead of relying
on fixed schemas or fine-tuned models, our method builds on internal
representations of large language models (LLMs) to embed both entity mentions
and user-provided open-ended type descriptions into a shared semantic space. We
show that internal representations, specifically the value vectors from
mid-layer transformer blocks, encode fine-grained type information more
effectively than commonly used top-layer embeddings. To refine these
representations, we train a lightweight contrastive projection network that
aligns type-compatible entities while separating unrelated types. The resulting
entity embeddings are compact, type-aware, and well-suited for nearest-neighbor
search. Evaluated on three benchmarks, NER Retriever significantly outperforms
both lexical and dense sentence-level retrieval baselines. Our findings provide
empirical support for representation selection within LLMs and demonstrate a
practical solution for scalable, schema-free entity retrieval. The NER
Retriever Codebase is publicly available at
https://github.com/ShacharOr100/ner_retriever
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Regional Spurious Correlations in <span class="highlight-title">Vision</span> Transformers via
  Token Discarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Solha Kang, Esla Timothy Anzaku, Wesley De Neve, Arnout Van Messem, Joris Vankerschaver, Francois Rameau, Utku Ozbulak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to their powerful feature association capabilities, neural network-based
computer vision models have the ability to detect and exploit unintended
patterns within the data, potentially leading to correct predictions based on
incorrect or unintended but statistically relevant signals. These clues may
vary from simple color aberrations to small texts within the image. In
situations where these unintended signals align with the predictive task,
models can mistakenly link these features with the task and rely on them for
making predictions. This phenomenon is referred to as spurious correlations,
where patterns appear to be associated with the task but are actually
coincidental. As a result, detection and mitigation of spurious correlations
have become crucial tasks for building trustworthy, reliable, and generalizable
machine learning models. In this work, we present a novel method to detect
spurious correlations in vision transformers, a type of neural network
architecture that gained significant popularity in recent years. Using both
supervised and self-supervised trained models, we present large-scale
experiments on the ImageNet dataset demonstrating the ability of the proposed
method to identify spurious correlations. We also find that, even if the same
architecture is used, the training methodology has a significant impact on the
model's reliance on spurious correlations. Furthermore, we show that certain
classes in the ImageNet dataset contain spurious signals that are easily
detected by the models and discuss the underlying reasons for those spurious
signals. In light of our findings, we provide an exhaustive list of the
aforementioned images and call for caution in their use in future research
efforts. Lastly, we present a case study investigating spurious signals in
invasive breast mass classification, grounding our work in real-world
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoPBO: LLM-powered <span class="highlight-title">Optimization</span> for Local Search PBO Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.04007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.04007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyuan Li, Yi Chu, Yiwen Sun, Mengchuan Zou, Shaowei Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pseudo-Boolean Optimization (PBO) provides a powerful framework for modeling
combinatorial problems through pseudo-Boolean (PB) constraints. Local search
solvers have shown excellent performance in PBO solving, and their efficiency
is highly dependent on their internal heuristics to guide the search. Still,
their design often requires significant expert effort and manual tuning in
practice. While Large Language Models (LLMs) have demonstrated potential in
automating algorithm design, their application to optimizing PBO solvers
remains unexplored. In this work, we introduce AutoPBO, a novel LLM-powered
framework to automatically enhance PBO local search solvers. We conduct
experiments on a broad range of four public benchmarks, including one
real-world benchmark, a benchmark from PB competition, an integer linear
programming optimization benchmark, and a crafted combinatorial benchmark, to
evaluate the performance improvement achieved by AutoPBO and compare it with
six state-of-the-art competitors, including two local search PBO solvers NuPBO
and OraSLS, two complete PB solvers PBO-IHS and RoundingSat, and two mixed
integer programming (MIP) solvers Gurobi and SCIP. AutoPBO demonstrates
significant improvements over previous local search approaches, while
maintaining competitive performance compared to state-of-the-art competitors.
The results suggest that AutoPBO offers a promising approach to automating
local search solver design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question
  Answering with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyan Gong, Juan Li, Zhiqiang Liu, Lei Liang, Huajun Chen, Wen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current temporal knowledge graph question answering (TKGQA) methods primarily
focus on implicit temporal constraints, lacking the capability of handling more
complex temporal queries, and struggle with limited reasoning abilities and
error propagation in decomposition frameworks. We propose RTQA, a novel
framework to address these challenges by enhancing reasoning over TKGs without
requiring training. Following recursive thinking, RTQA recursively decomposes
questions into sub-problems, solves them bottom-up using LLMs and TKG
knowledge, and employs multi-path answer aggregation to improve fault
tolerance. RTQA consists of three core components: the Temporal Question
Decomposer, the Recursive Solver, and the Answer Aggregator. Experiments on
MultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements
in "Multiple" and "Complex" categories, outperforming state-of-the-art methods.
Our code and data are available at https://github.com/zjukg/RTQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Policy Reflexion: Reusable Reflective Memory and Rule Admissibility
  for Resource-Efficient LLM Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunlong Wu, Zhibo Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) agents achieve impressive single-task performance
but commonly exhibit repeated failures, inefficient exploration, and limited
cross-task adaptability. Existing reflective strategies (e.g., Reflexion,
ReAct) improve per-episode behavior but typically produce ephemeral,
task-specific traces that are not reused across tasks. Reinforcement-learning
based alternatives can produce transferable policies but require substantial
parameter updates and compute. In this work we introduce Meta-Policy Reflexion
(MPR): a hybrid framework that consolidates LLM-generated reflections into a
structured, predicate-like Meta-Policy Memory (MPM) and applies that memory at
inference time through two complementary mechanisms soft memory-guided decoding
and hard rule admissibility checks(HAC). MPR (i) externalizes reusable
corrective knowledge without model weight updates, (ii) enforces domain
constraints to reduce unsafe or invalid actions, and (iii) retains the
adaptability of language-based reflection. We formalize the MPM representation,
present algorithms for update and decoding, and validate the approach in a
text-based agent environment following the experimental protocol described in
the provided implementation (AlfWorld-based). Empirical results reported in the
supplied material indicate consistent gains in execution accuracy and
robustness when compared to Reflexion baselines; rule admissibility further
improves stability. We analyze mechanisms that explain these gains, discuss
scalability and failure modes, and outline future directions for multimodal and
multi?agent extensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Promptception: How Sensitive Are Large Multimodal Models to Prompts? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Insaf Ismithdeen, Muhammad Uzair Khattak, Salman Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of Large Multimodal Models (LMMs) in recent years, prompt
design for LMMs in Multiple-Choice Question Answering (MCQA) remains poorly
understood. We show that even minor variations in prompt phrasing and structure
can lead to accuracy deviations of up to 15% for certain prompts and models.
This variability poses a challenge for transparent and fair LMM evaluation, as
models often report their best-case performance using carefully selected
prompts. To address this, we introduce Promptception, a systematic framework
for evaluating prompt sensitivity in LMMs. It consists of 61 prompt types,
spanning 15 categories and 6 supercategories, each targeting specific aspects
of prompt formulation, and is used to evaluate 10 LMMs ranging from lightweight
open-source models to GPT-4o and Gemini 1.5 Pro, across 3 MCQA benchmarks:
MMStar, MMMU-Pro, MVBench. Our findings reveal that proprietary models exhibit
greater sensitivity to prompt phrasing, reflecting tighter alignment with
instruction semantics, while open-source models are steadier but struggle with
nuanced and complex phrasing. Based on this analysis, we propose Prompting
Principles tailored to proprietary and open-source LMMs, enabling more robust
and fair model evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuroBreak: Unveil Internal Jailbreak Mechanisms in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuhan Zhang, Ye Zhang, Bowen Shi, Yuyou Gan, Tianyu Du, Shouling Ji, Dazhan Deng, Yingcai Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In deployment and application, large language models (LLMs) typically undergo
safety alignment to prevent illegal and unethical outputs. However, the
continuous advancement of jailbreak attack techniques, designed to bypass
safety mechanisms with adversarial prompts, has placed increasing pressure on
the security defenses of LLMs. Strengthening resistance to jailbreak attacks
requires an in-depth understanding of the security mechanisms and
vulnerabilities of LLMs. However, the vast number of parameters and complex
structure of LLMs make analyzing security weaknesses from an internal
perspective a challenging task. This paper presents NeuroBreak, a top-down
jailbreak analysis system designed to analyze neuron-level safety mechanisms
and mitigate vulnerabilities. We carefully design system requirements through
collaboration with three experts in the field of AI security. The system
provides a comprehensive analysis of various jailbreak attack methods. By
incorporating layer-wise representation probing analysis, NeuroBreak offers a
novel perspective on the model's decision-making process throughout its
generation steps. Furthermore, the system supports the analysis of critical
neurons from both semantic and functional perspectives, facilitating a deeper
exploration of security mechanisms. We conduct quantitative evaluations and
case studies to verify the effectiveness of our system, offering mechanistic
insights for developing next-generation defense strategies against evolving
jailbreak attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAC-MIL: Spatial-Aware Correlated Multiple Instance <span class="highlight-title">Learning</span> for
  Histopathology Whole Slide Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Bai, Zitong Yu, Haowen Tian, Xijing Wang, Shuo Yan, Lin Wang, Honglin Li, Xitong Ling, Bo Zhang, Zheng Zhang, Wufan Wang, Hui Gao, Xiangyang Gong, Wendong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Spatial-Aware Correlated Multiple Instance Learning (SAC-MIL) for
performing WSI classification. SAC-MIL consists of a positional encoding module
to encode position information and a SAC block to perform full instance
correlations. The positional encoding module utilizes the instance coordinates
within the slide to encode the spatial relationships instead of the instance
index in the input WSI sequence. The positional encoding module can also handle
the length extrapolation issue where the training and testing sequences have
different lengths. The SAC block is an MLP-based method that performs full
instance correlation in linear time complexity with respect to the sequence
length. Due to the simple structure of MLP, it is easy to deploy since it does
not require custom CUDA kernels, compared to Transformer-based methods for WSI
classification. SAC-MIL has achieved state-of-the-art performance on the
CAMELYON-16, TCGA-LUNG, and TCGA-BRAC datasets. The code will be released upon
acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expanding Foundational Language Capabilities in Open-Source LLMs through
  a Korean Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junghwan Lim, Gangwon Jo, Sungmin Lee, Jiyoung Park, Dongseok Kim, Jihwan Kim, Junhyeok Lee, Wai Ting Cheung, Dahye Choi, Kibong Choi, Jaeyeon Huh, Beomgyu Kim, Jangwoong Kim, Taehyun Kim, Haesol Lee, Jeesoo Lee, Dongpin Oh, Changseok Song, Daewon Suh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Llama-3-Motif, a language model consisting of 102 billion
parameters, specifically designed to enhance Korean capabilities while
retaining strong performance in English. Developed on the Llama 3 architecture,
Llama-3-Motif employs advanced training techniques, including LlamaPro and
Masked Structure Growth, to effectively scale the model without altering its
core Transformer architecture. Using the MoAI platform for efficient training
across hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully
curated dataset that maintains a balanced ratio of Korean and English data.
Llama-3-Motif shows decent performance on Korean-specific benchmarks,
outperforming existing models and achieving results comparable to GPT-4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Feature Fusion Network with Text Difference Enhancement for
  Remote Sensing Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijun Zhou, Yikui Zhai, Zilu Ying, Tingfeng Xian, Wenlve Zhou, Zhiheng Zhou, Xiaolin Tian, Xudong Jia, Hongsheng Zhang, C. L. Philip Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although deep learning has advanced remote sensing change detection (RSCD),
most methods rely solely on image modality, limiting feature representation,
change pattern modeling, and generalization especially under illumination and
noise disturbances. To address this, we propose MMChange, a multimodal RSCD
method that combines image and text modalities to enhance accuracy and
robustness. An Image Feature Refinement (IFR) module is introduced to highlight
key regions and suppress environmental noise. To overcome the semantic
limitations of image features, we employ a vision language model (VLM) to
generate semantic descriptions of bitemporal images. A Textual Difference
Enhancement (TDE) module then captures fine grained semantic shifts, guiding
the model toward meaningful changes. To bridge the heterogeneity between
modalities, we design an Image Text Feature Fusion (ITFF) module that enables
deep cross modal integration. Extensive experiments on LEVIRCD, WHUCD, and
SYSUCD demonstrate that MMChange consistently surpasses state of the art
methods across multiple metrics, validating its effectiveness for multimodal
RSCD. Code is available at: https://github.com/yikuizhai/MMChange.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese
  Misinformation Fact-Checking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiling Guo, Xinwei Yang, Chen Huang, Tong Zhang, Yong Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of large language models (LLMs) to fact-check
misinformation remains uncertain, despite their growing use. To this end, we
present CANDY, a benchmark designed to systematically evaluate the capabilities
and limitations of LLMs in fact-checking Chinese misinformation. Specifically,
we curate a carefully annotated dataset of ~20k instances. Our analysis shows
that current LLMs exhibit limitations in generating accurate fact-checking
conclusions, even when enhanced with chain-of-thought reasoning and few-shot
prompting. To understand these limitations, we develop a taxonomy to categorize
flawed LLM-generated explanations for their conclusions and identify factual
fabrication as the most common failure mode. Although LLMs alone are unreliable
for fact-checking, our findings indicate their considerable potential to
augment human performance when deployed as assistive tools in scenarios. Our
dataset and code can be accessed at https://github.com/SCUNLP/CANDY
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ World Model Implanting for Test-time Adaptation of Embodied Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minjong Yoo, Jinwoo Jang, Sihyung Yoon, Honguk Woo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In embodied AI, a persistent challenge is enabling agents to robustly adapt
to novel domains without requiring extensive data collection or retraining. To
address this, we present a world model implanting framework (WorMI) that
combines the reasoning capabilities of large language models (LLMs) with
independently learned, domain-specific world models through test-time
composition. By allowing seamless implantation and removal of the world models,
the embodied agent's policy achieves and maintains cross-domain adaptability.
In the WorMI framework, we employ a prototype-based world model retrieval
approach, utilizing efficient trajectory-based abstract representation
matching, to incorporate relevant models into test-time composition. We also
develop a world-wise compound attention method that not only integrates the
knowledge from the retrieved world models but also aligns their intermediate
representations with the reasoning model's representation within the agent's
policy. This framework design effectively fuses domain-specific knowledge from
multiple world models, ensuring robust adaptation to unseen domains. We
evaluate our WorMI on the VirtualHome and ALFWorld benchmarks, demonstrating
superior zero-shot and few-shot performance compared to several LLM-based
approaches across a range of unseen domains. These results highlight the
frameworks potential for scalable, real-world deployment in embodied agent
scenarios where adaptability and data efficiency are essential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Handling Infinite Domain Parameters in <span class="highlight-title">Planning</span> Through Best-First
  Search with Delayed Partial Expansions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03953v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03953v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ángel Aso-Mollar, Diego Aineto, Enrico Scala, Eva Onaindia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In automated planning, control parameters extend standard action
representations through the introduction of continuous numeric decision
variables. Existing state-of-the-art approaches have primarily handled control
parameters as embedded constraints alongside other temporal and numeric
restrictions, and thus have implicitly treated them as additional constraints
rather than as decision points in the search space. In this paper, we propose
an efficient alternative that explicitly handles control parameters as true
decision points within a systematic search scheme. We develop a best-first,
heuristic search algorithm that operates over infinite decision spaces defined
by control parameters and prove a notion of completeness in the limit under
certain conditions. Our algorithm leverages the concept of delayed partial
expansion, where a state is not fully expanded but instead incrementally
expands a subset of its successors. Our results demonstrate that this novel
search algorithm is a competitive alternative to existing approaches for
solving planning problems involving control parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of the Thirty-Fourth International Joint
  Conference on Artificial Intelligence (IJCAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer
  <span class="highlight-title">Learning</span> in a U-Net Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alvaro Aranibar Roque, Helga Sebastian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pneumothorax, the abnormal accumulation of air in the pleural space, can be
life-threatening if undetected. Chest X-rays are the first-line diagnostic
tool, but small cases may be subtle. We propose an automated deep-learning
pipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothorax
regions. Trained on the SIIM-ACR dataset with data augmentation and a combined
binary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 and
Dice score of 0.8241 on the independent PTX-498 dataset. These results
demonstrate that the model can accurately localize pneumothoraces and support
radiologists.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 page, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based
  Role-Playing Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Wu, Liang Cao, Xinyu Wu, Zhiwei Lin, Rui Niu, Jingbei Li, Zhiyong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent significant advancements in Large Language Models (LLMs) have greatly
propelled the development of Role-Playing Conversational Agents (RPCAs). These
systems aim to create immersive user experiences through consistent persona
adoption. However, current RPCA research faces dual limitations. First,
existing work predominantly focuses on the textual modality, entirely
overlooking critical paralinguistic features including intonation, prosody, and
rhythm in speech, which are essential for conveying character emotions and
shaping vivid identities. Second, the speech-based role-playing domain suffers
from a long-standing lack of standardized evaluation benchmarks. Most current
spoken dialogue datasets target only fundamental capability assessments,
featuring thinly sketched or ill-defined character profiles. Consequently, they
fail to effectively quantify model performance on core competencies like
long-term persona consistency. To address this critical gap, we introduce
VoxRole, the first comprehensive benchmark specifically designed for the
evaluation of speech-based RPCAs. The benchmark comprises 13335 multi-turn
dialogues, totaling 65.6 hours of speech from 1228 unique characters across 261
movies. To construct this resource, we propose a novel two-stage automated
pipeline that first aligns movie audio with scripts and subsequently employs an
LLM to systematically build multi-dimensional profiles for each character.
Leveraging VoxRole, we conduct a multi-dimensional evaluation of contemporary
spoken dialogue models, revealing crucial insights into their respective
strengths and limitations in maintaining persona consistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by
  Self-Play Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Zhang, Shaoming Duan, Jinhang Su, Chuanyi Liu, Peiyi Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the significant advancements of self-play fine-tuning (SPIN), which
can transform a weak large language model (LLM) into a strong one through
competitive interactions between models of varying capabilities, it still faces
challenges in the Text-to-SQL task. SPIN does not generate new information, and
the large number of correct SQL queries produced by the opponent model during
self-play reduces the main model's ability to generate accurate SQL queries. To
address this challenge, we propose a new self-play fine-tuning method tailored
for the Text-to-SQL task, called SPFT-SQL. Prior to self-play, we introduce a
verification-based iterative fine-tuning approach, which synthesizes
high-quality fine-tuning data iteratively based on the database schema and
validation feedback to enhance model performance, while building a model base
with varying capabilities. During the self-play fine-tuning phase, we propose
an error-driven loss method that incentivizes incorrect outputs from the
opponent model, enabling the main model to distinguish between correct SQL and
erroneous SQL generated by the opponent model, thereby improving its ability to
generate correct SQL. Extensive experiments and in-depth analyses on six
open-source LLMs and five widely used benchmarks demonstrate that our approach
outperforms existing state-of-the-art (SOTA) methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented
  Generation via Distribution Self-Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03934v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03934v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqing Huang, Rongyang Zhang, Qimeng Wang, Chengqiang Lu, Yan Gao, <span class="highlight-author">Yi Wu</span>, Yao Hu, Xuyang Zhi, Guiquan Liu, Xin Li, Hao Wang, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have revolutionized
natural language processing through their remarkable capabilities in
understanding and executing diverse tasks. While supervised fine-tuning,
particularly in Retrieval-Augmented Generation (RAG) scenarios, effectively
enhances task-specific performance, it often leads to catastrophic forgetting,
where models lose their previously acquired knowledge and general capabilities.
Existing solutions either require access to general instruction data or face
limitations in preserving the model's original distribution. To overcome these
limitations, we propose SelfAug, a self-distribution alignment method that
aligns input sequence logits to preserve the model's semantic distribution,
thereby mitigating catastrophic forgetting and improving downstream
performance. Extensive experiments demonstrate that SelfAug achieves a superior
balance between downstream learning and general capability retention. Our
comprehensive empirical analysis reveals a direct correlation between
distribution shifts and the severity of catastrophic forgetting in RAG
scenarios, highlighting how the absence of RAG capabilities in general
instruction tuning leads to significant distribution shifts during fine-tuning.
Our findings not only advance the understanding of catastrophic forgetting in
RAG contexts but also provide a practical solution applicable across diverse
fine-tuning scenarios. Our code is publicly available at
https://github.com/USTC-StarTeam/SelfAug.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03918v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03918v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengxiao Tang, Yufeng Li, Zongzong Wu, Ming Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Complex Question Answering (QA) is a fundamental and challenging task in NLP.
While large language models (LLMs) exhibit impressive performance in QA, they
suffer from significant performance degradation when facing complex and
abstract QA tasks due to insufficient reasoning capabilities. Works such as
Chain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning
abilities, but they face issues such as in-layer redundancy in tree structures
and single paths in chain structures. Although some studies utilize
Retrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the
challenge of effectively utilizing large amounts of information involving
multiple entities and hops remains critical. To address this, we propose the
Matrix of Thought (MoT), a novel and efficient LLM thought structure. MoT
explores the problem in both horizontal and vertical dimensions through the
"column-cell communication" mechanism, enabling LLMs to actively engage in
multi-strategy and deep-level thinking, reducing redundancy within the column
cells and enhancing reasoning capabilities. Furthermore, we develop a
fact-correction mechanism by constructing knowledge units from retrieved
knowledge graph triples and raw text to enhance the initial knowledge for LLM
reasoning and correct erroneous answers. This leads to the development of an
efficient and accurate QA framework (MTQA). Experimental results show that our
framework outperforms state-of-the-art methods on four widely-used datasets in
terms of F1 and EM scores, with reasoning time only 14.4\% of the baseline
methods, demonstrating both its efficiency and accuracy. The code for this
framework is available at https://github.com/lyfiter/mtqa.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Foundation Model for Chest X-ray Interpretation with Grounded
  Reasoning via Online Reinforcement <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qika Lin, Yifan Zhu, Bin Pu, Ling Huang, Haoran Luo, Jingying Ma, Zhen Peng, Tianzhe Zhao, Fangzhi Xu, Jian Zhang, Kai He, Zhonghong Ou, Swapnil Mishra, Mengling Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical foundation models (FMs) have shown tremendous promise amid the rapid
advancements in artificial intelligence (AI) technologies. However, current
medical FMs typically generate answers in a black-box manner, lacking
transparent reasoning processes and locally grounded interpretability, which
hinders their practical clinical deployments. To this end, we introduce
DeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. It
leverages a sequential training pipeline: initially fine-tuned on curated CXR
instruction data to equip with fundamental CXR interpretation capabilities,
then exposed to high-quality synthetic reasoning samples to enable cold-start
reasoning, and finally refined via online reinforcement learning to enhance
both grounded reasoning quality and generation performance. Thus, the model
produces both an answer and reasoning steps tied to the image's local regions
for each query. Quantitative evaluation demonstrates substantial improvements
in report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) and
visual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent)
tasks. To facilitate robust assessment, we propose Report Arena, a benchmarking
framework using advanced language models to evaluate answer quality, further
highlighting the superiority of DeepMedix-R1. Expert review of generated
reasoning steps reveals greater interpretability and clinical plausibility
compared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overall
preference). Collectively, our work advances medical FM development toward
holistic, transparent, and clinically actionable modeling for CXR
interpretation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion <span class="highlight-title">Generative</span> Models Meet Compressed Sensing, with Applications
  to Image Data and Financial Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03898v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03898v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyi Guo, Jiatu Li, Wenpin Tang, David D. Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops dimension reduction techniques for accelerating diffusion
model inference in the context of synthetic data generation. The idea is to
integrate compressed sensing into diffusion models: (i) compress the data into
a latent space, (ii) train a diffusion model in the latent space, and (iii)
apply a compressed sensing algorithm to the samples generated in the latent
space, facilitating the efficiency of both model training and inference. Under
suitable sparsity assumptions on data, the proposed algorithm is proved to
enjoy faster convergence by combining diffusion model inference with sparse
recovery. As a byproduct, we obtain an optimal value for the latent space
dimension. We also conduct numerical experiments on a range of datasets,
including image data (handwritten digits, medical images, and climate data) and
financial time series for stress testing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FaMA: LLM-Empowered Agentic Assistant for Consumer-to-Consumer
  Marketplace 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yineng Yan, Xidong Wang, Jin Seng Cheng, Ran Hu, Wentao Guan, Nahid Farahmand, Hengte Lin, Yue Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of agentic AI, powered by Large Language Models (LLMs), marks a
paradigm shift from reactive generative systems to proactive, goal-oriented
autonomous agents capable of sophisticated planning, memory, and tool use. This
evolution presents a novel opportunity to address long-standing challenges in
complex digital environments. Core tasks on Consumer-to-Consumer (C2C)
e-commerce platforms often require users to navigate complex Graphical User
Interfaces (GUIs), making the experience time-consuming for both buyers and
sellers. This paper introduces a novel approach to simplify these interactions
through an LLM-powered agentic assistant. This agent functions as a new,
conversational entry point to the marketplace, shifting the primary interaction
model from a complex GUI to an intuitive AI agent. By interpreting natural
language commands, the agent automates key high-friction workflows. For
sellers, this includes simplified updating and renewal of listings, and the
ability to send bulk messages. For buyers, the agent facilitates a more
efficient product discovery process through conversational search. We present
the architecture for Facebook Marketplace Assistant (FaMA), arguing that this
agentic, conversational paradigm provides a lightweight and more accessible
alternative to traditional app interfaces, allowing users to manage their
marketplace activities with greater efficiency. Experiments show FaMA achieves
a 98% task success rate on solving complex tasks on the marketplace and enables
up to a 2x speedup on interaction time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reactive In-Air Clothing <span class="highlight-title">Manipulation</span> with Confidence-Aware Dense
  Correspondence and Visuotactile Affordance <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neha Sunil, Megha Tippur, Arnau Saumell, Edward Adelson, Alberto Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulating clothing is challenging due to complex configurations, variable
material dynamics, and frequent self-occlusion. Prior systems often flatten
garments or assume visibility of key features. We present a dual-arm
visuotactile framework that combines confidence-aware dense visual
correspondence and tactile-supervised grasp affordance to operate directly on
crumpled and suspended garments. The correspondence model is trained on a
custom, high-fidelity simulated dataset using a distributional loss that
captures cloth symmetries and generates correspondence confidence estimates.
These estimates guide a reactive state machine that adapts folding strategies
based on perceptual uncertainty. In parallel, a visuotactile grasp affordance
network, self-supervised using high-resolution tactile feedback, determines
which regions are physically graspable. The same tactile classifier is used
during execution for real-time grasp validation. By deferring action in
low-confidence states, the system handles highly occluded table-top and in-air
configurations. We demonstrate our task-agnostic grasp selection module in
folding and hanging tasks. Moreover, our dense descriptors provide a reusable
intermediate representation for other planning modalities, such as extracting
grasp targets from human video demonstrations, paving the way for more
generalizable and scalable garment manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CoRL 2025. Project website:
  https://mhtippur.github.io/inairclothmanipulation/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Peptidomic-Based Prediction Model for Coronary Heart Disease Using a
  Multilayer Perceptron Neural Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03884v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03884v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jesus Celis-Porras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coronary heart disease (CHD) is a leading cause of death worldwide and
contributes significantly to annual healthcare expenditures. To develop a
non-invasive diagnostic approach, we designed a model based on a multilayer
perceptron (MLP) neural network, trained on 50 key urinary peptide biomarkers
selected via genetic algorithms. Treatment and control groups, each comprising
345 individuals, were balanced using the Synthetic Minority Over-sampling
Technique (SMOTE). The neural network was trained using a stratified validation
strategy. Using a network with three hidden layers of 60 neurons each and an
output layer of two neurons, the model achieved a precision, sensitivity, and
specificity of 95.67 percent, with an F1-score of 0.9565. The area under the
ROC curve (AUC) reached 0.9748 for both classes, while the Matthews correlation
coefficient (MCC) and Cohen's kappa coefficient were 0.9134 and 0.9131,
respectively, demonstrating its reliability in detecting CHD. These results
indicate that the model provides a highly accurate and robust non-invasive
diagnostic tool for coronary heart disease.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures, Submitted to arXiv for public dissemination</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SalientFusion: Context-Aware Compositional Zero-Shot Food Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Song, Xiaoou Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Food recognition has gained significant attention, but the rapid emergence of
new dishes requires methods for recognizing unseen food categories, motivating
Zero-Shot Food Learning (ZSFL). We propose the task of Compositional Zero-Shot
Food Recognition (CZSFR), where cuisines and ingredients naturally align with
attributes and objects in Compositional Zero-Shot learning (CZSL). However,
CZSFR faces three challenges: (1) Redundant background information distracts
models from learning meaningful food features, (2) Role confusion between
staple and side dishes leads to misclassification, and (3) Semantic bias in a
single attribute can lead to confusion of understanding. Therefore, we propose
SalientFusion, a context-aware CZSFR method with two components: SalientFormer,
which removes background redundancy and uses depth features to resolve role
confusion; DebiasAT, which reduces the semantic bias by aligning prompts with
visual features. Using our proposed benchmarks, CZSFood-90 and CZSFood-164, we
show that SalientFusion achieves state-of-the-art results on these benchmarks
and the most popular general datasets for the general CZSL. The code is
avaliable at https://github.com/Jiajun-RUC/SalientFusion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>34th International Conference on Artificial Neural Networks - ICANN
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive <span class="highlight-title">Survey</span> on Trustworthiness in Reasoning with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanbo Wang, Yongcan Yu, Jian Liang, Ran He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Long-CoT reasoning has advanced LLM performance across
various tasks, including language understanding, complex problem solving, and
code generation. This paradigm enables models to generate intermediate
reasoning steps, thereby improving both accuracy and interpretability. However,
despite these advancements, a comprehensive understanding of how CoT-based
reasoning affects the trustworthiness of language models remains
underdeveloped. In this paper, we survey recent work on reasoning models and
CoT techniques, focusing on five core dimensions of trustworthy reasoning:
truthfulness, safety, robustness, fairness, and privacy. For each aspect, we
provide a clear and structured overview of recent studies in chronological
order, along with detailed analyses of their methodologies, findings, and
limitations. Future research directions are also appended at the end for
reference and discussion. Overall, while reasoning techniques hold promise for
enhancing model trustworthiness through hallucination mitigation, harmful
content detection, and robustness improvement, cutting-edge reasoning models
themselves often suffer from comparable or even greater vulnerabilities in
safety, robustness, and privacy. By synthesizing these insights, we hope this
work serves as a valuable and timely resource for the AI safety community to
stay informed on the latest progress in reasoning trustworthiness. A full list
of related papers can be found at
\href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages. This survey considers papers published up to June 30, 2025.
  Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expedition & Expansion: Leveraging <span class="highlight-title">Semantic</span> Representations for
  Goal-Directed Exploration in Continuous Cellular Automata 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03863v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03863v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sina Khajehabdollahi, Gautier Hamon, Marko Cvjetko, Pierre-Yves Oudeyer, Clément Moulin-Frier, Cédric Colas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering diverse visual patterns in continuous cellular automata (CA) is
challenging due to the vastness and redundancy of high-dimensional behavioral
spaces. Traditional exploration methods like Novelty Search (NS) expand locally
by mutating known novel solutions but often plateau when local novelty is
exhausted, failing to reach distant, unexplored regions. We introduce
Expedition and Expansion (E&E), a hybrid strategy where exploration alternates
between local novelty-driven expansions and goal-directed expeditions. During
expeditions, E&E leverages a Vision-Language Model (VLM) to generate linguistic
goals--descriptions of interesting but hypothetical patterns that drive
exploration toward uncharted regions. By operating in semantic spaces that
align with human perception, E&E both evaluates novelty and generates goals in
conceptually meaningful ways, enhancing the interpretability and relevance of
discovered behaviors. Tested on Flow Lenia, a continuous CA known for its rich,
emergent behaviors, E&E consistently uncovers more diverse solutions than
existing exploration methods. A genealogical analysis further reveals that
solutions originating from expeditions disproportionately influence long-term
exploration, unlocking new behavioral niches that serve as stepping stones for
subsequent search. These findings highlight E&E's capacity to break through
local novelty boundaries and explore behavioral landscapes in human-aligned,
interpretable ways, offering a promising template for open-ended exploration in
artificial life and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous Monitoring of Large-Scale <span class="highlight-title">Generative</span> AI via Deterministic
  Knowledge Graph Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kishor Datta Gupta, Mohd Ariful Haque, Hasmot Ali, Marufa Kamal, Syed Bahauddin Alam, Mohammad Ashiqur Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI (GEN AI) models have revolutionized diverse application domains
but present substantial challenges due to reliability concerns, including
hallucinations, semantic drift, and inherent biases. These models typically
operate as black-boxes, complicating transparent and objective evaluation.
Current evaluation methods primarily depend on subjective human assessment,
limiting scalability, transparency, and effectiveness. This research proposes a
systematic methodology using deterministic and Large Language Model
(LLM)-generated Knowledge Graphs (KGs) to continuously monitor and evaluate GEN
AI reliability. We construct two parallel KGs: (i) a deterministic KG built
using explicit rule-based methods, predefined ontologies, domain-specific
dictionaries, and structured entity-relation extraction rules, and (ii) an
LLM-generated KG dynamically derived from real-time textual data streams such
as live news articles. Utilizing real-time news streams ensures authenticity,
mitigates biases from repetitive training, and prevents adaptive LLMs from
bypassing predefined benchmarks through feedback memorization. To quantify
structural deviations and semantic discrepancies, we employ several established
KG metrics, including Instantiated Class Ratio (ICR), Instantiated Property
Ratio (IPR), and Class Instantiation (CI). An automated real-time monitoring
framework continuously computes deviations between deterministic and
LLM-generated KGs. By establishing dynamic anomaly thresholds based on
historical structural metric distributions, our method proactively identifies
and flags significant deviations, thus promptly detecting semantic anomalies or
hallucinations. This structured, metric-driven comparison between deterministic
and dynamically generated KGs delivers a robust and scalable evaluation
framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MillGNN: <span class="highlight-title">Learning</span> Multi-Scale Lead-Lag Dependencies for Multi-Variate
  Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binqing Wu, Zongjiang Shang, Jianlong Huang, Ling Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-variate time series (MTS) forecasting is crucial for various
applications. Existing methods have shown promising results owing to their
strong ability to capture intra- and inter-variate dependencies. However, these
methods often overlook lead-lag dependencies at multiple grouping scales,
failing to capture hierarchical lead-lag effects in complex systems. To this
end, we propose MillGNN, a novel \underline{g}raph \underline{n}eural
\underline{n}etwork-based method that learns \underline{m}ult\underline{i}ple
grouping scale \underline{l}ead-\underline{l}ag dependencies for MTS
forecasting, which can comprehensively capture lead-lag effects considering
variate-wise and group-wise dynamics and decays. Specifically, MillGNN
introduces two key innovations: (1) a scale-specific lead-lag graph learning
module that integrates cross-correlation coefficients and dynamic decaying
features derived from real-time inputs and time lags to learn lead-lag
dependencies for each scale, which can model evolving lead-lag dependencies
with statistical interpretability and data-driven flexibility; (2) a
hierarchical lead-lag message passing module that passes lead-lag messages at
multiple grouping scales in a structured way to simultaneously propagate intra-
and inter-scale lead-lag effects, which can capture multi-scale lead-lag
effects with a balance of comprehensiveness and efficiency. Experimental
results on 11 datasets demonstrate the superiority of MillGNN for long-term and
short-term MTS forecasting, compared with 16 state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CIKM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-Inverse Reinforcement <span class="highlight-title">Learning</span> for Mean Field Games via
  Probabilistic Context Variables 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Chen, Xiao Lin, Bo Yan, Libo Zhang, Jiamou Liu, Neset Özkan Tan, Michael Witbrock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing suitable reward functions for numerous interacting intelligent
agents is challenging in real-world applications. Inverse reinforcement
learning (IRL) in mean field games (MFGs) offers a practical framework to infer
reward functions from expert demonstrations. While promising, the assumption of
agent homogeneity limits the capability of existing methods to handle
demonstrations with heterogeneous and unknown objectives, which are common in
practice. To this end, we propose a deep latent variable MFG model and an
associated IRL method. Critically, our method can infer rewards from different
yet structurally similar tasks without prior knowledge about underlying
contexts or modifying the MFG model itself. Our experiments, conducted on
simulated scenarios and a real-world spatial taxi-ride pricing problem,
demonstrate the superiority of our approach over state-of-the-art IRL methods
in MFGs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INGRID: Intelligent <span class="highlight-title">Generative</span> <span class="highlight-title">Robot</span>ic Design Using Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanglu Jia, Ceng Zhang, Gregory S. Chirikjian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of large language models (LLMs) into robotic systems has
accelerated progress in embodied artificial intelligence, yet current
approaches remain constrained by existing robotic architectures, particularly
serial mechanisms. This hardware dependency fundamentally limits the scope of
robotic intelligence. Here, we present INGRID (Intelligent Generative Robotic
Design), a framework that enables the automated design of parallel robotic
mechanisms through deep integration with reciprocal screw theory and kinematic
synthesis methods. We decompose the design challenge into four progressive
tasks: constraint analysis, kinematic joint generation, chain construction, and
complete mechanism design. INGRID demonstrates the ability to generate novel
parallel mechanisms with both fixed and variable mobility, discovering
kinematic configurations not previously documented in the literature. We
validate our approach through three case studies demonstrating how INGRID
assists users in designing task-specific parallel robots based on desired
mobility requirements. By bridging the gap between mechanism theory and machine
learning, INGRID enables researchers without specialized robotics training to
create custom parallel mechanisms, thereby decoupling advances in robotic
intelligence from hardware constraints. This work establishes a foundation for
mechanism intelligence, where AI systems actively design robotic hardware,
potentially transforming the development of embodied AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Leiden to Pleasure Island: The Constant Potts Model for Community
  Detection as a Hedonic Game 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Lopes Felipe, Konstantin Avrachenkov, Daniel Sadoc Menasche
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Community detection is one of the fundamental problems in data science which
consists of partitioning nodes into disjoint communities. We present a
game-theoretic perspective on the Constant Potts Model (CPM) for partitioning
networks into disjoint communities, emphasizing its efficiency, robustness, and
accuracy. Efficiency: We reinterpret CPM as a potential hedonic game by
decomposing its global Hamiltonian into local utility functions, where the
local utility gain of each agent matches the corresponding increase in global
utility. Leveraging this equivalence, we prove that local optimization of the
CPM objective via better-response dynamics converges in pseudo-polynomial time
to an equilibrium partition. Robustness: We introduce and relate two stability
criteria: a strict criterion based on a novel notion of robustness, requiring
nodes to simultaneously maximize neighbors and minimize non-neighbors within
communities, and a relaxed utility function based on a weighted sum of these
objectives, controlled by a resolution parameter. Accuracy: In community
tracking scenarios, where initial partitions are used to bootstrap the Leiden
algorithm with partial ground-truth information, our experiments reveal that
robust partitions yield higher accuracy in recovering ground-truth communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript submitted to Physica A: Statistical Mechanics and its
  Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gravity Well Echo Chamber Modeling With An LLM-Based Confirmation Bias
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Jackson, Georgiy Lapin, Jeremy E. Thompson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media echo chambers play a central role in the spread of
misinformation, yet existing models often overlook the influence of individual
confirmation bias. An existing model of echo chambers is the "gravity well"
model, which creates an analog between echo chambers and spatial gravity wells.
We extend this established model by introducing a dynamic confirmation bias
variable that adjusts the strength of pull based on a user's susceptibility to
belief-reinforcing content. This variable is calculated for each user through
comparisons between their posting history and their responses to posts of a
wide range of viewpoints.
  Incorporating this factor produces a confirmation-bias-integrated gravity
well model that more accurately identifies echo chambers and reveals
community-level markers of information health. We validated the approach on
nineteen Reddit communities, demonstrating improved detection of echo chambers.
  Our contribution is a framework for systematically capturing the role of
confirmation bias in online group dynamics, enabling more effective
identification of echo chambers. By flagging these high-risk environments, the
model supports efforts to curb the spread of misinformation at its most common
points of amplification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Multidimensional AI-powered Framework for Analyzing Tourist Perception
  in Historic Urban Quarters: A Case Study in Shanghai 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizhen Tan, Yufan Wu, Yuxuan Liu, Haoran Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Historic urban quarters play a vital role in preserving cultural heritage
while serving as vibrant spaces for tourism and everyday life. Understanding
how tourists perceive these environments is essential for sustainable,
human-centered urban planning. This study proposes a multidimensional
AI-powered framework for analyzing tourist perception in historic urban
quarters using multimodal data from social media. Applied to twelve historic
quarters in central Shanghai, the framework integrates focal point extraction,
color theme analysis, and sentiment mining. Visual focus areas are identified
from tourist-shared photos using a fine-tuned semantic segmentation model. To
assess aesthetic preferences, dominant colors are extracted using a clustering
method, and their spatial distribution across quarters is analyzed. Color
themes are further compared between social media photos and real-world street
views, revealing notable shifts. This divergence highlights potential gaps
between visual expectations and the built environment, reflecting both
stylistic preferences and perceptual bias. Tourist reviews are evaluated
through a hybrid sentiment analysis approach combining a rule-based method and
a multi-task BERT model. Satisfaction is assessed across four dimensions:
tourist activities, built environment, service facilities, and business
formats. The results reveal spatial variations in aesthetic appeal and
emotional response. Rather than focusing on a single technical innovation, this
framework offers an integrated, data-driven approach to decoding tourist
perception and contributes to informed decision-making in tourism, heritage
conservation, and the design of aesthetically engaging public spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Agentic Model Context Protocol Framework for Medical Concept
  Standardization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaerong Ahn, Andrew Wen, Nan Wang, Heling Jia, Zhiyi Yue, Sunyang Fu, Hongfang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Observational Medical Outcomes Partnership (OMOP) common data model (CDM)
provides a standardized representation of heterogeneous health data to support
large-scale, multi-institutional research. One critical step in data
standardization using OMOP CDM is the mapping of source medical terms to OMOP
standard concepts, a procedure that is resource-intensive and error-prone.
While large language models (LLMs) have the potential to facilitate this
process, their tendency toward hallucination makes them unsuitable for clinical
deployment without training and expert validation. Here, we developed a
zero-training, hallucination-preventive mapping system based on the Model
Context Protocol (MCP), a standardized and secure framework allowing LLMs to
interact with external resources and tools. The system enables explainable
mapping and significantly improves efficiency and accuracy with minimal effort.
It provides real-time vocabulary lookups and structured reasoning outputs
suitable for immediate use in both exploratory and production environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Would an LLM Do? Evaluating Policymaking Capabilities of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Le Coz, Jia An Liu, Debarun Bhattacharjya, Georgina Curto, Serge Stinckwich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are increasingly being adopted in high-stakes
domains. Their capacity to process vast amounts of unstructured data, explore
flexible scenarios, and handle a diversity of contextual factors can make them
uniquely suited to provide new insights for the complexity of social
policymaking. This article evaluates whether LLMs' are aligned with domain
experts (and among themselves) to inform social policymaking on the subject of
homelessness alleviation - a challenge affecting over 150 million people
worldwide. We develop a novel benchmark comprised of decision scenarios with
policy choices across four geographies (South Bend, USA; Barcelona, Spain;
Johannesburg, South Africa; Macau SAR, China). The policies in scope are
grounded in the conceptual framework of the Capability Approach for human
development. We also present an automated pipeline that connects the
benchmarked policies to an agent-based model, and we explore the social impact
of the recommended policies through simulated social scenarios. The paper
results reveal promising potential to leverage LLMs for social policy making.
If responsible guardrails and contextual calibrations are introduced in
collaboration with local domain experts, LLMs can provide humans with valuable
insights, in the form of alternative policies at scale.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Learning</span> to Deliberate: Meta-policy Collaboration for Agentic LLMs with
  Multi-agent Reinforcement <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Yang, Jesse Thomason
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent systems of large language models (LLMs) show promise for complex
reasoning, but their effectiveness is often limited by fixed collaboration
protocols. These frameworks typically focus on macro-level orchestration while
overlooking agents' internal deliberative capabilities. This critical
meta-cognitive blindspot treats agents as passive executors unable to adapt
their strategy based on internal cognitive states like uncertainty or
confidence. We introduce the Meta-Policy Deliberation Framework (MPDF), where
agents learn a decentralized policy over a set of high-level meta-cognitive
actions: Persist, Refine, and Concede. To overcome the instability of
traditional policy gradients in this setting, we develop SoftRankPO, a novel
reinforcement learning algorithm. SoftRankPO stabilizes training by shaping
advantages based on the rank of rewards mapped through smooth normal quantiles,
making the learning process robust to reward variance. Experiments show that
MPDF with SoftRankPO achieves a a 4-5% absolute gain in average accuracy across
five mathematical and general reasoning benchmarks compared to six
state-of-the-art heuristic and learning-based multi-agent reasoning algorithms.
Our work presents a paradigm for learning adaptive, meta-cognitive policies for
multi-agent LLM systems, shifting the focus from designing fixed protocols to
learning dynamic, deliberative strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging LLM-Based Agents for Intelligent Supply Chain <span class="highlight-title">Planning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongzhi Qi, Jiaheng Yin, Jianshen Zhang, Dongyang Geng, Zhengyu Chen, Hao Hu, Wei Qi, Zuo-Jun Max Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In supply chain management, planning is a critical concept. The movement of
physical products across different categories, from suppliers to warehouse
management, to sales, and logistics transporting them to customers, entails the
involvement of many entities. It covers various aspects such as demand
forecasting, inventory management, sales operations, and replenishment. How to
collect relevant data from an e-commerce platform's perspective, formulate
long-term plans, and dynamically adjust them based on environmental changes,
while ensuring interpretability, efficiency, and reliability, is a practical
and challenging problem. In recent years, the development of AI technologies,
especially the rapid progress of large language models, has provided new tools
to address real-world issues. In this work, we construct a Supply Chain
Planning Agent (SCPA) framework that can understand domain knowledge,
comprehend the operator's needs, decompose tasks, leverage or create new tools,
and return evidence-based planning reports. We deploy this framework in
JD.com's real-world scenario, demonstrating the feasibility of LLM-agent
applications in the supply chain. It effectively reduced labor and improved
accuracy, stock availability, and other key metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Align-then-Slide: A complete evaluation framework for Ultra-Long
  Document-Level Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Guo, Daimeng Wei, Yuanchang Luo, Xiaoyu Chen, Zhanglin Wu, Huan Yang, Hengchao Shang, Zongyao Li, Zhiqiang Rao, Jinlong Yang, Hao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have ushered in a new era for document-level
machine translation (\textit{doc}-mt), yet their whole-document outputs
challenge existing evaluation methods that assume sentence-by-sentence
alignment. We introduce \textit{\textbf{Align-then-Slide}}, a complete
evaluation framework for ultra-long doc-mt. In the Align stage, we
automatically infer sentence-level source-target correspondences and rebuild
the target to match the source sentence number, resolving omissions and
many-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we
calculate averaged metric scores under 1-, 2-, 3- and 4-chunk for
multi-granularity assessment. Experiments on the WMT benchmark show a Pearson
correlation of 0.929 between our method with expert MQM rankings. On a newly
curated real-world test set, our method again aligns closely with human
judgments. Furthermore, preference data produced by Align-then-Slide enables
effective CPO training and its direct use as a reward model for GRPO, both
yielding translations preferred over a vanilla SFT baseline. The results
validate our framework as an accurate, robust, and actionable evaluation tool
for doc-mt systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under preview</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Measuring How (Not Just Whether) VLMs Build Common Ground 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saki Imai, Mert İnan, Anthony Sicilia, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision language models (VLMs) increasingly claim reasoning skills, yet
current benchmarks evaluate them in single-turn or question answering settings.
However, grounding is an interactive process in which people gradually develop
shared understanding through ongoing communication. We introduce a four-metric
suite (grounding efficiency, content alignment, lexical adaptation, and
human-likeness) to systematically evaluate VLM performance in interactive
grounding contexts. We deploy the suite on 150 self-play sessions of
interactive referential games between three proprietary VLMs and compare them
with human dyads. All three models diverge from human patterns on at least
three metrics, while GPT4o-mini is the closest overall. We find that (i) task
success scores do not indicate successful grounding and (ii) high
image-utterance alignment does not necessarily predict task success. Our metric
suite and findings offer a framework for future research on VLM grounding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAMVAD: A Multi-Agent System for Simulating Judicial Deliberation
  <span class="highlight-title">Dynamic</span>s in India 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prathamesh Devadiga, Omkaar Jayadev Shetty, Pooja Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the complexities of judicial deliberation is crucial for
assessing the efficacy and fairness of a justice system. However, empirical
studies of judicial panels are constrained by significant ethical and practical
barriers. This paper introduces SAMVAD, an innovative Multi-Agent System (MAS)
designed to simulate the deliberation process within the framework of the
Indian justice system.
  Our system comprises agents representing key judicial roles: a Judge, a
Prosecution Counsel, a Defense Counsel, and multiple Adjudicators (simulating a
judicial bench), all powered by large language models (LLMs). A primary
contribution of this work is the integration of Retrieval-Augmented Generation
(RAG), grounded in a domain-specific knowledge base of landmark Indian legal
documents, including the Indian Penal Code and the Constitution of India. This
RAG functionality enables the Judge and Counsel agents to generate legally
sound instructions and arguments, complete with source citations, thereby
enhancing both the fidelity and transparency of the simulation.
  The Adjudicator agents engage in iterative deliberation rounds, processing
case facts, legal instructions, and arguments to reach a consensus-based
verdict. We detail the system architecture, agent communication protocols, the
RAG pipeline, the simulation workflow, and a comprehensive evaluation plan
designed to assess performance, deliberation quality, and outcome consistency.
  This work provides a configurable and explainable MAS platform for exploring
legal reasoning and group decision-making dynamics in judicial simulations,
specifically tailored to the Indian legal context and augmented with verifiable
legal grounding via RAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SiLVERScore: <span class="highlight-title">Semantic</span>ally-Aware Embeddings for Sign Language Generation
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saki Imai, Mert İnan, Anthony Sicilia, Malihe Alikhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating sign language generation is often done through back-translation,
where generated signs are first recognized back to text and then compared to a
reference using text-based metrics. However, this two-step evaluation pipeline
introduces ambiguity: it not only fails to capture the multimodal nature of
sign language-such as facial expressions, spatial grammar, and prosody-but also
makes it hard to pinpoint whether evaluation errors come from sign generation
model or the translation system used to assess it. In this work, we propose
SiLVERScore, a novel semantically-aware embedding-based evaluation metric that
assesses sign language generation in a joint embedding space. Our contributions
include: (1) identifying limitations of existing metrics, (2) introducing
SiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness
to semantic and prosodic variations, and (4) exploring generalization
challenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore
achieves near-perfect discrimination between correct and random pairs (ROC AUC
= 0.99, overlap < 7%), substantially outperforming traditional metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Fundamental Structure in Reward Functions Enables Efficient
  Sparse-Reward <span class="highlight-title">Learning</span>? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What fundamental properties of reward functions enable efficient
sparse-reward reinforcement learning? We address this question through the lens
of low-rank structure in reward matrices, showing that such structure induces a
sharp transition from exponential to polynomial sample complexity, the first
result of this kind for sparse-reward RL. We introduce Policy-Aware Matrix
Completion (PAMC), which connects matrix completion theory with reinforcement
learning via a new analysis of policy-dependent sampling. Our framework
provides: (i) impossibility results for general sparse reward observation, (ii)
reward-free representation learning from dynamics, (iii) distribution-free
confidence sets via conformal prediction, and (iv) robust completion guarantees
that degrade gracefully when low-rank structure is only approximate.
Empirically, we conduct a pre-registered evaluation across 100 systematically
sampled domains, finding exploitable structure in over half. PAMC improves
sample efficiency by factors between 1.6 and 2.1 compared to strong
exploration, structured, and representation-learning baselines, while adding
only about 20 percent computational overhead.These results establish structural
reward learning as a promising new paradigm, with immediate implications for
robotics, healthcare, and other safety-critical, sample-expensive applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACING: Actor-Critic for Instruction <span class="highlight-title">Learning</span> in Black-Box LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salma Kharrat, Fares Fourati, Marco Canini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effectiveness of Large Language Models (LLMs) in solving tasks depends
significantly on the quality of their instructions, which often require
substantial human effort to craft. This underscores the need for automated
instruction optimization. However, optimizing instructions is particularly
challenging when working with black-box LLMs, where model parameters and
gradients are inaccessible. We introduce ACING, an actor-critic reinforcement
learning framework that formulates instruction optimization as a stateless,
continuous-action problem, enabling exploration of infinite instruction spaces
using only black-box feedback. ACING automatically discovers prompts that
outperform human-written prompts in 76% of instruction-induction tasks, with
gains of up to 33 points and a 10-point median improvement over the best
automatic baseline in 33 tasks spanning instruction-induction, summarization,
and chain-of-thought reasoning. Extensive ablations highlight its robustness
and efficiency. An implementation of ACING is available at
https://github.com/salmakh1/ACING.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modular Techniques for Synthetic Long-Context Data Generation in
  Language Model Training and Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seganrasan Subramanian, Abhigya Verma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of large language models (LLMs) to process and reason over long
textual inputs is critical for a wide range of real-world applications.
However, progress in this area is significantly constrained by the absence of
high-quality, diverse, and verifiable long-context datasets suitable for both
training and evaluation. This work introduces a modular, extensible framework
for synthetic long-context data generation via prompt-based interaction with
LLMs. The framework supports multiple training and alignment objectives,
including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO),
and Group Relative Policy Optimization (GRPO). It encompasses four core
generation paradigms: multi-turn conversational dialogues, document-grounded
input-output pairs, verifiable instruction-response tasks, and long-context
reasoning examples. Through templated prompting, a model-agnostic architecture,
and metadata-enriched outputs, the proposed approach facilitates scalable,
controllable, and purpose-aligned dataset creation for advancing long-context
capabilities in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Demographic-aware fine-grained classification of pediatric wrist
  fractures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.12964v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.12964v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ammar Ahmed, Ali Shariq Imran, Zenun Kastrati, Sher Muhammad Daudpota
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wrist pathologies are frequently observed, particularly among children who
constitute the majority of fracture cases. Computer vision presents a promising
avenue, contingent upon the availability of extensive datasets, a notable
challenge in medical imaging. Therefore, reliance solely on one modality, such
as images, proves inadequate, especially in an era of diverse and plentiful
data types. This study addresses the problem using a multifaceted approach:
framing it as a fine-grained recognition task, fusing patient metadata with
X-rays, and leveraging weights from a separate fine-grained dataset rather than
from a coarse-grained dataset like ImageNet. Unlike prior work, this is the
first application of metadata integration for wrist pathology recognition. Our
results show that combining fine-grained transformer approach, fine-grained
pre-training, and metadata integration improves diagnostic accuracy by 2% on
small custom curated dataset and over 10% on a larger fracture dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding Space Is Rocket Science -- Only Top Reasoning Models Can
  Solve Spatial Understanding Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Hoehing, Mayug Maniparambil, Ellen Rushe, Noel E. O'Connor, Anthony Ventresque
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose RocketScience, an open-source contrastive VLM benchmark that tests
for spatial relation understanding. It is comprised of entirely new real-world
image-text pairs covering mostly relative spatial understanding and the order
of objects. The benchmark is designed to be very easy for humans and hard for
the current generation of VLMs, and this is empirically verified. Our results
show a striking lack of spatial relation understanding in open source and
frontier commercial VLMs and a surprisingly high performance of reasoning
models. Additionally, we perform a disentanglement analysis to separate the
contributions of object localization and spatial reasoning in
chain-of-thought-based models and find that the performance on the benchmark is
bottlenecked by spatial reasoning and not object localization capabilities. We
release the dataset with a CC-BY-4.0 license and make the evaluation code
available at: https://github.com/nilshoehing/rocketscience
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiniCPM4: Ultra-Efficient LLMs on End Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.07900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.07900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengda Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Baoxi Ji, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Xin Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Lushi Pu, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Zheng Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Xiaoyue Xu, Yukun Yan, Jiarui Yuan, Jinqian Zhang, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Chuyue Zhou, Ge Zhou, Jie Zhou, Wei Zhou, Yanghao Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces MiniCPM4, a highly efficient large language model (LLM)
designed explicitly for end-side devices. We achieve this efficiency through
systematic innovation in four key dimensions: model architecture, training
data, training algorithms, and inference systems. Specifically, in terms of
model architecture, we propose InfLLM v2, a trainable sparse attention
mechanism that accelerates both prefilling and decoding phases for long-context
processing. Regarding training data, we propose UltraClean, an efficient and
accurate pre-training data filtering and generation strategy, and UltraChat v2,
a comprehensive supervised fine-tuning dataset. These datasets enable
satisfactory model performance to be achieved using just 8 trillion training
tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient
pre-training strategy search, and improve existing post-training methods by
introducing chunk-wise rollout for load-balanced reinforcement learning and
data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose
CPM.cu that integrates sparse attention, model quantization, and speculative
sampling to achieve efficient prefilling and decoding. To meet diverse
on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B
parameters, respectively. Furthermore, we construct a hybrid reasoning model,
MiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning
mode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform
similar-sized open-source models across benchmarks, with the 8B variants
showing significant speed improvements on long sequence understanding and
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MiniCPM4 Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pilot Study on <span class="highlight-title">Generative</span> AI and Critical Thinking in Higher Education
  Classrooms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        W. F. Lamberti, S. R. Lawrence, D. White, S. Kim, S. Abdullah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI (GAI) tools have seen rapid adoption in educational settings,
yet their role in fostering critical thinking remains underexplored. While
previous studies have examined GAI as a tutor for specific lessons or as a tool
for completing assignments, few have addressed how students critically evaluate
the accuracy and appropriateness of GAI-generated responses. This pilot study
investigates students' ability to apply structured critical thinking when
assessing Generative AI outputs in introductory Computational and Data Science
courses. Given that GAI tools often produce contextually flawed or factually
incorrect answers, we designed learning activities that require students to
analyze, critique, and revise AI-generated solutions. Our findings offer
initial insights into students' ability to engage critically with GAI content
and lay the groundwork for more comprehensive studies in future semesters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transplant Then Regenerate: A New Paradigm for Text Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.14723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.14723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzhan Wang, Hongyu Zhang, Beijun Shen, Xiaodong Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is a critical technique in deep learning. Traditional
methods like Back-translation typically focus on lexical-level rephrasing,
which primarily produces variations with the same semantics. While large
language models (LLMs) have enhanced text augmentation by their "knowledge
emergence" capability, controlling the style and structure of these outputs
remains challenging and requires meticulous prompt engineering. In this paper,
we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.
The core idea of LMTransplant is transplant-then-regenerate: incorporating seed
text into a context expanded by LLM, and asking the LLM to regenerate a variant
based on the expanded context. This strategy allows the model to create more
diverse and creative content-level variants by fully leveraging the knowledge
embedded in LLMs, while preserving the core attributes of the original text. We
evaluate LMTransplant across various text-related tasks, demonstrating its
superior performance over existing text augmentation methods. Moreover,
LMTransplant demonstrates exceptional scalability as the size of augmented data
grows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomation, Not Automation: Activities and Needs of European
  Fact-checkers as a Basis for Designing Human-Centered AI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12143v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12143v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Hrckova, Robert Moro, Ivan Srba, Jakub Simko, Maria Bielikova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To mitigate the negative effects of false information more effectively, the
development of Artificial Intelligence (AI) systems to assist fact-checkers is
needed. Nevertheless, the lack of focus on the needs of these stakeholders
results in their limited acceptance and skepticism toward automating the whole
fact-checking process. In this study, we conducted semi-structured in-depth
interviews with Central European fact-checkers. Their activities and problems
were analyzed using iterative content analysis. The most significant problems
were validated with a survey of European fact-checkers, in which we collected
24 responses from 20 countries, i.e., 62% of active European signatories of the
International Fact-Checking Network (IFCN). Our contributions include an
in-depth examination of the variability of fact-checking work in
non-English-speaking regions, which still remained largely uncovered. By
aligning them with the knowledge from prior studies, we created conceptual
models that help to understand the fact-checking processes. In addition, we
mapped our findings on the fact-checkers' activities and needs to the relevant
tasks for AI research, while providing a discussion on three AI tasks that were
not covered by previous similar studies. The new opportunities identified for
AI researchers and developers have implications for the focus of AI research in
this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 13 figures, 2 annexes. Accepted to ACM Journal on
  Responsible Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Plan Verification for LLM-Based Embodied Task Completion Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ananth Hariharan, Vardhan Dongre, Dilek Hakkani-Tür, Gokhan Tur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) based task plans and corresponding human
demonstrations for embodied AI may be noisy, with unnecessary actions,
redundant navigation, and logical errors that reduce policy quality. We propose
an iterative verification framework in which a Judge LLM critiques action
sequences and a Planner LLM applies the revisions, yielding progressively
cleaner and more spatially coherent trajectories. Unlike rule-based approaches,
our method relies on natural language prompting, enabling broad generalization
across error types including irrelevant actions, contradictions, and missing
steps. On a set of manually annotated actions from the TEACh embodied AI
dataset, our framework achieves up to 90% recall and 100% precision across four
state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout).
The refinement loop converges quickly, with 96.5% of sequences requiring at
most three iterations, while improving both temporal efficiency and spatial
action organization. Crucially, the method preserves human error-recovery
patterns rather than collapsing them, supporting future work on robust
corrective behavior. By establishing plan verification as a reliable LLM
capability for spatial planning and action refinement, we provide a scalable
path to higher-quality training data for imitation learning in embodied AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Depth-Breadth Synergy in RLVR: Unlocking LLM Reasoning Gains with
  Adaptive Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.13755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.13755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhicheng Yang, Zhijiang Guo, Yinya Huang, Yongxin Wang, Dongchun Xie, Yiwei Wang, Xiaodan Liang, Jing Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Verifiable Reward (RLVR) has emerged as a
powerful paradigm for unlocking reasoning capabilities in large language
models, yet its full potential is hindered by two under-explored dimensions:
Depth-the hardest problem a model can sample; Breadth-the number of instances
consumed in a single iteration. We dissect the popular GRPO algorithm and
reveal a systematic bias: the cumulative-advantage disproportionately weights
samples with medium accuracy, while down-weighting the low-accuracy instances
that are crucial for pushing reasoning boundaries. To rectify the depth
neglect, we introduce Difficulty Adaptive Rollout Sampling (DARS), which
re-weights hard problems through targeted multi-stage rollouts, thereby
increasing the number of positive rollouts for hard problems. Empirically,
naively enlarging rollout size only accelerates convergence and even hurts
Pass@K. Our DARS, in contrast, delivers consistent Pass@K gains without extra
inference cost at convergence. Just as we adaptively expanded the depth of
exploration, we now ask whether aggressively scaling the breadth of training
data can further amplify reasoning gains. To this end, we intensely scale batch
size and replace PPO's mini-batch iterations with full-batch updates over
multiple epochs. Increasing breadth significantly enhances Pass@1 performance.
Large-breadth training sustains high token-level entropy, indicating continued
exploration and reduced gradient noise. We further present DARS-B, which
augments DARS with large breadth, and demonstrate simultaneous gains in Pass@K
and Pass@1. The results confirm that breadth and adaptive exploration across
depth operate as orthogonal dimensions in RLVR, which are key to unleashing the
reasoning power of RLVR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The KG-ER Conceptual Schema Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrico Franconi, Benoît Groz, Jan Hidders, Nina Pardal, Sławek Staworko, Jan Van den Bussche, Piotr Wieczorek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose KG-ER, a conceptual schema language for knowledge graphs that
describes the structure of knowledge graphs independently of their
representation (relational databases, property graphs, RDF) while helping to
capture the semantics of the information stored in a knowledge graph.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Embedding Sampling Method for Diverse Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sania Waheed, Na Min An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Captioning for state-of-the-art VLMs has significantly improved over
time; however, this comes at the cost of increased computational complexity,
making them less accessible for resource-constrained applications such as
mobile devices and assistive technologies. Alternatively, comparably smaller
VLMs prioritize high-level scene descriptions, overlooking finer details that
contribute to a richer understanding of an image. In this paper, we introduce a
training-free framework that enhances caption diversity and informativeness by
explicitly attending to distinct image regions using a comparably small VLM,
BLIP, as the backbone. Our approach leverages structured segmentation to
produce hierarchical representations that capture both global and localized
semantics. Without requiring additional model training, we demonstrate that our
method allows smaller VLMs to achieve performance comparable to larger models
in terms of image-caption alignment, semantic integrity, and diversity. We
evaluate our framework on MSCOCO, Flickr30k, and Nocaps test datasets,
achieving a Div-2 score of 0.735, 0.750, and 0.748 for each dataset,
respectively, while maintaining strong image-caption relevancy and semantic
integrity with the human-annotated captions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rapid Word <span class="highlight-title">Learning</span> Through Meta In-Context <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14791v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14791v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Wang, Guangyuan Jiang, Tal Linzen, Brenden M. Lake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can quickly learn a new word from a few illustrative examples, and
then systematically and flexibly use it in novel contexts. Yet the abilities of
current language models for few-shot word learning, and methods for improving
these abilities, are underexplored. In this study, we introduce a novel method,
Meta-training for IN-context learNing Of Words (Minnow). This method trains
language models to generate new examples of a word's usage given a few
in-context examples, using a special placeholder token to represent the new
word. This training is repeated on many new words to develop a general
word-learning ability. We find that training models from scratch with Minnow on
human-scale child-directed language enables strong few-shot word learning,
comparable to a large language model (LLM) pre-trained on orders of magnitude
more data. Furthermore, through discriminative and generative evaluations, we
demonstrate that finetuning pre-trained LLMs with Minnow improves their ability
to discriminate between new words, identify syntactic categories of new words,
and generate reasonable new usages and definitions for new words, based on one
or a few in-context examples. These findings highlight the data efficiency of
Minnow and its potential to improve language model performance in word learning
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Style Transfer to Calvin and Hobbes comics using Stable Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asvin Kumar Venkataramanan, Sloke Shrestha, Sundar Sripada Venugopalaswamy Sriraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This project report summarizes our journey to perform stable diffusion
fine-tuning on a dataset containing Calvin and Hobbes comics. The purpose is to
convert any given input image into the comic style of Calvin and Hobbes,
essentially performing style transfer. We train stable-diffusion-v1.5 using Low
Rank Adaptation (LoRA) to efficiently speed up the fine-tuning process. The
diffusion itself is handled by a Variational Autoencoder (VAE), which is a
U-net. Our results were visually appealing for the amount of training time and
the quality of input data that went into training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated authorship</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Street-Level AI: Are Large Language Models Ready for Real-World
  Judgments? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.08193v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.08193v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaurab Pokharel, Shafkat Farabi, Patrick J. Fowler, Sanmay Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A surge of recent work explores the ethical and societal implications of
large-scale AI models that make "moral" judgments. Much of this literature
focuses either on alignment with human judgments through various thought
experiments or on the group fairness implications of AI judgments. However, the
most immediate and likely use of AI is to help or fully replace the so-called
street-level bureaucrats, the individuals deciding to allocate scarce social
resources or approve benefits. There is a rich history underlying how
principles of local justice determine how society decides on prioritization
mechanisms in such domains. In this paper, we examine how well LLM judgments
align with human judgments, as well as with socially and politically determined
vulnerability scoring systems currently used in the domain of homelessness
resource allocation. Crucially, we use real data on those needing services
(maintaining strict confidentiality by only using local large models) to
perform our analyses. We find that LLM prioritizations are extremely
inconsistent in several ways: internally on different runs, between different
LLMs, and between LLMs and the vulnerability scoring systems. At the same time,
LLMs demonstrate qualitative consistency with lay human judgments in pairwise
testing. Findings call into question the readiness of current generation AI
systems for naive integration in high-stakes societal decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted for publication as a full paper at the
  AAAI/ACM Conference on AI, Ethics, and Society (AIES 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unisolver: PDE-Conditional Transformers Towards Universal Neural PDE
  Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17527v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17527v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Zhou, Yuezhou Ma, Haixu Wu, Haowen Wang, Mingsheng Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep models have recently emerged as promising tools to solve partial
differential equations (PDEs), known as neural PDE solvers. While neural
solvers trained from either simulation data or physics-informed loss can solve
PDEs reasonably well, they are mainly restricted to a few instances of PDEs,
e.g. a certain equation with a limited set of coefficients. This limits their
generalization to diverse PDEs, preventing them from being practical surrogate
models of numerical solvers. In this paper, we present Unisolver, a novel
Transformer model trained on diverse data and conditioned on diverse PDEs,
aiming towards a universal neural PDE solver capable of solving a wide scope of
PDEs. Instead of purely scaling up data and parameters, Unisolver stems from
the theoretical analysis of the PDE-solving process. Inspired by the
mathematical structure of PDEs that a PDE solution is fundamentally governed by
a series of PDE components such as equation symbols and boundary conditions, we
define a complete set of PDE components and flexibly embed them as domain-wise
and point-wise deep conditions for Transformer PDE solvers. Integrating
physical insights with recent Transformer advances, Unisolver achieves
consistent state-of-the-art on three challenging large-scale benchmarks,
showing impressive performance and generalizability. Code is available at
https://github.com/thuml/Unisolver.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AudioCodecBench: A Comprehensive Benchmark for Audio Codec Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Wang, Hao Chen, Siyu Wu, Zhiyue Wu, Hao Zhou, Chengfeng Zhang, Ting Wang, Haodi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have been widely applied in speech
and music. This tendency has led to a focus on audio tokenization for Large
Models (LMs). Unlike semantic-only text tokens, audio tokens must both capture
global semantic content and preserve fine-grained acoustic details. Moreover,
they provide a discrete method for speech and music that can be effectively
integrated into MLLMs. However, existing research is unsuitable in the
definitions of semantic tokens and acoustic tokens. In addition, the evaluation
of different codecs typically concentrates on specific domains or tasks, such
as reconstruction or Automatic Speech Recognition (ASR) task, which prevents
fair and comprehensive comparisons. To address these problems, this paper
provides suitable definitions for semantic and acoustic tokens and introduces a
systematic evaluation framework. This framework allows for a comprehensive
assessment of codecs' capabilities which evaluate across four dimensions: audio
reconstruction metric, codebook index (ID) stability, decoder-only transformer
perplexity, and performance on downstream probe tasks. Our results show the
correctness of the provided suitable definitions and the correlation among
reconstruction metrics, codebook ID stability, downstream probe tasks and
perplexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Graph Retrieval-Augmented Generation for Customized Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13958v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13958v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen, Yilin Xiao, Chuang Zhou, Yi Chang, Xiao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable capabilities in a
wide range of tasks, yet their application to specialized domains remains
challenging due to the need for deep expertise. Retrieval-Augmented generation
(RAG) has emerged as a promising solution to customize LLMs for professional
fields by seamlessly integrating external knowledge bases, enabling real-time
access to domain-specific expertise during inference. Despite its potential,
traditional RAG systems, based on flat text retrieval, face three critical
challenges: (i) complex query understanding in professional contexts, (ii)
difficulties in knowledge integration across distributed sources, and (iii)
system efficiency bottlenecks at scale. This survey presents a systematic
analysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new
paradigm that revolutionizes domain-specific LLM applications. GraphRAG
addresses traditional RAG limitations through three key innovations: (i)
graph-structured knowledge representation that explicitly captures entity
relationships and domain hierarchies, (ii) efficient graph-based retrieval
techniques that enable context-preserving knowledge retrieval with multihop
reasoning ability, and (iii) structure-aware knowledge integration algorithms
that leverage retrieved knowledge for accurate and logical coherent generation
of LLMs. In this survey, we systematically analyze the technical foundations of
GraphRAG and examine current implementations across various professional
domains, identifying key technical challenges and promising research
directions. All the related resources of GraphRAG, including research papers,
open-source data, and projects, are collected for the community in
https://github.com/DEEP-PolyU/Awesome-GraphRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoPETIII: The Tracer Frontier. What Frontier? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02807v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02807v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zacharia Mesbah, Léo Mottay, Romain Modzelewski, Pierre Decazes, Sébastien Hapdey, Su Ruan, Sébastien Thureau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For the last three years, the AutoPET competition gathered the medical
imaging community around a hot topic: lesion segmentation on Positron Emitting
Tomography (PET) scans. Each year a different aspect of the problem is
presented; in 2024 the multiplicity of existing and used tracers was at the
core of the challenge. Specifically, this year's edition aims to develop a
fully automatic algorithm capable of performing lesion segmentation on a PET/CT
scan, without knowing the tracer, which can either be a FDG or PSMA-based
tracer. In this paper we describe how we used the nnUNetv2 framework to train
two sets of 6 fold ensembles of models to perform fully automatic PET/CT lesion
segmentation as well as a MIP-CNN to choose which set of models to use for
segmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DMN-Guided Prompting: A Framework for Controlling LLM Behavior 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.11701v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.11701v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaghayegh Abedi, Amin Jalali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown considerable potential in automating
decision logic within knowledge-intensive processes. However, their
effectiveness largely depends on the strategy and quality of prompting. Since
decision logic is typically embedded in prompts, it becomes challenging for end
users to modify or refine it. Decision Model and Notation (DMN) offers a
standardized graphical approach for defining decision logic in a structured,
user-friendly manner. This paper introduces a DMN-guided prompting framework
that breaks down complex decision logic into smaller, manageable components,
guiding LLMs through structured decision pathways. We implemented the framework
in a graduate-level course where students submitted assignments. The
assignments and DMN models representing feedback instructions served as inputs
to our framework. The instructor evaluated the generated feedback and labeled
it for performance assessment. Our approach demonstrated promising results,
outperforming chain-of-thought (CoT) prompting in our case study. Students also
responded positively to the generated feedback, reporting high levels of
perceived usefulness in a survey based on the Technology Acceptance Model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Large Language Models, Decision Model and Notation, Automated
  Feedback, Prompt Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FFHFlow: Diverse and Uncertainty-Aware Dexterous Grasp Generation via
  Flow Variational Inference <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15161v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15161v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Feng, Jianxiang Feng, Zhaopeng Chen, Rudolph Triebel, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthesizing diverse, uncertainty-aware grasps for multi-fingered hands from
partial observations remains a critical challenge in robot learning. Prior
generative methods struggle to model the intricate grasp distribution of
dexterous hands and often fail to reason about shape uncertainty inherent in
partial point clouds, leading to unreliable or overly conservative grasps. We
propose FFHFlow, a flow-based variational framework that generates diverse,
robust multi-finger grasps while explicitly quantifying perceptual uncertainty
in the partial point clouds. Our approach leverages a normalizing flow-based
deep latent variable model to learn a hierarchical grasp manifold, overcoming
the mode collapse and rigid prior limitations of conditional Variational
Autoencoders (cVAEs). By exploiting the invertibility and exact likelihoods of
flows, FFHFlow introspects shape uncertainty in partial observations and
identifies novel object structures, enabling risk-aware grasp synthesis. To
further enhance reliability, we integrate a discriminative grasp evaluator with
the flow likelihoods, formulating an uncertainty-aware ranking strategy that
prioritizes grasps robust to shape ambiguity. Extensive experiments in
simulation and real-world setups demonstrate that FFHFlow outperforms
state-of-the-art baselines (including diffusion models) in grasp diversity and
success rate, while achieving run-time efficient sampling. We also showcase its
practical value in cluttered and confined environments, where diversity-driven
sampling excels by mitigating collisions (Project Page:
https://sites.google.com/view/ffhflow/home/).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First two authors contributed equally, whose ordering decided via
  coin-tossing. Accepted for CoRL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kolb-Based Experiential <span class="highlight-title">Learning</span> for Generalist Agents with Human-Level
  Kaggle Data Science Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03562v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03562v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antoine Grosnit, Alexandre Maraval, Refinath S N, Zichao Zhao, James Dora, Giuseppe Paolo, Albert Thomas, Jonas Gonzalez, Abhineet Kumar, Khyati Khandelwal, Abdelhakim Benechehab, Hamza Cherkaoui, Youssef Attia El-Hili, Kun Shao, Jianye Hao, Jun Yao, Balázs Kégl, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human expertise emerges through iterative cycles of interaction, reflection,
and internal model updating, which are central to cognitive theories such as
Kolb's experiential learning and Vygotsky's zone of proximal development. In
contrast, current AI systems, particularly LLM agents, rely on static
pre-training or rigid workflows, lacking mechanisms for continual adaptation.
Recent studies identified early cognitive traits in LLM agents (reflection,
revision, and self-correction) suggesting foundational elements of human-like
experiential learning. Thus the key question: Can we design LLM agents capable
of structured, cognitively grounded learning similar to human processes? In
response, we propose a computational framework of Kolb's learning cycle with
Vygotsky's ZPD for autonomous agents. Our architecture separates extrinsic
(environment interaction) and intrinsic (internal reflection/abstraction)
functions, enabling cognitively grounded scaffolded learning, where the agent
initially learns within structured environments, followed by open-ended
generalisation. This approach empowers agents to master complex tasks ; domains
that traditional fine-tuning or simple reflective methods could not tackle
effectively. Its potential is powerfully demonstrated via direct comparison
with humans in real-world Kaggle data science competitions. Learning fully
automated data science code generation across 81 tasks, our system, Agent K,
demonstrated the ability to perform the entire workflow autonomously, achieving
an Elo-MMR score of 1694, beyond median score of the Kaggle Masters (the top 2%
among 200,000 users) of our study. With 9 gold, 8 silver, and 12 bronze medals
level performance - including 4 gold and 4 silver on prize-awarding
competitions - Agent K is the 1st AI system to successfully integrate Kolb- and
Vygotsky-inspired human cognitive learning, marking a major step toward
generalist AI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StreetViewAI: Making Street View Accessible Using Context-Aware
  Multimodal AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.08524v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.08524v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jon E. Froehlich, Alexander Fiannaca, Nimer Jaber, Victor Tsaran, Shaun Kane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive streetscape mapping tools such as Google Street View (GSV) and
Meta Mapillary enable users to virtually navigate and experience real-world
environments via immersive 360{\deg} imagery but remain fundamentally
inaccessible to blind users. We introduce StreetViewAI, the first-ever
accessible street view tool, which combines context-aware, multimodal AI,
accessible navigation controls, and conversational speech. With StreetViewAI,
blind users can virtually examine destinations, engage in open-world
exploration, or virtually tour any of the over 220 billion images and 100+
countries where GSV is deployed. We iteratively designed StreetViewAI with a
mixed-visual ability team and performed an evaluation with eleven blind users.
Our findings demonstrate the value of an accessible street view in supporting
POI investigations and remote route planning. We close by enumerating key
guidelines for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to UIST'25 v2. Fixed a missing word in the PDF v3. Fixed a
  typo in an author's name</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theory of Mind Using Active Inference: A Framework for Multi-Agent
  Cooperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.00401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.00401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riddhi J. Pitliya, Ozan Çatal, Toon Van de Maele, Corrado Pezzato, Tim Verbelen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Theory of Mind (ToM) -- the ability to understand that others can have
differing knowledge and goals -- enables agents to reason about others' beliefs
while planning their own actions. We present a novel approach to multi-agent
cooperation by implementing ToM within active inference. Unlike previous active
inference approaches to multi-agent cooperation, our method neither relies on
task-specific shared generative models nor requires explicit communication. In
our framework, ToM-equipped agents maintain distinct representations of their
own and others' beliefs and goals. ToM agents then use an extended and adapted
version of the sophisticated inference tree-based planning algorithm to
systematically explore joint policy spaces through recursive reasoning. We
evaluate our approach through collision avoidance and foraging simulations.
Results suggest that ToM agents cooperate better compared to non-ToM
counterparts by being able to avoid collisions and reduce redundant efforts.
Crucially, ToM agents accomplish this by inferring others' beliefs solely from
observable behaviour and considering them when planning their own actions. Our
approach shows potential for generalisable and scalable multi-agent systems
while providing computational insights into ToM mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TriCLIP-3D: A Unified Parameter-Efficient Framework for Tri-Modal 3D
  <span class="highlight-title">Visual</span> Grounding based on CLIP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.14904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.14904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Li, Zanyi Wang, Zeyi Huang, Guang Dai, Jingdong Wang, Mengmeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D visual grounding allows an embodied agent to understand visual information
in real-world 3D environments based on human instructions, which is crucial for
embodied intelligence. Existing 3D visual grounding methods typically rely on
separate encoders for different modalities (e.g., RGB images, text, and 3D
point clouds), resulting in large and complex models that are inefficient to
train. While some approaches use pre-trained 2D multi-modal models like CLIP
for 3D tasks, they still struggle with aligning point cloud data to 2D
encoders. As a result, these methods continue to depend on 3D encoders for
feature extraction, further increasing model complexity and training
inefficiency. In this paper, we propose a unified 2D pre-trained multi-modal
network to process all three modalities (RGB images, text, and point clouds),
significantly simplifying the architecture. By leveraging a 2D CLIP bi-modal
model with adapter-based fine-tuning, this framework effectively adapts to the
tri-modal setting, improving both adaptability and performance across
modalities. Our Geometric-Aware 2D-3D Feature Recovery and Fusion (GARF) module
is designed to fuse geometric multi-scale features from point clouds and
images. We then integrate textual features for final modality fusion and
introduce a multi-modal decoder to facilitate deep cross-modal understanding.
Together, our method achieves unified feature extraction and fusion across the
three modalities, enabling an end-to-end 3D visual grounding model. Compared to
the baseline, our method reduces the number of trainable parameters by
approximately 58\%, while achieving a 6.52\% improvement in the 3D detection
task and a 6.25\% improvement in the 3D visual grounding task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text
  Pairing <span class="chip">ICCV25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.22627v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.22627v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Girella, Davide Talon, Ziyue Liu, Zanxi Ruan, Yiming Wang, Marco Cristani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fashion design is a complex creative process that blends visual and textual
expressions. Designers convey ideas through sketches, which define spatial
structure and design elements, and textual descriptions, capturing material,
texture, and stylistic details. In this paper, we present LOcalized Text and
Sketch for fashion image generation (LOTS), an approach for compositional
sketch-text based generation of complete fashion outlooks. LOTS leverages a
global description with paired localized sketch + text information for
conditioning and introduces a novel step-based merging strategy for diffusion
adaptation. First, a Modularized Pair-Centric representation encodes sketches
and text into a shared latent space while preserving independent localized
features; then, a Diffusion Pair Guidance phase integrates both local and
global conditioning via attention-based guidance within the diffusion model's
multi-step denoising process. To validate our method, we build on Fashionpedia
to release Sketchy, the first fashion dataset where multiple text-sketch pairs
are provided per image. Quantitative results show LOTS achieves
state-of-the-art image generation performance on both global and localized
metrics, while qualitative examples and a human evaluation study highlight its
unprecedented level of design customization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICCV25 (Oral). Project page:
  https://intelligolabs.github.io/lots/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepVIS: Bridging Natural Language and Data <span class="highlight-title">Visual</span>ization Through
  Step-wise Reasoning <span class="chip">IEEE VIS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01700v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01700v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Shuai, Boyan Li, Siyu Yan, Yuyu Luo, Weikai Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although data visualization is powerful for revealing patterns and
communicating insights, creating effective visualizations requires familiarity
with authoring tools and often disrupts the analysis flow. While large language
models show promise for automatically converting analysis intent into
visualizations, existing methods function as black boxes without transparent
reasoning processes, which prevents users from understanding design rationales
and refining suboptimal outputs. To bridge this gap, we propose integrating
Chain-of-Thought (CoT) reasoning into the Natural Language to Visualization
(NL2VIS) pipeline. First, we design a comprehensive CoT reasoning process for
NL2VIS and develop an automatic pipeline to equip existing datasets with
structured reasoning steps. Second, we introduce nvBench-CoT, a specialized
dataset capturing detailed step-by-step reasoning from ambiguous natural
language descriptions to finalized visualizations, which enables
state-of-the-art performance when used for model fine-tuning. Third, we develop
DeepVIS, an interactive visual interface that tightly integrates with the CoT
reasoning process, allowing users to inspect reasoning steps, identify errors,
and make targeted adjustments to improve visualization outcomes. Quantitative
benchmark evaluations, two use cases, and a user study collectively demonstrate
that our CoT framework effectively enhances NL2VIS quality while providing
insightful reasoning steps to users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE VIS 2025 full paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ First Order Model-Based RL through Decoupled Backpropagation <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Amigo, Rooholla Khorrambakht, Elliot Chane-Sane, Nicolas Mansard, Ludovic Righetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is growing interest in reinforcement learning (RL) methods that
leverage the simulator's derivatives to improve learning efficiency. While
early gradient-based approaches have demonstrated superior performance compared
to derivative-free methods, accessing simulator gradients is often impractical
due to their implementation cost or unavailability. Model-based RL (MBRL) can
approximate these gradients via learned dynamics models, but the solver
efficiency suffers from compounding prediction errors during training rollouts,
which can degrade policy performance. We propose an approach that decouples
trajectory generation from gradient computation: trajectories are unrolled
using a simulator, while gradients are computed via backpropagation through a
learned differentiable model of the simulator. This hybrid design enables
efficient and consistent first-order policy optimization, even when simulator
gradients are unavailable, as well as learning a critic from simulation
rollouts, which is more accurate. Our method achieves the sample efficiency and
speed of specialized optimizers such as SHAC, while maintaining the generality
of standard approaches like PPO and avoiding ill behaviors observed in other
first-order MBRL methods. We empirically validate our algorithm on benchmark
control tasks and demonstrate its effectiveness on a real Go2 quadruped robot,
across both quadrupedal and bipedal locomotion tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoRL 2025. Project website: https://machines-in-motion.github.io/DMO/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span> training of implicit <span class="highlight-title">generative</span> models for multivariate and
  heavy-tailed distributions with an invariant statistical loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22381v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22381v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José Manuel de Frutos, Manuel A. Vázquez, Pablo Olmos, Joaquín Míguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional implicit generative models are capable of learning highly complex
data distributions. However, their training involves distinguishing real data
from synthetically generated data using adversarial discriminators, which can
lead to unstable training dynamics and mode dropping issues. In this work, we
build on the \textit{invariant statistical loss} (ISL) method introduced in
\cite{de2024training}, and extend it to handle heavy-tailed and multivariate
data distributions.
  The data generated by many real-world phenomena can only be properly
characterised using heavy-tailed probability distributions, and traditional
implicit methods struggle to effectively capture their asymptotic behavior. To
address this problem, we introduce a generator trained with ISL, that uses
input noise from a generalised Pareto distribution (GPD). We refer to this
generative scheme as Pareto-ISL for conciseness. Our experiments demonstrate
that Pareto-ISL accurately models the tails of the distributions while still
effectively capturing their central characteristics.
  The original ISL function was conceived for 1D data sets. When the actual
data is $n$-dimensional, a straightforward extension of the method was obtained
by targeting the $n$ marginal distributions of the data. This approach is
computationally infeasible and ineffective in high-dimensional spaces. To
overcome this, we extend the 1D approach using random projections and define a
new loss function suited for multivariate data, keeping problems tractable by
adjusting the number of projections. We assess its performance in
multidimensional generative modeling and explore its potential as a pretraining
technique for generative adversarial networks (GANs) to prevent mode collapse,
reporting promising results and highlighting its robustness across various
hyperparameter settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Calibration Error in Neural Networks Through Evidence-Based
  Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00265v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00265v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koffi Ismael Ouattara, Ioannis Krontiris, Theo Dimitrakos, Frank Kargl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trustworthiness in neural networks is crucial for their deployment in
critical applications, where reliability, confidence, and uncertainty play
pivotal roles in decision-making. Traditional performance metrics such as
accuracy and precision fail to capture these aspects, particularly in cases
where models exhibit overconfidence. To address these limitations, this paper
introduces a novel framework for quantifying the trustworthiness of neural
networks by incorporating subjective logic into the evaluation of Expected
Calibration Error (ECE). This method provides a comprehensive measure of trust,
disbelief, and uncertainty by clustering predicted probabilities and fusing
opinions using appropriate fusion operators. We demonstrate the effectiveness
of this approach through experiments on MNIST and CIFAR-10 datasets, where
post-calibration results indicate improved trustworthiness. The proposed
framework offers a more interpretable and nuanced assessment of AI models, with
potential applications in sensitive domains such as healthcare and autonomous
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the preprint of the paper accepted to Fusion 2025 (28th
  International Conference on Information Fusion, Rio de Janeiro, Brazil, July
  7-10, 2025). The published version is available at
  https://doi.org/10.23919/FUSION65864.2025.11124121</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extending FKG.in: Towards a Food Claim Traceability Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.16117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.16117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saransh Kumar Gupta, Rizwan Gulzar Mir, Lipika Dey, Partha Pratim Das, Anirban Sen, Ramesh Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The global food landscape is rife with scientific, cultural, and commercial
claims about what foods are, what they do, what they should not do, or should
not do. These range from rigorously studied health benefits (probiotics improve
gut health) and misrepresentations (soaked almonds make one smarter) to vague
promises (superfoods boost immunity) and culturally rooted beliefs (cold foods
cause coughs). Despite their widespread influence, the infrastructure for
tracing, verifying, and contextualizing these claims remains fragmented and
underdeveloped. In this paper, we propose a Food Claim-Traceability Network
(FCN) as an extension of FKG[.]in, a knowledge graph of Indian food that we
have been incrementally building. We also present the ontology design and the
semi-automated knowledge curation workflow that we used to develop a proof of
concept of FKG[.]in-FCN using Reddit data and Large Language Models. FCN
integrates curated data inputs, structured schemas, and provenance-aware
pipelines for food-related claim extraction and validation. While directly
linked to the Indian food knowledge graph as an application, our methodology
remains application-agnostic and adaptable to other geographic, culinary, or
regulatory settings. By modeling food claims and their traceability in a
structured, verifiable, and explainable way, we aim to contribute to more
transparent and accountable food knowledge ecosystems, supporting researchers,
policymakers, and most importantly, everyday consumers in navigating a world
saturated with dietary assertions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 1 table, 45 references, ACM International
  Conference on Multimedia 2025 - Multi-modal Food Computing Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling
  Paradigms for Text-to-Music Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08570v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08570v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Or Tal, Felix Kreuk, Yossi Adi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in text-to-music generation has enabled models to synthesize
high-quality musical segments, full compositions, and even respond to
fine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)
systems differ significantly in many dimensions, such as training datasets,
modeling paradigms, and architectural choices. This diversity complicates
efforts to evaluate models fairly and identify which design choices influence
performance the most. While factors like data and architecture are important,
in this study we focus exclusively on the modeling paradigm. We conduct a
systematic empirical analysis to isolate its effects, offering insights into
associated trade-offs and emergent behaviors that can guide future
text-to-music generation systems. Specifically, we compare the two arguably
most common modeling paradigms: auto-regressive decoding and conditional
flow-matching. We conduct a controlled comparison by training all models from
scratch using identical datasets, training configurations, and similar backbone
architectures. Performance is evaluated across multiple axes, including
generation quality, robustness to inference configurations, scalability,
adherence to both textual and temporally aligned conditioning, and editing
capabilities in the form of audio inpainting. This comparative study sheds
light on distinct strengths and limitations of each paradigm, providing
actionable insights that can inform future architectural and training decisions
in the evolving landscape of text-to-music generation. Audio sampled examples
are available at: https://huggingface.co/spaces/ortal1602/ARvsFM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Oyster-I: Beyond Refusal -- Constructive Safety Alignment for
  Responsible Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranjie Duan, Jiexi Liu, Xiaojun Jia, Shiji Zhao, Ruoxi Cheng, Fengxiang Wang, Cheng Wei, Yong Xie, Chang Liu, Defeng Li, Yinpeng Dong, Yichi Zhang, Yuefeng Chen, Chongwen Wang, Xingjun Ma, Xingxing Wei, Yang Liu, Hang Su, Jun Zhu, Xinfeng Li, Yitong Sun, Jie Zhang, Jinzhao Hu, Sha Xu, Yitong Yang, Jialing Tao, Hui Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) typically deploy safety mechanisms to prevent
harmful content generation. Most current approaches focus narrowly on risks
posed by malicious actors, often framing risks as adversarial events and
relying on defensive refusals. However, in real-world settings, risks also come
from non-malicious users seeking help while under psychological distress (e.g.,
self-harm intentions). In such cases, the model's response can strongly
influence the user's next actions. Simple refusals may lead them to repeat,
escalate, or move to unsafe platforms, creating worse outcomes. We introduce
Constructive Safety Alignment (CSA), a human-centric paradigm that protects
against malicious misuse while actively guiding vulnerable users toward safe
and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic
anticipation of user reactions, fine-grained risk boundary discovery, and
interpretable reasoning control, turning safety into a trust-building process.
Oy1 achieves state-of-the-art safety among open models while retaining high
general capabilities. On our Constructive Benchmark, it shows strong
constructive engagement, close to GPT-5, and unmatched robustness on the
Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from
refusal-first to guidance-first safety, CSA redefines the model-user
relationship, aiming for systems that are not just safe, but meaningfully
helpful. We release Oy1, code, and the benchmark to support responsible,
user-centered AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report Code & Model weights available:
  https://github.com/Alibaba-AAIG/Oyster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing FKG.in: automating Indian food composition analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05248v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05248v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saransh Kumar Gupta, Lipika Dey, Partha Pratim Das, Geeta Trilok-Kumar, Ramesh Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to compute food composition data for
Indian recipes using a knowledge graph for Indian food (FKG[.]in) and LLMs. The
primary focus is to provide a broad overview of an automated food composition
analysis workflow and describe its core functionalities: nutrition data
aggregation, food composition analysis, and LLM-augmented information
resolution. This workflow aims to complement FKG[.]in and iteratively
supplement food composition data from verified knowledge bases. Additionally,
this paper highlights the challenges of representing Indian food and accessing
food composition data digitally. It also reviews three key sources of food
composition data: the Indian Food Composition Tables, the Indian Nutrient
Databank, and the Nutritionix API. Furthermore, it briefly outlines how users
can interact with the workflow to obtain diet-based health recommendations and
detailed food composition information for numerous recipes. We then explore the
complex challenges of analyzing Indian recipe information across dimensions
such as structure, multilingualism, and uncertainty as well as present our
ongoing work on LLM-based solutions to address these issues. The methods
proposed in this workshop paper for AI-driven knowledge curation and
information resolution are application-agnostic, generalizable, and replicable
for any domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures, 30 references, International Conference on
  Pattern Recognition 2024 - Multimedia Assisted Dietary Management Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Parameter Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20790v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20790v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucius Bushnaq, Dan Braun, Lee Sharkey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key step in reverse engineering neural networks is to decompose them into
simpler parts that can be studied in relative isolation. Linear parameter
decomposition -- a framework that has been proposed to resolve several issues
with current decomposition methods -- decomposes neural network parameters into
a sum of sparsely used vectors in parameter space. However, the current main
method in this framework, Attribution-based Parameter Decomposition (APD), is
impractical on account of its computational cost and sensitivity to
hyperparameters. In this work, we introduce \textit{Stochastic Parameter
Decomposition} (SPD), a method that is more scalable and robust to
hyperparameters than APD, which we demonstrate by decomposing models that are
slightly larger and more complex than was possible to decompose with APD. We
also show that SPD avoids other issues, such as shrinkage of the learned
parameters, and better identifies ground truth mechanisms in toy models. By
bridging causal mediation analysis and network decomposition methods, this
demonstration opens up new research possibilities in mechanistic
interpretability by removing barriers to scaling linear parameter decomposition
methods to larger models. We release a library for running SPD and reproducing
our experiments at https://github.com/goodfire-ai/spd/tree/spd-paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion on language model encodings for protein sequence generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03726v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03726v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viacheslav Meshchaninov, Pavel Strashnov, Andrey Shevtsov, Fedor Nikolaev, Nikita Ivanisenko, Olga Kardymon, Dmitry Vetrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein sequence design has seen significant advances through discrete
diffusion and autoregressive approaches, yet the potential of continuous
diffusion remains underexplored. Here, we present DiMA, a latent diffusion
framework that operates on protein language model representations. Through
systematic exploration of architectural choices and diffusion components, we
develop a robust methodology that generalizes across multiple protein encoders
ranging from 8M to 3B parameters. We demonstrate that our framework achieves
consistently high performance across sequence-only (ESM-2, ESMc),
dual-decodable (CHEAP), and multimodal (SaProt) representations using the same
architecture and training approach. We extensively evaluate existing methods
alongside DiMA using multiple metrics across two protein modalities, covering
quality, diversity, novelty, and distribution matching of generated proteins.
DiMA consistently produces novel, high-quality and diverse protein sequences
and achieves strong results compared to baselines such as autoregressive,
discrete diffusion and flow matching language models. The model demonstrates
versatile functionality, supporting conditional generation tasks including
protein family-generation, motif scaffolding and infilling, and fold-specific
sequence design. This work provides a universal continuous diffusion framework
for protein sequence generation, offering both architectural insights and
practical applicability across various protein design scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WASP: A Weight-Space Approach to Detecting Learned Spuriousness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18970v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18970v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristian Daniel Păduraru, Antonio Bărbălau, Radu Filipescu, Andrei Liviu Nicolicioiu, Elena Burceanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is of crucial importance to train machine learning models such that they
clearly understand what defines each class in a given task. Though there is a
sum of works dedicated to identifying the spurious correlations featured by a
dataset that may impact the model's understanding of the classes, all current
approaches rely solely on data or error analysis. That is, they cannot point
out spurious correlations learned by the model that are not already pointed out
by the counterexamples featured in the validation or training sets. We propose
a method that transcends this limitation, switching the focus from analyzing a
model's predictions to analyzing the model's weights, the mechanism behind the
making of the decisions, which proves to be more insightful. Our proposed
Weight-space Approach to detecting Spuriousness (WASP) relies on analyzing the
weights of foundation models as they drift towards capturing various (spurious)
correlations while being fine-tuned on a given dataset. We demonstrate that
different from previous works, our method (i) can expose spurious correlations
featured by a dataset even when they are not exposed by training or validation
counterexamples, (ii) it works for multiple modalities such as image and text,
and (iii) it can uncover previously untapped spurious correlations learned by
ImageNet-1k classifiers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferable Mask Transformer: Cross-domain <span class="highlight-title">Semantic</span> Segmentation with
  Region-adaptive Transferability <span class="highlight-title">Estimation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.05774v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.05774v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianhua Liu, Zhengyu Li, Yanru Wu, Jingge Wang, Yang Tan, Ruizhe Zhao, Guan Wang, Yang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Vision Transformers (ViTs) have set new benchmarks in
semantic segmentation. However, when adapting pretrained ViTs to new target
domains, significant performance degradation often occurs due to distribution
shifts, resulting in suboptimal global attention. Since self-attention
mechanisms are inherently data-driven, they may fail to effectively attend to
key objects when source and target domains exhibit differences in texture,
scale, or object co-occurrence patterns. While global and patch-level domain
adaptation methods provide partial solutions, region-level adaptation with
dynamically shaped regions is crucial due to spatial heterogeneity in
transferability across different image areas. We present Transferable Mask
Transformer (TMT), a novel region-level adaptation framework for semantic
segmentation that aligns cross-domain representations through spatial
transferability analysis. TMT consists of two key components: (1) An Adaptive
Cluster-based Transferability Estimator (ACTE) that dynamically segments images
into structurally and semantically coherent regions for localized
transferability assessment, and (2) A Transferable Masked Attention (TMA)
module that integrates region-specific transferability maps into ViTs'
attention mechanisms, prioritizing adaptation in regions with low
transferability and high semantic uncertainty. Comprehensive evaluations across
20 cross-domain pairs demonstrate TMT's superiority, achieving an average 2%
MIoU improvement over vanilla fine-tuning and a 1.28% increase compared to
state-of-the-art baselines. The source code will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PIN: A Knowledge-Intensive <span class="highlight-title">Dataset</span> for Paired and Interleaved Multimodal
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wang, Yuxiang Zhang, Minghao Liu, Yin Zhang, Yatai Ji, Weihao Xuan, Nie Lin, Kang Zhu, Zhiqiang Lin, Yiming Ren, Chunyang Jiang, Yiyao Yu, Zekun Wang, Tiezhen Wang, Wenhao Huang, Jie Fu, Qunshu Liu, Yujiu Yang, Ge Zhang, Ruibin Yuan, Bei Chen, Wenhu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large multimodal models (LMMs) have leveraged
extensive multimodal datasets to enhance capabilities in complex
knowledge-driven tasks. However, persistent challenges in perceptual and
reasoning errors limit their efficacy, particularly in interpreting intricate
visual data and deducing multimodal relationships. To address these issues, we
introduce PIN (Paired and INterleaved multimodal documents), a novel data
format designed to foster a deeper integration of visual and textual knowledge.
The PIN format uniquely combines semantically rich Markdown files, which
preserve fine-grained textual structures, with holistic overall images that
capture the complete document layout. Following this format, we construct and
release two large-scale, open-source datasets: PIN-200M (~200 million
documents) and PIN-14M (~14 million), compiled from diverse web and scientific
sources in both English and Chinese. To maximize usability, we provide detailed
statistical analyses and equip the datasets with quality signals, enabling
researchers to easily filter and select data for specific tasks. Our work
provides the community with a versatile data format and substantial resources,
offering a foundation for new research in pre-training strategies and the
development of more powerful knowledge-intensive LMMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report v1.0</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conditional Video Generation for High-Efficiency Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.15269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.15269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangqiu Yi, Jingyu Xu, Jiawei Shao, Chi Zhang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceptual studies demonstrate that conditional diffusion models excel at
reconstructing video content aligned with human visual perception. Building on
this insight, we propose a video compression framework that leverages
conditional diffusion models for perceptually optimized reconstruction.
Specifically, we reframe video compression as a conditional generation task,
where a generative model synthesizes video from sparse, yet informative
signals. Our approach introduces three key modules: (1) Multi-granular
conditioning that captures both static scene structure and dynamic
spatio-temporal cues; (2) Compact representations designed for efficient
transmission without sacrificing semantic richness; (3) Multi-condition
training with modality dropout and role-aware embeddings, which prevent
over-reliance on any single modality and enhance robustness. Extensive
experiments show that our method significantly outperforms both traditional and
neural codecs on perceptual quality metrics such as Fr\'echet Video Distance
(FVD) and LPIPS, especially under high compression ratios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Critical methodology flaws invalidate key results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DaMoC: Efficiently Selecting the Optimal Large Language Model for
  Fine-tuning Domain Tasks Based on Data and Model Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Huang, Huang Wei, Yinggui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) excel in general tasks but struggle with
domain-specific ones, requiring fine-tuning with specific data. With many
open-source LLMs available, selecting the best model for fine-tuning downstream
tasks is challenging, primarily focusing on how to quickly identify the optimal
LLM. We introduce a Data and Model Compression Framework (DaMoC) that addresses
this challenge by: 1) Data Level: A systematic categorization of data filtering
methodologies for LLMs is first established, classifying them into three
distinct paradigms: (1) distribution-aware methods, (2) quality-aware methods,
and (3) hybrid approaches considering both dimensions. Further, we enhance the
density of key tokens in the text achieving token compression. Subsequently, we
use an LLM to iterative rewrite the text to optimize its expression. 2) Model
Level: We use layer similarity scores to assess each layer's importance and
remove those with lower importance. Then, we introduce a sparse merging
paradigm to preserve as much of the original model's capability as possible.
Extensive experiments on four datasets, medical Q&A, financial Q&A, general
Q&A, and reading comprehension, show that we can select the optimal LLM while
saving approximately 20-fold in training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CP-Bench: Evaluating Large Language Models for Constraint Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.06052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.06052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kostis Michailidis, Dimos Tsouros, Tias Guns
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constraint Programming (CP) is widely used to solve combinatorial problems,
but its core process, namely constraint modelling, requires significant
expertise and is considered to be a bottleneck for wider adoption. Aiming to
alleviate this bottleneck, recent studies have explored using Large Language
Models (LLMs) to transform combinatorial problem descriptions into executable
constraint models. However, the existing evaluation datasets for constraint
modelling are often limited to small, homogeneous, or domain-specific
instances, which do not capture the diversity of real-world scenarios. This
work addresses this gap by introducing CP-Bench, a novel benchmark that
includes a diverse set of well-known combinatorial problems sourced from the CP
community, structured explicitly for evaluating LLM-driven CP modelling. With
this dataset, and given the variety of constraint modelling frameworks, we
compare and evaluate the modelling capabilities of LLMs for three distinct
constraint modelling systems, which vary in abstraction level and underlying
syntax. Notably, the results show higher performance when modelling with a
high-level Python-based framework. Additionally, we systematically evaluate the
use of prompt-based and inference-time compute methods across different LLMs,
which further increase accuracy, reaching up to 70% on this highly challenging
benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECAI 25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Axiomatics of Restricted Choices by Linear Orders of Sets with Minimum
  as Fallback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.03315v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.03315v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Sauerwald, Kenneth Skiba, Eduardo Fermé, Thomas Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study how linear orders can be employed to realise choice functions for
which the set of potential choices is restricted, i.e., the possible choice is
not possible among the full powerset of all alternatives. In such restricted
settings, constructing a choice function via a relation on the alternatives is
not always possible. However, we show that one can always construct a choice
function via a linear order on sets of alternatives, even when a fallback value
is encoded as the minimal element in the linear order. The axiomatics of such
choice functions are presented for the general case and the case of
union-closed input restrictions. Restricted choice structures have applications
in knowledge representation and reasoning, and here we discuss their
applications for theory change and abstract argumentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structure Transfer: an Inference-Based Calculus for the Transformation
  of Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03249v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03249v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Raggi, Gem Stapleton, Mateja Jamnik, Aaron Stockdill, Grecia Garcia Garcia, Peter C-H. Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation choice is of fundamental importance to our ability to
communicate and reason effectively. A major unsolved problem, addressed in this
paper, is how to devise representational-system (RS) agnostic techniques that
drive representation transformation and choice. We present a novel calculus,
called structure transfer, that enables representation transformation across
diverse RSs. Specifically, given a source representation drawn from a source
RS, the rules of structure transfer allow us to generate a target
representation for a target RS. The generality of structure transfer comes in
part from its ability to ensure that the source representation and the
generated target representation satisfy any specified relation (such as
semantic equivalence). This is done by exploiting schemas, which encode
knowledge about RSs. Specifically, schemas can express preservation of
information across relations between any pair of RSs, and this knowledge is
used by structure transfer to derive a structure for the target representation
which ensures that the desired relation holds. We formalise this using
Representational Systems Theory, building on the key concept of a construction
space. The abstract nature of construction spaces grants them the generality to
model RSs of diverse kinds, including formal languages, geometric figures and
diagrams, as well as informal notations. Consequently, structure transfer is a
system-agnostic calculus that can be used to identify alternative
representations in a wide range of practical settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond holography: the entropic quantum gravity foundations of image
  processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.14048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.14048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ginestra Bianconi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, thanks to the development of artificial intelligence (AI) there is
increasing scientific attention in establishing the connections between
theoretical physics and AI. Traditionally, these connections have been focusing
mostly on the relation between string theory and image processing and involve
important theoretical paradigms such as holography. Recently G. Bianconi has
formulated the Gravity from Entropy (GfE) approach to quantum gravity in which
gravity is derived from the geometric quantum relative entropy (GQRE) between
two metrics associated with the Lorentzian spacetime. Here it is demonstrated
that the famous Perona-Malik algorithm for image processing is the gradient
flow of the GfE action in its simple warm-up scenario. Specifically, this
algorithm is the outcome of the minimization of the GQRE between two Euclidean
metrics: the one of the support of the image and the one induced by the image.
As the Perona-Malik algorithm is known to preserve sharp contours, this implies
that the GfE action, does not in general lead to uniform images upon iteration
of the gradient flow dynamics as it would be intuitively expected from entropic
actions maximising classical entropies. Rather, the outcome of the minimization
of the GQRE is compatible with the preservation of complex structures. These
results provide the geometrical and information theory foundations for the
Perona-Malik algorithm and might contribute to establish deeper connections
between GfE, machine learning and brain research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>(7 pages, 1 figure)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MultiGen: Child-Friendly Multilingual Speech Generator with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.08715v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.08715v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxue Gao, Huayun Zhang, Nancy F. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative speech models have demonstrated significant potential in improving
human-machine interactions, offering valuable real-world applications such as
language learning for children. However, achieving high-quality, child-friendly
speech generation remains challenging, particularly for low-resource languages
across diverse languages and cultural contexts. In this paper, we propose
MultiGen, a multilingual speech generation model with child-friendly
interaction, leveraging LLM architecture for speech generation tailored for
low-resource languages. We propose to integrate age-appropriate multilingual
speech generation using LLM architectures, which can be used to facilitate
young children's communication with AI systems through culturally relevant
context in three low-resource languages: Singaporean accent Mandarin, Malay,
and Tamil. Experimental results from both objective metrics and subjective
evaluations demonstrate the superior performance of the proposed MultiGen
compared to baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AImoclips: A Benchmark for Evaluating Emotion Conveyance in
  Text-to-Music Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00813v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00813v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyehun Go, Satbyul Han, Ahyeon Choi, Eunjin Choi, Juhan Nam, Jeong Mi Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-music (TTM) generation have enabled controllable
and expressive music creation using natural language prompts. However, the
emotional fidelity of TTM systems remains largely underexplored compared to
human preference or text alignment. In this study, we introduce AImoclips, a
benchmark for evaluating how well TTM systems convey intended emotions to human
listeners, covering both open-source and commercial models. We selected 12
emotion intents spanning four quadrants of the valence-arousal space, and used
six state-of-the-art TTM systems to generate over 1,000 music clips. A total of
111 participants rated the perceived valence and arousal of each clip on a
9-point Likert scale. Our results show that commercial systems tend to produce
music perceived as more pleasant than intended, while open-source systems tend
to perform the opposite. Emotions are more accurately conveyed under
high-arousal conditions across all models. Additionally, all systems exhibit a
bias toward emotional neutrality, highlighting a key limitation in affective
controllability. This benchmark offers valuable insights into model-specific
emotion rendering characteristics and supports future development of
emotionally aligned TTM systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in HCMIR25: 3rd Workshop on Human-Centric Music
  Information Research</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is an Ultra Large Natural Image-Based Foundation Model Superior to a
  Retina-Specific Model for Detecting Ocular and Systemic Diseases? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06289v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06289v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingshan Hou, Yukun Zhou, Jocelyn Hui Lin Goh, Ke Zou, Samantha Min Er Yew, Sahana Srinivasan, Meng Wang, Thaddaeus Lo, Xiaofeng Lei, Siegfried K. Wagner, Mark A. Chia, Dawei Yang, Hongyang Jiang, An Ran Ran, Rui Santos, Gabor Mark Somfai, Juan Helen Zhou, Haoyu Chen, Qingyu Chen, Carol Y. Cheung, Pearse A. Keane, Yih Chung Tham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of foundation models (FMs) is transforming medical domain. In
ophthalmology, RETFound, a retina-specific FM pre-trained sequentially on 1.4
million natural images and 1.6 million retinal images, has demonstrated high
adaptability across clinical applications. Conversely, DINOv2, a
general-purpose vision FM pre-trained on 142 million natural images, has shown
promise in non-medical domains. However, its applicability to clinical tasks
remains underexplored. To address this, we conducted head-to-head evaluations
by fine-tuning RETFound and three DINOv2 models (large, base, small) for ocular
disease detection and systemic disease prediction tasks, across eight
standardized open-source ocular datasets, as well as the Moorfields AlzEye and
the UK Biobank datasets. DINOv2-large model outperformed RETFound in detecting
diabetic retinopathy (AUROC=0.850-0.952 vs 0.823-0.944, across three datasets,
all P<=0.007) and multi-class eye diseases (AUROC=0.892 vs. 0.846, P<0.001). In
glaucoma, DINOv2-base model outperformed RETFound (AUROC=0.958 vs 0.940,
P<0.001). Conversely, RETFound achieved superior performance over all DINOv2
models in predicting heart failure, myocardial infarction, and ischaemic stroke
(AUROC=0.732-0.796 vs 0.663-0.771, all P<0.001). These trends persisted even
with 10% of the fine-tuning data. These findings showcase the distinct
scenarios where general-purpose and domain-specific FMs excel, highlighting the
importance of aligning FM selection with task-specific requirements to optimise
clinical performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Ophthalmology Science and is currently in press</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Defending LVLMs Against <span class="highlight-title">Vision</span> Attacks through Partial-Perception
  Super<span class="highlight-title">vision</span> <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Zhou, Tianlin Li, Qing Guo, Dongxia Wang, Yun Lin, Yang Liu, Jin Song Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have raised significant concerns regarding the vulnerability
of Large Vision Language Models (LVLMs) to maliciously injected or perturbed
input images, which can mislead their responses. Existing defense methods show
that such vision attacks are sensitive to image modifications especially
cropping, using majority voting across responses of modified images as
corrected responses. However, these modifications often result in partial
images and distort the semantics, which reduces response quality on clean
images after voting. Instead of directly using responses from partial images
for voting, we investigate using them to supervise the LVLM's responses to the
original images. We propose a black-box, training-free method called DPS
(Defense through Partial-Perception Supervision). In this approach, the model
is prompted using the responses generated by a model that perceives only a
partial image. With DPS, the model can adjust its response based on partial
image understanding when under attack, while confidently maintaining its
original response for clean input. Our findings show that the weak model can
supervise the strong model: when faced with an attacked input, the strong model
becomes less confident and adjusts its response based on the weak model's
partial understanding, effectively defending against the attack. With clean
input, it confidently maintains its original response. Empirical experiments
show our method outperforms the baseline, cutting the average attack success
rate by 76.3% across six datasets on three popular models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Artificial Intelligence Reshaping the Landscape of the International
  Academic Community of Geosciences? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Li, Yuntian Li, Wenxin Zhao, Shan Ye, Yun Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Through bibliometric analysis and topic modeling, we find that artificial
intelligence (AI) is positively transforming geosciences research, with a
notable increase in AI-related scientific output in recent years. We are
encouraged to observe that earth scientists from developing countries have
gained better visibility in the recent AI for Science (AI4S) paradigm and that
AI is also improving the landscape of international collaboration in
geoscience-related research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>miscommunication in the authorization process from the first author</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Analysis of Action-Value Temporal-Difference Methods That Learn State
  Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.09523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.09523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brett Daley, Prabhat Nagarajan, Martha White, Marlos C. Machado
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The hallmark feature of temporal-difference (TD) learning is bootstrapping:
using value predictions to generate new value predictions. The vast majority of
TD methods for control learn a policy by bootstrapping from a single
action-value function (e.g., Q-learning and Sarsa). Significantly less
attention has been given to methods that bootstrap from two asymmetric value
functions: i.e., methods that learn state values as an intermediate step in
learning action values. Existing algorithms in this vein can be categorized as
either QV-learning or AV-learning. Though these algorithms have been
investigated to some degree in prior work, it remains unclear if and when it is
advantageous to learn two value functions instead of just one -- and whether
such approaches are theoretically sound in general. In this paper, we analyze
these algorithmic families in terms of convergence and sample efficiency. We
find that while both families are more efficient than Expected Sarsa in the
prediction setting, only AV-learning methods offer any major benefit over
Q-learning in the control setting. Finally, we introduce a new AV-learning
algorithm called Regularized Dueling Q-learning (RDQ), which significantly
outperforms Dueling DQN in the MinAtar benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in RLC/RLJ 2025. Camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beacon: Post-Training Quantization with Integrated Grid Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Zhang, Rayan Saab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization is a widely used compression technique for reducing the memory
and computation costs of large pre-trained models. A key challenge in
per-channel post-training quantization (PTQ) is selecting appropriate scaling
factors to replace weight values with values from a scaled integer grid.
Existing methods typically fix the scale at the outset via heuristic tuning or
grid search. We propose Beacon, a simple and effective algorithm that
eliminates the need for such manual tuning. Beacon performs per-channel PTQ
directly using an unscaled grid and automatically determines the optimal
scaling factors by exploiting the geometry of scalar quantization. It does not
rely on back-propagation or large calibration sets. Despite its simplicity and
tuning-free nature, Beacon achieves competitive performance compared to
state-of-the-art methods, making it a practical solution for efficient model
deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Optimization</span> of Module Transferability in Single Image Super-Resolution:
  Universality Assessment and Cycle Residual Blocks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotong Cheng, Zhiqi Zhang, Hao Li, Xinshang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has substantially advanced the field of Single Image
Super-Resolution (SISR). However, existing research has predominantly focused
on raw performance gains, with little attention paid to quantifying the
transferability of architectural components. In this paper, we introduce the
concept of "Universality" and its associated definitions, which extend the
traditional notion of "Generalization" to encompass the ease of transferability
of modules. We then propose the Universality Assessment Equation (UAE), a
metric that quantifies how readily a given module can be transplanted across
models and reveals the combined influence of multiple existing metrics on
transferability. Guided by the UAE results of standard residual blocks and
other plug-and-play modules, we further design two optimized modules: the Cycle
Residual Block (CRB) and the Depth-Wise Cycle Residual Block (DCRB). Through
comprehensive experiments on natural-scene benchmarks, remote-sensing datasets,
and other low-level tasks, we demonstrate that networks embedded with the
proposed plug-and-play modules outperform several state-of-the-art methods,
achieving a PSNR improvement of up to 0.83 dB or enabling a 71.3% reduction in
parameters with negligible loss in reconstruction fidelity. Similar
optimization approaches could be applied to a broader range of basic modules,
offering a new paradigm for the design of plug-and-play modules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted to IET Image Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoDiff: Conditional Diffusion Model for Collaborative 3D Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14891v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14891v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Huang, Shuo Wang, Yongcai Wang, Lei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative 3D object detection holds significant importance in the field
of autonomous driving, as it greatly enhances the perception capabilities of
each individual agent by facilitating information exchange among multiple
agents. However, in practice, due to pose estimation errors and time delays,
the fusion of information across agents often results in feature
representations with spatial and temporal noise, leading to detection errors.
Diffusion models naturally have the ability to denoise noisy samples to the
ideal data, which motivates us to explore the use of diffusion models to
address the noise problem between multi-agent systems. In this work, we propose
CoDiff, a novel robust collaborative perception framework that leverages the
potential of diffusion models to generate more comprehensive and clearer
feature representations. To the best of our knowledge, this is the first work
to apply diffusion models to multi-agent collaborative perception.
Specifically, we project high-dimensional feature map into the latent space of
a powerful pre-trained autoencoder. Within this space, individual agent
information serves as a condition to guide the diffusion model's sampling. This
process denoises coarse feature maps and progressively refines the fused
features. Experimental study on both simulated and real-world datasets
demonstrates that the proposed framework CoDiff consistently outperforms
existing relevant methods in terms of the collaborative object detection
performance, and exhibits highly desired robustness when the pose and delay
information of agents is with high-level noise. The code is released at
https://github.com/HuangZhe885/CoDiff
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span> Offline Imitation <span class="highlight-title">Learning</span> Through State-level Trajectory
  Stitching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.22524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.22524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuze Wang, Yunpeng Mei, Hongjie Cao, Yetian Yuan, Gang Wang, Jian Sun, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning (IL) has proven effective for enabling robots to acquire
visuomotor skills through expert demonstrations. However, traditional IL
methods are limited by their reliance on high-quality, often scarce, expert
data, and suffer from covariate shift. To address these challenges, recent
advances in offline IL have incorporated suboptimal, unlabeled datasets into
the training. In this paper, we propose a novel approach to enhance policy
learning from mixed-quality offline datasets by leveraging task-relevant
trajectory fragments and rich environmental dynamics. Specifically, we
introduce a state-based search framework that stitches state-action pairs from
imperfect demonstrations, generating more diverse and informative training
trajectories. Experimental results on standard IL benchmarks and real-world
robotic tasks showcase that our proposed method significantly improves both
generalization and performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transferable Belief Model on Quantum Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08949v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08949v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianli Zhou, Hao Luo, Lipeng Pan, Yong Deng, Eloi Bosse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transferable belief model, as a semantic interpretation of
Dempster-Shafer theory, enables agents to perform reasoning and decision making
in imprecise and incomplete environments. The model offers distinct semantics
for handling unreliable testimonies, allowing for a more reasonable and general
process of belief transfer compared to the Bayesian approach. However, because
both the belief masses and the structure of focal sets must be considered when
updating belief functions-leading to extra computational complexity during
reasoning-the transferable belief model has gradually lost favor among
researchers in recent developments. In this paper, we implement the
transferable belief model on quantum circuits and demonstrate that belief
functions offer a more concise and effective alternative to Bayesian approaches
within the quantum computing framework. Furthermore, leveraging the unique
characteristics of quantum computing, we propose several novel belief transfer
approaches. More broadly, this paper introduces a new perspective on basic
information representation for quantum AI models, suggesting that belief
functions are more suitable than Bayesian approach for handling uncertainty on
quantum circuits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Science Across Languages: Assessing LLM Multilingual Translation of
  Scientific Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Calzi Kleidermacher, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific research is inherently global. However, the vast majority of
academic journals are published exclusively in English, creating barriers for
non-native-English-speaking researchers. In this study, we leverage large
language models (LLMs) to translate published scientific articles while
preserving their native JATS XML formatting, thereby developing a practical,
automated approach for implementation by academic journals. Using our approach,
we translate articles across multiple scientific disciplines into 28 languages.
To evaluate translation accuracy, we introduce a novel question-and-answer (QA)
benchmarking method, in which an LLM generates comprehension-based questions
from the original text and then answers them based on the translated text. Our
benchmark results show an average performance of 95.9%, showing that the key
scientific details are accurately conveyed. In a user study, we translate the
scientific papers of 15 researchers into their native languages, finding that
the authors consistently found the translations to accurately capture the
original information in their articles. Interestingly, a third of the authors
found many technical terms "overtranslated," expressing a preference to keep
terminology more familiar in English untranslated. Finally, we demonstrate how
in-context learning techniques can be used to align translations with
domain-specific preferences such as mitigating overtranslation, highlighting
the adaptability and utility of LLM-driven scientific translation. The code and
translated articles are available at https://hankleid.github.io/ProjectMundo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long Input Sequence Network for Long Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Ma, Yikai Hou, Xiang Li, Yinggang Sun, Haining Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Short fixed-length inputs are the main bottleneck of deep learning methods in
long time-series forecasting tasks. Prolonging input length causes overfitting,
rapidly deteriorating accuracy. Our research indicates that the overfitting is
a combination reaction of the multi-scale pattern coupling in time series and
the fixed focusing scale of current models. First, we find that the patterns
exhibited by a time series across various scales are reflective of its
multi-periodic nature, where each scale corresponds to specific period length.
Second, We find that the token size predominantly dictates model behavior, as
it determines the scale at which the model focuses and the context size it can
accommodate. Our idea is to decouple the multi-scale temporal patterns of time
series and to model each pattern with its corresponding period length as token
size. We introduced a novel series-decomposition module(MPSD), and a
Multi-Token Pattern Recognition neural network(MTPR), enabling the model to
handle \textit{inputs up to $10\times$ longer}. Sufficient context enhances
performance(\textit{38% maximum precision improvement}), and the decoupling
approach offers \textit{Low complexity($0.22\times$ cost)} and \textit{high
interpretability}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>rewrite and authorship changing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Breaking the Context Bottleneck on Long Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Ma, Yikai Hou, Xiang Li, Yinggang Sun, Haining Yu, Zhou Fang, Jiaxing Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term time-series forecasting is essential for planning and
decision-making in economics, energy, and transportation, where long foresight
is required. To obtain such long foresight, models must be both efficient and
effective in processing long sequence. Recent advancements have enhanced the
efficiency of these models; however, the challenge of effectively leveraging
longer sequences persists. This is primarily due to the tendency of these
models to overfit when presented with extended inputs, necessitating the use of
shorter input lengths to maintain tolerable error margins. In this work, we
investigate the multiscale modeling method and propose the Logsparse
Decomposable Multiscaling (LDM) framework for the efficient and effective
processing of long sequences. We demonstrate that by decoupling patterns at
different scales in time series, we can enhance predictability by reducing
non-stationarity, improve efficiency through a compact long input
representation, and simplify the architecture by providing clear task
assignments. Experimental results demonstrate that LDM not only outperforms all
baselines in long-term forecasting benchmarks, but also reducing both training
time and memory costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>rewrite and authorship changing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EZhouNet:A framework based on graph neural network and anchor interval
  for the respiratory sound event detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Chu, Qiuhao Wang, Enze Zhou, Qian Liu, Gang Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auscultation is a key method for early diagnosis of respiratory and pulmonary
diseases, relying on skilled healthcare professionals. However, the process is
often subjective, with variability between experts. As a result, numerous deep
learning-based automatic classification methods have emerged, most of which
focus on respiratory sound classification. In contrast, research on respiratory
sound event detection remains limited. Existing sound event detection methods
typically rely on frame-level predictions followed by post-processing to
generate event-level outputs, making interval boundaries challenging to learn
directly. Furthermore, many approaches can only handle fixed-length audio,
limiting their applicability to variable-length respiratory sounds.
Additionally, the impact of respiratory sound location information on detection
performance has not been extensively explored. To address these issues, we
propose a graph neural network-based framework with anchor intervals, capable
of handling variable-length audio and providing more precise temporal
localization for abnormal respiratory sound events. Our method improves both
the flexibility and applicability of respiratory sound detection. Experiments
on the SPRSound 2024 and HF Lung V1 datasets demonstrate the effectiveness of
the proposed approach, and incorporating respiratory position information
enhances the discrimination between abnormal sounds. The reference
implementation is available at https://github.com/chumingqian/EzhouNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeCopilot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Azul Garza, Reneé Rosillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce TimeCopilot, the first open-source agentic framework for
forecasting that combines multiple Time Series Foundation Models (TSFMs) with
Large Language Models (LLMs) through a single unified API. TimeCopilot
automates the forecasting pipeline: feature analysis, model selection,
cross-validation, and forecast generation, while providing natural language
explanations and supporting direct queries about the future. The framework is
LLM-agnostic, compatible with both commercial and open-source models, and
supports ensembles across diverse forecasting families. Results on the
large-scale GIFT-Eval benchmark show that TimeCopilot achieves state-of-the-art
probabilistic forecasting performance at low cost. Our framework provides a
practical foundation for reproducible, explainable, and accessible agentic
forecasting systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ensemble of Pathology Foundation Models for MIDOG 2025 Track 2: Atypical
  Mitosis Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02591v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02591v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mieko Ochi, Bae Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitotic figures are classified into typical and atypical variants, with
atypical counts correlating strongly with tumor aggressiveness. Accurate
differentiation is therefore essential for patient prognostication and resource
allocation, yet remains challenging even for expert pathologists. Here, we
leveraged Pathology Foundation Models (PFMs) pre-trained on large
histopathology datasets and applied parameter-efficient fine-tuning via
low-rank adaptation. In addition, we incorporated ConvNeXt V2, a
state-of-the-art convolutional neural network architecture, to complement PFMs.
During training, we employed a fisheye transform to emphasize mitoses and
Fourier Domain Adaptation using ImageNet target images. Finally, we ensembled
multiple PFMs to integrate complementary morphological insights, achieving
competitive balanced accuracy on the Preliminary Evaluation Phase dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Can I Publish My LLM Benchmark Without Giving the True Answers Away? <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.18102v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.18102v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Ishida, Thanawat Lodkaew, Ikko Yamane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Publishing a large language model (LLM) benchmark on the Internet risks
contaminating future LLMs: the benchmark may be unintentionally (or
intentionally) used to train or select a model. A common mitigation is to keep
the benchmark private and let participants submit their models or predictions
to the organizers. However, this strategy will require trust in a single
organization and still permits test-set overfitting through repeated queries.
To overcome this issue, we propose a way to publish benchmarks without
completely disclosing the ground-truth answers to the questions, while still
maintaining the ability to openly evaluate LLMs. Our main idea is to inject
randomness to the answers by preparing several logically correct answers, and
only include one of them as the solution in the benchmark. This reduces the
best possible accuracy, i.e., Bayes accuracy, of the benchmark. Not only is
this helpful to keep us from disclosing the ground truth, but this approach
also offers a test for detecting data contamination. In principle, even fully
capable models should not surpass the Bayes accuracy. If a model surpasses this
ceiling despite this expectation, this is a strong signal of data
contamination. We present experimental evidence that our method can detect data
contamination accurately on a wide range of benchmarks, models, and training
methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of the paper presented as an Oral at the ICML 2025
  Workshop on the Impact of Memorization on Trustworthy Foundation Models</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-09-03T00:00:00Z">2025-09-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">45</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory <span class="highlight-title">Optimization</span> for Convex Hull Support Point Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Greer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper evaluates several improvements to the memory layout of convex
hulls to improve computation times for support point queries. The support point
query is a fundamental part of common collision algorithms, and the work
presented achieves a significant speedup depending on the number of vertices of
the convex hull.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Avoidance of an unexpected obstacle without reinforcement <span class="highlight-title">learning</span>: Why
  not using advanced control-theoretic tools? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03721v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03721v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cédric Join, Michel Fliess
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This communication on collision avoidance with unexpected obstacles is
motivated by some critical appraisals on reinforcement learning (RL) which
"requires ridiculously large numbers of trials to learn any new task" (Yann
LeCun). We use the classic Dubins' car in order to replace RL with
flatness-based control, combined with the HEOL feedback setting, and the latest
model-free predictive control approach. The two approaches lead to convincing
computer experiments where the results with the model-based one are only
slightly better. They exhibit a satisfactory robustness with respect to
randomly generated mismatches/disturbances, which become excellent in the
model-free case. Those properties would have been perhaps difficult to obtain
with today's popular machine learning techniques in AI. Finally, we should
emphasize that our two methods require a low computational burden.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE 2025 - 13th International Conference on Systems and Control
  (ICSC) - October 22-24, 2025 - Marrakesh, Morocco</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-Cost Open-Source Ambidextrous <span class="highlight-title">Robot</span>ic Hand with 23 Direct-Drive
  servos for American Sign Language Alphabet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kelvin Daniel Gonzalez Amador
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accessible communication through sign language is vital for deaf communities,
1 yet robotic solutions are often costly and limited. This study presents
VulcanV3, a low- 2 cost, open-source, 3D-printed ambidextrous robotic hand
capable of reproducing the full 3 American Sign Language (ASL) alphabet (52
signs for right- and left-hand configurations). 4 The system employs 23
direct-drive servo actuators for precise finger and wrist movements, 5
controlled by an Arduino Mega with dual PCA9685 modules. Unlike most humanoid
upper- 6 limb systems, which rarely employ direct-drive actuation, VulcanV3
achieves complete ASL 7 coverage with a reversible design. All CAD files and
code are released under permissive 8 open-source licenses to enable
replication. Empirical tests confirmed accurate reproduction 9 of all 52 ASL
handshapes, while a participant study (n = 33) achieved 96.97% recognition 10
accuracy, improving to 98.78% after video demonstration. VulcanV3 advances
assistive 11 robotics by combining affordability, full ASL coverage, and
ambidexterity in an openly 12 shared platform, contributing to accessible
communication technologies and inclusive 13 innovation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, 4 tables. Submitted as preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Virtuoso: A Latent Diffusion Transformer Model for
  Goal-Conditioned Trajectory <span class="highlight-title">Planning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Guillen-Perez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to generate a diverse and plausible distribution of future
trajectories is a critical capability for autonomous vehicle planning systems.
While recent generative models have shown promise, achieving high fidelity,
computational efficiency, and precise control remains a significant challenge.
In this paper, we present the \textbf{Efficient Virtuoso}, a conditional latent
diffusion model for goal-conditioned trajectory planning. Our approach
introduces a novel two-stage normalization pipeline that first scales
trajectories to preserve their geometric aspect ratio and then normalizes the
resulting PCA latent space to ensure a stable training target. The denoising
process is performed efficiently in this low-dimensional latent space by a
simple MLP denoiser, which is conditioned on a rich scene context fused by a
powerful Transformer-based StateEncoder. We demonstrate that our method
achieves state-of-the-art performance on the Waymo Open Motion Dataset,
reaching a \textbf{minADE of 0.25}. Furthermore, through a rigorous ablation
study on goal representation, we provide a key insight: while a single endpoint
goal can resolve strategic ambiguity, a richer, multi-step sparse route is
essential for enabling the precise, high-fidelity tactical execution that
mirrors nuanced human driving behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cooperative Grasping for Collective Object Transport in Constrained
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Alvear, George Turkiyyah, Shinkyu Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel framework for decision-making in cooperative grasping for
two-robot object transport in constrained environments. The core of the
framework is a Conditional Embedding (CE) model consisting of two neural
networks that map grasp configuration information into an embedding space. The
resulting embedding vectors are then used to identify feasible grasp
configurations that allow two robots to collaboratively transport an object. To
ensure generalizability across diverse environments and object geometries, the
neural networks are trained on a dataset comprising a range of environment maps
and object shapes. We employ a supervised learning approach with negative
sampling to ensure that the learned embeddings effectively distinguish between
feasible and infeasible grasp configurations. Evaluation results across a wide
range of environments and objects in simulations demonstrate the model's
ability to reliably identify feasible grasp configurations. We further validate
the framework through experiments on a physical robotic platform, confirming
its practical applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can the Waymo Open Motion <span class="highlight-title">Dataset</span> Support Realistic Behavioral Modeling?
  A Validation Study with Naturalistic Trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanlin Zhang, Sungyong Chung, Nachuan Li, Dana Monzer, Hani S. Mahmassani, Samer H. Hamdar, Alireza Talebpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Waymo Open Motion Dataset (WOMD) has become a popular resource for
data-driven modeling of autonomous vehicles (AVs) behavior. However, its
validity for behavioral analysis remains uncertain due to proprietary
post-processing, the absence of error quantification, and the segmentation of
trajectories into 20-second clips. This study examines whether WOMD accurately
captures the dynamics and interactions observed in real-world AV operations.
Leveraging an independently collected naturalistic dataset from Level 4 AV
operations in Phoenix, Arizona (PHX), we perform comparative analyses across
three representative urban driving scenarios: discharging at signalized
intersections, car-following, and lane-changing behaviors. For the discharging
analysis, headways are manually extracted from aerial video to ensure
negligible measurement error. For the car-following and lane-changing cases, we
apply the Simulation-Extrapolation (SIMEX) method to account for empirically
estimated error in the PHX data and use Dynamic Time Warping (DTW) distances to
quantify behavioral differences. Results across all scenarios consistently show
that behavior in PHX falls outside the behavioral envelope of WOMD. Notably,
WOMD underrepresents short headways and abrupt decelerations. These findings
suggest that behavioral models calibrated solely on WOMD may systematically
underestimate the variability, risk, and complexity of naturalistic driving.
Caution is therefore warranted when using WOMD for behavior modeling without
proper validation against independently collected data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Instrument <span class="highlight-title">Planning</span> and Perception for Novel Measurements of
  <span class="highlight-title">Dynamic</span> Phenomena 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itai Zilberstein, Alberto Candela, Steve Chien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advancements in onboard computing mean remote sensing agents can employ
state-of-the-art computer vision and machine learning at the edge. These
capabilities can be leveraged to unlock new rare, transient, and pinpoint
measurements of dynamic science phenomena. In this paper, we present an
automated workflow that synthesizes the detection of these dynamic events in
look-ahead satellite imagery with autonomous trajectory planning for a
follow-up high-resolution sensor to obtain pinpoint measurements. We apply this
workflow to the use case of observing volcanic plumes. We analyze
classification approaches including traditional machine learning algorithms and
convolutional neural networks. We present several trajectory planning
algorithms that track the morphological features of a plume and integrate these
algorithms with the classifiers. We show through simulation an order of
magnitude increase in the utility return of the high-resolution instrument
compared to baselines while maintaining efficient runtimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appears in Proceedings of 18th Symposium on Advanced Space
  Technologies in Robotics and Automation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ sam-llm: interpretable lane change trajectoryprediction via parametric
  finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Cao, Yunxiao Shi, Min Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces SAM-LLM, a novel hybrid architecture that bridges the
gap between the contextual reasoning of Large Language Models (LLMs) and the
physical precision of kinematic lane change models for autonomous driving. The
system is designed for interpretable lane change trajectory prediction by
finetuning an LLM to output the core physical parameters of a trajectory model
instead of raw coordinates. For lane-keeping scenarios, the model predicts
discrete coordinates, but for lane change maneuvers, it generates the
parameters for an enhanced Sinusoidal Acceleration Model (SAM), including
lateral displacement, maneuver duration, initial lateral velocity, and
longitudinal velocity change. This parametric approach yields a complete,
continuous, and physically plausible trajectory model that is inherently
interpretable and computationally efficient, achieving an 80% reduction in
output size compared to coordinate-based methods. The SAM-LLM achieves a
state-of-the-art overall intention prediction accuracy of 98.73%, demonstrating
performance equivalent to traditional LLM predictors while offering significant
advantages in explainability and resource efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SmartPoser: Arm Pose <span class="highlight-title">Estimation</span> with a Smartphone and Smartwatch Using
  UWB and IMU Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan DeVrio, Vimal Mollyn, Chris Harrison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to track a user's arm pose could be valuable in a wide range of
applications, including fitness, rehabilitation, augmented reality input, life
logging, and context-aware assistants. Unfortunately, this capability is not
readily available to consumers. Systems either require cameras, which carry
privacy issues, or utilize multiple worn IMUs or markers. In this work, we
describe how an off-the-shelf smartphone and smartwatch can work together to
accurately estimate arm pose. Moving beyond prior work, we take advantage of
more recent ultra-wideband (UWB) functionality on these devices to capture
absolute distance between the two devices. This measurement is the perfect
complement to inertial data, which is relative and suffers from drift. We
quantify the performance of our software-only approach using off-the-shelf
devices, showing it can estimate the wrist and elbow joints with a \hl{median
positional error of 11.0~cm}, without the user having to provide training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two listed authors contributed equally. Published at UIST
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cost-Optimized Systems Engineering for IoT-Enabled <span class="highlight-title">Robot</span> Nurse in
  Infectious Pandemic Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Mhamud Hussen Sifat, Md Maruf, Md Rokunuzzaman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The utilization of robotic technology has gained traction in healthcare
facilities due to progress in the field that enables time and cost savings,
minimizes waste, and improves patient care. Digital healthcare technologies
that leverage automation, such as robotics and artificial intelligence, have
the potential to enhance the sustainability and profitability of healthcare
systems in the long run. However, the recent COVID-19 pandemic has amplified
the need for cyber-physical robots to automate check-ups and medication
administration. A robot nurse is controlled by the Internet of Things (IoT) and
can serve as an automated medical assistant while also allowing supervisory
control based on custom commands. This system helps reduce infection risk and
improves outcomes in pandemic settings. This research presents a test case with
a nurse robot that can assess a patient's health status and take action
accordingly. We also evaluate the system's performance in medication
administration, health-status monitoring, and life-cycle considerations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 10 figures, 4 tables, 1 algorithm. Corresponding author: Md
  Maruf (maruf.mte.17@gmail.com)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EclipseTouch: Touch Segmentation on Ad Hoc Surfaces using Worn Infrared
  Shadow Casting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03430v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03430v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vimal Mollyn, Nathan DeVrio, Chris Harrison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to detect touch events on uninstrumented, everyday surfaces has
been a long-standing goal for mixed reality systems. Prior work has shown that
virtual interfaces bound to physical surfaces offer performance and ergonomic
benefits over tapping at interfaces floating in the air. A wide variety of
approaches have been previously developed, to which we contribute a new
headset-integrated technique called \systemname. We use a combination of a
computer-triggered camera and one or more infrared emitters to create
structured shadows, from which we can accurately estimate hover distance (mean
error of 6.9~mm) and touch contact (98.0\% accuracy). We discuss how our
technique works across a range of conditions, including surface material,
interaction orientation, and environmental lighting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to UIST 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Organizing Aerial Swarm <span class="highlight-title">Robot</span>ics for Resilient Load Transportation
  : A Table-Mechanics-Inspired Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Quan, Jiwen Xu, Runxiao Liu, Yi Ding, Jiaxing Che, Kai-Yuan Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In comparison with existing approaches, which struggle with scalability,
communication dependency, and robustness against dynamic failures, cooperative
aerial transportation via robot swarms holds transformative potential for
logistics and disaster response. Here, we present a physics-inspired
cooperative transportation approach for flying robot swarms that imitates the
dissipative mechanics of table-leg load distribution. By developing a
decentralized dissipative force model, our approach enables autonomous
formation stabilization and adaptive load allocation without the requirement of
explicit communication. Based on local neighbor robots and the suspended
payload, each robot dynamically adjusts its position. This is similar to
energy-dissipating table leg reactions. The stability of the resultant control
system is rigorously proved. Simulations demonstrate that the tracking errors
of the proposed approach are 20%, 68%, 55.5%, and 21.9% of existing approaches
under the cases of capability variation, cable uncertainty, limited vision, and
payload variation, respectively. In real-world experiments with six flying
robots, the cooperative aerial transportation system achieved a 94% success
rate under single-robot failure, disconnection events, 25% payload variation,
and 40% cable length uncertainty, demonstrating strong robustness under outdoor
winds up to Beaufort scale 4. Overall, this physics-inspired approach bridges
swarm intelligence and mechanical stability principles, offering a scalable
framework for heterogeneous aerial systems to collectively handle complex
transportation tasks in communication-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ANNIE: Be Careful of Your <span class="highlight-title">Robots</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Huang, Zixuan Wang, Zishen Wan, Yapeng Tian, Haobo Xu, Yinhe Han, Yiming Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of vision-language-action (VLA) models into embodied AI (EAI)
robots is rapidly advancing their ability to perform complex, long-horizon
tasks in humancentric environments. However, EAI systems introduce critical
security risks: a compromised VLA model can directly translate adversarial
perturbations on sensory input into unsafe physical actions. Traditional safety
definitions and methodologies from the machine learning community are no longer
sufficient. EAI systems raise new questions, such as what constitutes safety,
how to measure it, and how to design effective attack and defense mechanisms in
physically grounded, interactive settings. In this work, we present the first
systematic study of adversarial safety attacks on embodied AI systems, grounded
in ISO standards for human-robot interactions. We (1) formalize a principled
taxonomy of safety violations (critical, dangerous, risky) based on physical
constraints such as separation distance, velocity, and collision boundaries;
(2) introduce ANNIEBench, a benchmark of nine safety-critical scenarios with
2,400 video-action sequences for evaluating embodied safety; and (3)
ANNIE-Attack, a task-aware adversarial framework with an attack leader model
that decomposes long-horizon goals into frame-level perturbations. Our
evaluation across representative EAI models shows attack success rates
exceeding 50% across all safety categories. We further demonstrate sparse and
adaptive attack strategies and validate the real-world impact through physical
robot experiments. These results expose a previously underexplored but highly
consequential attack surface in embodied AI systems, highlighting the urgent
need for security-driven defenses in the physical AI era. Code is available at
https://github.com/RLCLab/Annie.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dependency Chain Analysis of ROS 2 DDS QoS Policies: From Lifecycle
  Tutorial to Static Verification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03381v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03381v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghoon Lee, Junha Kang, Kyung-Joon Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot Operating System 2 (ROS 2) relies on the Data Distribution Service
(DDS), which offers more than 20 Quality of Service (QoS) policies governing
availability, reliability, and resource usage. Yet ROS 2 users lack clear
guidance on safe policy combinations and validation processes prior to
deployment, which often leads to trial-and-error tuning and unexpected runtime
failures. To address these challenges, we analyze DDS Publisher-Subscriber
communication over a life cycle divided into Discovery, Data Exchange, and
Disassociation, and provide a user oriented tutorial explaining how 16 QoS
policies operate in each phase. Building on this analysis, we derive a QoS
dependency chain that formalizes inter-policy relationships and classifies 41
dependency violation rules, capturing constraints that commonly cause
communication failures in practice. Finally, we introduce QoS Guard, a ROS 2
package that statically validates DDS XML profiles offline, flags conflicts,
and enables safe, predeployment tuning without establishing a live ROS 2
session. Together, these contributions give ROS 2 users both conceptual insight
and a concrete tool that enables early detection of misconfigurations,
improving the reliability and resource efficiency of ROS 2 based robotic
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Safety Assurance in Electric Vehicles: A Case Study on AI-Driven SOC
  <span class="highlight-title">Estimation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Skoglund, Fredrik Warg, Aria Mirzai, Anders Thorsen, Karl Lundgren, Peter Folkesson, Bastian Havers-zulka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating Artificial Intelligence (AI) technology in electric vehicles (EV)
introduces unique challenges for safety assurance, particularly within the
framework of ISO 26262, which governs functional safety in the automotive
domain. Traditional assessment methodologies are not geared toward evaluating
AI-based functions and require evolving standards and practices. This paper
explores how an independent assessment of an AI component in an EV can be
achieved when combining ISO 26262 with the recently released ISO/PAS 8800,
whose scope is AI safety for road vehicles. The AI-driven State of Charge (SOC)
battery estimation exemplifies the process. Key features relevant to the
independent assessment of this extended evaluation approach are identified. As
part of the evaluation, robustness testing of the AI component is conducted
using fault injection experiments, wherein perturbed sensor inputs are
systematically introduced to assess the component's resilience to input
variance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, EVS38,
  https://evs38-program.org/en/evs-38-proceedings/all</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallel-Constraint Model Predictive Control: Exploiting Parallel
  Computation for Improving Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Fontanari, Gianni Lunardi, Matteo Saveriano, Andrea Del Prete
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring constraint satisfaction is a key requirement for safety-critical
systems, which include most robotic platforms. For example, constraints can be
used for modeling joint position/velocity/torque limits and collision
avoidance. Constrained systems are often controlled using Model Predictive
Control, because of its ability to naturally handle constraints, relying on
numerical optimization. However, ensuring constraint satisfaction is
challenging for nonlinear systems/constraints. A well-known tool to make
controllers safe is the so-called control-invariant set (a.k.a. safe set). In
our previous work, we have shown that safety can be improved by letting the
safe-set constraint recede along the MPC horizon. In this paper, we push that
idea further by exploiting parallel computation to improve safety. We solve
several MPC problems at the same time, where each problem instantiates the
safe-set constraint at a different time step along the horizon. Finally, the
controller can select the best solution according to some user-defined
criteria. We validated this idea through extensive simulations with a 3-joint
robotic arm, showing that significant improvements can be achieved in terms of
safety and performance, even using as little as 4 computational cores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vibration Damping in Underactuated Cable-suspended Artwork -- Flying
  Belt Motion Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Goubej, Lauria Clarke, Martin Hrabačka, David Tolar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a comprehensive refurbishment of the interactive robotic
art installation Standards and Double Standards by Rafael Lozano-Hemmer. The
installation features an array of belts suspended from the ceiling, each
actuated by stepper motors and dynamically oriented by a vision-based tracking
system that follows the movements of exhibition visitors. The original system
was limited by oscillatory dynamics, resulting in torsional and pendulum-like
vibrations that constrained rotational speed and reduced interactive
responsiveness. To address these challenges, the refurbishment involved
significant upgrades to both hardware and motion control algorithms. A detailed
mathematical model of the flying belt system was developed to accurately
capture its dynamic behavior, providing a foundation for advanced control
design. An input shaping method, formulated as a convex optimization problem,
was implemented to effectively suppress vibrations, enabling smoother and
faster belt movements. Experimental results demonstrate substantial
improvements in system performance and audience interaction. This work
exemplifies the integration of robotics, control engineering, and interactive
art, offering new solutions to technical challenges in real-time motion control
and vibration damping for large-scale kinetic installations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring persuasive Interactions with <span class="highlight-title">generative</span> social <span class="highlight-title">robots</span>: An
  experimental framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephan Vonschallen, Larissa Julia Corina Finsler, Theresa Schmiedel, Friederike Eyssel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating generative AI such as large language models into social robots
has improved their ability to engage in natural, human-like communication. This
study presents a method to examine their persuasive capabilities. We designed
an experimental framework focused on decision making and tested it in a pilot
that varied robot appearance and self-knowledge. Using qualitative analysis, we
evaluated interaction quality, persuasion effectiveness, and the robot's
communicative strategies. Participants generally experienced the interaction
positively, describing the robot as competent, friendly, and supportive, while
noting practical limits such as delayed responses and occasional
speech-recognition errors. Persuasiveness was highly context dependent and
shaped by robot behavior: participants responded well to polite, reasoned
suggestions and expressive gestures, but emphasized the need for more
personalized, context-aware arguments and clearer social roles. These findings
suggest that generative social robots can influence user decisions, but their
effectiveness depends on communicative nuance and contextual relevance. We
propose refinements to the framework to further study persuasive dynamics
between robots and human users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A shortened version of this paper was accepted as poster for the
  Thirteenth International Conference on Human-Agent Interaction (HAI2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of Embodiment in Intuitive <span class="highlight-title">Whole-Body</span> Teleoperation for Mobile
  <span class="highlight-title">Manipulation</span> <span class="chip">Humanoids</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03222v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03222v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophia Bianchi Moyen, Rickmer Krohn, Sophie Lueth, Kay Pompetzki, Jan Peters, Vignesh Prasad, Georgia Chalvatzaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intuitive Teleoperation interfaces are essential for mobile manipulation
robots to ensure high quality data collection while reducing operator workload.
A strong sense of embodiment combined with minimal physical and cognitive
demands not only enhances the user experience during large-scale data
collection, but also helps maintain data quality over extended periods. This
becomes especially crucial for challenging long-horizon mobile manipulation
tasks that require whole-body coordination. We compare two distinct robot
control paradigms: a coupled embodiment integrating arm manipulation and base
navigation functions, and a decoupled embodiment treating these systems as
separate control entities. Additionally, we evaluate two visual feedback
mechanisms: immersive virtual reality and conventional screen-based
visualization of the robot's field of view. These configurations were
systematically assessed across a complex, multi-stage task sequence requiring
integrated planning and execution. Our results show that the use of VR as a
feedback modality increases task completion time, cognitive workload, and
perceived effort of the teleoperator. Coupling manipulation and navigation
leads to a comparable workload on the user as decoupling the embodiments, while
preliminary experiments suggest that data acquired by coupled teleoperation
leads to better imitation learning performance. Our holistic view on intuitive
teleoperation interfaces provides valuable insight into collecting
high-quality, high-dimensional mobile manipulation data at scale with the human
operator in mind. Project
website:https://sophiamoyen.github.io/role-embodiment-wbc-moma-teleop/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, Accepted at the IEEE-RAS International Conference
  on Humanoid Robots (Humanoids) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Active Training for Deep <span class="highlight-title">LiDAR</span> Odometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beibei Zhou, Zhiyuan Zhang, Zhenbo Song, Jianhui Guo, Hui Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust and efficient deep LiDAR odometry models are crucial for accurate
localization and 3D reconstruction, but typically require extensive and diverse
training data to adapt to diverse environments, leading to inefficiencies. To
tackle this, we introduce an active training framework designed to selectively
extract training data from diverse environments, thereby reducing the training
load and enhancing model generalization. Our framework is based on two key
strategies: Initial Training Set Selection (ITSS) and Active Incremental
Selection (AIS). ITSS begins by breaking down motion sequences from general
weather into nodes and edges for detailed trajectory analysis, prioritizing
diverse sequences to form a rich initial training dataset for training the base
model. For complex sequences that are difficult to analyze, especially under
challenging snowy weather conditions, AIS uses scene reconstruction and
prediction inconsistency to iteratively select training samples, refining the
model to handle a wide range of real-world scenarios. Experiments across
datasets and weather conditions validate our approach's effectiveness. Notably,
our method matches the performance of full-dataset training with just 52\% of
the sequence volume, demonstrating the training efficiency and robustness of
our active training paradigm. By optimizing the training process, our approach
sets the stage for more agile and reliable LiDAR odometry systems, capable of
navigating diverse environmental conditions with greater precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralised self-organisation of pivoting cube ensembles using
  geometric deep <span class="highlight-title">learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadezhda Dobreva, Emmanuel Blazquez, Jai Grover, Dario Izzo, Yuzhen Qin, Dominik Dold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a decentralized model for autonomous reconfiguration of
homogeneous pivoting cube modular robots in two dimensions. Each cube in the
ensemble is controlled by a neural network that only gains information from
other cubes in its local neighborhood, trained using reinforcement learning.
Furthermore, using geometric deep learning, we include the grid symmetries of
the cube ensemble in the neural network architecture. We find that even the
most localized versions succeed in reconfiguring to the target shape, although
reconfiguration happens faster the more information about the whole ensemble is
available to individual cubes. Near-optimal reconfiguration is achieved with
only nearest neighbor interactions by using multiple information passing
between cubes, allowing them to accumulate more global information about the
ensemble. Compared to standard neural network architectures, using geometric
deep learning approaches provided only minor benefits. Overall, we successfully
demonstrate mostly local control of a modular self-assembling system, which is
transferable to other space-relevant systems with different action spaces, such
as sliding cube modular robots and CubeSat swarms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forbal: Force Balanced 2-5 Degree of Freedom <span class="highlight-title">Robot</span> Manipulator Built
  from a Five Bar Linkage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Vyas, Matteo Bottin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A force balanced manipulator design based on the closed chain planar five bar
linkage is developed and experimentally validated. We present 2 variants as a
modular design: Forbal-2, a planar 2-DOF manipulator, and its extension to
5-DOF spatial motion called Forbal-5. The design considerations in terms of
geometric, kinematic, and dynamic design that fulfill the force balance
conditions while maximizing workspace are discussed. Then, the inverse
kinematics of both variants are derived from geometric principles.
  We validate the improvements from force balancing the manipulator through
comparative experiments with counter mass balanced and unbalanced
configurations. The results show how the balanced configuration yields a
reduction in the average reaction moments of up to 66\%, a reduction of average
joint torques of up to 79\%, as well as a noticeable reduction in position
error for Forbal-2. For Forbal-5, which has a higher end effector payload mass,
the joint torques are reduced up to 84\% for the balanced configuration.
Experimental results validate that the balanced manipulator design is suitable
for applications where the reduction of joint torques and reaction
forces/moments helps achieve millimeter level precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Population-aware Online Mirror Descent for Mean-Field Games with Common
  Noise by Deep Reinforcement <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zida Wu, Mathieu Lauriere, Matthieu Geist, Olivier Pietquin, Ankur Mehta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mean Field Games (MFGs) offer a powerful framework for studying large-scale
multi-agent systems. Yet, learning Nash equilibria in MFGs remains a
challenging problem, particularly when the initial distribution is unknown or
when the population is subject to common noise. In this paper, we introduce an
efficient deep reinforcement learning (DRL) algorithm designed to achieve
population-dependent Nash equilibria without relying on averaging or historical
sampling, inspired by Munchausen RL and Online Mirror Descent. The resulting
policy is adaptable to various initial distributions and sources of common
noise. Through numerical experiments on seven canonical examples, we
demonstrate that our algorithm exhibits superior convergence properties
compared to state-of-the-art algorithms, particularly a DRL version of
Fictitious Play for population-dependent policies. The performance in the
presence of common noise underscores the robustness and adaptability of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE 64rd Conference on Decision and Control (CDC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-aware Test-Time Training (UT$^3$) for Efficient On-the-fly
  Domain Adaptive Dense Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uddeshya Upadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are increasingly being used in autonomous
systems. However, DNNs do not generalize well to domain shift. Adapting to a
continuously evolving environment is a safety-critical challenge inevitably
faced by all autonomous systems deployed to the real world. Recent work on
test-time training proposes methods that adapt to a new test distribution on
the fly by optimizing the DNN model for each test input using self-supervision.
However, these techniques result in a sharp increase in inference time as
multiple forward and backward passes are required for a single test sample (for
test-time training) before finally making the prediction based on the
fine-tuned features. This is undesirable for real-world robotics applications
where these models may be deployed to resource constraint hardware with strong
latency requirements. In this work, we propose a new framework (called UT$^3$)
that leverages test-time training for improved performance in the presence of
continuous domain shift while also decreasing the inference time, making it
suitable for real-world applications. Our method proposes an uncertainty-aware
self-supervision task for efficient test-time training that leverages the
quantified uncertainty to selectively apply the training leading to sharp
improvements in the inference time while performing comparably to standard
test-time training protocol. Our proposed protocol offers a continuous setting
to identify the selected keyframes, allowing the end-user to control how often
to apply test-time training. We demonstrate the efficacy of our method on a
dense regression task - monocular depth estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CTBC: Contact-Triggered Blind Climbing for Wheeled <span class="highlight-title">Biped</span>al <span class="highlight-title">Robots</span> with
  Instruction <span class="highlight-title">Learning</span> and Reinforcement <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rankun Li, Hao Wang, Qi Li, Zhuo Han, Yifei Chu, Linqi Ye, Wende Xie, Wenlong Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, wheeled bipedal robots have gained increasing attention due
to their advantages in mobility, such as high-speed locomotion on flat terrain.
However, their performance on complex environments (e.g., staircases) remains
inferior to that of traditional legged robots. To overcome this limitation, we
propose a general contact-triggered blind climbing (CTBC) framework for wheeled
bipedal robots. Upon detecting wheel-obstacle contact, the robot triggers a
leg-lifting motion to overcome the obstacle. By leveraging a strongly-guided
feedforward trajectory, our method enables the robot to rapidly acquire agile
leg-lifting skills, significantly enhancing its capability to traverse
unstructured terrains. The approach has been experimentally validated and
successfully deployed on LimX Dynamics' wheeled bipedal robot, Tron1.
Real-world tests demonstrate that Tron1 can reliably climb obstacles well
beyond its wheel radius using only proprioceptive feedback.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DUViN: Diffusion-Based Underwater <span class="highlight-title">Visual</span> <span class="highlight-title">Navigation</span> via
  Knowledge-Transferred Depth Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinghe Yang, Minh-Quan Le, Mingming Gong, Ye Pu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous underwater navigation remains a challenging problem due to limited
sensing capabilities and the difficulty of constructing accurate maps in
underwater environments. In this paper, we propose a Diffusion-based Underwater
Visual Navigation policy via knowledge-transferred depth features, named DUViN,
which enables vision-based end-to-end 4-DoF motion control for underwater
vehicles in unknown environments. DUViN guides the vehicle to avoid obstacles
and maintain a safe and perception awareness altitude relative to the terrain
without relying on pre-built maps. To address the difficulty of collecting
large-scale underwater navigation datasets, we propose a method that ensures
robust generalization under domain shifts from in-air to underwater
environments by leveraging depth features and introducing a novel model
transfer strategy. Specifically, our training framework consists of two phases:
we first train the diffusion-based visual navigation policy on in-air datasets
using a pre-trained depth feature extractor. Secondly, we retrain the extractor
on an underwater depth estimation task and integrate the adapted extractor into
the trained navigation policy from the first step. Experiments in both
simulated and real-world underwater environments demonstrate the effectiveness
and generalization of our approach. The experimental videos are available at
https://www.youtube.com/playlist?list=PLqt2s-RyCf1gfXJgFzKjmwIqYhrP4I-7Y.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IL-<span class="highlight-title">SLAM</span>: Intelligent Line-assisted <span class="highlight-title">SLAM</span> Based on Feature Awareness for
  <span class="highlight-title">Dynamic</span> Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolan Zhang, Thanh Nguyen Canh, Chenghao Li, Ruidong Yang, Yonghoon Ji, Nak Young Chong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Simultaneous Localization and Mapping (SLAM) plays a crucial role in
autonomous systems. Traditional SLAM methods, based on static environment
assumptions, struggle to handle complex dynamic environments. Recent dynamic
SLAM systems employ geometric constraints and deep learning to remove dynamic
features, yet this creates a new challenge: insufficient remaining point
features for subsequent SLAM processes. Existing solutions address this by
continuously introducing additional line and plane features to supplement point
features, achieving robust tracking and pose estimation. However, current
methods continuously introduce additional features regardless of necessity,
causing two problems: unnecessary computational overhead and potential
performance degradation from accumulated low-quality additional features and
noise. To address these issues, this paper proposes a feature-aware mechanism
that evaluates whether current features are adequate to determine if line
feature support should be activated. This decision mechanism enables the system
to introduce line features only when necessary, significantly reducing
computational complexity of additional features while minimizing the
introduction of low-quality features and noise. In subsequent processing, the
introduced line features assist in obtaining better initial camera poses
through tracking, local mapping, and loop closure, but are excluded from global
optimization to avoid potential negative impacts from low-quality additional
features in long-term process. Extensive experiments on TUM datasets
demonstrate substantial improvements in both ATE and RPE metrics compared to
ORB-SLAM3 baseline and superior performance over other dynamic SLAM and
multi-feature methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to International Conference on Robotic Computing and
  Communication(IEEE IRC)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VendiRL: A Framework for Self-Supervised Reinforcement <span class="highlight-title">Learning</span> of
  Diversely Diverse Skills 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02930v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02930v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik M. Lintunen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In self-supervised reinforcement learning (RL), one of the key challenges is
learning a diverse set of skills to prepare agents for unknown future tasks.
Despite impressive advances, scalability and evaluation remain prevalent
issues. Regarding scalability, the search for meaningful skills can be obscured
by high-dimensional feature spaces, where relevant features may vary across
downstream task domains. For evaluating skill diversity, defining what
constitutes "diversity" typically requires a hard commitment to a specific
notion of what it means for skills to be diverse, potentially leading to
inconsistencies in how skill diversity is understood, making results across
different approaches hard to compare, and leaving many forms of diversity
unexplored. To address these issues, we adopt a measure of sample diversity
that translates ideas from ecology to machine learning -- the Vendi Score --
allowing the user to specify and evaluate any desired form of diversity. We
demonstrate how this metric facilitates skill evaluation and introduce VendiRL,
a unified framework for learning diversely diverse sets of skills. Given
distinct similarity functions, VendiRL motivates distinct forms of diversity,
which could support skill-diversity pretraining in new and richly interactive
environments where optimising for various forms of diversity may be desirable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages including appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximate constrained stochastic optimal control via parameterized
  input inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahbaz P Qadri Syed, He Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate methods to solve stochastic optimal control (SOC) problems have
received significant interest from researchers in the past decade.
Probabilistic inference approaches to SOC have been developed to solve
nonlinear quadratic Gaussian problems. In this work, we propose an
Expectation-Maximization (EM) based inference procedure to generate
state-feedback controls for constrained SOC problems. We consider the
inequality constraints for the state and controls and also the structural
constraints for the controls. We employ barrier functions to address state and
control constraints. We show that the expectation step leads to smoothing of
the state-control pair while the the maximization step on the non-zero subsets
of the control parameters allows inference of structured stochastic optimal
controllers. We demonstrate the effectiveness of the algorithm on unicycle
obstacle avoidance, four-unicycle formation control, and quadcopter navigation
in windy environment examples. In these examples, we perform an empirical study
on the parametric effect of barrier functions on the state constraint
satisfaction. We also present a comparative study of smoothing algorithms on
the performance of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sim2Val: Leveraging Correlation Across Test Platforms for
  Variance-Reduced Metric <span class="highlight-title">Estimation</span> <span class="chip">CoRL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.20553v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.20553v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachel Luo, Heng Yang, Michael Watson, Apoorva Sharma, Sushant Veer, Edward Schmerling, Marco Pavone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based robotic systems demand rigorous validation to assure reliable
performance, but extensive real-world testing is often prohibitively expensive,
and if conducted may still yield insufficient data for high-confidence
guarantees. In this work we introduce Sim2Val, a general estimation framework
that leverages paired data across test platforms, e.g., paired simulation and
real-world observations, to achieve better estimates of real-world metrics via
the method of control variates. By incorporating cheap and abundant auxiliary
measurements (for example, simulator outputs) as control variates for costly
real-world samples, our method provably reduces the variance of Monte Carlo
estimates and thus requires significantly fewer real-world samples to attain a
specified confidence bound on the mean performance. We provide theoretical
analysis characterizing the variance and sample-efficiency improvement, and
demonstrate empirically in autonomous driving and quadruped robotics settings
that our approach achieves high-probability bounds with markedly improved
sample efficiency. Our technique can lower the real-world testing burden for
validating the performance of the stack, thereby enabling more efficient and
cost-effective experimental evaluation of robotic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Robot Learning (CoRL) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Embodied AI: Emerging Risks and Opportunities for Policy Action 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jared Perlo, Alexander Robey, Fazl Barez, Luciano Floridi, Jakob Mökander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of embodied AI (EAI) is rapidly advancing. Unlike virtual AI, EAI
systems can exist in, learn from, reason about, and act in the physical world.
With recent advances in AI models and hardware, EAI systems are becoming
increasingly capable across wider operational domains. While EAI systems can
offer many benefits, they also pose significant risks, including physical harm
from malicious use, mass surveillance, as well as economic and societal
disruption. These risks require urgent attention from policymakers, as existing
policies governing industrial robots and autonomous vehicles are insufficient
to address the full range of concerns EAI systems present. To help address this
issue, this paper makes three contributions. First, we provide a taxonomy of
the physical, informational, economic, and social risks EAI systems pose.
Second, we analyze policies in the US, EU, and UK to assess how existing
frameworks address these risks and to identify critical gaps. We conclude by
offering policy recommendations for the safe and beneficial deployment of EAI
systems, such as mandatory testing and certification schemes, clarified
liability frameworks, and strategies to manage EAI's potentially transformative
economic and societal impacts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LanternNet: A Hub-and-Spoke System to Seek and Suppress Spotted
  Lanternfly Populations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.20800v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.20800v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinil Polepalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The invasive spotted lanternfly (SLF) poses a significant threat to
agriculture and ecosystems, causing widespread damage. Current control methods,
such as egg scraping, pesticides, and quarantines, prove labor-intensive,
environmentally hazardous, and inadequate for long-term SLF suppression. This
research introduces LanternNet, a novel autonomous robotic Hub-and-Spoke system
designed for scalable detection and suppression of SLF populations. A central,
tree-mimicking hub utilizes a YOLOv8 computer vision model for precise SLF
identification. Three specialized robotic spokes perform targeted tasks: pest
neutralization, environmental monitoring, and navigation/mapping. Field
deployment across multiple infested sites over 5 weeks demonstrated
LanternNet's efficacy. Quantitative analysis revealed significant reductions (p
< 0.01, paired t-tests) in SLF populations and corresponding improvements in
tree health indicators across the majority of test sites. Compared to
conventional methods, LanternNet offers substantial cost advantages and
improved scalability. Furthermore, the system's adaptability for enhanced
autonomy and targeting of other invasive species presents significant potential
for broader ecological impact. LanternNet demonstrates the transformative
potential of integrating robotics and AI for advanced invasive species
management and improved environmental outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controlling Deformable Objects with Non-negligible <span class="highlight-title">Dynamic</span>s: a
  Shape-Regulation Approach to End-Point Positioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastien Tiburzio, Tomás Coleman, Daniel Feliu-Talegon, Cosimo Della Santina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model-based manipulation of deformable objects has traditionally dealt with
objects while neglecting their dynamics, thus mostly focusing on very
lightweight objects at steady state. At the same time, soft robotic research
has made considerable strides toward general modeling and control, despite soft
robots and deformable objects being very similar from a mechanical standpoint.
In this work, we leverage these recent results to develop a control-oriented,
fully dynamic framework of slender deformable objects grasped at one end by a
robotic manipulator. We introduce a dynamic model of this system using
functional strain parameterizations and describe the manipulation challenge as
a regulation control problem. This enables us to define a fully model-based
control architecture, for which we can prove analytically closed-loop stability
and provide sufficient conditions for steady state convergence to the desired
state. The nature of this work is intended to be markedly experimental. We
provide an extensive experimental validation of the proposed ideas, tasking a
robot arm with controlling the distal end of six different cables, in a given
planar position and orientation in space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 18 figures. Accepted for publication as a Regular Paper in
  the IEEE Transactions on Robotics (T-RO)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stretchable Electrohydraulic Artificial Muscle for Full Motion Ranges in
  Musculoskeletal Antagonistic Joints <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11017v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11017v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Kazemipour, Ronan Hinchet, Robert K. Katzschmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial muscles play a crucial role in musculoskeletal robotics and
prosthetics to approximate the force-generating functionality of biological
muscle. However, current artificial muscle systems are typically limited to
either contraction or extension, not both. This limitation hinders the
development of fully functional artificial musculoskeletal systems. We address
this challenge by introducing an artificial antagonistic muscle system capable
of both contraction and extension. Our design integrates non-stretchable
electrohydraulic soft actuators (HASELs) with electrostatic clutches within an
antagonistic musculoskeletal framework. This configuration enables an
antagonistic joint to achieve a full range of motion without displacement loss
due to tendon slack. We implement a synchronization method to coordinate muscle
and clutch units, ensuring smooth motion profiles and speeds. This approach
facilitates seamless transitions between antagonistic muscles at operational
frequencies of up to 3.2 Hz. While our prototype utilizes electrohydraulic
actuators, this muscle-clutch concept is adaptable to other non-stretchable
artificial muscles, such as McKibben actuators, expanding their capability for
extension and full range of motion in antagonistic setups. Our design
represents a significant advancement in the development of fundamental
components for more functional and efficient artificial musculoskeletal
systems, bringing their capabilities closer to those of their biological
counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to the IEEE International Conference on
  Robotics and Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point Cloud Recombination: Systematic Real Data Augmentation Using
  <span class="highlight-title">Robot</span>ic Targets for <span class="highlight-title">LiDAR</span> Perception Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.02476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.02476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hubert Padusinski, Christian Steinhauser, Christian Scherl, Julian Gaal, Jacob Langner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The validation of LiDAR-based perception of intelligent mobile systems
operating in open-world applications remains a challenge due to the variability
of real environmental conditions. Virtual simulations allow the generation of
arbitrary scenes under controlled conditions but lack physical sensor
characteristics, such as intensity responses or material-dependent effects. In
contrast, real-world data offers true sensor realism but provides less control
over influencing factors, hindering sufficient validation. Existing approaches
address this problem with augmentation of real-world point cloud data by
transferring objects between scenes. However, these methods do not consider
validation and remain limited in controllability because they rely on empirical
data. We solve these limitations by proposing Point Cloud Recombination, which
systematically augments captured point cloud scenes by integrating point clouds
acquired from physical target objects measured in controlled laboratory
environments. Thus enabling the creation of vast amounts and varieties of
repeatable, physically accurate test scenes with respect to phenomena-aware
occlusions with registered 3D meshes. Using the Ouster OS1-128 Rev7 sensor, we
demonstrate the augmentation of real-world urban and rural scenes with humanoid
targets featuring varied clothing and poses, for repeatable positioning. We
show that the recombined scenes closely match real sensor outputs, enabling
targeted testing, scalable failure analysis, and improved system safety. By
providing controlled yet sensor-realistic data, our method enables trustworthy
conclusions about the limitations of specific sensors in compound with their
algorithms, e.g., object detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print for IEEE IAVVC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Coarse-to-Fine Approach to <span class="highlight-title">Multi-Modal</span>ity 3D Occupancy Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01197v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01197v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Shi, Song Wang, Junbo Chen, Jianke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual grounding aims to identify objects or regions in a scene based on
natural language descriptions, essential for spatially aware perception in
autonomous driving. However, existing visual grounding tasks typically depend
on bounding boxes that often fail to capture fine-grained details. Not all
voxels within a bounding box are occupied, resulting in inaccurate object
representations. To address this, we introduce a benchmark for 3D occupancy
grounding in challenging outdoor scenes. Built on the nuScenes dataset, it
integrates natural language with voxel-level occupancy annotations, offering
more precise object perception compared to the traditional grounding task.
Moreover, we propose GroundingOcc, an end-to-end model designed for 3D
occupancy grounding through multi-modal learning. It combines visual, textual,
and point cloud features to predict object location and occupancy information
from coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder
for feature extraction, an occupancy head for voxel-wise predictions, and a
grounding head to refine localization. Additionally, a 2D grounding module and
a depth estimation module enhance geometric understanding, thereby boosting
model performance. Extensive experiments on the benchmark demonstrate that our
method outperforms existing baselines on 3D occupancy grounding. The dataset is
available at https://github.com/RONINGOD/GroundingOcc.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hey, Teacher, (Don't) Leave Those Kids Alone: Standardizing HRI
  Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.00024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.00024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexis E. Block
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating a standardized introduction course becomes more critical as the
field of human-robot interaction (HRI) becomes more established. This paper
outlines the key components necessary to provide an undergraduate with a
sufficient foundational understanding of the interdisciplinary nature of this
field and provides proposed course content. It emphasizes the importance of
creating a course with theoretical and experimental components to accommodate
all different learning preferences. This manuscript also advocates creating or
adopting a universal platform to standardize the hands-on component of
introductory HRI courses, regardless of university funding or size. Next, it
recommends formal training in how to read scientific articles and staying
up-to-date with the latest relevant papers. Finally, it provides detailed
lecture content and project milestones for a 15-week semester. By creating a
standardized course, researchers can ensure consistency and quality are
maintained across institutions, which will help students as well as industrial
and academic employers understand what foundational knowledge is expected.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the Designing an Intro to HRI Course Workshop at HRI
  2024 (arXiv:2403.05588)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong
  <span class="highlight-title">Learning</span> in Physical Embodied Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01415v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01415v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingcong Lei, Honghao Cai, Binbin Que, Zezhou Cui, Liangchen Tan, Junkun Hong, Gehan Hu, Shuangyu Zhu, Yimou Wu, Shaohan Jiang, Ge Wang, Zhen Li, Shuguang Cui, Yiming Zhao, Yatong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present RoboMemory, a brain-inspired multi-memory framework for lifelong
learning in physical embodied systems, addressing critical challenges in
real-world environments: continuous learning, multi-module memory latency, task
correlation capture, and infinite-loop mitigation in closed-loop planning.
Grounded in cognitive neuroscience, it integrates four core modules: the
Information Preprocessor (thalamus-like), the Lifelong Embodied Memory System
(hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and
the Low-Level Executer (cerebellum-like) to enable long-term planning and
cumulative learning. The Lifelong Embodied Memory System, central to the
framework, alleviates inference speed issues in complex memory frameworks via
parallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic
submodules. It incorporates a dynamic Knowledge Graph (KG) and consistent
architectural design to enhance memory consistency and scalability. Evaluations
on EmbodiedBench show RoboMemory outperforms the open-source baseline
(Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the
closed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing
new SOTA. Ablation studies validate key components (critic, spatial memory,
long-term memory), while real-world deployment confirms its lifelong learning
capability with significantly improved success rates across repeated tasks.
RoboMemory alleviates high latency challenges with scalability, serving as a
foundational reference for integrating multi-modal memory systems in physical
robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Lloyd-Based Algorithm for Uncertainty-Aware Multi-<span class="highlight-title">Robot</span>
  Under-Canopy Flocking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.18840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.18840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Boldrer, Vit Kratky, Viktor Walter, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this letter, we present a distributed algorithm for flocking in complex
environments that operates at constant altitude, without explicit
communication, no a priori information about the environment, and by using only
on-board sensing and computation capabilities. We provide sufficient conditions
to guarantee that each robot reaches its goal region in a finite time, avoiding
collisions with obstacles and other robots without exceeding a desired maximum
distance from a predefined set of neighbors (flocking or proximity constraint).
The proposed approach allows to operate in crowded scenarios and to deal with
tracking errors and on-board sensing errors, without violating safety and
proximity constraints. The algorithm was verified through simulations with
varying number of UAVs and also through numerous real-world experiments in a
dense forest involving up to four UAVs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sem-RaDiff: Diffusion-Based 3D <span class="highlight-title">Radar</span> <span class="highlight-title">Semantic</span> Perception in Cluttered
  Agricultural Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruibin Zhang, Fei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and robust environmental perception is crucial for robot autonomous
navigation. While current methods typically adopt optical sensors (e.g.,
camera, LiDAR) as primary sensing modalities, their susceptibility to visual
occlusion often leads to degraded performance or complete system failure. In
this paper, we focus on agricultural scenarios where robots are exposed to the
risk of onboard sensor contamination. Leveraging radar's strong penetration
capability, we introduce a radar-based 3D environmental perception framework as
a viable alternative. It comprises three core modules designed for dense and
accurate semantic perception: 1) Parallel frame accumulation to enhance
signal-to-noise ratio of radar raw data. 2) A diffusion model-based
hierarchical learning framework that first filters radar sidelobe artifacts
then generates fine-grained 3D semantic point clouds. 3) A specifically
designed sparse 3D network optimized for processing large-scale radar raw data.
We conducted extensive benchmark comparisons and experimental evaluations on a
self-built dataset collected in real-world agricultural field scenes. Results
demonstrate that our method achieves superior structural and semantic
prediction performance compared to existing methods, while simultaneously
reducing computational and memory costs by 51.3% and 27.5%, respectively.
Furthermore, our approach achieves complete reconstruction and accurate
classification of thin structures such as poles and wires-which existing
methods struggle to perceive-highlighting its potential for dense and accurate
3D radar perception.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Communication Efficient <span class="highlight-title">Robot</span>ic Mixed Reality with Gaussian Splatting
  Cross-Layer <span class="highlight-title">Optimization</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.08624v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.08624v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxuan Liu, He Li, Zongze Li, Shuai Wang, Wei Xu, Kejiang Ye, Derrick Wing Kwan Ng, Chengzhong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Realizing low-cost communication in robotic mixed reality (RoboMR) systems
presents a challenge, due to the necessity of uploading high-resolution images
through wireless channels. This paper proposes Gaussian splatting (GS) RoboMR
(GSMR), which enables the simulator to opportunistically render a
photo-realistic view from the robot's pose by calling ``memory'' from a GS
model, thus reducing the need for excessive image uploads. However, the GS
model may involve discrepancies compared to the actual environments. To this
end, a GS cross-layer optimization (GSCLO) framework is further proposed, which
jointly optimizes content switching (i.e., deciding whether to upload image or
not) and power allocation (i.e., adjusting to content profiles) across
different frames by minimizing a newly derived GSMR loss function. The GSCLO
problem is addressed by an accelerated penalty optimization (APO) algorithm
that reduces computational complexity by over $10$x compared to traditional
branch-and-bound and search algorithms. Moreover, variants of GSCLO are
presented to achieve robust, low-power, and multi-robot GSMR. Extensive
experiments demonstrate that the proposed GSMR paradigm and GSCLO method
achieve significant improvements over existing benchmarks on both wheeled and
legged robots in terms of diverse metrics in various scenarios. For the first
time, it is found that RoboMR can be achieved with ultra-low communication
costs, and mixture of data is useful for enhancing GS performance in dynamic
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 18 figures, to appear in IEEE Transactions on Cognitive
  Communications and Networking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RMMI: Reactive Mobile <span class="highlight-title">Manipulation</span> using an Implicit Neural Map <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.16206v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.16206v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Marticorena, Tobias Fischer, Jesse Haviland, Niko Suenderhauf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile manipulator robots operating in complex domestic and industrial
environments must effectively coordinate their base and arm motions while
avoiding obstacles. While current reactive control methods gracefully achieve
this coordination, they rely on simplified and idealised geometric
representations of the environment to avoid collisions. This limits their
performance in cluttered environments. To address this problem, we introduce
RMMI, a reactive control framework that leverages the ability of neural Signed
Distance Fields (SDFs) to provide a continuous and differentiable
representation of the environment's geometry. RMMI formulates a quadratic
program that optimises jointly for robot base and arm motion, maximises the
manipulability, and avoids collisions through a set of inequality constraints.
These constraints are constructed by querying the SDF for the distance and
direction to the closest obstacle for a large number of sampling points on the
robot. We evaluate RMMI both in simulation and in a set of real-world
experiments. For reaching in cluttered environments, we observe a 25% increase
in success rate. For additional details, code, and experiment videos, please
visit https://rmmi.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, accepted to the IEEE/RSJ International Conference
  on Intelligent Robots and Systems (IROS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dexonomy: Synthesizing All Dexterous Grasp Types in a Grasp Taxonomy <span class="chip">RSS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.18829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.18829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Chen, Yubin Ke, Lin Peng, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizable dexterous grasping with suitable grasp types is a fundamental
skill for intelligent robots. Developing such skills requires a large-scale and
high-quality dataset that covers numerous grasp types (i.e., at least those
categorized by the GRASP taxonomy), but collecting such data is extremely
challenging. Existing automatic grasp synthesis methods are often limited to
specific grasp types or object categories, hindering scalability. This work
proposes an efficient pipeline capable of synthesizing contact-rich,
penetration-free, and physically plausible grasps for any grasp type, object,
and articulated hand. Starting from a single human-annotated template for each
hand and grasp type, our pipeline tackles the complicated synthesis problem
with two stages: optimize the object to fit the hand template first, and then
locally refine the hand to fit the object in simulation. To validate the
synthesized grasps, we introduce a contact-aware control strategy that allows
the hand to apply the appropriate force at each contact point to the object.
Those validated grasps can also be used as new grasp templates to facilitate
future synthesis. Experiments show that our method significantly outperforms
previous type-unaware grasp synthesis baselines in simulation. Using our
algorithm, we construct a dataset containing 10.7k objects and 9.5M grasps,
covering 31 grasp types in the GRASP taxonomy. Finally, we train a
type-conditional generative model that successfully performs the desired grasp
type from single-view object point clouds, achieving an 82.3% success rate in
real-world experiments. Project page: https://pku-epic.github.io/Dexonomy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Robotics: Science and Systems (RSS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BODex: Scalable and Efficient <span class="highlight-title">Robot</span>ic Dexterous Grasp Synthesis Using
  Bilevel <span class="highlight-title">Optimization</span> <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16490v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16490v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Chen, Yubin Ke, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic dexterous grasping is important for interacting with the environment.
To unleash the potential of data-driven models for dexterous grasping, a
large-scale, high-quality dataset is essential. While gradient-based
optimization offers a promising way for constructing such datasets, previous
works suffer from limitations, such as inefficiency, strong assumptions in the
grasp quality energy, or limited object sets for experiments. Moreover, the
lack of a standard benchmark for comparing different methods and datasets
hinders progress in this field. To address these challenges, we develop a
highly efficient synthesis system and a comprehensive benchmark with MuJoCo for
dexterous grasping. We formulate grasp synthesis as a bilevel optimization
problem, combining a novel lower-level quadratic programming (QP) with an
upper-level gradient descent process. By leveraging recent advances in
CUDA-accelerated robotic libraries and GPU-based QP solvers, our system can
parallelize thousands of grasps and synthesize over 49 grasps per second on a
single 3090 GPU. Our synthesized grasps for Shadow, Allegro, and Leap hands all
achieve a success rate above 75% in simulation, with a penetration depth under
1 mm, outperforming existing baselines on nearly all metrics. Compared to the
previous large-scale dataset, DexGraspNet, our dataset significantly improves
the performance of learning models, with a success rate from around 40% to 80%
in simulation. Real-world testing of the trained model on the Shadow Hand
achieves an 81% success rate across 20 diverse objects. The codes and datasets
are released on our project page: https://pku-epic.github.io/BODex.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span>: <span class="highlight-title">Learning</span> Embodied Intelligence from Physical Simulators and
  World Models <span class="chip">3DV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.00917v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.00917v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxiao Long, Qingrui Zhao, Kaiwen Zhang, Zihao Zhang, Dingrui Wang, Yumeng Liu, Zhengjie Shu, Yi Lu, Shouzheng Wang, Xinzhe Wei, Wei Li, Wei Yin, Yao Yao, Jia Pan, Qiu Shen, Ruigang Yang, Xun Cao, Qionghai Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pursuit of artificial general intelligence (AGI) has placed embodied
intelligence at the forefront of robotics research. Embodied intelligence
focuses on agents capable of perceiving, reasoning, and acting within the
physical world. Achieving robust embodied intelligence requires not only
advanced perception and control, but also the ability to ground abstract
cognition in real-world interactions. Two foundational technologies, physical
simulators and world models, have emerged as critical enablers in this quest.
Physical simulators provide controlled, high-fidelity environments for training
and evaluating robotic agents, allowing safe and efficient development of
complex behaviors. In contrast, world models empower robots with internal
representations of their surroundings, enabling predictive planning and
adaptive decision-making beyond direct sensory input. This survey
systematically reviews recent advances in learning embodied AI through the
integration of physical simulators and world models. We analyze their
complementary roles in enhancing autonomy, adaptability, and generalization in
intelligent robots, and discuss the interplay between external simulation and
internal modeling in bridging the gap between simulated training and real-world
deployment. By synthesizing current progress and identifying open challenges,
this survey aims to provide a comprehensive perspective on the path toward more
capable and generalizable embodied AI systems. We also maintain an active
repository that contains up-to-date literature and open-source projects at
https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update with recent progresses. 49pages, 25figures, 6tables, github
  repository avalible in
  https://github.com/NJU3DV-LoongGroup/Embodied-World-Models-Survey</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">28</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContraGS: Codebook-Condensed and Trainable Gaussian Splatting for Fast,
  Memory-Efficient Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sankeerth Durvasula, Sharanshangar Muhunthan, Zain Moustafa, Richard Chen, Ruofan Liang, Yushi Guan, Nilesh Ahuja, Nilesh Jain, Selvakumar Panneer, Nandita Vijaykumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting (3DGS) is a state-of-art technique to model real-world
scenes with high quality and real-time rendering. Typically, a higher quality
representation can be achieved by using a large number of 3D Gaussians.
However, using large 3D Gaussian counts significantly increases the GPU device
memory for storing model parameters. A large model thus requires powerful GPUs
with high memory capacities for training and has slower training/rendering
latencies due to the inefficiencies of memory access and data movement. In this
work, we introduce ContraGS, a method to enable training directly on compressed
3DGS representations without reducing the Gaussian Counts, and thus with a
little loss in model quality. ContraGS leverages codebooks to compactly store a
set of Gaussian parameter vectors throughout the training process, thereby
significantly reducing memory consumption. While codebooks have been
demonstrated to be highly effective at compressing fully trained 3DGS models,
directly training using codebook representations is an unsolved challenge.
ContraGS solves the problem of learning non-differentiable parameters in
codebook-compressed representations by posing parameter estimation as a
Bayesian inference problem. To this end, ContraGS provides a framework that
effectively uses MCMC sampling to sample over a posterior distribution of these
compressed representations. With ContraGS, we demonstrate that ContraGS
significantly reduces the peak memory during training (on average 3.49X) and
accelerated training and rendering (1.36X and 1.88X on average, respectively),
while retraining close to state-of-art quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STA-Net: A Decoupled Shape and Texture Attention Network for Lightweight
  Plant Disease Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongsen Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Responding to rising global food security needs, precision agriculture and
deep learning-based plant disease diagnosis have become crucial. Yet, deploying
high-precision models on edge devices is challenging. Most lightweight networks
use attention mechanisms designed for generic object recognition, which poorly
capture subtle pathological features like irregular lesion shapes and complex
textures. To overcome this, we propose a twofold solution: first, using a
training-free neural architecture search method (DeepMAD) to create an
efficient network backbone for edge devices; second, introducing the
Shape-Texture Attention Module (STAM). STAM splits attention into two branches
-- one using deformable convolutions (DCNv4) for shape awareness and the other
using a Gabor filter bank for texture awareness. On the public CCMT plant
disease dataset, our STA-Net model (with 401K parameters and 51.1M FLOPs)
reached 89.00% accuracy and an F1 score of 88.96%. Ablation studies confirm
STAM significantly improves performance over baseline and standard attention
models. Integrating domain knowledge via decoupled attention thus presents a
promising path for edge-deployed precision agriculture AI. The source code is
available at https://github.com/RzMY/STA-Net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping on a Budget: Optimizing Spatial Data Collection for ML 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Livia Betti, Farooq Sanni, Gnouyaro Sogoyou, Togbe Agbagla, Cullen Molitor, Tamma Carleton, Esther Rolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In applications across agriculture, ecology, and human development, machine
learning with satellite imagery (SatML) is limited by the sparsity of labeled
training data. While satellite data cover the globe, labeled training datasets
for SatML are often small, spatially clustered, and collected for other
purposes (e.g., administrative surveys or field measurements). Despite the
pervasiveness of this issue in practice, past SatML research has largely
focused on new model architectures and training algorithms to handle scarce
training data, rather than modeling data conditions directly. This leaves
scientists and policymakers who wish to use SatML for large-scale monitoring
uncertain about whether and how to collect additional data to maximize
performance. Here, we present the first problem formulation for the
optimization of spatial training data in the presence of heterogeneous data
collection costs and realistic budget constraints, as well as novel methods for
addressing this problem. In experiments simulating different problem settings
across three continents and four tasks, our strategies reveal substantial gains
from sample optimization. Further experiments delineate settings for which
optimized sampling is particularly effective. The problem formulation and
methods we introduce are designed to generalize across application domains for
SatML; we put special emphasis on a specific problem setting where our
coauthors can immediately use our findings to augment clustered agricultural
surveys for SatML monitoring in Togo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Singular Value Few-shot Adaptation of <span class="highlight-title">Vision</span>-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03740v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03740v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Koleilat, Hassan Rivaz, Yiming Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) like CLIP have shown impressive zero-shot and
few-shot learning capabilities across diverse applications. However, adapting
these models to new fine-grained domains remains difficult due to reliance on
prompt engineering and the high cost of full model fine-tuning. Existing
adaptation approaches rely on augmented components, such as prompt tokens and
adapter modules, which could limit adaptation quality, destabilize the model,
and compromise the rich knowledge learned during pretraining. In this work, we
present \textbf{CLIP-SVD}, a novel \textit{multi-modal} and
\textit{parameter-efficient} adaptation technique that leverages Singular Value
Decomposition (SVD) to modify the internal parameter space of CLIP without
injecting additional modules. Specifically, we fine-tune only the singular
values of the CLIP parameter matrices to rescale the basis vectors for domain
adaptation while retaining the pretrained model. This design enables enhanced
adaptation performance using only \textbf{0.04\%} of the model's total
parameters and better preservation of its generalization ability. CLIP-SVD
achieves state-of-the-art classification results on 11 natural and 10
biomedical datasets, outperforming previous methods in both accuracy and
generalization under few-shot settings. Additionally, we leverage a natural
language-based approach to analyze the effectiveness and dynamics of the CLIP
adaptation to allow interpretability of CLIP-SVD. The code is publicly
available at https://github.com/HealthX-Lab/CLIP-SVD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LayoutGKN: Graph Similarity <span class="highlight-title">Learning</span> of Floor Plans <span class="chip">BMVC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Casper van Engelenburg, Jan van Gemert, Seyran Khademi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Floor plans depict building layouts and are often represented as graphs to
capture the underlying spatial relationships. Comparison of these graphs is
critical for applications like search, clustering, and data visualization. The
most successful methods to compare graphs \ie, graph matching networks, rely on
costly intermediate cross-graph node-level interactions, therefore being slow
in inference time. We introduce \textbf{LayoutGKN}, a more efficient approach
that postpones the cross-graph node-level interactions to the end of the joint
embedding architecture. We do so by using a differentiable graph kernel as a
distance function on the final learned node-level embeddings. We show that
LayoutGKN computes similarity comparably or better than graph matching networks
while significantly increasing the speed.
\href{https://github.com/caspervanengelenburg/LayoutGKN}{Code and data} are
open.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BMVC (2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer <span class="highlight-title">Learning</span>-Based CNN Models for Plant Species Identification
  Using Leaf Venation Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bandita Bharadwaj, Ankur Mishra, Saurav Bharadwaj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study evaluates the efficacy of three deep learning architectures:
ResNet50, MobileNetV2, and EfficientNetB0 for automated plant species
classification based on leaf venation patterns, a critical morphological
feature with high taxonomic relevance. Using the Swedish Leaf Dataset
comprising images from 15 distinct species (75 images per species, totalling
1,125 images), the models were demonstrated using standard performance metrics
during training and testing phases. ResNet50 achieved a training accuracy of
94.11% but exhibited overfitting, reflected by a reduced testing accuracy of
88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated better
generalization capabilities, attaining a testing accuracy of 93.34% and an F1
score of 93.23%, indicating its suitability for lightweight, real-time
applications. EfficientNetB0 outperformed both models, achieving a testing
accuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,
highlighting its robustness in venation-based classification. The findings
underscore the potential of deep learning, particularly EfficientNetB0, in
developing scalable and accurate tools for automated plant taxonomy using
venation traits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QuantV2X: A Fully Quantized Multi-Agent System for Cooperative
  Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seth Z. Zhao, Huizhi Zhang, Zhaowei Li, Juntong Peng, Anthony Chui, Zewei Zhou, Zonglin Meng, Hao Xiang, Zhiyu Huang, Fujia Wang, Ran Tian, Chenfeng Xu, Bolei Zhou, Jiaqi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooperative perception through Vehicle-to-Everything (V2X) communication
offers significant potential for enhancing vehicle perception by mitigating
occlusions and expanding the field of view. However, past research has
predominantly focused on improving accuracy metrics without addressing the
crucial system-level considerations of efficiency, latency, and real-world
deployability. Noticeably, most existing systems rely on full-precision models,
which incur high computational and transmission costs, making them impractical
for real-time operation in resource-constrained environments. In this paper, we
introduce \textbf{QuantV2X}, the first fully quantized multi-agent system
designed specifically for efficient and scalable deployment of multi-modal,
multi-agent V2X cooperative perception. QuantV2X introduces a unified
end-to-end quantization strategy across both neural network models and
transmitted message representations that simultaneously reduces computational
load and transmission bandwidth. Remarkably, despite operating under low-bit
constraints, QuantV2X achieves accuracy comparable to full-precision systems.
More importantly, when evaluated under deployment-oriented metrics, QuantV2X
reduces system-level latency by 3.2$\times$ and achieves a +9.5 improvement in
mAP30 over full-precision baselines. Furthermore, QuantV2X scales more
effectively, enabling larger and more capable models to fit within strict
memory budgets. These results highlight the viability of a fully quantized
multi-agent intermediate fusion system for real-world deployment. The system
will be publicly released to promote research in this field:
https://github.com/ucla-mobility/QuantV2X.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LuxDiT: Lighting <span class="highlight-title">Estimation</span> with Video Diffusion Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruofan Liang, Kai He, Zan Gojcic, Igor Gilitschenski, Sanja Fidler, Nandita Vijaykumar, Zian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating scene lighting from a single image or video remains a longstanding
challenge in computer vision and graphics. Learning-based approaches are
constrained by the scarcity of ground-truth HDR environment maps, which are
expensive to capture and limited in diversity. While recent generative models
offer strong priors for image synthesis, lighting estimation remains difficult
due to its reliance on indirect visual cues, the need to infer global
(non-local) context, and the recovery of high-dynamic-range outputs. We propose
LuxDiT, a novel data-driven approach that fine-tunes a video diffusion
transformer to generate HDR environment maps conditioned on visual input.
Trained on a large synthetic dataset with diverse lighting conditions, our
model learns to infer illumination from indirect visual cues and generalizes
effectively to real-world scenes. To improve semantic alignment between the
input and the predicted environment map, we introduce a low-rank adaptation
finetuning strategy using a collected dataset of HDR panoramas. Our method
produces accurate lighting predictions with realistic angular high-frequency
details, outperforming existing state-of-the-art techniques in both
quantitative and qualitative evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://research.nvidia.com/labs/toronto-ai/LuxDiT/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Insights from <span class="highlight-title">Gradient</span> <span class="highlight-title">Dynamic</span>s: <span class="highlight-title">Gradient</span> Autoscaled Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent-Daniel Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient dynamics play a central role in determining the stability and
generalization of deep neural networks. In this work, we provide an empirical
analysis of how variance and standard deviation of gradients evolve during
training, showing consistent changes across layers and at the global scale in
convolutional networks. Motivated by these observations, we propose a
hyperparameter-free gradient normalization method that aligns gradient scaling
with their natural evolution. This approach prevents unintended amplification,
stabilizes optimization, and preserves convergence guarantees. Experiments on
the challenging CIFAR-100 benchmark with ResNet-20, ResNet-56, and VGG-16-BN
demonstrate that our method maintains or improves test accuracy even under
strong generalization. Beyond practical performance, our study highlights the
importance of directly tracking gradient dynamics, aiming to bridge the gap
between theoretical expectations and empirical behaviors, and to provide
insights for future optimization research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongpei Zheng, Lintao Xiang, Qijun Yang, Qian Lin, Hujun Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Large Multimodal Models (LMMs) has led to remarkable
progress in 2D visual understanding; however, extending these capabilities to
3D scene understanding remains a significant challenge. Existing approaches
predominantly rely on text-only supervision, which fails to provide the
geometric constraints required for learning robust 3D spatial representations.
In this paper, we introduce Reg3D, a novel Reconstructive Geometry Instruction
Tuning framework that addresses this limitation by incorporating geometry-aware
supervision directly into the training process. Our key insight is that
effective 3D understanding necessitates reconstructing underlying geometric
structures rather than merely describing them. Unlike existing methods that
inject 3D information solely at the input level, Reg3D adopts a
dual-supervision paradigm that leverages 3D geometric information both as input
and as explicit learning targets. Specifically, we design complementary
object-level and frame-level reconstruction tasks within a dual-encoder
architecture, enforcing geometric consistency to encourage the development of
spatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,
ScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performance
improvements, establishing a new training paradigm for spatially aware
multimodal models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point
  Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josafat-Mattias Burmeister, Andreas Tockner, Stefan Reder, Markus Engel, Rico Richter, Jan-Peter Mund, Jürgen Döllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Close-range laser scanning provides detailed 3D captures of forest stands but
requires efficient software for processing 3D point cloud data and extracting
individual trees. Although recent studies have introduced deep learning methods
for tree instance segmentation, these approaches require large annotated
datasets and substantial computational resources. As a resource-efficient
alternative, we present a revised version of the treeX algorithm, an
unsupervised method that combines clustering-based stem detection with region
growing for crown delineation. While the original treeX algorithm was developed
for personal laser scanning (PLS) data, we provide two parameter presets, one
for ground-based laser scanning (stationary terrestrial - TLS and PLS), and one
for UAV-borne laser scanning (ULS). We evaluated the method on six public
datasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, Wytham
Woods) and compared it to six open-source methods (original treeX, treeiso,
RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the original
treeX algorithm, our revision reduces runtime and improves accuracy, with
instance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.
For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the original
algorithm fails to segment any correct instances. For TLS and PLS data, our
algorithm achieves accuracy similar to recent open-source methods, including
deep learning. Given its algorithmic design, we see two main applications for
our method: (1) as a resource-efficient alternative to deep learning approaches
in scenarios where the data characteristics align with the method design
(sufficient stem visibility and point density), and (2) for the semi-automatic
generation of labels for deep learning models. To enable broader adoption, we
provide an open-source Python implementation in the pointtree package.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight image segmentation for echocardiography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anders Kjelsrud, Lasse Løvstakken, Erik Smistad, Håvard Dalen, Gilles Van De Vyver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of the left ventricle in echocardiography can enable
fully automatic extraction of clinical measurements such as volumes and
ejection fraction. While models configured by nnU-Net perform well, they are
large and slow, thus limiting real-time use. We identified the most effective
components of nnU-Net for cardiac segmentation through an ablation study,
incrementally evaluating data augmentation schemes, architectural
modifications, loss functions, and post-processing techniques. Our analysis
revealed that simple affine augmentations and deep supervision drive
performance, while complex augmentations and large model capacity offer
diminishing returns. Based on these insights, we developed a lightweight U-Net
(2M vs 33M parameters) that achieves statistically equivalent performance to
nnU-Net on CAMUS (N=500) with Dice scores of 0.93/0.85/0.89 vs 0.93/0.86/0.89
for LV/MYO/LA ($p>0.05$), while being 16 times smaller and 4 times faster
(1.35ms vs 5.40ms per frame) than the default nnU-Net configuration.
Cross-dataset evaluation on an internal dataset (N=311) confirms comparable
generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 6 figures, The 2025 IEEE International Ultrasonics Symposium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing Fine Structure in Protoplanetary Disks with Physics
  Constrained Neural Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aviad Levis, Nhan Luong, Richard Teague, Katherine. L. Bouman, Marcelo Barraza-Alfaro, Kevin Flaherty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protoplanetary disks are the birthplaces of planets, and resolving their
three-dimensional structure is key to understanding disk evolution. The
unprecedented resolution of ALMA demands modeling approaches that capture
features beyond the reach of traditional methods. We introduce a computational
framework that integrates physics-constrained neural fields with differentiable
rendering and present RadJAX, a GPU-accelerated, fully differentiable line
radiative transfer solver achieving up to 10,000x speedups over conventional
ray tracers, enabling previously intractable, high-dimensional neural
reconstructions. Applied to ALMA CO observations of HD 163296, this framework
recovers the vertical morphology of the CO-rich layer, revealing a pronounced
narrowing and flattening of the emission surface beyond 400 au - a feature
missed by existing approaches. Our work establish a new paradigm for extracting
complex disk structure and advancing our understanding of protoplanetary
evolution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi Attribute Bias Mitigation via Representation <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rajeev Ranjan Dwivedi, Ankur Kumar, Vinod K Kurmi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real world images frequently exhibit multiple overlapping biases, including
textures, watermarks, gendered makeup, scene object pairings, etc. These biases
collectively impair the performance of modern vision models, undermining both
their robustness and fairness. Addressing these biases individually proves
inadequate, as mitigating one bias often permits or intensifies others. We
tackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), a
lean two stage framework that needs group labels only while training and
minimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)
deliberately identifies the influence of known shortcuts by training encoders
for each attribute and integrating them with the main backbone, compelling the
classifier to explicitly recognize these biases. Then Gradient Suppression Fine
Tuning prunes those very bias directions from the backbone's gradients, leaving
a single compact network that ignores all the shortcuts it just learned to
recognize. Moreover we find that existing bias metrics break under subgroup
imbalance and train test distribution shifts, so we introduce Scaled Bias
Amplification (SBA): a test time measure that disentangles model induced bias
amplification from distributional differences. We validate GMBM on FB CMNIST,
CelebA, and COCO, where we boost worst group accuracy, halve multi attribute
bias amplification, and set a new low in SBA even as bias complexity and
distribution shifts intensify, making GMBM the first practical, end to end
multibias solution for visual recognition. Project page:
http://visdomlab.github.io/GMBM/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECAI 2025 (28th European Conference on Artificial Intelligence)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG
  2025 Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungho Choe, Xiaoli Qin, Abubakr Shafique, Amanda Dy, Susan Done, Dimitrios Androutsos, April Khademi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counting mitotic figures is time-intensive for pathologists and leads to
inter-observer variability. Artificial intelligence (AI) promises a solution by
automatically detecting mitotic figures while maintaining decision consistency.
However, AI tools are susceptible to domain shift, where a significant drop in
performance can occur due to differences in the training and testing sets,
including morphological diversity between organs, species, and variations in
staining protocols. Furthermore, the number of mitoses is much less than the
count of normal nuclei, which introduces severely imbalanced data for the
detection task. In this work, we formulate mitosis detection as a pixel-level
segmentation and propose a teacher-student model that simultaneously addresses
mitosis detection (Track 1) and atypical mitosis classification (Track 2). Our
method is based on a UNet segmentation backbone that integrates domain
generalization modules, namely contrastive representation learning and
domain-adversarial training. A teacher-student strategy is employed to generate
pixel-level pseudo-masks not only for annotated mitoses and hard negatives but
also for normal nuclei, thereby enhancing feature discrimination and improving
robustness against domain shift. For the classification task, we introduce a
multi-scale CNN classifier that leverages feature maps from the segmentation
model within a multi-task learning paradigm. On the preliminary test set, the
algorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of
0.8414 in Track 2, demonstrating the effectiveness of integrating
segmentation-based detection and classification into a unified framework for
robust mitosis analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figures, final submission for MIDOG 2025 challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Efficient General Feature Prediction in Masked Skeleton Modeling <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengkai Sun, Zefan Zhang, Jianfeng Dong, Zhiyong Cheng, Xiaojun Chang, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in the masked autoencoder (MAE) paradigm have significantly
propelled self-supervised skeleton-based action recognition. However, most
existing approaches limit reconstruction targets to raw joint coordinates or
their simple variants, resulting in computational redundancy and limited
semantic representation. To address this, we propose a novel General Feature
Prediction framework (GFP) for efficient mask skeleton modeling. Our key
innovation is replacing conventional low-level reconstruction with high-level
feature prediction that spans from local motion patterns to global semantic
representations. Specifically, we introduce a collaborative learning framework
where a lightweight target generation network dynamically produces diversified
supervision signals across spatial-temporal hierarchies, avoiding reliance on
pre-computed offline features. The framework incorporates constrained
optimization to ensure feature diversity while preventing model collapse.
Experiments on NTU RGB+D 60, NTU RGB+D 120 and PKU-MMD demonstrate the benefits
of our approach: Computational efficiency (with 6.2$\times$ faster training
than standard masked skeleton modeling methods) and superior representation
quality, achieving state-of-the-art performance in various downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICCV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,
  but Not Direct the Play? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ouxiang Li, Yuan Wang, Xinting Hu, Huijuan Huang, Rui Chen, Jiarong Ou, Xin Tao, Pengfei Wan, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) generation aims to synthesize images from textual
prompts, which jointly specify what must be shown and imply what can be
inferred, thereby corresponding to two core capabilities: composition and
reasoning. However, with the emerging advances of T2I models in reasoning
beyond composition, existing benchmarks reveal clear limitations in providing
comprehensive evaluations across and within these capabilities. Meanwhile,
these advances also enable models to handle more complex prompts, whereas
current benchmarks remain limited to low scene density and simplified
one-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a
comprehensive and complex benchmark that evaluates both composition and
reasoning capabilities of T2I models. To ensure comprehensiveness, we structure
composition around scene graph elements (instance, attribute, and relation) and
reasoning around the philosophical framework of inference (deductive,
inductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To
increase complexity, driven by the inherent complexities of real-world
scenarios, we curate each prompt with high compositional density for
composition and multi-step inference for reasoning. We also pair each prompt
with a checklist that specifies individual yes/no questions to assess each
intended element independently to facilitate fine-grained and reliable
evaluation. In statistics, our benchmark comprises 1,080 challenging prompts
and around 13,500 checklist questions. Experiments across 27 current T2I models
reveal that their composition capability still remains limited in complex
high-density scenarios, while the reasoning capability lags even further behind
as a critical bottleneck, with all models struggling to infer implicit elements
from prompts. Our project page: https://t2i-corebench.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://t2i-corebench.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comprehensive Persian offline handwritten database for investigating
  the effects of heritability and family relationships on handwriting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abbas Zohrevand, Javad Sadri, Zahra Imani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a comprehensive database for research and investigation
on the effects of inheritance on handwriting. A database has been created that
can be used to answer questions such as: Is there a genetic component to
handwriting? Is handwriting inherited? Do family relationships affect
handwriting? Varieties of samples of handwritten components such as: digits,
letters, shapes and free paragraphs of 210 families including (grandparents,
parents, uncles, aunts, siblings, cousins, nephews and nieces) have been
collected using specially designed forms, and family relationships of all
writers are captured. To the best of our knowledge, no such database is
presently available. Based on comparisons and investigation of features of
handwritings of family members, similarities among their features and writing
styles are detected. Our database is freely available to the pattern
recognition community and hope it will pave the way for investigations on the
effects of inheritance and family relationships on handwritings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning
  via Synthetic Instruction Data <span class="chip">ICCV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honglu Zhou, Xiangyu Peng, Shrikant Kendre, Michael S. Ryoo, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Next-generation AI companions must go beyond general video understanding to
resolve spatial and temporal references in dynamic, real-world environments.
Existing Video Large Language Models (Video LLMs), while capable of
coarse-level comprehension, struggle with fine-grained, spatiotemporal
reasoning, especially when user queries rely on time-based event references for
temporal anchoring, or gestural cues for spatial anchoring to clarify object
references and positions. To bridge this critical gap, we introduce Strefer, a
synthetic instruction data generation framework designed to equip Video LLMs
with spatiotemporal referring and reasoning capabilities. Strefer produces
diverse instruction-tuning data using a data engine that pseudo-annotates
temporally dense, fine-grained video metadata, capturing rich spatial and
temporal information in a structured manner, including subjects, objects, their
locations as masklets, and their action descriptions and timelines. Our
approach enhances the ability of Video LLMs to interpret spatial and temporal
references, fostering more versatile, space-time-aware reasoning essential for
real-world AI companions. Without using proprietary models, costly human
annotation, or the need to annotate large volumes of new videos, experimental
evaluations show that models trained with data produced by Strefer outperform
baselines on tasks requiring spatial and temporal disambiguation. Additionally,
these models exhibit enhanced space-time-aware reasoning, establishing a new
foundation for perceptually grounded, instruction-tuned Video LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This technical report serves as the archival version of our paper
  accepted at the ICCV 2025 Workshop. For more information, please visit our
  project website: https://strefer.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepSea MOT: A benchmark <span class="highlight-title">dataset</span> for multi-object tracking on deep-sea
  video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Barnard, Elaine Liu, Kristine Walz, Brian Schlining, Nancy Jacobsen Stout, Lonny Lundsten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Benchmarking multi-object tracking and object detection model performance is
an essential step in machine learning model development, as it allows
researchers to evaluate model detection and tracker performance on
human-generated 'test' data, facilitating consistent comparisons between models
and trackers and aiding performance optimization. In this study, a novel
benchmark video dataset was developed and used to assess the performance of
several Monterey Bay Aquarium Research Institute object detection models and a
FathomNet single-class object detection model together with several trackers.
The dataset consists of four video sequences representing midwater and benthic
deep-sea habitats. Performance was evaluated using Higher Order Tracking
Accuracy, a metric that balances detection, localization, and association
accuracy. To the best of our knowledge, this is the first publicly available
benchmark for multi-object tracking in deep-sea video footage. We provide the
benchmark data, a clearly documented workflow for generating additional
benchmark videos, as well as example Python notebooks for computing metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures, dataset available at
  https://huggingface.co/datasets/MBARI-org/DeepSea-MOT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OneCAT: Decoder-Only Auto-Regressive Model for Unified Understanding and
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.03498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.03498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Li, Xinyu Peng, Yaoming Wang, Zelin Peng, Xin Chen, Rongxiang Weng, Jingang Wang, Xunliang Cai, Wenrui Dai, Hongkai Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce OneCAT, a unified multimodal model that seamlessly integrates
understanding, generation, and editing within a novel, pure decoder-only
transformer architecture. Our framework uniquely eliminates the need for
external components such as Vision Transformers (ViT) or vision tokenizer
during inference, leading to significant efficiency gains, especially for
high-resolution inputs. This is achieved through a modality-specific
Mixture-of-Experts (MoE) structure trained with a single autoregressive (AR)
objective, which also natively supports dynamic resolutions. Furthermore, we
pioneer a multi-scale visual autoregressive mechanism within the Large Language
Model (LLM) that drastically reduces decoding steps compared to diffusion-based
methods while maintaining state-of-the-art performance. Our findings
demonstrate the powerful potential of pure autoregressive modeling as a
sufficient and elegant foundation for unified multimodal intelligence. As a
result, OneCAT sets a new performance standard, outperforming existing
open-source unified multimodal models across benchmarks for multimodal
generation, editing, and understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OFTSR: One-Step Flow for Image Super-Resolution with Tunable
  Fidelity-Realism Trade-offs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09465v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09465v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanzhi Zhu, Ruiqing Wang, Shilin Lu, Junnan Li, Hanshu Yan, Kai Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in diffusion and flow-based generative models have
demonstrated remarkable success in image restoration tasks, achieving superior
perceptual quality compared to traditional deep learning approaches. However,
these methods either require numerous sampling steps to generate high-quality
images, resulting in significant computational overhead, or rely on common
model distillation, which usually imposes a fixed fidelity-realism trade-off
and thus lacks flexibility. In this paper, we introduce OFTSR, a novel
flow-based framework for one-step image super-resolution that can produce
outputs with tunable levels of fidelity and realism. Our approach first trains
a conditional flow-based super-resolution model to serve as a teacher model. We
then distill this teacher model by applying a specialized constraint.
Specifically, we force the predictions from our one-step student model for same
input to lie on the same sampling ODE trajectory of the teacher model. This
alignment ensures that the student model's single-step predictions from initial
states match the teacher's predictions from a closer intermediate state.
Through extensive experiments on datasets including FFHQ (256$\times$256),
DIV2K, and ImageNet (256$\times$256), we demonstrate that OFTSR achieves
state-of-the-art performance for one-step image super-resolution, while having
the ability to flexibly tune the fidelity-realism trade-off. Codes:
\href{https://github.com/yuanzhi-zhu/OFTSR}{https://github.com/yuanzhi-zhu/OFTSR}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast rigid alignment of heterogeneous images in sliced Wasserstein
  distance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13756v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13756v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Shi, Amit Singer, Eric J. Verbeke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many applications of computer vision rely on the alignment of similar but
non-identical images. We present a fast algorithm for aligning heterogeneous
images based on optimal transport. Our approach combines the speed of fast
Fourier methods with the robustness of sliced probability metrics and allows us
to efficiently compute the alignment between two $L \times L$ images using the
sliced 2-Wasserstein distance in $O(L^2 \log L)$ operations. We show that our
method is robust to translations, rotations and deformations in the images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Embeddings to Accuracy: Comparing Foundation Models for
  Radiographic Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.10823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.10823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xue Li, Jameson Merkow, Noel C. F. Codella, Alberto Santamaria-Pang, Naiteek Sangani, Alexander Ersoy, Christopher Burt, John W. Garrett, Richard J. Bruce, Joshua D. Warner, Tyler Bradshaw, Ivan Tarapov, Matthew P. Lungren, Alan B. McMillan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models provide robust embeddings for diverse tasks, including
medical imaging. We evaluate embeddings from seven general and medical-specific
foundation models (e.g., DenseNet121, BiomedCLIP, MedImageInsight, Rad-DINO,
CXR-Foundation) for training lightweight adapters in multi-class radiography
classification. Using a dataset of 8,842 radiographs across seven classes, we
trained adapters with algorithms like K-Nearest Neighbors, logistic regression,
SVM, random forest, and MLP. The combination of MedImageInsight embeddings with
an SVM or MLP adapter achieved the highest mean area under the curve (mAUC) of
93.1%. This performance was statistically superior to other models, including
MedSigLIP with an MLP (91.0%), Rad-DINO with an SVM (90.7%), and CXR-Foundation
with logistic regression (88.6%). In contrast, models like BiomedCLIP (82.8%)
and Med-Flamingo (78.5%) showed lower performance. Crucially, these lightweight
adapters are computationally efficient, training in minutes and performing
inference in seconds on a CPU, making them practical for clinical use. A
fairness analysis of the top-performing MedImageInsight adapter revealed
minimal performance disparities across patient gender (within 1.8%) and age
groups (std. dev < 1.4%), with no significant statistical differences. These
findings confirm that embeddings from specialized foundation models,
particularly MedImageInsight, can power accurate, efficient, and equitable
diagnostic tools using simple, lightweight adapters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cryo-em images are intrinsically low dimensional 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.11249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.11249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luke Evans, Octavian-Vlad Murad, Lars Dingeldein, Pilar Cossio, Roberto Covino, Marina Meila
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation-based inference provides a powerful framework for cryo-electron
microscopy, employing neural networks in methods like CryoSBI to infer
biomolecular conformations via learned latent representations. This latent
space represents a rich opportunity, encoding valuable information about the
physical system and the inference process. Harnessing this potential hinges on
understanding the underlying geometric structure of these representations. We
investigate this structure by applying manifold learning techniques to CryoSBI
representations of hemagglutinin (simulated and experimental). We reveal that
these high-dimensional data inherently populate low-dimensional, smooth
manifolds, with simulated data effectively covering the experimental
counterpart. By characterizing the manifold's geometry using Diffusion Maps and
identifying its principal axes of variation via coordinate interpretation
methods, we establish a direct link between the latent structure and key
physical parameters. Discovering this intrinsic low-dimensionality and
interpretable geometric organization not only validates the CryoSBI approach
but enables us to learn more from the data structure and provides opportunities
for improving future inference strategies by exploiting this revealed manifold
geometry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Dataset</span> Distillation as Pushforward Optimal Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Ye Tan, Emma Slade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset distillation aims to find a synthetic training set such that training
on the synthetic data achieves similar performance to training on real data,
with orders of magnitude less computational requirements. Existing methods can
be broadly categorized as either bi-level optimization problems that have
neural network training heuristics as the lower level problem, or disentangled
methods that bypass the bi-level optimization by matching distributions of
data. The latter method has the major advantages of speed and scalability in
terms of size of both training and distilled datasets. We demonstrate that when
equipped with an encoder-decoder structure, the empirically successful
disentangled methods can be reformulated as an optimal quantization problem,
where a finite set of points is found to approximate the underlying probability
measure by minimizing the expected projection distance. In particular, we link
existing disentangled dataset distillation methods to the classical optimal
quantization and Wasserstein barycenter problems, demonstrating consistency of
distilled datasets for diffusion-based generative priors. We propose Dataset
Distillation by Optimal Quantization, based on clustering in a latent space.
Compared to the previous SOTA method D\textsuperscript{4}M, we achieve better
performance and inter-model generalization on the ImageNet-1K dataset with
trivial additional computation, and SOTA performance in higher image-per-class
settings. Using the distilled noise initializations in a stronger diffusion
transformer model, we obtain SOTA distillation performance on ImageNet-1K and
its subsets, outperforming diffusion guidance methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Modified abstract, additional experiments based on diffusion
  transformers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TexVerse: A Universe of 3D Objects with High-Resolution Textures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.10868v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.10868v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Zhang, Li Zhang, Rui Ma, Nan Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce TexVerse, a large-scale 3D dataset featuring high-resolution
textures. While recent advances in large-scale 3D datasets have enhanced
high-resolution geometry generation, creating high-resolution textures
end-to-end remains underexplored due to the lack of suitable datasets. TexVerse
fills this gap with a curated collection of over 858K unique high-resolution 3D
models sourced from Sketchfab, including more than 158K models with physically
based rendering (PBR) materials. Each model encompasses all of its
high-resolution variants, bringing the total to 1.6M 3D instances. TexVerse
also includes specialized subsets: TexVerse-Skeleton, with 69K rigged models,
and TexVerse-Animation, with 54K animated models, both preserving original
skeleton and animation data uploaded by the user. We also provide detailed
model annotations describing overall characteristics, structural components,
and intricate features. TexVerse offers a high-quality data resource with
wide-ranging potential applications in texture synthesis, PBR material
development, animation, and various 3D vision and graphics tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/yiboz2001/TexVerse</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LumiNet: Latent Intrinsics Meets Diffusion Models for Indoor Scene
  Relighting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00177v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00177v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyan Xing, Konrad Groh, Sezer Karaoglu, Theo Gevers, Anand Bhattad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LumiNet, a novel architecture that leverages generative models
and latent intrinsic representations for effective lighting transfer. Given a
source image and a target lighting image, LumiNet synthesizes a relit version
of the source scene that captures the target's lighting. Our approach makes two
key contributions: a data curation strategy from the StyleGAN-based relighting
model for our training, and a modified diffusion-based ControlNet that
processes both latent intrinsic properties from the source image and latent
extrinsic properties from the target image. We further improve lighting
transfer through a learned adaptor (MLP) that injects the target's latent
extrinsic properties via cross-attention and fine-tuning.
  Unlike traditional ControlNet, which generates images with conditional maps
from a single scene, LumiNet processes latent representations from two
different images - preserving geometry and albedo from the source while
transferring lighting characteristics from the target. Experiments demonstrate
that our method successfully transfers complex lighting phenomena including
specular highlights and indirect illumination across scenes with varying
spatial layouts and materials, outperforming existing approaches on challenging
indoor scenes using only images as input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corrects an evaluation bug in Table 1 due to a data normalization
  error. Thanks to the Sony PlayStation team for discovering and reporting the
  issue. The paper's core contributions, qualitative results, and user study
  are unaffected. We also include a minor update to the method to further
  improve result quality. Project page: https://luminet-relight.github.io/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-09-02T00:00:00Z">2025-09-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">53</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable Skill <span class="highlight-title">Learning</span> for Construction <span class="highlight-title">Robots</span> with Crowdsourced
  Natural Language Instructions, Composable Skills Standardization, and Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02876v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02876v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongrui Yu, Vineet R. Kamat, Carol C. Menassa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The quasi-repetitive nature of construction work and the resulting lack of
generalizability in programming construction robots presents persistent
challenges to the broad adoption of robots in the construction industry. Robots
cannot achieve generalist capabilities as skills learnt from one domain cannot
readily transfer to another work domain or be directly used to perform a
different set of tasks. Human workers have to arduously reprogram their
scene-understanding, path-planning, and manipulation components to enable the
robots to perform alternate work tasks. The methods presented in this paper
resolve a significant proportion of such reprogramming workload by proposing a
generalizable learning architecture that directly teaches robots versatile
task-performance skills through crowdsourced online natural language
instructions. A Large Language Model (LLM), a standardized and modularized
hierarchical modeling approach, and Building Information Modeling-Robot sematic
data pipeline are developed to address the multi-task skill transfer problem.
The proposed skill standardization scheme and LLM-based hierarchical skill
learning framework were tested with a long-horizon drywall installation
experiment using a full-scale industrial robotic manipulator. The resulting
robot task learning scheme achieves multi-task reprogramming with minimal
effort and high quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review for ASCE OPEN: Multidisciplinary Journal of Civil
  Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Robot</span>ic 3D Flower Pose <span class="highlight-title">Estimation</span> for Small-Scale Urban Farms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harsh Muriki, Hong Ray Teo, Ved Sengupta, Ai-Ping Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The small scale of urban farms and the commercial availability of low-cost
robots (such as the FarmBot) that automate simple tending tasks enable an
accessible platform for plant phenotyping. We have used a FarmBot with a custom
camera end-effector to estimate strawberry plant flower pose (for robotic
pollination) from acquired 3D point cloud models. We describe a novel algorithm
that translates individual occupancy grids along orthogonal axes of a point
cloud to obtain 2D images corresponding to the six viewpoints. For each image,
2D object detection models for flowers are used to identify 2D bounding boxes
which can be converted into the 3D space to extract flower point clouds. Pose
estimation is performed by fitting three shapes (superellipsoids, paraboloids
and planes) to the flower point clouds and compared with manually labeled
ground truth. Our method successfully finds approximately 80% of flowers
scanned using our customized FarmBot platform and has a mean flower pose error
of 7.7 degrees, which is sufficient for robotic pollination and rivals previous
results. All code will be made available at
https://github.com/harshmuriki/flowerPose.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Embodiment <span class="highlight-title">Locomotion</span> at Scale with extreme Embodiment
  Randomization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nico Bohlinger, Jan Peters
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a single, general locomotion policy trained on a diverse
collection of 50 legged robots. By combining an improved embodiment-aware
architecture (URMAv2) with a performance-based curriculum for extreme
Embodiment Randomization, our policy learns to control millions of
morphological variations. Our policy achieves zero-shot transfer to unseen
real-world humanoid and quadruped robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving the Resilience of Quadrotors in Underground Environments by
  Combining <span class="highlight-title">Learning</span>-based and Safety Controllers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Ronald Ward, Mark Paral, Kristopher Riordan, Mykel J. Kochenderfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomously controlling quadrotors in large-scale subterranean environments
is applicable to many areas such as environmental surveying, mining operations,
and search and rescue. Learning-based controllers represent an appealing
approach to autonomy, but are known to not generalize well to
`out-of-distribution' environments not encountered during training. In this
work, we train a normalizing flow-based prior over the environment, which
provides a measure of how far out-of-distribution the quadrotor is at any given
time. We use this measure as a runtime monitor, allowing us to switch between a
learning-based controller and a safe controller when we are sufficiently
out-of-distribution. Our methods are benchmarked on a point-to-point navigation
task in a simulated 3D cave environment based on real-world point cloud data
from the DARPA Subterranean Challenge Final Event Dataset. Our experimental
results show that our combined controller simultaneously possesses the liveness
of the learning-based controller (completing the task quickly) and the safety
of the safety controller (avoiding collision).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and awarded best paper at the 11th International Conference
  on Control, Decision and Information Technologies (CoDIT 2025 -
  https://codit2025.org/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Impact of Adaptive Emotional Alignment on Mental State Attribution
  and User Empathy in HRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgia Buracchio, Ariele Callegari, Massimo Donini, Cristina Gena, Antonio Lieto, Alberto Lillo, Claudio Mattutino, Alessandro Mazzei, Linda Pigureddu, Manuel Striani, Fabiana Vernero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents an experiment on the effects of adaptive emotional
alignment between agents, considered a prerequisite for empathic communication,
in Human-Robot Interaction (HRI). Using the NAO robot, we investigate the
impact of an emotionally aligned, empathic, dialogue on these aspects: (i) the
robot's persuasive effectiveness, (ii) the user's communication style, and
(iii) the attribution of mental states and empathy to the robot. In an
experiment with 42 participants, two conditions were compared: one with neutral
communication and another where the robot provided responses adapted to the
emotions expressed by the users. The results show that emotional alignment does
not influence users' communication styles or have a persuasive effect. However,
it significantly influences attribution of mental states to the robot and its
perceived empathy
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>autohor copy of the paper accepted at ROMAN2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ac<span class="highlight-title">robot</span>ics: A Generalist Approahc To <span class="highlight-title">Quadrupedal</span> <span class="highlight-title">Robots</span>' Parkour 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guillaume Gagné-Labelle, Vassil Atanassov, Ioannis Havoutis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Climbing, crouching, bridging gaps, and walking up stairs are just a few of
the advantages that quadruped robots have over wheeled robots, making them more
suitable for navigating rough and unstructured terrain. However, executing such
manoeuvres requires precise temporal coordination and complex agent-environment
interactions. Moreover, legged locomotion is inherently more prone to slippage
and tripping, and the classical approach of modeling such cases to design a
robust controller thus quickly becomes impractical. In contrast, reinforcement
learning offers a compelling solution by enabling optimal control through trial
and error. We present a generalist reinforcement learning algorithm for
quadrupedal agents in dynamic motion scenarios. The learned policy rivals
state-of-the-art specialist policies trained using a mixture of experts
approach, while using only 25% as many agents during training. Our experiments
also highlight the key components of the generalist locomotion policy and the
primary factors contributing to its success.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supplementary material can be found here:
  https://drive.google.com/drive/folders/18h25azbCFfPF4fhSsRfxKrnZo3dPKs_j?usp=sharing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2nd Place Solution for CVPR2024 E2E Challenge: End-to-End Autonomous
  <span class="highlight-title">Driving</span> Using <span class="highlight-title">Vision</span> Language Model <span class="chip">CVPR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilong Guo, Yi Luo, Long Sha, Dongxu Wang, Panqu Wang, Chenyang Xu, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end autonomous driving has drawn tremendous attention recently. Many
works focus on using modular deep neural networks to construct the end-to-end
archi-tecture. However, whether using powerful large language models (LLM),
especially multi-modality Vision Language Models (VLM) could benefit the
end-to-end driving tasks remain a question. In our work, we demonstrate that
combining end-to-end architectural design and knowledgeable VLMs yield
impressive performance on the driving tasks. It is worth noting that our method
only uses a single camera and is the best camera-only solution across the
leaderboard, demonstrating the effectiveness of vision-based driving approach
and the potential for end-to-end driving tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2nd place in CVPR 2024 End-to-End Driving at Scale Challenge</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Manipulation</span> as in Simulation: Enabling Accurate Geometry Perception in
  <span class="highlight-title">Robots</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghuan Liu, Zhengbang Zhu, Xiaoshen Han, Peng Hu, Haotong Lin, Xinyao Li, Jingxiao Chen, Jiafeng Xu, Yichu Yang, Yunfeng Lin, Xinghang Li, Yong Yu, Weinan Zhang, Tao Kong, Bingyi Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern robotic manipulation primarily relies on visual observations in a 2D
color space for skill learning but suffers from poor generalization. In
contrast, humans, living in a 3D world, depend more on physical properties-such
as distance, size, and shape-than on texture when interacting with objects.
Since such 3D geometric information can be acquired from widely available depth
cameras, it appears feasible to endow robots with similar perceptual
capabilities. Our pilot study found that using depth cameras for manipulation
is challenging, primarily due to their limited accuracy and susceptibility to
various types of noise. In this work, we propose Camera Depth Models (CDMs) as
a simple plugin on daily-use depth cameras, which take RGB images and raw depth
signals as input and output denoised, accurate metric depth. To achieve this,
we develop a neural data engine that generates high-quality paired data from
simulation by modeling a depth camera's noise pattern. Our results show that
CDMs achieve nearly simulation-level accuracy in depth prediction, effectively
bridging the sim-to-real gap for manipulation tasks. Notably, our experiments
demonstrate, for the first time, that a policy trained on raw simulated depth,
without the need for adding noise or real-world fine-tuning, generalizes
seamlessly to real-world robots on two challenging long-horizon tasks involving
articulated, reflective, and slender objects, with little to no performance
degradation. We hope our findings will inspire future research in utilizing
simulation data and 3D information in general robot policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 18 figures, project page:
  https://manipulation-as-in-simulation.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fault-tolerant Model Predictive Control for Spacecraft 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Stöckner, Pedro Roque, Maria Charitidou, Dimos V. Dimarogonas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the cost and critical functions of satellite constellations, ensuring
mission longevity and safe decommissioning is essential for space
sustainability. This article presents a Model Predictive Control for spacecraft
trajectory and setpoint stabilization under multiple actuation failures. The
proposed solution allows us to efficiently control the faulty spacecraft
enabling safe navigation towards servicing or collision-free trajectories. The
proposed scheme ensures closed-loop asymptotic stability and is shown to be
recursively feasible. We demonstrate its efficacy through open-source numerical
results and realistic experiments using the ATMOS platform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been submitted to CDC2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coral: A Unifying Abstraction Layer for Composable <span class="highlight-title">Robot</span>ics Software 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Steven Swanbeck, Mitch Pryor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the multitude of excellent software components and tools available in
the robotics and broader software engineering communities, successful
integration of software for robotic systems remains a time-consuming and
challenging task for users of all knowledge and skill levels. And with robotics
software often being built into tightly coupled, monolithic systems, even minor
alterations to improve performance, adjust to changing task requirements, or
deploy to new hardware can require significant engineering investment. To help
solve this problem, this paper presents Coral, an abstraction layer for
building, deploying, and coordinating independent software components that
maximizes composability to allow for rapid system integration without modifying
low-level code. Rather than replacing existing tools, Coral complements them by
introducing a higher-level abstraction that constrains the integration process
to semantically meaningful choices, reducing the configuration burden without
limiting adaptability to diverse domains, systems, and tasks. We describe Coral
in detail and demonstrate its utility in integrating software for scenarios of
increasing complexity, including LiDAR-based SLAM and multi-robot corrosion
mitigation tasks. By enabling practical composability in robotics software,
Coral offers a scalable solution to a broad range of robotics system
integration challenges, improving component reusability, system
reconfigurability, and accessibility to both expert and non-expert users. We
release Coral open source.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ U-ARM : Ultra low-cost general teleoperation interface for <span class="highlight-title">robot</span>
  <span class="highlight-title">manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02437v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02437v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwen Zou, Zhaoye Zhou, Chenyang Shi, Zewei Ye, Junda Huang, Yan Ding, Bo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose U-Arm, a low-cost and rapidly adaptable leader-follower
teleoperation framework designed to interface with most of commercially
available robotic arms. Our system supports teleoperation through three
structurally distinct 3D-printed leader arms that share consistent control
logic, enabling seamless compatibility with diverse commercial robot
configurations. Compared with previous open-source leader-follower interfaces,
we further optimized both the mechanical design and servo selection, achieving
a bill of materials (BOM) cost of only \$50.5 for the 6-DoF leader arm and
\$56.8 for the 7-DoF version. To enhance usability, we mitigate the common
challenge in controlling redundant degrees of freedom by %engineering methods
mechanical and control optimizations. Experimental results demonstrate that
U-Arm achieves 39\% higher data collection efficiency and comparable task
success rates across multiple manipulation scenarios compared with Joycon,
another low-cost teleoperation interface. We have open-sourced all CAD models
of three configs and also provided simulation support for validating
teleoperation workflows. We also open-sourced real-world manipulation data
collected with U-Arm. The project website is
https://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals
  with <span class="highlight-title">Visual</span> Impairments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Xu, Qianwei Wang, Vineet Kamat, Carol Menassa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indoor built environments like homes and offices often present complex and
cluttered layouts that pose significant challenges for individuals who are
blind or visually impaired, especially when performing tasks that involve
locating and gathering multiple objects. While many existing assistive
technologies focus on basic navigation or obstacle avoidance, few systems
provide scalable and efficient multi-object search capabilities in real-world,
partially observable settings. To address this gap, we introduce OpenGuide, an
assistive mobile robot system that combines natural language understanding with
vision-language foundation models (VLM), frontier-based exploration, and a
Partially Observable Markov Decision Process (POMDP) planner. OpenGuide
interprets open-vocabulary requests, reasons about object-scene relationships,
and adaptively navigates and localizes multiple target items in novel
environments. Our approach enables robust recovery from missed detections
through value decay and belief-space reasoning, resulting in more effective
exploration and object localization. We validate OpenGuide in simulated and
real-world experiments, demonstrating substantial improvements in task success
rate and search efficiency over prior methods. This work establishes a
foundation for scalable, human-centered robotic assistance in assisted living
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-Informed Machine <span class="highlight-title">Learning</span> with Adaptive Grids for Optical
  Micro<span class="highlight-title">robot</span> Depth <span class="highlight-title">Estimation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lan Wei, Lou Genoud, Dandan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical microrobots actuated by optical tweezers (OT) offer great potential
for biomedical applications such as cell manipulation and microscale assembly.
These tasks demand accurate three-dimensional perception to ensure precise
control in complex and dynamic biological environments. However, the
transparent nature of microrobots and low-contrast microscopic imaging
challenge conventional deep learning methods, which also require large
annotated datasets that are costly to obtain. To address these challenges, we
propose a physics-informed, data-efficient framework for depth estimation of
optical microrobots. Our method augments convolutional feature extraction with
physics-based focus metrics, such as entropy, Laplacian of Gaussian, and
gradient sharpness, calculated using an adaptive grid strategy. This approach
allocates finer grids over microrobot regions and coarser grids over background
areas, enhancing depth sensitivity while reducing computational complexity. We
evaluate our framework on multiple microrobot types and demonstrate significant
improvements over baseline models. Specifically, our approach reduces mean
squared error (MSE) by over 60% and improves the coefficient of determination
(R^2) across all test cases. Notably, even when trained on only 20% of the
available data, our model outperforms ResNet50 trained on the full dataset,
highlighting its robustness under limited data conditions. Our code is
available at: https://github.com/LannWei/CBS2025.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE International Conference on Cyborg and Bionic Systems (CBS
  2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-Guided Long Horizon <span class="highlight-title">Manipulation</span> with LLM-based <span class="highlight-title">Planning</span> and
  <span class="highlight-title">Visual</span> Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changshi Zhou, Haichuan Xu, Ningquan Gu, Zhipeng Wang, Bin Cheng, Pengpeng Zhang, Yanchao Dong, Mitsuhiro Hayashibe, Yanmin Zhou, Bin He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language-guided long-horizon manipulation of deformable objects presents
significant challenges due to high degrees of freedom, complex dynamics, and
the need for accurate vision-language grounding. In this work, we focus on
multi-step cloth folding, a representative deformable-object manipulation task
that requires both structured long-horizon planning and fine-grained visual
perception. To this end, we propose a unified framework that integrates a Large
Language Model (LLM)-based planner, a Vision-Language Model (VLM)-based
perception system, and a task execution module. Specifically, the LLM-based
planner decomposes high-level language instructions into low-level action
primitives, bridging the semantic-execution gap, aligning perception with
action, and enhancing generalization. The VLM-based perception module employs a
SigLIP2-driven architecture with a bidirectional cross-attention fusion
mechanism and weight-decomposed low-rank adaptation (DoRA) fine-tuning to
achieve language-conditioned fine-grained visual grounding. Experiments in both
simulation and real-world settings demonstrate the method's effectiveness. In
simulation, it outperforms state-of-the-art baselines by 2.23, 1.87, and 33.3
on seen instructions, unseen instructions, and unseen tasks, respectively. On a
real robot, it robustly executes multi-step folding sequences from language
instructions across diverse cloth materials and configurations, demonstrating
strong generalization in practical scenarios. Project page:
https://language-guided.netlify.app/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-Inspired Soft Anthropomorphic Hand System for Neuromorphic Object
  and Pose Recognition Using Multimodal Signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengyi Wang, Xiangyu Fu, Nitish Thakor, Gordon Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human somatosensory system integrates multimodal sensory feedback,
including tactile, proprioceptive, and thermal signals, to enable comprehensive
perception and effective interaction with the environment. Inspired by the
biological mechanism, we present a sensorized soft anthropomorphic hand
equipped with diverse sensors designed to emulate the sensory modalities of the
human hand. This system incorporates biologically inspired encoding schemes
that convert multimodal sensory data into spike trains, enabling
highly-efficient processing through Spiking Neural Networks (SNNs). By
utilizing these neuromorphic signals, the proposed framework achieves 97.14%
accuracy in object recognition across varying poses, significantly
outperforming previous studies on soft hands. Additionally, we introduce a
novel differentiator neuron model to enhance material classification by
capturing dynamic thermal responses. Our results demonstrate the benefits of
multimodal sensory fusion and highlight the potential of neuromorphic
approaches for achieving efficient, robust, and human-like perception in
robotic systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive <span class="highlight-title">Navigation</span> Strategy for Low-Thrust Proximity Operations in
  Circular Relative Orbit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Ruggiero, Mauro Mancini, Elisa Capello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an adaptive observer-based navigation strategy for
spacecraft in Circular Relative Orbit (CRO) scenarios, addressing challenges in
proximity operations like formation flight and uncooperative target inspection.
The proposed method adjusts observer gains based on the estimated state to
achieve fast convergence and low noise sensitivity in state estimation. A
Lyapunov-based analysis ensures stability and accuracy, while simulations using
vision-based sensor data validate the approach under realistic conditions.
Compared to classical observers with time-invariant gains, the proposed method
enhances trajectory tracking precision and reduces control input switching,
making it a promising solution for autonomous spacecraft localization and
control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted and presented at the 35th AAS/AIAA Space
  Flight Mechanics Meeting, 2025, Kaua'i, Hawai</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Reliability in LLM-Integrated <span class="highlight-title">Robot</span>ic Systems: A Unified
  Approach to Security and Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxiao Zhang, Xiangrui Kong, Conan Dewitt, Thomas Bräunl, Jin B. Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating large language models (LLMs) into robotic systems has
revolutionised embodied artificial intelligence, enabling advanced
decision-making and adaptability. However, ensuring reliability, encompassing
both security against adversarial attacks and safety in complex environments,
remains a critical challenge. To address this, we propose a unified framework
that mitigates prompt injection attacks while enforcing operational safety
through robust validation mechanisms. Our approach combines prompt assembling,
state management, and safety validation, evaluated using both performance and
security metrics. Experiments show a 30.8% improvement under injection attacks
and up to a 325% improvement in complex environment settings under adversarial
conditions compared to baseline scenarios. This work bridges the gap between
safety and security in LLM-based robotic systems, offering actionable insights
for deploying reliable LLM-integrated mobile robots in real-world settings. The
framework is open-sourced with simulation and physical deployment demos at
https://llmeyesim.vercel.app/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Systematic Evaluation of Trade-Offs in Motion <span class="highlight-title">Planning</span> Algorithms for
  Optimal Industrial <span class="highlight-title">Robot</span>ic Work Cell Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        G. de Mathelin, C. Hartl-Nesic, A. Kugi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of industrial robotic work cells depends on optimizing
various hyperparameters referring to the cell layout, such as robot base
placement, tool placement, and kinematic design. Achieving this requires a
bilevel optimization approach, where the high-level optimization adjusts these
hyperparameters, and the low-level optimization computes robot motions.
However, computing the optimal robot motion is computationally infeasible,
introducing trade-offs in motion planning to make the problem tractable. These
trade-offs significantly impact the overall performance of the bilevel
optimization, but their effects still need to be systematically evaluated. In
this paper, we introduce metrics to assess these trade-offs regarding
optimality, time gain, robustness, and consistency. Through extensive
simulation studies, we investigate how simplifications in motion-level
optimization affect the high-level optimization outcomes, balancing
computational complexity with solution quality. The proposed algorithms are
applied to find the time-optimal kinematic design for a modular robot in two
palletization scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted to IFAC for publication under a Creative
  Commons Licence CC-BY-NC-ND</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Learning</span> Social Heuristics for Human-Aware Path <span class="highlight-title">Planning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Eirale, Matteo Leonetti, Marcello Chiaberge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social robotic navigation has been at the center of numerous studies in
recent years. Most of the research has focused on driving the robotic agent
along obstacle-free trajectories, respecting social distances from humans, and
predicting their movements to optimize navigation. However, in order to really
be socially accepted, the robots must be able to attain certain social norms
that cannot arise from conventional navigation, but require a dedicated
learning process. We propose Heuristic Planning with Learned Social Value
(HPLSV), a method to learn a value function encapsulating the cost of social
navigation, and use it as an additional heuristic in heuristic-search path
planning. In this preliminary work, we apply the methodology to the common
social scenario of joining a queue of people, with the intention of
generalizing to further human activities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Geometric Method for Base Parameter Analysis in <span class="highlight-title">Robot</span> Inertia
  Identification Based on Projective Geometric Algebra 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangzhen Sun, Ye Ding, Xiangyang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel geometric method for analytically determining the
base inertial parameters of robotic systems. The rigid body dynamics is
reformulated using projective geometric algebra, leading to a new
identification model named ``tetrahedral-point (TP)" model. Based on the rigid
body TP model, coefficients in the regresoor matrix of the identification model
are derived in closed-form, exhibiting clear geometric interpretations.
Building directly from the dynamic model, three foundational principles for
base parameter analysis are proposed: the shared points principle, fixed points
principle, and planar rotations principle. With these principles, algorithms
are developed to automatically determine all the base parameters. The core
algorithm, referred to as Dynamics Regressor Nullspace Generator (DRNG),
achieves $O(1)$-complexity theoretically following an $O(N)$-complexity
preprocessing stage, where $N$ is the number of rigid bodies. The proposed
method and algorithms are validated across four robots: Puma560, Unitree Go2, a
2RRU-1RRS parallel kinematics mechanism (PKM), and a 2PRS-1PSR PKM. In all
cases, the algorithms successfully identify the complete set of base
parameters. Notably, the approach demonstrates high robustness and
computational efficiency, particularly in the cases of PKMs. Through the
comprehensive demonstrations, the method is shown to be general, robust, and
efficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Align-Then-stEer: Adapting the <span class="highlight-title">Vision</span>-Language Action Models through
  Unified Latent Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhang, Chenwei Wang, Ouyang Lu, Yuan Zhao, Yunfei Ge, Zhenglong Sun, Xiu Li, Chi Zhang, Chenjia Bai, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models pre-trained on large, diverse datasets
show remarkable potential for general-purpose robotic manipulation. However, a
primary bottleneck remains in adapting these models to downstream tasks,
especially when the robot's embodiment or the task itself differs from the
pre-training data. This discrepancy leads to a significant mismatch in action
distributions, demanding extensive data and compute for effective fine-tuning.
To address this challenge, we introduce \textbf{Align-Then-stEer
(\texttt{ATE})}, a novel, data-efficient, and plug-and-play adaptation
framework. \texttt{ATE} first aligns disparate action spaces by constructing a
unified latent space, where a variational autoencoder constrained by reverse KL
divergence embeds adaptation actions into modes of the pre-training action
latent distribution. Subsequently, it steers the diffusion- or flow-based VLA's
generation process during fine-tuning via a guidance mechanism that pushes the
model's output distribution towards the target domain. We conduct extensive
experiments on cross-embodiment and cross-task manipulation in both simulation
and real world. Compared to direct fine-tuning of representative VLAs, our
method improves the average multi-task success rate by up to \textbf{9.8\%} in
simulation and achieves a striking \textbf{32\% success rate gain} in a
real-world cross-embodiment setting. Our work presents a general and
lightweight solution that greatly enhances the practicality of deploying VLA
models to new robotic platforms and tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first three authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizing Unsupervised <span class="highlight-title">Lidar</span> Odometry Model from Normal to Snowy
  Weather Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beibei Zhou, Zhiyuan Zhang, Zhenbo Song, Jianhui Guo, Hui Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based LiDAR odometry is crucial for autonomous driving and
robotic navigation, yet its performance under adverse weather, especially
snowfall, remains challenging. Existing models struggle to generalize across
conditions due to sensitivity to snow-induced noise, limiting real-world use.
In this work, we present an unsupervised LiDAR odometry model to close the gap
between clear and snowy weather conditions. Our approach focuses on effective
denoising to mitigate the impact of snowflake noise and outlier points on pose
estimation, while also maintaining computational efficiency for real-time
applications.
  To achieve this, we introduce a Patch Spatial Measure (PSM) module that
evaluates the dispersion of points within each patch, enabling effective
detection of sparse and discrete noise.
  We further propose a Patch Point Weight Predictor (PPWP) to assign adaptive
point-wise weights, enhancing their discriminative capacity within local
regions. To support real-time performance, we first apply an intensity
threshold mask to quickly suppress dense snowflake clusters near the LiDAR, and
then perform multi-modal feature fusion to refine the point-wise weight
prediction, improving overall robustness under adverse weather. Our model is
trained in clear weather conditions and rigorously tested across various
scenarios, including snowy and dynamic. Extensive experimental results confirm
the effectiveness of our method, demonstrating robust performance in both clear
and snowy weather. This advancement enhances the model's generalizability and
paves the way for more reliable autonomous systems capable of operating across
a wider range of environmental conditions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIRAGE: Multimodal Intention Recognition and Admittance-Guided
  Enhancement in VR-based Multi-object Teleoperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Sun, Xian Wang, Abhishek Kumar, Chengbin Cui, Lik-Hang Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective human-robot interaction (HRI) in multi-object teleoperation tasks
faces significant challenges due to perceptual ambiguities in virtual reality
(VR) environments and the limitations of single-modality intention recognition.
This paper proposes a shared control framework that combines a virtual
admittance (VA) model with a Multimodal-CNN-based Human Intention Perception
Network (MMIPN) to enhance teleoperation performance and user experience. The
VA model employs artificial potential fields to guide operators toward target
objects by adjusting admittance force and optimizing motion trajectories. MMIPN
processes multimodal inputs, including gaze movement, robot motions, and
environmental context, to estimate human grasping intentions, helping to
overcome depth perception challenges in VR. Our user study evaluated four
conditions across two factors, and the results showed that MMIPN significantly
improved grasp success rates, while the VA model enhanced movement efficiency
by reducing path lengths. Gaze data emerged as the most crucial input modality.
These findings demonstrate the effectiveness of combining multimodal cues with
implicit guidance in VR-based teleoperation, providing a robust solution for
multi-object grasping tasks and enabling more natural interactions across
various applications in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ISMAR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Geometric Control of Mechanical Systems with Symmetries Based on Sliding
  Modes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo Espindola, Yu Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a framework for designing sliding mode controllers
for a class of mechanical systems with symmetry, both unconstrained and
constrained, that evolve on principal fiber bundles. Control laws are developed
based on the reduced motion equations by exploring symmetries, leading to a
sliding mode control strategy where the reaching stage is executed on the base
space, and the sliding stage is performed on the structure group. Thus, design
complexity is reduced, and difficult choices for coordinate representations
when working with a particular Lie group are avoided. For this purpose, a
sliding subgroup is constructed on the structure group based on a kinematic
controller, and the sliding variable will converge to the identity of the state
manifold upon reaching the sliding subgroup. A reaching law based on a general
sliding vector field is then designed on the base space using the local form of
the mechanical connection to drive the sliding variable to the sliding
subgroup, and its time evolution is given according to the appropriate
covariant derivative. Almost global asymptotic stability and local exponential
stability are demonstrated using a Lyapunov analysis. We apply the results to a
fully actuated system (a rigid spacecraft actuated by reaction wheels) and a
subactuated nonholonomic system (unicycle mobile robot actuated by wheels),
which is also simulated for illustration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 3 figures, journal submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Autonomy Framework for a Future Mars Science Helicopter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Di Pierno, Robert Hewitt, Stephan Weiss, Roland Brockers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous aerial vehicles, such as NASA's Ingenuity, enable rapid planetary
surface exploration beyond the reach of ground-based robots. Thus, NASA is
studying a Mars Science Helicopter (MSH), an advanced concept capable of
performing long-range science missions and autonomously navigating challenging
Martian terrain. Given significant Earth-Mars communication delays and mission
complexity, an advanced autonomy framework is required to ensure safe and
efficient operation by continuously adapting behavior based on mission
objectives and real-time conditions, without human intervention. This study
presents a deterministic high-level control framework for aerial exploration,
integrating a Finite State Machine (FSM) with Behavior Trees (BTs) to achieve a
scalable, robust, and computationally efficient autonomy solution for critical
scenarios like deep space exploration. In this paper we outline key
capabilities of a possible MSH and detail the FSM-BT hybrid autonomy framework
which orchestrates them to achieve the desired objectives. Monte Carlo
simulations and real field tests validate the framework, demonstrating its
robustness and adaptability to both discrete events and real-time system
feedback. These inputs trigger state transitions or dynamically adjust behavior
execution, enabling reactive and context-aware responses. The framework is
middleware-agnostic, supporting integration with systems like F-Prime and
extending beyond aerial robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, IEEE CASE 2025 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ensemble-Based <span class="highlight-title">Event Camera</span> Place Recognition Under Varying Illumination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Therese Joseph, Tobias Fischer, Michael Milford
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to conventional cameras, event cameras provide a high dynamic range
and low latency, offering greater robustness to rapid motion and challenging
lighting conditions. Although the potential of event cameras for visual place
recognition (VPR) has been established, developing robust VPR frameworks under
severe illumination changes remains an open research problem. In this paper, we
introduce an ensemble-based approach to event camera place recognition that
combines sequence-matched results from multiple event-to-frame reconstructions,
VPR feature extractors, and temporal resolutions. Unlike previous event-based
ensemble methods, which only utilise temporal resolution, our broader fusion
strategy delivers significantly improved robustness under varied lighting
conditions (e.g., afternoon, sunset, night), achieving a 57% relative
improvement in Recall@1 across day-night transitions. We evaluate our approach
on two long-term driving datasets (with 8 km per traverse) without metric
subsampling, thereby preserving natural variations in speed and stop duration
that influence event density. We also conduct a comprehensive analysis of key
design choices, including binning strategies, polarity handling, reconstruction
methods, and feature extractors, to identify the most critical components for
robust performance. Additionally, we propose a modification to the standard
sequence matching framework that enhances performance at longer sequence
lengths. To facilitate future research, we will release our codebase and
benchmarking framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Robust</span>ness Enhancement for Multi-Quadrotor Centralized Transportation
  System via Online Tuning and <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhua Gao, Kohji Tomita, Akiya Kamimura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an adaptive-neuro geometric control for a centralized
multi-quadrotor cooperative transportation system, which enhances both
adaptivity and disturbance rejection. Our strategy is to coactively tune the
model parameters and learn the external disturbances in real-time. To realize
this, we augmented the existing geometric control with multiple neural networks
and adaptive laws, where the estimated model parameters and the weights of the
neural networks are simultaneously tuned and adjusted online. The
Lyapunov-based adaptation guarantees bounded estimation errors without
requiring either pre-training or the persistent excitation (PE) condition. The
proposed control system has been proven to be stable in the sense of Lyapunov
under certain preconditions, and its enhanced robustness under scenarios of
disturbed environment and model-unmatched plant was demonstrated by numerical
simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Identification using Adaptive Laws and Neural Networks for
  Multi-Quadrotor Centralized Transportation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianhua Gao, Kohji Tomita, Akiya Kamimura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces an adaptive-neuro identification method that enhances
the robustness of a centralized multi-quadrotor transportation system. This
method leverages online tuning and learning on decomposed error subspaces,
enabling efficient real-time compensation to time-varying disturbances and
model uncertainties acting on the payload. The strategy is to decompose the
high-dimensional error space into a set of low-dimensional subspaces. In this
way, the identification problem for unseen features is naturally transformed
into submappings (``slices'') addressed by multiple adaptive laws and shallow
neural networks, which are updated online via Lyapunov-based adaptation without
requiring persistent excitation (PE) and offline training. Due to the
model-free nature of neural networks, this approach can be well adapted to
highly coupled and nonlinear centralized transportation systems. It serves as a
feedforward compensator for the payload controller without explicitly relying
on the dynamics coupled with the payload, such as cables and quadrotors. The
proposed control system has been proven to be stable in the sense of Lyapunov,
and its enhanced robustness under time-varying disturbances and model
uncertainties was demonstrated by numerical simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoDrive-R$^2$: Incentivizing Reasoning and Self-Reflection Capacity
  for <span class="highlight-title">VLA</span> Model in Autonomous <span class="highlight-title">Driving</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenlong Yuan, Jing Tang, Jinguo Luo, Rui Chen, Chengxuan Qian, Lei Sun, Xiangxiang Chu, Yujun Cai, Dapeng Zhang, Shuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models in autonomous driving systems have
recently demonstrated transformative potential by integrating multimodal
perception with decision-making capabilities. However, the interpretability and
coherence of the decision process and the plausibility of action sequences
remain largely underexplored. To address these issues, we propose
AutoDrive-R$^2$, a novel VLA framework that enhances both reasoning and
self-reflection capabilities of autonomous driving systems through
chain-of-thought (CoT) processing and reinforcement learning (RL).
Specifically, we first propose an innovative CoT dataset named nuScenesR$^2$-6K
for supervised fine-tuning, which effectively builds cognitive bridges between
input information and output trajectories through a four-step logical chain
with self-reflection for validation. Moreover, to maximize both reasoning and
self-reflection during the RL stage, we further employ the Group Relative
Policy Optimization (GRPO) algorithm within a physics-grounded reward framework
that incorporates spatial alignment, vehicle dynamic, and temporal smoothness
criteria to ensure reliable and realistic trajectory planning. Extensive
evaluation results across both nuScenes and Waymo datasets demonstrates the
state-of-the-art performance and robust generalization capacity of our proposed
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Driven Marine <span class="highlight-title">Robot</span>ics: Emerging Trends in Underwater Perception and
  Ecosystem Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scarlett Raine, Tobias Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Marine ecosystems face increasing pressure due to climate change, driving the
need for scalable, AI-powered monitoring solutions. This paper examines the
rapid emergence of underwater AI as a major research frontier and analyzes the
factors that have transformed marine perception from a niche application into a
catalyst for AI innovation. We identify three convergent drivers: environmental
necessity for ecosystem-scale monitoring, democratization of underwater
datasets through citizen science platforms, and researcher migration from
saturated terrestrial computer vision domains. Our analysis reveals how unique
underwater challenges - turbidity, cryptic species detection, expert annotation
bottlenecks, and cross-ecosystem generalization - are driving fundamental
advances in weakly supervised learning, open-set recognition, and robust
perception under degraded conditions. We survey emerging trends in datasets,
scene understanding and 3D reconstruction, highlighting the paradigm shift from
passive observation toward AI-driven, targeted intervention capabilities. The
paper demonstrates how underwater constraints are pushing the boundaries of
foundation models, self-supervised learning, and perception, with
methodological innovations that extend far beyond marine applications to
benefit general computer vision, robotics, and environmental monitoring.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JARVIS: A Neuro-Symbolic Commonsense Reasoning Framework for
  Conversational Embodied Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.13266v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.13266v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizhi Zheng, Kaiwen Zhou, Jing Gu, Yue Fan, Jialu Wang, Zonglin Di, Xuehai He, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building a conversational embodied agent to execute real-life tasks has been
a long-standing yet quite challenging research goal, as it requires effective
human-agent communication, multi-modal understanding, long-range sequential
decision making, etc. Traditional symbolic methods have scaling and
generalization issues, while end-to-end deep learning models suffer from data
scarcity and high task complexity, and are often hard to explain. To benefit
from both worlds, we propose JARVIS, a neuro-symbolic commonsense reasoning
framework for modular, generalizable, and interpretable conversational embodied
agents. First, it acquires symbolic representations by prompting large language
models (LLMs) for language understanding and sub-goal planning, and by
constructing semantic maps from visual observations. Then the symbolic module
reasons for sub-goal planning and action generation based on task- and
action-level common sense. Extensive experiments on the TEACh dataset validate
the efficacy and efficiency of our JARVIS framework, which achieves
state-of-the-art (SOTA) results on all three dialog-based embodied tasks,
including Execution from Dialog History (EDH), Trajectory from Dialog (TfD),
and Two-Agent Task Completion (TATC) (e.g., our method boosts the unseen
Success Rate on EDH from 6.1\% to 15.8\%). Moreover, we systematically analyze
the essential factors that affect the task performance and also demonstrate the
superiority of our method in few-shot settings. Our JARVIS model ranks first in
the Alexa Prize SimBot Public Benchmark Challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19th International Conference on Neurosymbolic Learning and Reasoning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-Set <span class="highlight-title">LiDAR</span> Panoptic Segmentation Guided by Uncertainty-Aware
  <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.13265v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.13265v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Mohan, Julia Hindel, Florian Drews, Claudius Gläser, Daniele Cattaneo, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles that navigate in open-world environments may encounter
previously unseen object classes. However, most existing LiDAR panoptic
segmentation models rely on closed-set assumptions, failing to detect unknown
object instances. In this work, we propose ULOPS, an uncertainty-guided
open-set panoptic segmentation framework that leverages Dirichlet-based
evidential learning to model predictive uncertainty. Our architecture
incorporates separate decoders for semantic segmentation with uncertainty
estimation, embedding with prototype association, and instance center
prediction. During inference, we leverage uncertainty estimates to identify and
segment unknown instances. To strengthen the model's ability to differentiate
between known and unknown objects, we introduce three uncertainty-driven loss
functions. Uniform Evidence Loss to encourage high uncertainty in unknown
regions. Adaptive Uncertainty Separation Loss ensures a consistent difference
in uncertainty estimates between known and unknown objects at a global scale.
Contrastive Uncertainty Loss refines this separation at the fine-grained level.
To evaluate open-set performance, we extend benchmark settings on KITTI-360 and
introduce a new open-set evaluation for nuScenes. Extensive experiments
demonstrate that ULOPS consistently outperforms existing open-set LiDAR
panoptic segmentation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HDVIO2.0: Wind and Disturbance <span class="highlight-title">Estimation</span> with Hybrid <span class="highlight-title">Dynamic</span>s VIO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.00969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.00969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Cioffi, Leonard Bauersfeld, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-inertial odometry (VIO) is widely used for state estimation in
autonomous micro aerial vehicles using onboard sensors. Current methods improve
VIO by incorporating a model of the translational vehicle dynamics, yet their
performance degrades when faced with low-accuracy vehicle models or continuous
external disturbances, like wind. Additionally, incorporating rotational
dynamics in these models is computationally intractable when they are deployed
in online applications, e.g., in a closed-loop control system. We present
HDVIO2.0, which models full 6-DoF, translational and rotational, vehicle
dynamics and tightly incorporates them into a VIO with minimal impact on the
runtime. HDVIO2.0 builds upon the previous work, HDVIO, and addresses these
challenges through a hybrid dynamics model combining a point-mass vehicle model
with a learning-based component, with access to control commands and IMU
history, to capture complex aerodynamic effects. The key idea behind modeling
the rotational dynamics is to represent them with continuous-time functions.
HDVIO2.0 leverages the divergence between the actual motion and the predicted
motion from the hybrid dynamics model to estimate external forces as well as
the robot state. Our system surpasses the performance of state-of-the-art
methods in experiments using public and new drone dynamics datasets, as well as
real-world flights in winds up to 25 km/h. Unlike existing approaches, we also
show that accurate vehicle dynamics predictions are achievable without precise
knowledge of the full vehicle state.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Transactions on Robotics (T-RO) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PPF: Pre-training and Preservative Fine-tuning of <span class="highlight-title">Humanoid</span> <span class="highlight-title">Locomotion</span>
  via Model-Assumption-based Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.09833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.09833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunyoung Jung, Zhaoyuan Gu, Ye Zhao, Hae-Won Park, Sehoon Ha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid locomotion is a challenging task due to its inherent complexity and
high-dimensional dynamics, as well as the need to adapt to diverse and
unpredictable environments. In this work, we introduce a novel learning
framework for effectively training a humanoid locomotion policy that imitates
the behavior of a model-based controller while extending its capabilities to
handle more complex locomotion tasks, such as more challenging terrain and
higher velocity commands. Our framework consists of three key components:
pre-training through imitation of the model-based controller, fine-tuning via
reinforcement learning, and model-assumption-based regularization (MAR) during
fine-tuning. In particular, MAR aligns the policy with actions from the
model-based controller only in states where the model assumption holds to
prevent catastrophic forgetting. We evaluate the proposed framework through
comprehensive simulation tests and hardware experiments on a full-size humanoid
robot, Digit, demonstrating a forward speed of 1.5 m/s and robust locomotion
across diverse terrains, including slippery, sloped, uneven, and sandy
terrains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integration of Computer <span class="highlight-title">Vision</span> with Adaptive Control for Autonomous
  <span class="highlight-title">Driving</span> Using ADORE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.17985v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.17985v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abu Shad Ahammed, Md Shahi Amran Hossain, Sayeri Mukherjee, Roman Obermaisser, Md. Ziaur Rahman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring safety in autonomous driving requires a seamless integration of
perception and decision making under uncertain conditions. Although computer
vision (CV) models such as YOLO achieve high accuracy in detecting traffic
signs and obstacles, their performance degrades in drift scenarios caused by
weather variations or unseen objects. This work presents a simulated autonomous
driving system that combines a context aware CV model with adaptive control
using the ADORE framework. The CARLA simulator was integrated with ADORE via
the ROS bridge, allowing real-time communication between perception, decision,
and control modules. A simulated test case was designed in both clear and drift
weather conditions to demonstrate the robust detection performance of the
perception model while ADORE successfully adapted vehicle behavior to speed
limits and obstacles with low response latency. The findings highlight the
potential of coupling deep learning-based perception with rule-based adaptive
decision making to improve automotive safety critical system.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a cognitive architecture to enable natural language interaction
  in co-constructive task <span class="highlight-title">learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23760v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23760v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Scheibl, Birte Richter, Alissa Müller, Michael Beetz, Britta Wrede
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research addresses the question, which characteristics a cognitive
architecture must have to leverage the benefits of natural language in
Co-Constructive Task Learning (CCTL). To provide context, we first discuss
Interactive Task Learning (ITL), the mechanisms of the human memory system, and
the significance of natural language and multi-modality. Next, we examine the
current state of cognitive architectures, analyzing their capabilities to
inform a concept of CCTL grounded in multiple sources. We then integrate
insights from various research domains to develop a unified framework. Finally,
we conclude by identifying the remaining challenges and requirements necessary
to achieve CCTL in Human-Robot Interaction (HRI).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures, The paper has been accepted by the 2025 34th IEEE
  International Conference on Robot and Human Interactive Communication
  (ROMAN), IEEE Copyright Policy:
  https://www.ieee.org/publications/rights/copyright-policy</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Goal-Conditioned Data Augmentation for Offline Reinforcement <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingshuai Huang, Di Wu, Benoit Boulet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) enables policy learning from
pre-collected offline datasets, relaxing the need to interact directly with the
environment. However, limited by the quality of offline datasets, it generally
fails to learn well-qualified policies in suboptimal datasets. To address
datasets with insufficient optimal demonstrations, we introduce
Goal-cOnditioned Data Augmentation (GODA), a novel goal-conditioned
diffusion-based method for augmenting samples with higher quality. Leveraging
recent advancements in generative modelling, GODA incorporates a novel
return-oriented goal condition with various selection mechanisms. Specifically,
we introduce a controllable scaling technique to provide enhanced return-based
guidance during data sampling. GODA learns a comprehensive distribution
representation of the original offline datasets while generating new data with
selectively higher-return goals, thereby maximizing the utility of limited
optimal demonstrations. Furthermore, we propose a novel adaptive gated
conditioning method for processing noisy inputs and conditions, enhancing the
capture of goal-oriented guidance. We conduct experiments on the D4RL benchmark
and real-world challenges, specifically traffic signal control (TSC) tasks, to
demonstrate GODA's effectiveness in enhancing data quality and superior
performance compared to state-of-the-art data augmentation methods across
various offline RL algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExoStart: Efficient <span class="highlight-title">learning</span> for dexterous <span class="highlight-title">manipulation</span> with sensorized
  exoskeleton demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.11775v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.11775v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilin Si, Jose Enrique Chen, M. Emre Karagozler, Antonia Bronars, Jonathan Hutchinson, Thomas Lampe, Nimrod Gileadi, Taylor Howell, Stefano Saliceti, Lukasz Barczyk, Ilan Olivarez Correa, Tom Erez, Mohit Shridhar, Murilo Fernandes Martins, Konstantinos Bousmalis, Nicolas Heess, Francesco Nori, Maria Bauza Villalonga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in teleoperation systems have enabled high-quality data
collection for robotic manipulators, showing impressive results in learning
manipulation at scale. This progress suggests that extending these capabilities
to robotic hands could unlock an even broader range of manipulation skills,
especially if we could achieve the same level of dexterity that human hands
exhibit. However, teleoperating robotic hands is far from a solved problem, as
it presents a significant challenge due to the high degrees of freedom of
robotic hands and the complex dynamics occurring during contact-rich settings.
In this work, we present ExoStart, a general and scalable learning framework
that leverages human dexterity to improve robotic hand control. In particular,
we obtain high-quality data by collecting direct demonstrations without a robot
in the loop using a sensorized low-cost wearable exoskeleton, capturing the
rich behaviors that humans can demonstrate with their own hands. We also
propose a simulation-based dynamics filter that generates dynamically feasible
trajectories from the collected demonstrations and use the generated
trajectories to bootstrap an auto-curriculum reinforcement learning method that
relies only on simple sparse rewards. The ExoStart pipeline is generalizable
and yields robust policies that transfer zero-shot to the real robot. Our
results demonstrate that ExoStart can generate dexterous real-world hand
skills, achieving a success rate above 50% on a wide range of complex tasks
such as opening an AirPods case or inserting and turning a key in a lock. More
details and videos can be found in https://sites.google.com/view/exostart.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Touch and Bending Perception Using Electrical Impedance Tomography
  for <span class="highlight-title">Robot</span>ics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.13048v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.13048v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haofeng Chen, Bedrich Himmel, Bin Li, Xiaojie Wang, Matej Hoffmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electrical Impedance Tomography (EIT) offers a promising solution for
distributed tactile sensing with minimal wiring and full-surface coverage in
robotic applications. However, EIT-based tactile sensors face significant
challenges during surface bending. Deformation alters the baseline impedance
distribution and couples with touch-induced conductivity variations,
complicating signal interpretation. To address this challenge, we present a
novel sensing framework that integrates a deep neural network for interaction
state classification with a dynamic adaptive reference strategy to decouple
touch and deformation signals, while a data-driven regression model translates
EIT voltage changes into continuous bending angles. The framework is validated
using a magnetic hydrogel composite sensor that conforms to bendable surfaces.
Experimental evaluations demonstrate that the proposed framework achieves
precise and robust bending angle estimation, high accuracy in distinguishing
touch, bending, and idle states, and significantly improves touch localization
quality under bending deformation compared to conventional fixed-reference
methods. Real-time experiments confirm the system's capability to reliably
detect multi-touch interactions and track bending angles across varying
deformation conditions. This work paves the way for flexible EIT-based robotic
skins capable of rich multimodal sensing in robotics and human-robot
interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeLink: Safety-Critical Control Under <span class="highlight-title">Dynamic</span> and Irregular Unsafe
  Regions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.16551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.16551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songqiao Hu, Zidong Wang, Zeyi Liu, Zhen Shen, Xiao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control barrier functions (CBFs) provide a theoretical foundation for
safety-critical control in robotic systems. However, most existing methods rely
on the analytical expressions of unsafe state regions, which are often
impractical for irregular and dynamic unsafe regions. This paper introduces
SafeLink, a novel CBF construction method based on cost-sensitive incremental
random vector functional-link (RVFL) neural networks. By designing a valid cost
function, SafeLink assigns different sensitivities to safe and unsafe state
points, thereby eliminating false negatives in classification of unsafe state
points. Furthermore, an incremental update theorem is established, enabling
precise real-time adaptation to changes in unsafe regions. An analytical
expression for the gradient of SafeLink is also derived to facilitate control
input computation. The proposed method is validated on the endpoint position
control task of a nonlinear two-link manipulator. Experimental results
demonstrate that the method effectively learns the unsafe regions and rapidly
adapts as these regions change, achieving an update speed significantly faster
than comparison methods, while safely reaching the target position. The source
code is available at https://github.com/songqiaohu/SafeLink.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Exploratory Study on Human-<span class="highlight-title">Robot</span> Interaction using <span class="highlight-title">Semantic</span>s-based
  Situational Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.17376v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.17376v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianshu Ruan, Aniketh Ramesh, Rustam Stolkin, Manolis Chiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we investigate the impact of high-level semantics (evaluation
of the environment) on Human-Robot Teams (HRT) and Human-Robot Interaction
(HRI) in the context of mobile robot deployments. Although semantics has been
widely researched in AI, how high-level semantics can benefit the HRT paradigm
is underexplored, often fuzzy, and intractable. We applied a semantics-based
framework that could reveal different indicators of the environment (i.e. how
much semantic information exists) in a mock-up disaster response mission. In
such missions, semantics are crucial as the HRT should handle complex
situations and respond quickly with correct decisions, where humans might have
a high workload and stress. Especially when human operators need to shift their
attention between robots and other tasks, they will struggle to build
Situational Awareness (SA) quickly. The experiment suggests that the presented
semantics: 1) alleviate the perceived workload of human operators; 2) increase
the operator's trust in the SA; and 3) help to reduce the reaction time in
switching the level of autonomy when needed. Additionally, we find that
participants with higher trust in the system are encouraged by high-level
semantics to use teleoperation mode more.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perspective-Shifted Neuro-Symbolic World Models: A Framework for
  Socially-Aware <span class="highlight-title">Robot</span> <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20425v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20425v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Alcedo, Pedro U. Lima, Rachid Alami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating in environments alongside humans requires agents to reason under
uncertainty and account for the beliefs and intentions of those around them.
Under a sequential decision-making framework, egocentric navigation can
naturally be represented as a Markov Decision Process (MDP). However, social
navigation additionally requires reasoning about the hidden beliefs of others,
inherently leading to a Partially Observable Markov Decision Process (POMDP),
where agents lack direct access to others' mental states. Inspired by Theory of
Mind and Epistemic Planning, we propose (1) a neuro-symbolic model-based
reinforcement learning architecture for social navigation, addressing the
challenge of belief tracking in partially observable environments; and (2) a
perspective-shift operator for belief estimation, leveraging recent work on
Influence-based Abstractions (IBA) in structured multi-agent settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a regular paper at the 2025 IEEE International Conference
  on Robot & Human Interactive Communication (RO-MAN). \c{opyright} 2025 IEEE.
  The final version will appear in IEEE Xplore</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tactile SoftHand-A: 3D-Printed, Tactile, Highly-underactuated,
  Anthropomorphic <span class="highlight-title">Robot</span> Hand with an Antagonistic Tendon Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Li, Christopher J. Ford, Chenghua Lu, Yijiong Lin, Matteo Bianchi, Manuel G. Catalano, Efi Psomopoulou, Nathan F. Lepora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A challenging and important problem for tendon-driven multi-fingered robotic
hands is to ensure grasping adaptivity while minimizing the number of actuators
needed to provide human-like functionality. Inspired by the Pisa/IIT SoftHand,
this paper introduces a 3D-printed, highly-underactuated, tactile-sensorized,
five-finger robotic hand named the Tactile SoftHand-A, which features an
antagonistic mechanism to actively open and close the hand. Our proposed
dual-tendon design gives options that allow active control of specific (distal
or proximal interphalangeal) joints; for example, to adjust from an enclosing
to fingertip grasp or to manipulate an object with a fingertip. We also develop
and integrate a new design of fully 3D-printed vision-based tactile sensor
within the fingers that requires minimal hand assembly. A control scheme based
on analytically extracting contact location and slip from the tactile images is
used to coordinate the antagonistic tendon mechanism (using a marker
displacement density map, suitable for TacTip-based sensors). We perform
extensive testing of a single finger, the entire hand, and the tactile
capabilities to show the improvements in reactivity, load-bearing, and
manipulability in comparison to a SoftHand that lacks the antagonistic
mechanism. We also demonstrate the hand's reactivity to contact disturbances
including slip, and how this enables teleoperated control from human hand
gestures. Overall, this study points the way towards a class of low-cost,
accessible, 3D-printable, tactile, underactuated human-like robotic hands, and
we openly release the designs to facilitate others to build upon this work. The
designs are open-sourced at https://github.com/HaoranLi-Data/Tactile_SoftHand_A
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frontier Shepherding: A Bio-inspired Multi-<span class="highlight-title">robot</span> Framework for
  Large-Scale Exploration <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Lewis, Meysam Basiri, Pedro U. Lima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient exploration of large-scale environments remains a critical
challenge in robotics, with applications ranging from environmental monitoring
to search and rescue operations. This article proposes Frontier Shepherding
(FroShe), a bio-inspired multi-robot framework for large-scale exploration. The
framework heuristically models frontier exploration based on the shepherding
behavior of herding dogs, where frontiers are treated as a swarm of sheep
reacting to robots modeled as shepherding dogs. FroShe is robust across varying
environment sizes and obstacle densities, requiring minimal parameter tuning
for deployment across multiple agents. Simulation results demonstrate that the
proposed method performs consistently, regardless of environment complexity,
and outperforms state-of-the-art exploration strategies by an average of 20%
with three UAVs. The approach was further validated in real-world experiments
using single- and dual-drone deployments in a forest-like environment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 page article accepted at IEEE/RSJ International Conferenceo on
  Intelligent Robots and Systems (IROS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Security in Multi-<span class="highlight-title">Robot</span> Systems through Co-Observation
  <span class="highlight-title">Planning</span>, Reachability Analysis, and Network Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.13266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.13266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqi Yang, Roberto Tron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses security challenges in multi-robot systems (MRS) where
adversaries may compromise robot control, risking unauthorized access to
forbidden areas. We propose a novel multi-robot optimal planning algorithm that
integrates mutual observations and introduces reachability constraints for
enhanced security. This ensures that, even with adversarial movements,
compromised robots cannot breach forbidden regions without missing scheduled
co-observations. The reachability constraint uses ellipsoidal
over-approximation for efficient intersection checking and gradient
computation. To enhance system resilience and tackle feasibility challenges, we
also introduce sub-teams. These cohesive units replace individual robot
assignments along each route, enabling redundant robots to deviate for
co-observations across different trajectories, securing multiple sub-teams
without requiring modifications. We formulate the cross-trajectory
co-observation plan by solving a network flow coverage problem on the
checkpoint graph generated from the original unsecured MRS trajectories,
providing the same security guarantees against plan-deviation attacks. We
demonstrate the effectiveness and robustness of our proposed algorithm, which
significantly strengthens the security of multi-robot systems in the face of
adversarial threats.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, submitted to IEEE Transactions on Control of
  Network Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Experts to a Generalist: Toward General <span class="highlight-title">Whole-Body Control</span> for
  <span class="highlight-title">Humanoid</span> <span class="highlight-title">Robots</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.12779v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.12779v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wang, Ming Yang, Ziluo Ding, Yu Zhang, Weishuai Zeng, Xinrun Xu, Haobin Jiang, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving general agile whole-body control on humanoid robots remains a major
challenge due to diverse motion demands and data conflicts. While existing
frameworks excel in training single motion-specific policies, they struggle to
generalize across highly varied behaviors due to conflicting control
requirements and mismatched data distributions. In this work, we propose
BumbleBee (BB), an expert-generalist learning framework that combines motion
clustering and sim-to-real adaptation to overcome these challenges. BB first
leverages an autoencoder-based clustering method to group behaviorally similar
motions using motion features and motion descriptions. Expert policies are then
trained within each cluster and refined with real-world data through iterative
delta action modeling to bridge the sim-to-real gap. Finally, these experts are
distilled into a unified generalist controller that preserves agility and
robustness across all motion types. Experiments on two simulations and a real
humanoid robot demonstrate that BB achieves state-of-the-art general whole-body
control, setting a new benchmark for agile, robust, and generalizable humanoid
performance in the real world. The project webpage is available at
https://beingbeyond.github.io/BumbleBee/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robust</span> Deterministic Policy <span class="highlight-title">Gradient</span> for Disturbance Attenuation and Its
  Application to Quadrotor Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21057v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21057v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeho Lee, Donghwan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practical control systems pose significant challenges in identifying optimal
control policies due to uncertainties in the system model and external
disturbances. While $H_\infty$ control techniques are commonly used to design
robust controllers that mitigate the effects of disturbances, these methods
often require complex and computationally intensive calculations. To address
this issue, this paper proposes a reinforcement learning algorithm called
Robust Deterministic Policy Gradient (RDPG), which formulates the $H_\infty$
control problem as a two-player zero-sum dynamic game. In this formulation, one
player (the user) aims to minimize the cost, while the other player (the
adversary) seeks to maximize it. We then employ deterministic policy gradient
(DPG) and its deep reinforcement learning counterpart to train a robust control
policy with effective disturbance attenuation. In particular, for practical
implementation, we introduce an algorithm called robust deep deterministic
policy gradient (RDDPG), which employs a deep neural network architecture and
integrates techniques from the twin-delayed deep deterministic policy gradient
(TD3) to enhance stability and learning efficiency. To evaluate the proposed
algorithm, we implement it on an unmanned aerial vehicle (UAV) tasked with
following a predefined path in a disturbance-prone environment. The
experimental results demonstrate that the proposed method outperforms other
control approaches in terms of robustness against disturbances, enabling
precise real-time tracking of moving targets even under severe disturbance
conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Task <span class="highlight-title">Planning</span> for Heterogeneous Multi-Agent Systems <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anatoli A. Tziola, Savvas G. Loizou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a solution to the automatic task planning problem for
multi-agent systems. A formal framework is developed based on the
Nondeterministic Finite Automata with $\epsilon$-transitions, where given the
capabilities, constraints and failure modes of the agents involved, an initial
state of the system and a task specification, an optimal solution is generated
that satisfies the system constraints and the task specification. The resulting
solution is guaranteed to be complete and optimal; moreover a heuristic
solution that offers significant reduction of the computational requirements
while relaxing the completeness and optimality requirements is proposed. The
constructed system model is independent from the initial condition and the task
specification, alleviating the need to repeat the costly pre-processing cycle
for solving other scenarios, while allowing the incorporation of failure modes
on-the-fly. Two case studies are provided: a simple one to showcase the
concepts of the proposed methodology and a more elaborate one to demonstrate
the effectiveness and validity of the methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long version of paper submitted to the IEEE ICRA 2023 Conference, in
  IEEE Transactions on Automatic Control, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ N<span class="highlight-title">MPC</span>B: A Lightweight and Safety-Critical Motion Control Framework for
  Ackermann Mobile <span class="highlight-title">Robot</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.01752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.01752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longze Zheng, Qinghe Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-obstacle environments, real-time performance and safety in robot
motion control have long been challenging issues, as conventional methods often
struggle to balance the two. In this paper, we propose a novel motion control
framework composed of a Neural network-based path planner and a Model
Predictive Control (MPC) controller based on control Barrier function (NMPCB) .
The planner predicts the next target point through a lightweight neural network
and generates a reference trajectory for the controller. In the design of the
controller, we introduce the dual problem of control barrier function (CBF) as
the obstacle avoidance constraint, enabling it to ensure robot motion safety
while significantly reducing computation time. The controller directly outputs
control commands to the robot by tracking the reference trajectory. This
framework achieves a balance between real-time performance and safety. We
validate the feasibility of the framework through numerical simulations and
real-world experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Efficient LLM Grounding for Embodied Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14314v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14314v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhang, Shixin Yang, Chenjia Bai, Fei Wu, Xiu Li, Zhen Wang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounding the reasoning ability of large language models (LLMs) for embodied
tasks is challenging due to the complexity of the physical world. Especially,
LLM planning for multi-agent collaboration requires communication of agents or
credit assignment as the feedback to re-adjust the proposed plans and achieve
effective coordination. However, existing methods that overly rely on physical
verification or self-reflection suffer from excessive and inefficient querying
of LLMs. In this paper, we propose a novel framework for multi-agent
collaboration that introduces Reinforced Advantage feedback (ReAd) for
efficient self-refinement of plans. Specifically, we perform critic regression
to learn a sequential advantage function from LLM-planned data, and then treat
the LLM planner as an optimizer to generate actions that maximize the advantage
function. It endows the LLM with the foresight to discern whether the action
contributes to accomplishing the final task. We provide theoretical analysis by
extending advantage-weighted regression in reinforcement learning to
multi-agent systems. Experiments on Overcooked-AI and a difficult variant of
RoCoBench show that ReAd surpasses baselines in success rate, and also
significantly decreases the interaction steps of agents and query rounds of
LLMs, demonstrating its high efficiency for grounding LLMs. More results are
given at https://read-llm.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ACL'2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient <span class="highlight-title">Manipulation</span>-Enhanced <span class="highlight-title">Semantic</span> Mapping With
  Uncertainty-Informed Action Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Dengler, Jesper Mücke, Rohit Menon, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Service robots operating in cluttered human environments such as homes,
offices, and schools cannot rely on predefined object arrangements and must
continuously update their semantic and spatial estimates while dealing with
possible frequent rearrangements. Efficient and accurate mapping under such
conditions demands selecting informative viewpoints and targeted manipulations
to reduce occlusions and uncertainty. In this work, we present a
manipulation-enhanced semantic mapping framework for occlusion-heavy shelf
scenes that integrates evidential metric-semantic mapping with
reinforcement-learning-based next-best view planning and targeted action
selection. Our method thereby exploits uncertainty estimates from Dirichlet and
Beta distributions in the map prediction networks to guide both active sensor
placement and object manipulation, focusing on areas with high uncertainty and
selecting actions with high expected information gain. Furthermore, we
introduce an uncertainty-informed push strategy that targets occlusion-critical
objects and generates minimally invasive actions to reveal hidden regions by
reducing overall uncertainty in the scene. The experimental evaluation shows
that our framework enables to accurately map cluttered scenes, while
substantially reducing object displacement and achieving a 95% reduction in
planning time compared to the state-of-the-art, thereby realizing real-world
applicability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Co-Design of Soft Gripper with Neural Physics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20404v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20404v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sha Yi, Xueqian Bai, Adabhav Singh, Jianglong Ye, Michael T Tolley, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For robot manipulation, both the controller and end-effector design are
crucial. Soft grippers are generalizable by deforming to different geometries,
but designing such a gripper and finding its grasp pose remains challenging. In
this paper, we propose a co-design framework that generates an optimized soft
gripper's block-wise stiffness distribution and its grasping pose, using a
neural physics model trained in simulation. We derived a uniform-pressure
tendon model for a flexure-based soft finger, then generated a diverse dataset
by randomizing both gripper pose and design parameters. A neural network is
trained to approximate this forward simulation, yielding a fast, differentiable
surrogate. We embed that surrogate in an end-to-end optimization loop to
optimize the ideal stiffness configuration and best grasp pose. Finally, we
3D-print the optimized grippers of various stiffness by changing the structural
parameters. We demonstrate that our co-designed grippers significantly
outperform baseline designs in both simulation and hardware experiments. More
info: http://yswhynot.github.io/codesign-soft/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NetRoller: Interfacing General and Specialized Models for End-to-End
  Autonomous <span class="highlight-title">Driving</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.14589v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.14589v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ren Xin, Hongji Liu, Xiaodong Mei, Wenru Liu, Maosheng Ye, Zhili Chen, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating General Models (GMs) such as Large Language Models (LLMs), with
Specialized Models (SMs) in autonomous driving tasks presents a promising
approach to mitigating challenges in data diversity and model capacity of
existing specialized driving models. However, this integration leads to
problems of asynchronous systems, which arise from the distinct characteristics
inherent in GMs and SMs. To tackle this challenge, we propose NetRoller, an
adapter that incorporates a set of novel mechanisms to facilitate the seamless
integration of GMs and specialized driving models. Specifically, our mechanisms
for interfacing the asynchronous GMs and SMs are organized into three key
stages. NetRoller first harvests semantically rich and computationally
efficient representations from the reasoning processes of LLMs using an early
stopping mechanism, which preserves critical insights on driving context while
maintaining low overhead. It then applies learnable query embeddings,
nonsensical embeddings, and positional layer embeddings to facilitate robust
and efficient cross-modality translation. At last, it employs computationally
efficient Query Shift and Feature Shift mechanisms to enhance the performance
of SMs through few-epoch fine-tuning. Based on the mechanisms formalized in
these three stages, NetRoller enables specialized driving models to operate at
their native frequencies while maintaining situational awareness of the GM.
Experiments conducted on the nuScenes dataset demonstrate that integrating GM
through NetRoller significantly improves human similarity and safety in
planning tasks, and it also achieves noticeable precision improvements in
detection and mapping tasks for end-to-end autonomous driving. The code and
models are available at https://github.com/Rex-sys-hk/NetRoller .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-09-01T00:00:00Z">2025-09-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">52</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-vessel Interaction-Aware Trajectory Prediction and Collision Risk
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Mahbub Alam, Jose F. Rodrigues-Jr, Gabriel Spadon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate vessel trajectory prediction is essential for enhancing situational
awareness and preventing collisions. Still, existing data-driven models are
constrained mainly to single-vessel forecasting, overlooking vessel
interactions, navigation rules, and explicit collision risk assessment. We
present a transformer-based framework for multi-vessel trajectory prediction
with integrated collision risk analysis. For a given target vessel, the
framework identifies nearby vessels. It jointly predicts their future
trajectories through parallel streams encoding kinematic and derived physical
features, causal convolutions for temporal locality, spatial transformations
for positional encoding, and hybrid positional embeddings that capture both
local motion patterns and long-range dependencies. Evaluated on large-scale
real-world AIS data using joint multi-vessel metrics, the model demonstrates
superior forecasting capabilities beyond traditional single-vessel displacement
errors. By simulating interactions among predicted trajectories, the framework
further quantifies potential collision risks, offering actionable insights to
strengthen maritime safety and decision support.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear Model Predictive Control-Based Reverse Path-<span class="highlight-title">Planning</span> and
  Path-Tracking Control of a Vehicle with Trailer System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xincheng Cao, Haochong Chen, Bilin Aksun-Guvenc, Levent Guvenc, Brian Link, Peter J Richmond, Dokyung Yim, Shihong Fan, John Harber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reverse parking maneuvers of a vehicle with trailer system is a challenging
task to complete for human drivers due to the unstable nature of the system and
unintuitive controls required to orientate the trailer properly. This paper
hence proposes an optimization-based automation routine to handle the
path-planning and path-tracking control process of such type of maneuvers. The
proposed approach utilizes nonlinear model predictive control (NMPC) to
robustly guide the vehicle-trailer system into the desired parking space, and
an optional forward repositioning maneuver can be added as an additional stage
of the parking process to obtain better system configurations, before backward
motion can be attempted again to get a good final pose. The novelty of the
proposed approach is the simplicity of its formulation, as the path-planning
and path-tracking operations are only conducted on the trailer being viewed as
a standalone vehicle, before the control inputs are propagated to the tractor
vehicle via inverse kinematic relationships also derived in this paper.
Simulation case studies and hardware-in-the-loop tests are performed, and the
results demonstrate the efficacy of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ManiFlow: A General <span class="highlight-title">Robot</span> <span class="highlight-title">Manipulation</span> Policy via Consistency Flow
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Yan, Jiyue Zhu, Yuquan Deng, Shiqi Yang, Ri-Zhao Qiu, Xuxin Cheng, Marius Memmel, Ranjay Krishna, Ankit Goyal, Xiaolong Wang, Dieter Fox
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces ManiFlow, a visuomotor imitation learning policy for
general robot manipulation that generates precise, high-dimensional actions
conditioned on diverse visual, language and proprioceptive inputs. We leverage
flow matching with consistency training to enable high-quality dexterous action
generation in just 1-2 inference steps. To handle diverse input modalities
efficiently, we propose DiT-X, a diffusion transformer architecture with
adaptive cross-attention and AdaLN-Zero conditioning that enables fine-grained
feature interactions between action tokens and multi-modal observations.
ManiFlow demonstrates consistent improvements across diverse simulation
benchmarks and nearly doubles success rates on real-world tasks across
single-arm, bimanual, and humanoid robot setups with increasing dexterity. The
extensive evaluation further demonstrates the strong robustness and
generalizability of ManiFlow to novel objects and background changes, and
highlights its strong scaling capability with larger-scale datasets. Our
website: maniflow-policy.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EgoTouch: On-Body Touch Input Using AR/VR Headset <span class="highlight-title">Camera</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vimal Mollyn, Chris Harrison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In augmented and virtual reality (AR/VR) experiences, a user's arms and hands
can provide a convenient and tactile surface for touch input. Prior work has
shown on-body input to have significant speed, accuracy, and ergonomic benefits
over in-air interfaces, which are common today. In this work, we demonstrate
high accuracy, bare hands (i.e., no special instrumentation of the user) skin
input using just an RGB camera, like those already integrated into all modern
XR headsets. Our results show this approach can be accurate, and robust across
diverse lighting conditions, skin tones, and body motion (e.g., input while
walking). Finally, our pipeline also provides rich input metadata including
touch force, finger identification, angle of attack, and rotation. We believe
these are the requisite technical ingredients to more fully unlock on-skin
interfaces that have been well motivated in the HCI literature but have lacked
robust and practical methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at UIST 2024. More info at
  https://www.figlab.com/research/2024/egotouch</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-conflicting Energy Minimization in Reinforcement <span class="highlight-title">Learning</span> based
  <span class="highlight-title">Robot</span> Control <span class="chip">CoRL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Skand Peri, Akhil Perincherry, Bikram Pandit, Stefan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient robot control often requires balancing task performance with energy
expenditure. A common approach in reinforcement learning (RL) is to penalize
energy use directly as part of the reward function. This requires carefully
tuning weight terms to avoid undesirable trade-offs where energy minimization
harms task success. In this work, we propose a hyperparameter-free gradient
optimization method to minimize energy expenditure without conflicting with
task performance. Inspired by recent works in multitask learning, our method
applies policy gradient projection between task and energy objectives to derive
policy updates that minimize energy expenditure in ways that do not impact task
performance. We evaluate this technique on standard locomotion benchmarks of
DM-Control and HumanoidBench and demonstrate a reduction of 64% energy usage
while maintaining comparable task performance. Further, we conduct experiments
on a Unitree GO2 quadruped showcasing Sim2Real transfer of energy efficient
policies. Our method is easy to implement in standard RL pipelines with minimal
code changes, is applicable to any policy gradient method, and offers a
principled alternative to reward shaping for energy efficient control policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures. Accepted as Oral presentation at Conference on
  Robot Learning (CoRL) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fail2Progress: <span class="highlight-title">Learning</span> from Real-World <span class="highlight-title">Robot</span> Failures with Stein
  Variational Inference <span class="chip">CoRL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Huang, Novella Alvina, Mohanraj Devendran Shanthi, Tucker Hermans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skill effect models for long-horizon manipulation tasks are prone to failures
in conditions not covered by training data distributions. Therefore, enabling
robots to reason about and learn from failures is necessary. We investigate the
problem of efficiently generating a dataset targeted to observed failures.
After fine-tuning a skill effect model on this dataset, we evaluate the extent
to which the model can recover from failures and minimize future failures. We
propose Fail2Progress, an approach that leverages Stein variational inference
to generate multiple simulation environments in parallel, enabling efficient
data sample generation similar to observed failures. Our method is capable of
handling several challenging mobile manipulation tasks, including transporting
multiple objects, organizing a constrained shelf, and tabletop organization.
Through large-scale simulation and real-world experiments, we demonstrate that
our approach excels at learning from failures across different numbers of
objects. Furthermore, we show that Fail2Progress outperforms several baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: sites.google.com/view/fail2progress. 25 pages, 8
  figures. Accepted to the Conference on Robot Learning (CoRL) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constrained Decoding for <span class="highlight-title">Robot</span>ics Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01728v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01728v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parv Kapoor, Akila Ganlath, Changliu Liu, Sebastian Scherer, Eunsuk Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in the development of robotic foundation models have led to
promising end-to-end and general-purpose capabilities in robotic systems. These
models are pretrained on vast datasets of robot trajectories to process
multi-modal inputs and directly output a sequence of action that the system
then executes in the real world. Although this approach is attractive from the
perspective of improved generalization across diverse tasks, these models are
still data-driven and, therefore, lack explicit notions of behavioral
correctness and safety constraints. We address these limitations by introducing
a constrained decoding framework for robotics foundation models that enforces
logical constraints on action trajectories in dynamical systems. Our method
ensures that generated actions provably satisfy signal temporal logic (STL)
specifications at runtime without retraining, while remaining agnostic of the
underlying foundation model. We perform comprehensive evaluation of our
approach across state-of-the-art navigation foundation models and we show that
our decoding-time interventions are useful not only for filtering unsafe
actions but also for conditional action-generation. Videos available on our
website: https://constrained-robot-fms.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Articulated Object <span class="highlight-title">Estimation</span> in the Wild <span class="chip">CoRL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrhman Werby, Martin Büchner, Adrian Röfer, Chenguang Huang, Wolfram Burgard, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the 3D motion of articulated objects is essential in robotic
scene understanding, mobile manipulation, and motion planning. Prior methods
for articulation estimation have primarily focused on controlled settings,
assuming either fixed camera viewpoints or direct observations of various
object states, which tend to fail in more realistic unconstrained environments.
In contrast, humans effortlessly infer articulation by watching others
manipulate objects. Inspired by this, we introduce ArtiPoint, a novel
estimation framework that can infer articulated object models under dynamic
camera motion and partial observability. By combining deep point tracking with
a factor graph optimization framework, ArtiPoint robustly estimates articulated
part trajectories and articulation axes directly from raw RGB-D videos. To
foster future research in this domain, we introduce Arti4D, the first
ego-centric in-the-wild dataset that captures articulated object interactions
at a scene level, accompanied by articulation labels and ground-truth camera
poses. We benchmark ArtiPoint against a range of classical and learning-based
baselines, demonstrating its superior performance on Arti4D. We make code and
Arti4D publicly available at https://artipoint.cs.uni-freiburg.de.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9th Conference on Robot Learning (CoRL), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoTo: A Zero-shot Plug-in Interaction-aware <span class="highlight-title">Navigation</span> for General
  Mobile <span class="highlight-title">Manipulation</span> <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Wu, Angyuan Ma, Xiuwei Xu, Hang Yin, Yinan Liang, Ziwei Wang, Jiwen Lu, Haibin Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile manipulation stands as a core challenge in robotics, enabling robots
to assist humans across varied tasks and dynamic daily environments.
Conventional mobile manipulation approaches often struggle to generalize across
different tasks and environments due to the lack of large-scale training.
However, recent advances in manipulation foundation models demonstrate
impressive generalization capability on a wide range of fixed-base manipulation
tasks, which are still limited to a fixed setting. Therefore, we devise a
plug-in module named MoTo, which can be combined with any off-the-shelf
manipulation foundation model to empower them with mobile manipulation ability.
Specifically, we propose an interaction-aware navigation policy to generate
robot docking points for generalized mobile manipulation. To enable zero-shot
ability, we propose an interaction keypoints framework via vision-language
models (VLM) under multi-view consistency for both target object and robotic
arm following instructions, where fixed-base manipulation foundation models can
be employed. We further propose motion planning objectives for the mobile base
and robot arm, which minimize the distance between the two keypoints and
maintain the physical feasibility of trajectories. In this way, MoTo guides the
robot to move to the docking points where fixed-base manipulation can be
successfully performed, and leverages VLM generation and trajectory
optimization to achieve mobile manipulation in a zero-shot manner, without any
requirement on mobile manipulation expert data. Extensive experimental results
on OVMM and real-world demonstrate that MoTo achieves success rates of 2.68%
and 16.67% higher than the state-of-the-art mobile manipulation methods,
respectively, without requiring additional training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CoRL 2025. Project Page: https://gary3410.github.io/MoTo/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Retrieval with Importance Weights for Few-Shot Imitation <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amber Xie, Rahul Chand, Dorsa Sadigh, Joey Hejna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large-scale robot datasets have propelled recent progress in imitation
learning, learning from smaller task specific datasets remains critical for
deployment in new environments and unseen tasks. One such approach to few-shot
imitation learning is retrieval-based imitation learning, which extracts
relevant samples from large, widely available prior datasets to augment a
limited demonstration dataset. To determine the relevant data from prior
datasets, retrieval-based approaches most commonly calculate a prior data
point's minimum distance to a point in the target dataset in latent space.
While retrieval-based methods have shown success using this metric for data
selection, we demonstrate its equivalence to the limit of a Gaussian kernel
density (KDE) estimate of the target data distribution. This reveals two
shortcomings of the retrieval rule used in prior work. First, it relies on
high-variance nearest neighbor estimates that are susceptible to noise. Second,
it does not account for the distribution of prior data when retrieving data. To
address these issues, we introduce Importance Weighted Retrieval (IWR), which
estimates importance weights, or the ratio between the target and prior data
distributions for retrieval, using Gaussian KDEs. By considering the
probability ratio, IWR seeks to mitigate the bias of previous selection rules,
and by using reasonable modeling parameters, IWR effectively smooths estimates
using all data points. Across both simulation environments and real-world
evaluations on the Bridge dataset we find that our method, IWR, consistently
improves performance of existing retrieval-based methods, despite only
requiring minor modifications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Robot Learning 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speculative Design of Equitable <span class="highlight-title">Robot</span>ics: Queer Fictions and Futures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minja Axelsson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper examines the speculative topic of equitable robots through an
exploratory essay format. It focuses specifically on robots by and for LGBTQ+
populations. It aims to provoke thought and conversations in the field about
what aspirational queer robotics futures may look like, both in the arts and
sciences. First, it briefly reviews the state-of-the-art of queer robotics in
fiction and science, drawing together threads from each. Then, it discusses
queering robots through three speculative design proposals for queer robot
roles: 1) reflecting the queerness of their ''in-group'' queer users, building
and celebrating ''in-group'' identity, 2) a new kind of queer activism by
implementing queer robot identity performance to interact with ''out-group''
users, with a goal of reducing bigotry through familiarisation, and 3) a
network of queer-owned robots, through which the community could reach each
other, and distribute and access important resources. The paper then questions
whether robots should be queered, and what ethical implications this raises.
Finally, the paper makes suggestions for what aspirational queer robotics
futures may look like, and what would be required to get there.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the British Computer Society's Special Interest Group in
  Human Computer Interaction Conference (BCS HCI 2025), Futures track. 5 pages,
  no figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Learning</span> to Coordinate: Distributed Meta-Trajectory <span class="highlight-title">Optimization</span> Via
  Differentiable ADMM-DDP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingheng Wang, Yichao Gao, Tianchen Sun, Lin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed trajectory optimization via ADMM-DDP is a powerful approach for
coordinating multi-agent systems, but it requires extensive tuning of tightly
coupled hyperparameters that jointly govern local task performance and global
coordination. In this paper, we propose Learning to Coordinate (L2C), a general
framework that meta-learns these hyperparameters, modeled by lightweight
agent-wise neural networks, to adapt across diverse tasks and agent
configurations. L2C differentiates end-to-end through the ADMM-DDP pipeline in
a distributed manner. It also enables efficient meta-gradient computation by
reusing DDP components such as Riccati recursions and feedback gains. These
gradients correspond to the optimal solutions of distributed matrix-valued LQR
problems, coordinated across agents via an auxiliary ADMM framework that
becomes convex under mild assumptions. Training is further accelerated by
truncating iterations and meta-learning ADMM penalty parameters optimized for
rapid residual reduction, with provable Lipschitz-bounded gradient errors. On a
challenging cooperative aerial transport task, L2C generates dynamically
feasible trajectories in high-fidelity simulation using IsaacSIM, reconfigures
quadrotor formations for safe 6-DoF load manipulation in tight spaces, and
adapts robustly to varying team sizes and task conditions, while achieving up
to $88\%$ faster gradient computation than state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hybrid Input based Deep Reinforcement <span class="highlight-title">Learning</span> for Lane Change
  Decision-Making of Autonomous Vehicle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziteng Gao, Jiaqi Qu, Chaoyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lane change decision-making for autonomous vehicles is a complex but
high-reward behavior. In this paper, we propose a hybrid input based deep
reinforcement learning (DRL) algorithm, which realizes abstract lane change
decisions and lane change actions for autonomous vehicles within traffic flow.
Firstly, a surrounding vehicles trajectory prediction method is proposed to
reduce the risk of future behavior of surrounding vehicles to ego vehicle, and
the prediction results are input into the reinforcement learning model as
additional information. Secondly, to comprehensively leverage environmental
information, the model extracts feature from high-dimensional images and
low-dimensional sensor data simultaneously. The fusion of surrounding vehicle
trajectory prediction and multi-modal information are used as state space of
reinforcement learning to improve the rationality of lane change decision.
Finally, we integrate reinforcement learning macro decisions with end-to-end
vehicle control to achieve a holistic lane change process. Experiments were
conducted within the CARLA simulator, and the results demonstrated that the
utilization of a hybrid state space significantly enhances the safety of
vehicle lane change decisions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TransForSeg: A Multitask Stereo ViT for Joint Stereo Segmentation and 3D
  Force <span class="highlight-title">Estimation</span> in Catheterization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedram Fekri, Mehrdad Zadeh, Javad Dargahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the emergence of multitask deep learning models has enhanced
catheterization procedures by providing tactile and visual perception data
through an end-to-end architecture. This information is derived from a
segmentation and force estimation head, which localizes the catheter in X-ray
images and estimates the applied pressure based on its deflection within the
image. These stereo vision architectures incorporate a CNN-based
encoder-decoder that captures the dependencies between X-ray images from two
viewpoints, enabling simultaneous 3D force estimation and stereo segmentation
of the catheter. With these tasks in mind, this work approaches the problem
from a new perspective. We propose a novel encoder-decoder Vision Transformer
model that processes two input X-ray images as separate sequences. Given
sequences of X-ray patches from two perspectives, the transformer captures
long-range dependencies without the need to gradually expand the receptive
field for either image. The embeddings generated by both the encoder and
decoder are fed into two shared segmentation heads, while a regression head
employs the fused information from the decoder for 3D force estimation. The
proposed model is a stereo Vision Transformer capable of simultaneously
segmenting the catheter from two angles while estimating the generated forces
at its tip in 3D. This model has undergone extensive experiments on synthetic
X-ray images with various noise levels and has been compared against
state-of-the-art pure segmentation models, vision-based catheter force
estimation methods, and a multitask catheter segmentation and force estimation
approach. It outperforms existing models, setting a new state-of-the-art in
both catheter segmentation and force estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint version. This work is intended for future journal submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aleatoric Uncertainty from AI-based 6D Object Pose Predictors for
  Object-relative State <span class="highlight-title">Estimation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Jantos, Stephan Weiss, Jan Steinbrener
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning (DL) has become essential in various robotics applications due
to excelling at processing raw sensory data to extract task specific
information from semantic objects. For example, vision-based object-relative
navigation relies on a DL-based 6D object pose predictor to provide the
relative pose between the object and the robot as measurements to the robot's
state estimator. Accurately knowing the uncertainty inherent in such Deep
Neural Network (DNN) based measurements is essential for probabilistic state
estimators subsequently guiding the robot's tasks. Thus, in this letter, we
show that we can extend any existing DL-based object-relative pose predictor
for aleatoric uncertainty inference simply by including two multi-layer
perceptrons detached from the translational and rotational part of the DL
predictor. This allows for efficient training while freezing the existing
pre-trained predictor. We then use the inferred 6D pose and its uncertainty as
a measurement and corresponding noise covariance matrix in an extended Kalman
filter (EKF). Our approach induces minimal computational overhead such that the
state estimator can be deployed on edge devices while benefiting from the
dynamically inferred measurement uncertainty. This increases the performance of
the object-relative state estimation task compared to a fix-covariance
approach. We conduct evaluations on synthetic data and real-world data to
underline the benefits of aleatoric uncertainty inference for the
object-relative state estimation task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Robotics and Automation Letters
  (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum game models for interaction-aware decision-making in automated
  <span class="highlight-title">driving</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karim Essalmi, Fernando Garrido, Fawzi Nashashibi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-making in automated driving must consider interactions with
surrounding agents to be effective. However, traditional methods often neglect
or oversimplify these interactions because they are difficult to model and
solve, which can lead to overly conservative behavior of the ego vehicle. To
address this gap, we propose two quantum game models, QG-U1 (Quantum Game -
Unitary 1) and QG-G4 (Quantum Game - Gates 4), for interaction-aware
decision-making. These models extend classical game theory by incorporating
principles of quantum mechanics, such as superposition, interference, and
entanglement. Specifically, QG-U1 and QG-G4 are designed for two-player games
with two strategies per player and can be executed in real time on a standard
computer without requiring quantum hardware. We evaluate both models in merging
and roundabout scenarios and compare them with classical game-theoretic methods
and baseline approaches (IDM, MOBIL, and a utility-based technique). Results
show that QG-G4 achieves lower collision rates and higher success rates
compared to baseline methods, while both quantum models yield higher expected
payoffs than classical game approaches under certain parameter settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, submitted to ICAR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FGO-<span class="highlight-title">SLAM</span>: Enhancing Gaussian <span class="highlight-title">SLAM</span> with Globally Consistent Opacity
  Radiance Field <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Zhu, Yifan Zhao, Ziyu Chen, Biao Yu, Hui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual SLAM has regained attention due to its ability to provide perceptual
capabilities and simulation test data for Embodied AI. However, traditional
SLAM methods struggle to meet the demands of high-quality scene reconstruction,
and Gaussian SLAM systems, despite their rapid rendering and high-quality
mapping capabilities, lack effective pose optimization methods and face
challenges in geometric reconstruction. To address these issues, we introduce
FGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as the
scene representation to enhance geometric mapping performance. After initial
pose estimation, we apply global adjustment to optimize camera poses and sparse
point cloud, ensuring robust tracking of our approach. Additionally, we
maintain a globally consistent opacity radiance field based on 3D Gaussians and
introduce depth distortion and normal consistency terms to refine the scene
representation. Furthermore, after constructing tetrahedral grids, we identify
level sets to directly extract surfaces from 3D Gaussians. Results across
various real-world and large-scale synthetic datasets demonstrate that our
method achieves state-of-the-art tracking accuracy and mapping performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Who Owns The <span class="highlight-title">Robot</span>?: Four Ethical and Socio-technical Questions about
  Wellbeing <span class="highlight-title">Robots</span> in the Real World through Community Engagement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.02624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.02624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minja Axelsson, Jiaee Cheong, Rune Nyrup, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies indicate that robotic coaches can play a crucial role in
promoting wellbeing. However, the real-world deployment of wellbeing robots
raises numerous ethical and socio-technical questions and concerns. To explore
these questions, we undertake a community-centered investigation to examine
three different communities' perspectives on using robotic wellbeing coaches in
real-world environments. We frame our work as an anticipatory ethical
investigation, which we undertake to better inform the development of robotic
technologies with communities' opinions, with the ultimate goal of aligning
robot development with public interest. We conducted workshops with three
communities who are under-represented in robotics development: 1) members of
the public at a science festival, 2) women computer scientists at a conference,
and 3) humanities researchers interested in history and philosophy of science.
In the workshops, we collected qualitative data using the Social Robot
Co-Design Canvas on Ethics. We analysed the collected qualitative data with
Thematic Analysis, informed by notes taken during workshops. Through our
analysis, we identify four themes regarding key ethical and socio-technical
questions about the real-world use of wellbeing robots. We group participants'
insights and discussions around these broad thematic questions, discuss them in
light of state-of-the-art literature, and highlight areas for future
investigation. Finally, we provide the four questions as a broad framework that
roboticists can and should use during robotic development and deployment, in
order to reflect on the ethics and socio-technical dimensions of their robotic
applications, and to engage in dialogue with communities of robot users. The
four questions are: 1) Is the robot safe and how can we know that?, 2) Who is
the robot built for and with?, 3) Who owns the robot and the data?, and 4) Why
a robot?.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 8th AAAI/ACM Conference on AI, Ethics, and Society.
  23 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Reluctance to Ask for Help When Cooperating With <span class="highlight-title">Robots</span>:
  Insights to Integrate Artificial Agents in HRC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ane San Martin, Michael Hagenow, Julie Shah, Johan Kildal, Elena Lazkano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As robot technology advances, collaboration between humans and robots will
become more prevalent in industrial tasks. When humans run into issues in such
scenarios, a likely future involves relying on artificial agents or robots for
aid. This study identifies key aspects for the design of future user-assisting
agents. We analyze quantitative and qualitative data from a user study
examining the impact of on-demand assistance received from a remote human in a
human-robot collaboration (HRC) assembly task. We study scenarios in which
users require help and we assess their experiences in requesting and receiving
assistance. Additionally, we investigate participants' perceptions of future
non-human assisting agents and whether assistance should be on-demand or
unsolicited. Through a user study, we analyze the impact that such design
decisions (human or artificial assistant, on-demand or unsolicited help) can
have on elicited emotional responses, productivity, and preferences of humans
engaged in HRC tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures. Accepted for IEEE RO-MAN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Low-Level Neural Control of an Industrial-Grade 6D Magnetic
  Levitation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp Hartmann, Jannick Stranghöner, Klaus Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Magnetic levitation is poised to revolutionize industrial automation by
integrating flexible in-machine product transport and seamless manipulation. It
is expected to become the standard drive for automated manufacturing. However,
controlling such systems is inherently challenging due to their complex,
unstable dynamics. Traditional control approaches, which rely on hand-crafted
control engineering, typically yield robust but conservative solutions, with
their performance closely tied to the expertise of the engineering team. In
contrast, neural control learning presents a promising alternative. This paper
presents the first neural controller for 6D magnetic levitation. Trained
end-to-end on interaction data from a proprietary controller, it directly maps
raw sensor data and 6D reference poses to coil current commands. The neural
controller can effectively generalize to previously unseen situations while
maintaining accurate and robust control. These results underscore the practical
feasibility of learning-based neural control in complex physical systems and
suggest a future where such a paradigm could enhance or even substitute
traditional engineering approaches in demanding real-world applications. The
trained neural controller, source code, and demonstration videos are publicly
available at https://sites.google.com/view/neural-maglev.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TopoNav: Topological Graphs as a Key Enabler for Advanced Object
  <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiran Liu, Qiang Zhang, Daojie Peng, Lingfeng Zhang, Yihao Qin, Hang Zhou, Jun Ma, Renjing Xu, Yiding Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object Navigation (ObjectNav) has made great progress with large language
models (LLMs), but still faces challenges in memory management, especially in
long-horizon tasks and dynamic scenes. To address this, we propose TopoNav, a
new framework that leverages topological structures as spatial memory. By
building and updating a topological graph that captures scene connections,
adjacency, and semantic meaning, TopoNav helps agents accumulate spatial
knowledge over time, retrieve key information, and reason effectively toward
distant goals. Our experiments show that TopoNav achieves state-of-the-art
performance on benchmark ObjectNav datasets, with higher success rates and more
efficient paths. It particularly excels in diverse and complex environments, as
it connects temporary visual inputs with lasting spatial understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangled Multi-Context Meta-<span class="highlight-title">Learning</span>: Unlocking <span class="highlight-title">robust</span> and
  Generalized Task <span class="highlight-title">Learning</span> <span class="chip">CoRL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonsoo Kim, Jun-Gill Kang, Taehong Kim, Seongil Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In meta-learning and its downstream tasks, many methods rely on implicit
adaptation to task variations, where multiple factors are mixed together in a
single entangled representation. This makes it difficult to interpret which
factors drive performance and can hinder generalization. In this work, we
introduce a disentangled multi-context meta-learning framework that explicitly
assigns each task factor to a distinct context vector. By decoupling these
variations, our approach improves robustness through deeper task understanding
and enhances generalization by enabling context vector sharing across tasks
with shared factors. We evaluate our approach in two domains. First, on a
sinusoidal regression task, our model outperforms baselines on
out-of-distribution tasks and generalizes to unseen sine functions by sharing
context vectors associated with shared amplitudes or phase shifts. Second, in a
quadruped robot locomotion task, we disentangle the robot-specific properties
and the characteristics of the terrain in the robot dynamics model. By
transferring disentangled context vectors acquired from the dynamics model into
reinforcement learning, the resulting policy achieves improved robustness under
out-of-distribution conditions, surpassing the baselines that rely on a single
unified context. Furthermore, by effectively sharing context, our model enables
successful sim-to-real policy transfer to challenging terrains with
out-of-distribution robot-specific properties, using just 20 seconds of real
data from flat terrain, a result not achievable with single-task adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to The Conference on Robot Learning (CoRL) 2025 Project
  Page: seonsoo-p1.github.io/DMCM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Metamorphic Testing of Multimodal Human Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Helge Spieker, Nadjib Lazaar, Arnaud Gotlieb, Nassim Belmecheri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context: Predicting human trajectories is crucial for the safety and
reliability of autonomous systems, such as automated vehicles and mobile
robots. However, rigorously testing the underlying multimodal Human Trajectory
Prediction (HTP) models, which typically use multiple input sources (e.g.,
trajectory history and environment maps) and produce stochastic outputs
(multiple possible future paths), presents significant challenges. The primary
difficulty lies in the absence of a definitive test oracle, as numerous future
trajectories might be plausible for any given scenario. Objectives: This
research presents the application of Metamorphic Testing (MT) as a systematic
methodology for testing multimodal HTP systems. We address the oracle problem
through metamorphic relations (MRs) adapted for the complexities and stochastic
nature of HTP. Methods: We present five MRs, targeting transformations of both
historical trajectory data and semantic segmentation maps used as an
environmental context. These MRs encompass: 1) label-preserving geometric
transformations (mirroring, rotation, rescaling) applied to both trajectory and
map inputs, where outputs are expected to transform correspondingly. 2)
Map-altering transformations (changing semantic class labels, introducing
obstacles) with predictable changes in trajectory distributions. We propose
probabilistic violation criteria based on distance metrics between probability
distributions, such as the Wasserstein or Hellinger distance. Conclusion: This
study introduces tool, a MT framework for the oracle-less testing of
multimodal, stochastic HTP systems. It allows for assessment of model
robustness against input transformations and contextual changes without
reliance on ground-truth trajectories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Information and Software Technology</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward a Holistic Multi-Criteria Trajectory Evaluation Framework for
  Autonomous <span class="highlight-title">Driving</span> in Mixed Traffic Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nouhed Naidja, Stéphane Font, Marc Revilloud, Guillaume Sandou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a unified framework for the evaluation and optimization
of autonomous vehicle trajectories, integrating formal safety, comfort, and
efficiency criteria. An innovative geometric indicator, based on the analysis
of safety zones using adaptive ellipses, is used to accurately quantify
collision risks. Our method applies the Shoelace formula to compute the
intersection area in the case of misaligned and time-varying configurations.
Comfort is modeled using indicators centered on longitudinal and lateral jerk,
while efficiency is assessed by overall travel time. These criteria are
aggregated into a comprehensive objective function solved using a PSO based
algorithm. The approach was successfully validated under real traffic
conditions via experiments conducted in an urban intersection involving an
autonomous vehicle interacting with a human-operated vehicle, and in simulation
using data recorded from human driving in real traffic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Data-Driven Metrics for Social <span class="highlight-title">Robot</span> <span class="highlight-title">Navigation</span> Benchmarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pilar Bachiller-Burgos, Ulysses Bernardet, Luis V. Calderita, Pranup Chhetri, Anthony Francis, Noriaki Hirose, Noé Pérez, Dhruv Shah, Phani T. Singamaneni, Xuesu Xiao, Luis J. Manso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a joint effort towards the development of a data-driven
Social Robot Navigation metric to facilitate benchmarking and policy
optimization. We provide our motivations for our approach and describe our
proposal for storing rated social navigation trajectory datasets. Following
these guidelines, we compiled a dataset with 4427 trajectories -- 182 real and
4245 simulated -- and presented it to human raters, yielding a total of 4402
rated trajectories after data quality assurance. We also trained an RNN-based
baseline metric on the dataset and present quantitative and qualitative
results. All data, software, and model weights are publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An AI-Based Shopping Assistant System to Support the <span class="highlight-title">Visual</span>ly Impaired 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01246v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01246v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Larissa R. de S. Shibata, Ankit A. Ravankar, Jose Victorio Salazar Luces, Yasuhisa Hirata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shopping plays a significant role in shaping consumer identity and social
integration. However, for individuals with visual impairments, navigating in
supermarkets and identifying products can be an overwhelming and challenging
experience. This paper presents an AI-based shopping assistant prototype
designed to enhance the autonomy and inclusivity of visually impaired
individuals in supermarket environments. The system integrates multiple
technologies, including computer vision, speech recognition, text-to-speech
synthesis, and indoor navigation, into a single, user-friendly platform. Using
cameras for ArUco marker detection and real-time environmental scanning, the
system helps users navigate the store, identify product locations, provide
real-time auditory guidance, and gain context about their surroundings. The
assistant interacts with the user through voice commands and multimodal
feedback, promoting a more dynamic and engaging shopping experience. The system
was evaluated through experiments, which demonstrated its ability to guide
users effectively and improve their shopping experience. This paper contributes
to the development of inclusive AI-driven assistive technologies aimed at
enhancing accessibility and user independence for the shopping experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, Accepted for 2025 SICE-FES conference (IEEE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenMulti: <span class="highlight-title">Open-Vocabulary</span> Instance-Level Multi-Agent Distributed
  Implicit Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianyu Dou, Yinan Deng, Jiahui Wang, Xingsi Tang, Yi Yang, Yufeng Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent distributed collaborative mapping provides comprehensive and
efficient representations for robots. However, existing approaches lack
instance-level awareness and semantic understanding of environments, limiting
their effectiveness for downstream applications. To address this issue, we
propose OpenMulti, an open-vocabulary instance-level multi-agent distributed
implicit mapping framework. Specifically, we introduce a Cross-Agent Instance
Alignment module, which constructs an Instance Collaborative Graph to ensure
consistent instance understanding across agents. To alleviate the degradation
of mapping accuracy due to the blind-zone optimization trap, we leverage Cross
Rendering Supervision to enhance distributed learning of the scene.
Experimental results show that OpenMulti outperforms related algorithms in both
fine-grained geometric accuracy and zero-shot semantic accuracy. In addition,
OpenMulti supports instance-level retrieval tasks, delivering semantic
annotations for downstream applications. The project website of OpenMulti is
publicly available at https://openmulti666.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Robotics and Automation Letters. Project website:
  https://openmulti666.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel bio-inspired soft actuators for upper-limb exoskeletons: design,
  fabrication and feasibility study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyun Zhang, Gabrielle Naquila, Jung Hyun Bae, Zonghuan Wu, Ashwin Hingwe, Ashish Deshpande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft robots have been increasingly utilized as sophisticated tools in
physical rehabilitation, particularly for assisting patients with neuromotor
impairments. However, many soft robotics for rehabilitation applications are
characterized by limitations such as slow response times, restricted range of
motion, and low output force. There are also limited studies on the precise
position and force control of wearable soft actuators. Furthermore, not many
studies articulate how bellow-structured actuator designs quantitatively
contribute to the robots' capability. This study introduces a paradigm of upper
limb soft actuator design. This paradigm comprises two actuators: the
Lobster-Inspired Silicone Pneumatic Robot (LISPER) for the elbow and the
Scallop-Shaped Pneumatic Robot (SCASPER) for the shoulder. LISPER is
characterized by higher bandwidth, increased output force/torque, and high
linearity. SCASPER is characterized by high output force/torque and simplified
fabrication processes. Comprehensive analytical models that describe the
relationship between pressure, bending angles, and output force for both
actuators were presented so the geometric configuration of the actuators can be
set to modify the range of motion and output forces. The preliminary test on a
dummy arm is conducted to test the capability of the actuators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A novel parameter <span class="highlight-title">estimation</span> method for pneumatic soft hand control
  applying logarithmic decrement for pseudo rigid body modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyun Zhang, Kelvin HoLam Heung, Gabrielle J. Naquila, Ashwin Hingwe, Ashish D. Deshpande
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement in physical human-robot interaction (HRI) has
accelerated the development of soft robot designs and controllers. Controlling
soft robots, especially soft hand grasping, is challenging due to their
continuous deformation, motivating the use of reduced model-based controllers
for real-time dynamic performance. Most existing models, however, suffer from
computational inefficiency and complex parameter identification, limiting their
real-time applicability. To address this, we propose a paradigm coupling
Pseudo-Rigid Body Modeling with the Logarithmic Decrement Method for parameter
estimation (PRBM plus LDM). Using a soft robotic hand test bed, we validate
PRBM plus LDM for predicting position and force output from pressure input and
benchmark its performance. We then implement PRBM plus LDM as the basis for
closed-loop position and force controllers. Compared to a simple PID
controller, the PRBM plus LDM position controller achieves lower error (average
maximum error across all fingers: 4.37 degrees versus 20.38 degrees). For force
control, PRBM plus LDM outperforms constant pressure grasping in pinching tasks
on delicate objects: potato chip 86 versus 82.5, screwdriver 74.42 versus 70,
brass coin 64.75 versus 35. These results demonstrate PRBM plus LDM as a
computationally efficient and accurate modeling technique for soft actuators,
enabling stable and flexible grasping with precise force regulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SR-<span class="highlight-title">SLAM</span>: Scene-reliability Based RGB-D <span class="highlight-title">SLAM</span> in Diverse Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolan Zhang, Chenghao Li, Thanh Nguyen Canh, Lijun Wang, Nak Young Chong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual simultaneous localization and mapping (SLAM) plays a critical role in
autonomous robotic systems, especially where accurate and reliable measurements
are essential for navigation and sensing. In feature-based SLAM, the
quantityand quality of extracted features significantly influence system
performance. Due to the variations in feature quantity and quality across
diverse environments, current approaches face two major challenges: (1) limited
adaptability in dynamic feature culling and pose estimation, and (2)
insufficient environmental awareness in assessment and optimization strategies.
To address these issues, we propose SRR-SLAM, a scene-reliability based
framework that enhances feature-based SLAM through environment-aware
processing. Our method introduces a unified scene reliability assessment
mechanism that incorporates multiple metrics and historical observations to
guide system behavior. Based on this assessment, we develop: (i) adaptive
dynamic region selection with flexible geometric constraints, (ii)
depth-assisted self-adjusting clustering for efficient dynamic feature removal
in high-dimensional settings, and (iii) reliability-aware pose refinement that
dynamically integrates direct methods when features are insufficient.
Furthermore, we propose (iv) reliability-based keyframe selection and a
weighted optimization scheme to reduce computational overhead while improving
estimation accuracy. Extensive experiments on public datasets and real world
scenarios show that SRR-SLAM outperforms state-of-the-art dynamic SLAM methods,
achieving up to 90% improvement in accuracy and robustness across diverse
environments. These improvements directly contribute to enhanced measurement
precision and reliability in autonomous robotic sensing systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robix: A Unified Model for <span class="highlight-title">Robot</span> Interaction, Reasoning and <span class="highlight-title">Planning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Robix, a unified model that integrates robot reasoning, task
planning, and natural language interaction within a single vision-language
architecture. Acting as the high-level cognitive layer in a hierarchical robot
system, Robix dynamically generates atomic commands for the low-level
controller and verbal responses for human interaction, enabling robots to
follow complex instructions, plan long-horizon tasks, and interact naturally
with human within an end-to-end framework. Robix further introduces novel
capabilities such as proactive dialogue, real-time interruption handling, and
context-aware commonsense reasoning during task execution. At its core, Robix
leverages chain-of-thought reasoning and adopts a three-stage training
strategy: (1) continued pretraining to enhance foundational embodied reasoning
abilities including 3D spatial understanding, visual grounding, and
task-centric reasoning; (2) supervised finetuning to model human-robot
interaction and task planning as a unified reasoning-action sequence; and (3)
reinforcement learning to improve reasoning-action consistency and long-horizon
task coherence. Extensive experiments demonstrate that Robix outperforms both
open-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in
interactive task execution, demonstrating strong generalization across diverse
instruction types (e.g., open-ended, multi-stage, constrained, invalid, and
interrupted) and various user-involved tasks such as table bussing, grocery
shopping, and dietary filtering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report. Project page: https://robix-seed.github.io/robix/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Predictive Control for a Soft <span class="highlight-title">Robot</span>ic Finger with Stochastic
  Behavior based on Fokker-Planck Equation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumitaka Honji, Takahiro Wada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inherent flexibility of soft robots offers numerous advantages, such as
enhanced adaptability and improved safety. However, this flexibility can also
introduce challenges regarding highly uncertain and nonlinear motion. These
challenges become particularly problematic when using open-loop control
methods, which lack a feedback mechanism and are commonly employed in soft
robot control. Though one potential solution is model-based control, typical
deterministic models struggle with uncertainty as mentioned above. The idea is
to use the Fokker-Planck Equation (FPE), a master equation of a stochastic
process, to control not the state of soft robots but the probabilistic
distribution. In this study, we propose and implement a stochastic-based
control strategy, termed FPE-based Model Predictive Control (FPE-MPC), for a
soft robotic finger. Two numerical simulation case studies examine the
performance and characteristics of this control method, revealing its efficacy
in managing the uncertainty inherent in soft robotic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures, presented/published at 2025 IEEE 8th
  International Conference on Soft Robotics (RoboSoft)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> A Reactive Grasping Framework for Multi-DoF Grippers via Task Space
  Velocity Fields and Joint Space QP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonghyeon Lee, Tzu-Yuan Lin, Alexander Alexiev, <span class="highlight-author">Sangbae Kim</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a fast and reactive grasping framework for multi-DoF grippers that
combines task-space velocity fields with a joint-space Quadratic Program (QP)
in a hierarchical structure. Reactive, collision-free global motion planning is
particularly challenging for high-DoF systems, since simultaneous increases in
state dimensionality and planning horizon trigger a combinatorial explosion of
the search space, making real-time planning intractable. To address this, we
plan globally in a lower-dimensional task space, such as fingertip positions,
and track locally in the full joint space while enforcing all constraints. This
approach is realized by constructing velocity fields in multiple task-space
coordinates (or in some cases a subset of joint coordinates) and solving a
weighted joint-space QP to compute joint velocities that track these fields
with appropriately assigned priorities. Through simulation experiments with
privileged knowledge and real-world tests using the recent pose-tracking
algorithm FoundationPose, we verify that our method enables high-DoF arm-hand
systems to perform real-time, collision-free reaching motions while adapting to
dynamic environments and external disturbances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 12 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TARA: A Low-Cost 3D-Printed <span class="highlight-title">Robot</span>ic Arm for Accessible <span class="highlight-title">Robot</span>ics
  Education 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thays Leach Mitre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high cost of robotic platforms limits students' ability to gain practical
skills directly applicable in real-world scenarios. To address this challenge,
this paper presents TARA, a low-cost, 3D-printed robotic arm designed for
accessible robotics education. TARA includes an open-source repository with
design files, assembly instructions, and baseline code, enabling users to build
and customize the platform. The system balances affordability and
functionality, offering a highly capable robotic arm for approximately 200 USD,
significantly lower than industrial systems that often cost thousands of
dollars. Experimental validation confirmed accurate performance in basic
manipulation tasks. Rather than focusing on performance benchmarking, this work
prioritizes educational reproducibility, providing a platform that students and
educators can reliably replicate and extend.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures. Preprint submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking LLM Privacy Recognition for Social <span class="highlight-title">Robot</span> Decision Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.16124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.16124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dakota Sullivan, Shirley Zhang, Jennica Li, Heather Kirkorian, Bilge Mutlu, Kassem Fawaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While robots have previously utilized rule-based systems or probabilistic
models for user interaction, the rapid evolution of large language models
(LLMs) presents new opportunities to develop LLM-powered robots for enhanced
human-robot interaction (HRI). To fully realize these capabilities, however,
robots need to collect data such as audio, fine-grained images, video, and
locations. As a result, LLMs often process sensitive personal information,
particularly within private environments, such as homes. Given the tension
between utility and privacy risks, evaluating how current LLMs manage sensitive
data is critical. Specifically, we aim to explore the extent to which
out-of-the-box LLMs are privacy-aware in the context of household robots. In
this work, we present a set of privacy-relevant scenarios developed using the
Contextual Integrity (CI) framework. We first surveyed users' privacy
preferences regarding in-home robot behaviors and then examined how their
privacy orientations affected their choices of these behaviors (N = 450). We
then provided the same set of scenarios and questions to state-of-the-art LLMs
(N = 10) and found that the agreement between humans and LLMs was generally
low. To further investigate the capabilities of LLMs as potential privacy
controllers, we implemented four additional prompting strategies and compared
their results. We discuss the performance of the evaluated models as well as
the implications and potential of AI privacy awareness in human-robot
interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures. Dakota Sullivan and Shirley Zhang contributed
  equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Sim-to-Real</span> Reinforcement <span class="highlight-title">Learning</span> for <span class="highlight-title">Vision</span>-Based Dexterous
  <span class="highlight-title">Manipulation</span> on <span class="highlight-title">Humanoid</span>s <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20396v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20396v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toru Lin, Kartik Sachdev, Linxi Fan, Jitendra Malik, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning generalizable robot manipulation policies, especially for complex
multi-fingered humanoids, remains a significant challenge. Existing approaches
primarily rely on extensive data collection and imitation learning, which are
expensive, labor-intensive, and difficult to scale. Sim-to-real reinforcement
learning (RL) offers a promising alternative, but has mostly succeeded in
simpler state-based or single-hand setups. How to effectively extend this to
vision-based, contact-rich bimanual manipulation tasks remains an open
question. In this paper, we introduce a practical sim-to-real RL recipe that
trains a humanoid robot to perform three challenging dexterous manipulation
tasks: grasp-and-reach, box lift and bimanual handover. Our method features an
automated real-to-sim tuning module, a generalized reward formulation based on
contact and object goals, a divide-and-conquer policy distillation framework,
and a hybrid object representation strategy with modality-specific
augmentation. We demonstrate high success rates on unseen objects and robust,
adaptive policy behaviors -- highlighting that vision-based dexterous
manipulation via sim-to-real RL is not only viable, but also scalable and
broadly applicable to real-world humanoid manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at CoRL 2025. Project page can be found at
  https://toruowo.github.io/recipe/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Supervised <span class="highlight-title">Learning</span>-Based Path <span class="highlight-title">Planning</span> and Obstacle Avoidance
  Using PPO and B-Splines in Unknown Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahab Shokouhi, Oguzhan Oruc, May-Win Thein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces SmartBSP, an advanced self-supervised learning
framework for real-time path planning and obstacle avoidance in autonomous
robotics navigating through complex environments. The proposed system
integrates Proximal Policy Optimization (PPO) with Convolutional Neural
Networks (CNN) and Actor-Critic architecture to process limited LIDAR inputs
and compute spatial decision-making probabilities. The robot's perceptual field
is discretized into a grid format, which the CNN analyzes to produce a spatial
probability distribution. During the training process a nuanced cost function
is minimized that accounts for path curvature, endpoint proximity, and obstacle
avoidance. Simulations results in different scenarios validate the algorithm's
resilience and adaptability across diverse operational scenarios. Subsequently,
Real-time experiments, employing the Robot Operating System (ROS), were carried
out to assess the efficacy of the proposed algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Morphologically Symmetric Reinforcement <span class="highlight-title">Learning</span> for Ambidextrous
  Bimanual <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zechu Li, Yufeng Jin, Daniel Ordonez Apraez, Claudio Semini, Puze Liu, Georgia Chalvatzaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans naturally exhibit bilateral symmetry in their gross manipulation
skills, effortlessly mirroring simple actions between left and right hands.
Bimanual robots-which also feature bilateral symmetry-should similarly exploit
this property to perform tasks with either hand. Unlike humans, who often favor
a dominant hand for fine dexterous skills, robots should ideally execute
ambidextrous manipulation with equal proficiency. To this end, we introduce
SYMDEX (SYMmetric DEXterity), a reinforcement learning framework for
ambidextrous bi-manipulation that leverages the robot's inherent bilateral
symmetry as an inductive bias. SYMDEX decomposes complex bimanual manipulation
tasks into per-hand subtasks and trains dedicated policies for each. By
exploiting bilateral symmetry via equivariant neural networks, experience from
one arm is inherently leveraged by the opposite arm. We then distill the
subtask policies into a global ambidextrous policy that is independent of the
hand-task assignment. We evaluate SYMDEX on six challenging simulated
manipulation tasks and demonstrate successful real-world deployment on two of
them. Our approach strongly outperforms baselines on complex task in which the
left and right hands perform different roles. We further demonstrate SYMDEX's
scalability by extending it to a four-arm manipulation setup, where our
symmetry-aware policies enable effective multi-arm collaboration and
coordination. Our results highlight how structural symmetry as inductive bias
in policy learning enhances sample efficiency, robustness, and generalization
across diverse dexterous manipulation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dyna-LfLH: <span class="highlight-title">Learning</span> <span class="highlight-title">Agile</span> <span class="highlight-title">Navigation</span> in <span class="highlight-title">Dynamic</span> Environments from
  Learned Hallucination <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saad Abdul Ghani, Zizhao Wang, Peter Stone, Xuesu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Dynamic Learning from Learned Hallucination
(Dyna-LfLH), a self-supervised method for training motion planners to navigate
environments with dense and dynamic obstacles. Classical planners struggle with
dense, unpredictable obstacles due to limited computation, while learning-based
planners face challenges in acquiring high-quality demonstrations for imitation
learning or dealing with exploration inefficiencies in reinforcement learning.
Building on Learning from Hallucination (LfH), which synthesizes training data
from past successful navigation experiences in simpler environments, Dyna-LfLH
incorporates dynamic obstacles by generating them through a learned latent
distribution. This enables efficient and safe motion planner training. We
evaluate Dyna-LfLH on a ground robot in both simulated and real environments,
achieving up to a 25% improvement in success rate compared to baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at International Conference on Intelligent Robots and
  Systems (IROS) 2025 Hangzhou, China</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoK: Cybersecurity Assessment of <span class="highlight-title">Humanoid</span> Ecosystem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.17481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.17481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Priyanka Prakash Surve, Asaf Shabtai, Yuval Elovici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoids are progressing toward practical deployment across healthcare,
industrial, defense, and service sectors. While typically considered
cyber-physical systems (CPSs), their dependence on traditional networked
software stacks (e.g., Linux operating systems), robot operating system (ROS)
middleware, and over-the-air update channels, creates a distinct security
profile that exposes them to vulnerabilities conventional CPS models do not
fully address. Prior studies have mainly examined specific threats, such as
LiDAR spoofing or adversarial machine learning (AML). This narrow focus
overlooks how an attack targeting one component can cascade harm throughout the
robot's interconnected systems. We address this gap through a systematization
of knowledge (SoK) that takes a comprehensive approach, consolidating
fragmented research from robotics, CPS, and network security domains. We
introduce a seven-layer security model for humanoid robots, organizing 39 known
attacks and 35 defenses across the humanoid ecosystem-from hardware to
human-robot interaction. Building on this security model, we develop a
quantitative 39x35 attack-defense matrix with risk-weighted scoring, validated
through Monte Carlo analysis. We demonstrate our method by evaluating three
real-world robots: Pepper, G1 EDU, and Digit. The scoring analysis revealed
varying security maturity levels, with scores ranging from 39.9% to 79.5%
across the platforms. This work introduces a structured, evidence-based
assessment method that enables systematic security evaluation, supports
cross-platform benchmarking, and guides prioritization of security investments
in humanoid robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAVOR: Skill Affordance <span class="highlight-title">Learning</span> from Visuo-Haptic Perception for
  <span class="highlight-title">Robot</span>-Assisted Bite Acquisition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.02353v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.02353v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanxin Wu, Bo Ai, Tom Silver, Tapomayukh Bhattacharjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot-assisted feeding requires reliable bite acquisition, a challenging task
due to the complex interactions between utensils and food with diverse physical
properties. These interactions are further complicated by the temporal
variability of food properties-for example, steak becomes firm as it cools even
during a meal. To address this, we propose SAVOR, a novel approach for learning
skill affordances for bite acquisition-how suitable a manipulation skill (e.g.,
skewering, scooping) is for a given utensil-food interaction. In our
formulation, skill affordances arise from the combination of tool affordances
(what a utensil can do) and food affordances (what the food allows). Tool
affordances are learned offline through calibration, where different utensils
interact with a variety of foods to model their functional capabilities. Food
affordances are characterized by physical properties such as softness,
moisture, and viscosity, initially inferred through commonsense reasoning using
a visually-conditioned language model and then dynamically refined through
online multi-modal visuo-haptic perception using SAVOR-Net during interaction.
Our method integrates these offline and online estimates to predict skill
affordances in real time, enabling the robot to select the most appropriate
skill for each food item. Evaluated on 20 single-item foods and 10 in-the-wild
meals, our approach improves bite acquisition success rate by 13% over
state-of-the-art (SOTA) category-based methods (e.g. use skewer for fruits).
These results highlight the importance of modeling interaction-driven skill
affordances for generalizable and effective robot-assisted bite acquisition.
Website: https://emprise.cs.cornell.edu/savor/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Robot Learning, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wavelet Policy: Imitation Policy <span class="highlight-title">Learning</span> in the Scale Domain with
  Wavelet Transforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.04991v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.04991v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changchuan Yang, Yuhang Dong, Guanzhong Tian, Haizhou Ge, Hongrui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent imitation learning policies, often framed as time series prediction
tasks, directly map robotic observations into the action space, such as
high-dimensional visual data and proprioception. When deploying at the edge, we
found the underutilization of frequency domain analysis in robotic manipulation
trajectory prediction leads to neglecting the inherent rhythm information
embedded within action sequences, resulting in errors at critical moments. To
address this, we reframe imitation learning policies through the lens of
time-scale domain and introduce the Wavelet Policy. This novel approach employs
wavelet transforms (WT) and new Features Extractor (FE) for feature
preprocessing and extracts multi-scale features using the Single Encoder to
Multiple Decoder (SE2MD) architecture. Furthermore, to enhance feature mapping
in the scale domain and appropriately increase model capacity, we introduce a
Learnable Scale Domain Filter (LSDF) after each decoder, improving adaptability
under different visual conditions. Our results show that the Wavelet Policy
maintaining a comparable parameter count outperforms SOTA end-to-end methods on
four challenging simulation robotic arm tasks and real tasks, especially at
critical moments and remote settings simultaneously. We release the source code
and model checkpoint of simulation task at
https://github.com/lurenjia384/Wavelet_Policy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RALLY: Role-Adaptive LLM-Driven Yoked <span class="highlight-title">Navigation</span> for Agentic UAV Swarms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.01378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.01378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyao Wang, Rongpeng Li, Sizhao Li, Yuming Xiang, Haiping Wang, Zhifeng Zhao, Honggang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent control of Unmanned Aerial Vehicles (UAVs) swarms has emerged as
a critical research focus, and it typically requires the swarm to navigate
effectively while avoiding obstacles and achieving continuous coverage over
multiple mission targets. Although traditional Multi-Agent Reinforcement
Learning (MARL) approaches offer dynamic adaptability, they are hindered by the
semantic gap in numerical communication and the rigidity of homogeneous role
structures, resulting in poor generalization and limited task scalability.
Recent advances in Large Language Model (LLM)-based control frameworks
demonstrate strong semantic reasoning capabilities by leveraging extensive
prior knowledge. However, due to the lack of online learning and over-reliance
on static priors, these works often struggle with effective exploration,
leading to reduced individual potential and overall system performance. To
address these limitations, we propose a Role-Adaptive LLM-Driven Yoked
navigation algorithm RALLY. Specifically, we first develop an LLM-driven
semantic decision framework that uses structured natural language for efficient
semantic communication and collaborative reasoning. Afterward, we introduce a
dynamic role-heterogeneity mechanism for adaptive role switching and
personalized decision-making. Furthermore, we propose a Role-value Mixing
Network (RMIX)-based assignment strategy that integrates LLM offline priors
with MARL online policies to enable semi-offline training of role selection
strategies. Experiments in the Multi-Agent Particle Environment (MPE)
environment and a Software-In-The-Loop (SITL) platform demonstrate that RALLY
outperforms conventional approaches in terms of task coverage, convergence
speed, and generalization, highlighting its strong potential for collaborative
navigation in agentic multi-UAV systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ General agents contain world models <span class="chip">ICML 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.01622v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.01622v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Richens, David Abel, Alexis Bellot, Tom Everitt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Are world models a necessary ingredient for flexible, goal-directed
behaviour, or is model-free learning sufficient? We provide a formal answer to
this question, showing that any agent capable of generalizing to multi-step
goal-directed tasks must have learned a predictive model of its environment. We
show that this model can be extracted from the agent's policy, and that
increasing the agents performance or the complexity of the goals it can achieve
requires learning increasingly accurate world models. This has a number of
consequences: from developing safe and general agents, to bounding agent
capabilities in complex environments, and providing new algorithms for
eliciting world models from agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted ICML 2025. Typos corrected</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Force Myography based Torque <span class="highlight-title">Estimation</span> in Human Knee and Ankle Joints <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charlotte Marquardt, Arne Schulz, Miha Dezman, Gunther Kurz, Thorsten Stein, Tamim Asfour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The online adaptation of exoskeleton control based on muscle activity sensing
offers a promising approach to personalizing exoskeleton behavior based on the
user's biosignals. While electromyography (EMG)-based methods have demonstrated
improvements in joint torque estimation, EMG sensors require direct skin
contact and extensive post-processing. In contrast, force myography (FMG)
measures normal forces resulting from changes in muscle volume due to muscle
activity. We propose an FMG-based method to estimate knee and ankle joint
torques by integrating joint angles and velocities with muscle activity data.
We learn a model for joint torque estimation using Gaussian process regression
(GPR). The effectiveness of the proposed FMG-based method is validated on
isokinetic motions performed by ten participants. The model is compared to a
baseline model that uses only joint angle and velocity, as well as a model
augmented by EMG data. The results indicate that incorporating FMG into
exoskeleton control can improve the estimation of joint torque for the ankle
and knee joints in novel task characteristics within a single participant.
Although the findings suggest that this approach may not improve the
generalizability of estimates between multiple participants, they highlight the
need for further research into its potential applications in exoskeleton
control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This file corresponds to the manuscript presented at the IEEE
  International Conference on Robotics and Automation (ICRA), May 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large VLM-based <span class="highlight-title">Vision-Language-Action</span> Models for <span class="highlight-title">Robot</span>ic <span class="highlight-title">Manipulation</span>:
  A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.13073v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.13073v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation, a key frontier in robotics and embodied AI, requires
precise motor control and multimodal understanding, yet traditional rule-based
methods fail to scale or generalize in unstructured, novel environments. In
recent years, Vision-Language-Action (VLA) models, built upon Large
Vision-Language Models (VLMs) pretrained on vast image-text datasets, have
emerged as a transformative paradigm. This survey provides the first
systematic, taxonomy-oriented review of large VLM-based VLA models for robotic
manipulation. We begin by clearly defining large VLM-based VLA models and
delineating two principal architectural paradigms: (1) monolithic models,
encompassing single-system and dual-system designs with differing levels of
integration; and (2) hierarchical models, which explicitly decouple planning
from execution via interpretable intermediate representations. Building on this
foundation, we present an in-depth examination of large VLM-based VLA models:
(1) integration with advanced domains, including reinforcement learning,
training-free optimization, learning from human videos, and world model
integration; (2) synthesis of distinctive characteristics, consolidating
architectural traits, operational strengths, and the datasets and benchmarks
that support their development; (3) identification of promising directions,
including memory mechanisms, 4D perception, efficient adaptation, multi-agent
cooperation, and other emerging capabilities. This survey consolidates recent
advances to resolve inconsistencies in existing taxonomies, mitigate research
fragmentation, and fill a critical gap through the systematic integration of
studies at the intersection of large VLMs and robotic manipulation. We provide
a regularly updated project page to document ongoing progress:
https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page:
  https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Preference <span class="highlight-title">Optimization</span> for Long-Form Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13919v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13919v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Li, Xiaohan Wang, Yuhui Zhang, Orr Zohar, Zeyu Wang, Serena Yeung-Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in video large multimodal models
(video-LMMs), achieving effective temporal grounding in long-form videos
remains a challenge for existing models. To address this limitation, we propose
Temporal Preference Optimization (TPO), a novel post-training framework
designed to enhance the temporal grounding capabilities of video-LMMs through
preference learning. TPO adopts a self-training approach that enables models to
differentiate between well-grounded and less accurate temporal responses by
leveraging curated preference datasets at two granularities: localized temporal
grounding, which focuses on specific video segments, and comprehensive temporal
grounding, which captures extended temporal dependencies across entire video
sequences. By optimizing on these preference datasets, TPO significantly
enhances temporal understanding while reducing reliance on manually annotated
data. Extensive experiments on three long-form video understanding
benchmarks--LongVideoBench, MLVU, and Video-MME--demonstrate the effectiveness
of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO
establishes itself as the leading 7B model on the Video-MME benchmark,
underscoring the potential of TPO as a scalable and efficient solution for
advancing temporal reasoning in long-form video understanding. Project page:
https://ruili33.github.io/tpo_website.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NarraGuide: an LLM-based Narrative Mobile <span class="highlight-title">Robot</span> for Remote Place
  Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.01235v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.01235v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaxin Hu, Arissa J. Sato, Jingxin Du, Chenming Ye, Anjun Zhu, Pragathi Praveena, Bilge Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic telepresence enables users to navigate and experience remote
environments. However, effective navigation and situational awareness depend on
users' prior knowledge of the environment, limiting the usefulness of these
systems for exploring unfamiliar places. We explore how integrating
location-aware LLM-based narrative capabilities into a mobile robot can support
remote exploration. We developed a prototype system, called NarraGuide, that
provides narrative guidance for users to explore and learn about a remote place
through a dialogue-based interface. We deployed our prototype in a geology
museum, where remote participants (n=20) used the robot to tour the museum. Our
findings reveal how users perceived the robot's role, engaged in dialogue in
the tour, and expressed preferences for bystander encountering. Our work
demonstrates the potential of LLM-enabled robotic capabilities to deliver
location-aware narrative guidance and enrich the experience of exploring remote
environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViTaMIn: <span class="highlight-title">Learning</span> Contact-Rich Tasks Through <span class="highlight-title">Robot</span>-Free Visuo-Tactile
  <span class="highlight-title">Manipulation</span> Interface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06156v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06156v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangchen Liu, Chuanyu Li, Yihua Qin, Jing Xu, Pieter Abbeel, Rui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile information plays a crucial role for humans and robots to interact
effectively with their environment, particularly for tasks requiring the
understanding of contact properties. Solving such dexterous manipulation tasks
often relies on imitation learning from demonstration datasets, which are
typically collected via teleoperation systems and often demand substantial time
and effort. To address these challenges, we present ViTaMIn, an embodiment-free
manipulation interface that seamlessly integrates visual and tactile sensing
into a hand-held gripper, enabling data collection without the need for
teleoperation. Our design employs a compliant Fin Ray gripper with tactile
sensing, allowing operators to perceive force feedback during manipulation for
more intuitive operation. Additionally, we propose a multimodal representation
learning strategy to obtain pre-trained tactile representations, improving data
efficiency and policy robustness. Experiments on seven contact-rich
manipulation tasks demonstrate that ViTaMIn significantly outperforms baseline
methods, demonstrating its effectiveness for complex manipulation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DivScene: Towards <span class="highlight-title">Open-Vocabulary</span> Object <span class="highlight-title">Navigation</span> with Large <span class="highlight-title">Vision</span>
  Language Models in Diverse Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02730v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02730v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaowei Wang, Hongming Zhang, Tianqing Fang, Ye Tian, Yue Yang, Kaixin Ma, Xiaoman Pan, Yangqiu Song, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have achieved significant progress in
tasks like visual question answering and document understanding. However, their
potential to comprehend embodied environments and navigate within them remains
underexplored. In this work, we first study the challenge of open-vocabulary
object navigation by introducing DivScene, a large-scale dataset with 4,614
houses across 81 scene types and 5,707 kinds of target objects. Our dataset
provides a much greater diversity of target objects and scene types than
existing datasets, enabling a comprehensive task evaluation. We evaluated
various methods with LVLMs and LLMs on our dataset and found that current
models still fall short of open-vocab object navigation ability. Then, we
fine-tuned LVLMs to predict the next action with CoT explanations. We observe
that LVLM's navigation ability can be improved substantially with only
BFS-generated shortest paths without any human supervision, surpassing GPT-4o
by over 20% in success rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nav-SCOPE: Swarm <span class="highlight-title">Robot</span> Cooperative Perception and <span class="highlight-title">Coordinated</span> <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10049v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10049v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxi Li, Weining Lu, Qingquan Lin, Litong Meng, Haolu Li, Bin Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a lightweight systematic solution for multi-robot
coordinated navigation with decentralized cooperative perception. An
information flow is first created to facilitate real-time observation sharing
over unreliable ad-hoc networks. Then, the environmental uncertainties of each
robot are reduced by interaction fields that deliver complementary information.
Finally, path optimization is achieved, enabling self-organized coordination
with effective convergence, divergence, and collision avoidance. Our method is
fully interpretable and ready for deployment without gaps. Comprehensive
simulations and real-world experiments demonstrate reduced path redundancy,
robust performance across various tasks, and minimal demands on computation and
communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures, accepted in IEEE Transactions on Automation
  Science and Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmbodiedOne<span class="highlight-title">Vision</span>: Interleaved <span class="highlight-title">Vision</span>-Text-Action Pretraining for
  General <span class="highlight-title">Robot</span> Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21112v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21112v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, Dong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human ability to seamlessly perform multimodal reasoning and physical
interaction in the open world is a core goal for general-purpose embodied
intelligent systems. Recent vision-language-action (VLA) models, which are
co-trained on large-scale robot and visual-text data, have demonstrated notable
progress in general robot control. However, they still fail to achieve
human-level flexibility in interleaved reasoning and interaction. In this work,
introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is
a unified embodied foundation model that achieves superior performance in
multimodal embodied reasoning and robot control through interleaved
vision-text-action pre-training. The development of EO-1 is based on two key
pillars: (i) a unified architecture that processes multimodal inputs
indiscriminately (image, text, video, and action), and (ii) a massive,
high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains
over 1.5 million samples with emphasis on interleaved vision-text-action
comprehension. EO-1 is trained through synergies between auto-regressive
decoding and flow matching denoising on EO-Data1.5M, enabling seamless robot
action generation and multimodal embodied reasoning. Extensive experiments
demonstrate the effectiveness of interleaved vision-text-action learning for
open-world understanding and generalization, validated through a variety of
long-horizon, dexterous manipulation tasks across multiple embodiments. This
paper details the architecture of EO-1, the data construction strategy of
EO-Data1.5M, and the training methodology, offering valuable insights for
developing advanced embodied foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-08-31T00:00:00Z">2025-08-31</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolic <span class="highlight-title">Planning</span> and Multi-Agent Path Finding in Extremely Dense
  Environments with Movable Obstacles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Fu, Zhe Chen, Rahul Chandan, Alex Barbosa, Michael Caldara, Joey Durham, Federico Pecora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Block Rearrangement Problem (BRaP), a challenging component
of large warehouse management which involves rearranging storage blocks within
dense grids to achieve a target state. We formally define the BRaP as a graph
search problem. Building on intuitions from sliding puzzle problems, we propose
five search-based solution algorithms, leveraging joint configuration space
search, classical planning, multi-agent pathfinding, and expert heuristics. We
evaluate the five approaches empirically for plan quality and scalability.
Despite the exponential relation between search space size and block number,
our methods demonstrate efficiency in creating rearrangement plans for deeply
buried blocks in up to 80x80 grids.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-driven Dispensing of Coral Reseeding Devices for Broad-scale
  Restoration of the Great Barrier Reef 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01019v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01019v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scarlett Raine, Benjamin Moshirian, Tobias Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coral reefs are on the brink of collapse, with climate change, ocean
acidification, and pollution leading to a projected 70-90% loss of coral
species within the next decade. Restoration efforts are crucial, but their
success hinges on introducing automation to upscale efforts. We present
automated deployment of coral re-seeding devices powered by artificial
intelligence, computer vision, and robotics. Specifically, we perform automated
substrate classification, enabling detection of areas of the seafloor suitable
for coral growth, thus significantly reducing reliance on human experts and
increasing the range and efficiency of restoration. Real-world testing of the
algorithms on the Great Barrier Reef leads to deployment accuracy of 77.8%,
sub-image patch classification of 89.1%, and real-time model inference at 5.5
frames per second. Further, we present and publicly contribute a large
collection of annotated substrate image data to foster future research in this
area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Robust</span> Numerical Method for Solving Trigonometric Equations in <span class="highlight-title">Robot</span>ic
  Kinematics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.01010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.01010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hai-Jun Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a robust numerical method for solving systems of
trigonometric equations commonly encountered in robotic kinematics. Our
approach employs polynomial substitution techniques combined with eigenvalue
decomposition to handle singular matrices and edge cases effectively. The
method demonstrates superior numerical stability compared to traditional
approaches and has been implemented as an open-source Python package. For
non-singular matrices, we employ Weierstrass substitution to transform the
system into a quartic polynomial, ensuring all analytical solutions are found.
For singular matrices, we develop specialized geometric constraint methods
using SVD analysis. The solver demonstrates machine precision accuracy ($<
10^{-15}$ error) with 100\% success rate on extensive test cases, making it
particularly valuable for robotics applications such as inverse kinematics
problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Mean Field Game for Interactive Decision-Making with Varied
  Stylish Multi-Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liancheng Zheng, Zhen Tian, Yangfan He, Shuo Liu, Ke Gong, Huilin Chen, Zhihao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an MFG-based decision-making framework for autonomous
driving in heterogeneous traffic. To capture diverse human behaviors, we
propose a quantitative driving style representation that maps abstract traits
to parameters such as speed, safety factors, and reaction time. These
parameters are embedded into the MFG through a spatial influence field model.
To ensure safe operation in dense traffic, we introduce a safety-critical
lane-changing algorithm that leverages dynamic safety margins,
time-to-collision analysis, and multi-layered constraints. Real-world NGSIM
data is employed for style calibration and empirical validation. Experimental
results demonstrate zero collisions across six style combinations, two
15-vehicle scenarios, and NGSIM-based trials, consistently outperforming
conventional game-theoretic baselines. Overall, our approach provides a
scalable, interpretable, and behavior-aware planning framework for real-world
autonomous driving applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Step Model Predictive Path Integral for Manipulator Motion <span class="highlight-title">Planning</span>
  Using Configuration Space Distance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Li, Tetsuro Miyazaki, Kenji Kawashima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion planning for robotic manipulators is a fundamental problem in
robotics. Classical optimization-based methods typically rely on the gradients
of signed distance fields (SDFs) to impose collision-avoidance constraints.
However, these methods are susceptible to local minima and may fail when the
SDF gradients vanish. Recently, Configuration Space Distance Fields (CDFs) have
been introduced, which directly model distances in the robot's configuration
space. Unlike workspace SDFs, CDFs are differentiable almost everywhere and
thus provide reliable gradient information. On the other hand, gradient-free
approaches such as Model Predictive Path Integral (MPPI) control leverage
long-horizon rollouts to achieve collision avoidance. While effective, these
methods are computationally expensive due to the large number of trajectory
samples, repeated collision checks, and the difficulty of designing cost
functions with heterogeneous physical units. In this paper, we propose a
framework that integrates CDFs with MPPI to enable direct navigation in the
robot's configuration space. Leveraging CDF gradients, we unify the MPPI cost
in joint-space and reduce the horizon to one step, substantially cutting
computation while preserving collision avoidance in practice. We demonstrate
that our approach achieves nearly 100% success rates in 2D environments and
consistently high success rates in challenging 7-DOF Franka manipulator
simulations with complex obstacles. Furthermore, our method attains control
frequencies exceeding 750 Hz, substantially outperforming both
optimization-based and standard MPPI baselines. These results highlight the
effectiveness and efficiency of the proposed CDF-MPPI framework for
high-dimensional motion planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Effective Trajectory <span class="highlight-title">Planning</span> and an Optimized Path <span class="highlight-title">Planning</span> for a
  6-Degree-of-Freedom <span class="highlight-title">Robot</span> Manipulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takumu Okazaki, Akira Terui, Masahiko Mikawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An effective method for optimizing path planning for a specific model of a
6-degree-of-freedom (6-DOF) robot manipulator is presented as part of the
motion planning of the manipulator using computer algebra. We assume that we
are given a path in the form of a set of line segments that the end-effector
should follow. We also assume that we have a method to solve the inverse
kinematic problem of the manipulator at each via-point of the trajectory. The
proposed method consists of three steps. First, we calculate the feasible
region of the manipulator under a specific configuration of the end-effector.
Next, we aim to find a trajectory on the line segments and a sequence of joint
configurations the manipulator should follow to move the end-effector along the
specified trajectory. Finally, we find the optimal combination of solutions to
the inverse kinematic problem at each via-point along the trajectory by
reducing the problem to a shortest-path problem of the graph and applying
Dijkstra's algorithm. We show the effectiveness of the proposed method by
experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inverse Kinematics for a 6-Degree-of-Freedom <span class="highlight-title">Robot</span> Manipulator Using
  Comprehensive Gröbner Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takumu Okazaki, Akira Terui, Masahiko Mikawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an effective method for solving the inverse kinematic problem of a
specific model of 6-degree-of-freedom (6-DOF) robot manipulator using computer
algebra. It is known that when the rotation axes of three consecutive
rotational joints of a manipulator intersect at a single point, the inverse
kinematics problem can be divided into determining position and orientation. We
extend this method to more general manipulators in which the rotational axes of
two consecutive joints intersect. This extension broadens the class of 6-DOF
manipulators for which the inverse kinematics problem can be solved, and is
expected to enable more efficient solutions. The inverse kinematic problem is
solved using the Comprehensive Gr\"obner System (CGS) with joint parameters of
the robot appearing as parameters in the coefficients to prevent repetitive
calculations of the Gr\"obner bases. The effectiveness of the proposed method
is shown by experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DyPho-<span class="highlight-title">SLAM</span> : Real-time Photorealistic <span class="highlight-title">SLAM</span> in <span class="highlight-title">Dynamic</span> Environments <span class="chip">ICME 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Liu, Keyu Fan, Bin Lan, Houde Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual SLAM algorithms have been enhanced through the exploration of Gaussian
Splatting representations, particularly in generating high-fidelity dense maps.
While existing methods perform reliably in static environments, they often
encounter camera tracking drift and fuzzy mapping when dealing with the
disturbances caused by moving objects. This paper presents DyPho-SLAM, a
real-time, resource-efficient visual SLAM system designed to address the
challenges of localization and photorealistic mapping in environments with
dynamic objects. Specifically, the proposed system integrates prior image
information to generate refined masks, effectively minimizing noise from mask
misjudgment. Additionally, to enhance constraints for optimization after
removing dynamic obstacles, we devise adaptive feature extraction strategies
significantly improving the system's resilience. Experiments conducted on
publicly dynamic RGB-D datasets demonstrate that the proposed system achieves
state-of-the-art performance in camera pose estimation and dense map
reconstruction, while operating in real-time in dynamic scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICME 2025(Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ER-LoRA: Effective-Rank Guided Adaptation for Weather-Generalized Depth
  <span class="highlight-title">Estimation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilong Yan, Xin Zhang, Robby T. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monocular depth estimation under adverse weather conditions (e.g.\ rain, fog,
snow, and nighttime) remains highly challenging due to the lack of reliable
ground truth and the difficulty of learning from unlabeled real-world data.
Existing methods often rely on synthetic adverse data with pseudo-labels, which
suffer from domain gaps, or employ self-supervised learning, which violates
photometric assumptions in adverse scenarios. In this work, we propose to
achieve weather-generalized depth estimation by Parameter-Efficient Fine-Tuning
(PEFT) of Vision Foundation Models (VFMs), using only a small amount of
high-visibility (normal) data. While PEFT has shown strong performance in
semantic tasks such as segmentation, it remains underexplored for geometry --
centric tasks like depth estimation -- especially in terms of balancing
effective adaptation with the preservation of pretrained knowledge. To this
end, we introduce the Selecting-Tuning-Maintaining (STM) strategy, which
structurally decomposes the pretrained weights of VFMs based on two kinds of
effective ranks (entropy-rank and stable-rank). In the tuning phase, we
adaptively select the proper rank number as well as the task-aware singular
directions for initialization, based on the entropy-rank and full-tuned weight;
while in the maintaining stage, we enforce a principal direction regularization
based on the stable-rank. This design guarantees flexible task adaptation while
preserving the strong generalization capability of the pretrained VFM.
Extensive experiments on four real-world benchmarks across diverse weather
conditions demonstrate that STM not only outperforms existing PEFT methods and
full fine-tuning but also surpasses methods trained with adverse synthetic
data, and even the depth foundation model
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CARIS: A Context-Adaptable <span class="highlight-title">Robot</span> Interface System for Personalized and
  Scalable Human-<span class="highlight-title">Robot</span> Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felipe Arias-Russi, Yuanchen Bai, Angelique Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The human-robot interaction (HRI) field has traditionally used Wizard-of-Oz
(WoZ) controlled robots to explore navigation, conversational dynamics,
human-in-the-loop interactions, and more to explore appropriate robot behaviors
in everyday settings. However, existing WoZ tools are often limited to one
context, making them less adaptable across different settings, users, and
robotic platforms. To mitigate these issues, we introduce a Context-Adaptable
Robot Interface System (CARIS) that combines advanced robotic capabilities such
teleoperation, human perception, human-robot dialogue, and multimodal data
recording. Through pilot studies, we demonstrate the potential of CARIS to WoZ
control a robot in two contexts: 1) mental health companion and as a 2) tour
guide. Furthermore, we identified areas of improvement for CARIS, including
smoother integration between movement and communication, clearer functionality
separation, recommended prompts, and one-click communication options to enhance
the usability wizard control of CARIS. This project offers a publicly
available, context-adaptable tool for the HRI community, enabling researchers
to streamline data-driven approaches to intelligent robot behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MV-SSM: Multi-View State Space Modeling for 3D Human Pose <span class="highlight-title">Estimation</span> <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aviral Chharia, Wenbo Gou, Haoye Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While significant progress has been made in single-view 3D human pose
estimation, multi-view 3D human pose estimation remains challenging,
particularly in terms of generalizing to new camera configurations. Existing
attention-based transformers often struggle to accurately model the spatial
arrangement of keypoints, especially in occluded scenarios. Additionally, they
tend to overfit specific camera arrangements and visual scenes from training
data, resulting in substantial performance drops in new settings. In this
study, we introduce a novel Multi-View State Space Modeling framework, named
MV-SSM, for robustly estimating 3D human keypoints. We explicitly model the
joint spatial sequence at two distinct levels: the feature level from
multi-view images and the person keypoint level. We propose a Projective State
Space (PSS) block to learn a generalized representation of joint spatial
arrangements using state space modeling. Moreover, we modify Mamba's
traditional scanning into an effective Grid Token-guided Bidirectional Scanning
(GTBS), which is integral to the PSS block. Multiple experiments demonstrate
that MV-SSM achieves strong generalization, outperforming state-of-the-art
methods: +10.8 on AP25 (+24%) on the challenging three-camera setting in CMU
Panoptic, +7.0 on AP25 (+13%) on varying camera arrangements, and +15.3 PCP
(+38%) on Campus A1 in cross-dataset evaluations. Project Website:
https://aviralchharia.github.io/MV-SSM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025; Project Website: https://aviralchharia.github.io/MV-SSM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Risk-aware Spatial-temporal Trajectory <span class="highlight-title">Planning</span> Framework for
  Autonomous Vehicles Using QP-<span class="highlight-title">MPC</span> and <span class="highlight-title">Dynamic</span> Hazard Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tian, Zhihao Lin, Dezong Zhao, Christos Anagnostopoulos, Qiyuan Wang, Wenjing Zhao, Xiaodan Wang, Chongfeng Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory planning is a critical component in ensuring the safety,
stability, and efficiency of autonomous vehicles. While existing trajectory
planning methods have achieved progress, they often suffer from high
computational costs, unstable performance in dynamic environments, and limited
validation across diverse scenarios. To overcome these challenges, we propose
an enhanced QP-MPC-based framework that incorporates three key innovations: (i)
a novel cost function designed with a dynamic hazard field, which explicitly
balances safety, efficiency, and comfort; (ii) seamless integration of this
cost function into the QP-MPC formulation, enabling direct optimization of
desired driving behaviors; and (iii) extensive validation of the proposed
framework across complex tasks. The spatial safe planning is guided by a
dynamic hazard field (DHF) for risk assessment, while temporal safe planning is
based on a space-time graph. Besides, the quintic polynomial sampling and
sub-reward of comforts are used to ensure comforts during lane-changing. The
sub-reward of efficiency is used to maintain driving efficiency. Finally, the
proposed DHF-enhanced objective function integrates multiple objectives,
providing a proper optimization tasks for QP-MPC. Extensive simulations
demonstrate that the proposed framework outperforms benchmark optimization
methods in terms of efficiency, stability, and comfort across a variety of
scenarios likes lane-changing, overtaking, and crossing intersections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Making Physical Objects with <span class="highlight-title">Generative</span> AI and <span class="highlight-title">Robot</span>ic Assembly:
  Considering Fabrication Constraints, Sustainability, Time, Functionality, and
  Accessibility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.19131v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.19131v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Htet Kyaw, Se Hwan Jeon, Miana Smith, Neil Gershenfeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D generative AI enables rapid and accessible creation of 3D models from text
or image inputs. However, translating these outputs into physical objects
remains a challenge due to the constraints in the physical world. Recent
studies have focused on improving the capabilities of 3D generative AI to
produce fabricable outputs, with 3D printing as the main fabrication method.
However, this workshop paper calls for a broader perspective by considering how
fabrication methods align with the capabilities of 3D generative AI. As a case
study, we present a novel system using discrete robotic assembly and 3D
generative AI to make physical objects. Through this work, we identified five
key aspects to consider in a physical making process based on the capabilities
of 3D generative AI. 1) Fabrication Constraints: Current text-to-3D models can
generate a wide range of 3D designs, requiring fabrication methods that can
adapt to the variability of generative AI outputs. 2) Time: While generative AI
can generate 3D models in seconds, fabricating physical objects can take hours
or even days. Faster production could enable a closer iterative design loop
between humans and AI in the making process. 3) Sustainability: Although
text-to-3D models can generate thousands of models in the digital world,
extending this capability to the real world would be resource-intensive,
unsustainable and irresponsible. 4) Functionality: Unlike digital outputs from
3D generative AI models, the fabrication method plays a crucial role in the
usability of physical objects. 5) Accessibility: While generative AI simplifies
3D model creation, the need for fabrication equipment can limit participation,
making AI-assisted creation less inclusive. These five key aspects provide a
framework for assessing how well a physical making process aligns with the
capabilities of 3D generative AI and values in the world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACM CHI Conference on Human Factors in Computing Systems (CHI 2025),
  Workshop on Generative AI and Human-Computer Interaction, Yokohama, Japan,
  April 26 to May 1, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Computational Design and Fabrication of Modular <span class="highlight-title">Robots</span> with Untethered
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.05410v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.05410v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manas Bhargava, Takefumi Hiraki, Malina Strugaru, Yuhan Zhang, Michal Piovarci, Chiara Daraio, Daisuke Iwai, Bernd Bickel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural organisms utilize distributed actuation through their musculoskeletal
systems to adapt their gait for traversing diverse terrains or to morph their
bodies for varied tasks. A longstanding challenge in robotics is to emulate
this capability of natural organisms, which has motivated the development of
numerous soft robotic systems. However, such systems are generally optimized
for a single functionality, lack the ability to change form or function on
demand, or remain tethered to bulky control systems. To address these
limitations, we present a framework for designing and controlling robots that
utilize distributed actuation. We propose a novel building block that
integrates 3D-printed bones with liquid crystal elastomer (LCE) muscles as
lightweight actuators, enabling the modular assembly of musculoskeletal robots.
We developed LCE rods that contract in response to infrared radiation, thereby
providing localized, untethered control over the distributed skeletal network
and producing global deformations of the robot. To fully capitalize on the
extensive design space, we introduce two computational tools: one for
optimizing the robot's skeletal graph to achieve multiple target deformations,
and another for co-optimizing skeletal designs and control gaits to realize
desired locomotion. We validate our framework by constructing several robots
that demonstrate complex shape morphing, diverse control schemes, and
environmental adaptability. Our system integrates advances in modular material
building, untethered and distributed control, and computational design to
introduce a new generation of robots that brings us closer to the capabilities
of living organisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Online <span class="highlight-title">Learning</span> and Adaptive <span class="highlight-title">Planning</span> for <span class="highlight-title">Robot</span>ic Information
  Gathering Based on Streaming Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.13053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.13053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjeev Ramkumar Sudha, Joel Jose, Erlend M. Coates
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic information gathering (RIG) techniques refer to methods where mobile
robots are used to acquire data about the physical environment with a suite of
sensors. Informative planning is an important part of RIG where the goal is to
find sequences of actions or paths that maximize efficiency or the quality of
information collected. Many existing solutions solve this problem by assuming
that the environment is known in advance. However, real environments could be
unknown or time-varying, and adaptive informative planning remains an active
area of research. Adaptive planning and incremental online mapping are required
for mapping initially unknown or varying spatial fields. Gaussian process (GP)
regression is a widely used technique in RIG for mapping continuous spatial
fields. However, it falls short in many applications as its real-time
performance does not scale well to large datasets. To address these challenges,
this paper proposes an efficient adaptive informative planning approach for
mapping continuous scalar fields with GPs with streaming sparse GPs. Simulation
experiments are performed with a synthetic dataset and compared against
existing benchmarks. Finally, it is also verified with a real-world dataset to
further validate the efficacy of the proposed method. Results show that our
method achieves similar mapping accuracy to the baselines while reducing
computational complexity for longer missions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at 2025 European Conference on Mobile
  Robots</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safety-Critical Human-Machine Shared <span class="highlight-title">Driving</span> for Vehicle Collision
  Avoidance based on Hamilton-Jacobi reachability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10610v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10610v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiyue Zhao, Junzhi Zhang, Rui Zhou, Neda Masoud, Jianxiong Li, Helai Huang, Shijie Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Road safety continues to be a pressing global issue, with vehicle collisions
imposing significant human, societal, and economic burdens. Human-machine
shared collision avoidance in critical collision scenarios aims to aid drivers'
accident avoidance through intervening only when necessary. Existing methods
count on replanning collision-free trajectories and imposing human-machine
tracking, which usually interrupts the driver's intent and increases the risk
of conflict. This paper introduces a Reachability-Aware Reinforcement Learning
(RL) framework for shared control, guided by Hamilton-Jacobi (HJ) reachability
analysis. Machine intervention is activated only when the vehicle approaches
the Collision Avoidance Reachable Set (CARS), which represents states where
collision is unavoidable. First, we precompute the reachability distributions
and the CARS by solving the Bellman equation using offline data. To reduce
human-machine conflicts, we develop a driver model for sudden obstacles and
propose an authority allocation strategy considering key collision avoidance
features. Finally, we train a RL agent to reduce human-machine conflicts while
enforcing the hard constraint of avoiding entry into the CARS. The proposed
method was tested on a real vehicle platform. Results show that the controller
intervenes effectively near CARS to prevent collisions while maintaining
improved original driving task performance. Robustness analysis further
supports its flexibility across different driver attributes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unscented Kalman Filter with a Nonlinear Propagation Model for
  <span class="highlight-title">Navigation</span> Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.10082v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.10082v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Levy, Itzik Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The unscented Kalman filter is a nonlinear estimation algorithm commonly used
in navigation applications. The prediction of the mean and covariance matrix is
crucial to the stable behavior of the filter. This prediction is done by
propagating the sigma points according to the dynamic model at hand. In this
paper, we introduce an innovative method to propagate the sigma points
according to the nonlinear dynamic model of the navigation error state vector.
This improves the filter accuracy and navigation performance. We demonstrate
the benefits of our proposed approach using real sensor data recorded by an
autonomous underwater vehicle during several scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ YORI: Autonomous Cooking System Utilizing a Modular <span class="highlight-title">Robot</span>ic Kitchen and
  a Dual-Arm Proprioceptive Manipulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.11094v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.11094v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghun Noh, Hyunwoo Nam, Kyle Gillespie, Yeting Liu, Dennis Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Yummy Operations Robot Initiative (YORI), a
proprioceptive dual-arm robotic system that demonstrates autonomous multi-dish
cooking for scalable food service applications. YORI integrates a dual-arm
manipulator equipped with proprioceptive actuators, custom-designed tools,
appliances, and a structured kitchen environment to address the complexities of
cooking tasks. The proprioceptive actuators enable fast, precise,
force-controlled movements while mitigating the risks associated with
cooking-related impacts. The system's modular kitchen design and flexible
tool-changing mechanism support simultaneous multi-dish preparation through
torque control and optimization-based motion planning and scheduling. A
comprehensive scheduling framework with dynamic rescheduling ensures reliable
adaptation to new orders and delays. The system was publicly validated through
live demonstrations, reliably preparing steak-frites across multiple convention
sessions. This paper details YORI's design and explores future directions in
kitchen optimization, task planning, and food quality control, demonstrating
its potential as a scalable robotic cooking solution. A system introduction and
cooking videos are available online
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to IEEE Robotics & Automation Magazine
  for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on <span class="highlight-title">Vision-Language-Action</span> Models for Embodied AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14093v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14093v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied AI is widely recognized as a key element of artificial general
intelligence because it involves controlling embodied agents to perform tasks
in the physical world. Building on the success of large language models and
vision-language models, a new category of multimodal models -- referred to as
vision-language-action models (VLAs) -- has emerged to address
language-conditioned robotic tasks in embodied AI by leveraging their distinct
ability to generate actions. In recent years, a myriad of VLAs have been
developed, making it imperative to capture the rapidly evolving landscape
through a comprehensive survey. To this end, we present the first survey on
VLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized
into three major lines of research. The first line focuses on individual
components of VLAs. The second line is dedicated to developing control policies
adept at predicting low-level actions. The third line comprises high-level task
planners capable of decomposing long-horizon tasks into a sequence of subtasks,
thereby guiding VLAs to follow more general user instructions. Furthermore, we
provide an extensive summary of relevant resources, including datasets,
simulators, and benchmarks. Finally, we discuss the challenges faced by VLAs
and outline promising future directions in embodied AI. We have created a
project associated with this survey, which is available at
https://github.com/yueen-ma/Awesome-VLA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/yueen-ma/Awesome-VLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> HERMES: Human-to-<span class="highlight-title">Robot</span> Embodied <span class="highlight-title">Learning</span> from Multi-Source Motion Data
  for Mobile Dexterous <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20085v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20085v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhecheng Yuan, Tianming Wei, Langzhe Gu, Pu Hua, Tianhai Liang, Yuanpei Chen, <span class="highlight-author">Huazhe Xu</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging human motion data to impart robots with versatile manipulation
skills has emerged as a promising paradigm in robotic manipulation.
Nevertheless, translating multi-source human hand motions into feasible robot
behaviors remains challenging, particularly for robots equipped with
multi-fingered dexterous hands characterized by complex, high-dimensional
action spaces. Moreover, existing approaches often struggle to produce policies
capable of adapting to diverse environmental conditions. In this paper, we
introduce HERMES, a human-to-robot learning framework for mobile bimanual
dexterous manipulation. First, HERMES formulates a unified reinforcement
learning approach capable of seamlessly transforming heterogeneous human hand
motions from multiple sources into physically plausible robotic behaviors.
Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depth
image-based sim2real transfer method for improved generalization to real-world
scenarios. Furthermore, to enable autonomous operation in varied and
unstructured environments, we augment the navigation foundation model with a
closed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise
alignment of visual goals and effectively bridging autonomous navigation and
dexterous manipulation. Extensive experimental results demonstrate that HERMES
consistently exhibits generalizable behaviors across diverse, in-the-wild
scenarios, successfully performing numerous complex mobile bimanual dexterous
manipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Learning</span> from 10 Demos: Generalisable and Sample-Efficient Policy
  <span class="highlight-title">Learning</span> with Oriented Affordance Frames <span class="chip">CoRL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12124v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12124v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krishan Rana, Jad Abou-Chakra, Sourav Garg, Robert Lee, Ian Reid, Niko Suenderhauf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning has unlocked the potential for robots to exhibit highly
dexterous behaviours. However, it still struggles with long-horizon,
multi-object tasks due to poor sample efficiency and limited generalisation.
Existing methods require a substantial number of demonstrations to cover
possible task variations, making them costly and often impractical for
real-world deployment. We address this challenge by introducing oriented
affordance frames, a structured representation for state and action spaces that
improves spatial and intra-category generalisation and enables policies to be
learned efficiently from only 10 demonstrations. More importantly, we show how
this abstraction allows for compositional generalisation of independently
trained sub-policies to solve long-horizon, multi-object tasks. To seamlessly
transition between sub-policies, we introduce the notion of self-progress
prediction, which we directly derive from the duration of the training
demonstrations. We validate our method across three real-world tasks, each
requiring multi-step, multi-object interactions. Despite the small dataset, our
policies generalise robustly to unseen object appearances, geometries, and
spatial arrangements, achieving high success rates without reliance on
exhaustive training data. Video demonstration can be found on our project page:
https://affordance-policy.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the Conference on Robot Learning (CoRL), 2025. Videos can
  be found on our project website: https://affordance-policy.github.io</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-08-30T00:00:00Z">2025-08-30</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">31</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vehicle-in-Virtual-Environment (VVE) Method for Developing and
  Evaluating VRU Safety of Connected and Autonomous <span class="highlight-title">Driving</span> with Focus on
  Bicyclist Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochong Chen, Xincheng Cao, Bilin Aksun-Guvenc, Levent Guvenc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extensive research has already been conducted in the autonomous driving field
to help vehicles navigate safely and efficiently. At the same time, plenty of
current research on vulnerable road user (VRU) safety is performed which
largely concentrates on perception, localization, or trajectory prediction of
VRUs. However, existing research still exhibits several gaps, including the
lack of a unified planning and collision avoidance system for autonomous
vehicles, limited investigation into delay tolerant control strategies, and the
absence of an efficient and standardized testing methodology. Ensuring VRU
safety remains one of the most pressing challenges in autonomous driving,
particularly in dynamic and unpredictable environments. In this two year
project, we focused on applying the Vehicle in Virtual Environment (VVE) method
to develop, evaluate, and demonstrate safety functions for Vulnerable Road
Users (VRUs) using automated steering and braking of ADS. In this current
second year project report, our primary focus was on enhancing the previous
year results while also considering bicyclist safety.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe and Efficient Lane-Changing for Autonomous Vehicles: An Improved
  Double Quintic Polynomial Approach with Time-to-Collision Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Bai, Rui Xu, Teng Rui, Jiale Liu, Qi Wei Oung, Hoi Leong Lee, Zhen Tian, Fujiang Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving technology has made significant advancements in recent
years, yet challenges remain in ensuring safe and comfortable interactions with
human-driven vehicles (HDVs), particularly during lane-changing maneuvers. This
paper proposes an improved double quintic polynomial approach for safe and
efficient lane-changing in mixed traffic environments. The proposed method
integrates a time-to-collision (TTC) based evaluation mechanism directly into
the trajectory optimization process, ensuring that the ego vehicle proactively
maintains a safe gap from surrounding HDVs throughout the maneuver. The
framework comprises state estimation for both the autonomous vehicle (AV) and
HDVs, trajectory generation using double quintic polynomials, real-time TTC
computation, and adaptive trajectory evaluation. To the best of our knowledge,
this is the first work to embed an analytic TTC penalty directly into the
closed-form double-quintic polynomial solver, enabling real-time safety-aware
trajectory generation without post-hoc validation. Extensive simulations
conducted under diverse traffic scenarios demonstrate the safety, efficiency,
and comfort of the proposed approach compared to conventional methods such as
quintic polynomials, Bezier curves, and B-splines. The results highlight that
the improved method not only avoids collisions but also ensures smooth
transitions and adaptive decision-making in dynamic environments. This work
bridges the gap between model-based and adaptive trajectory planning
approaches, offering a stable solution for real-world autonomous driving
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Galaxea Open-World <span class="highlight-title">Dataset</span> and G0 Dual-System <span class="highlight-title">VLA</span> Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Jiang, Tianyuan Yuan, Yicheng Liu, Chenhao Lu, Jianning Cui, Xiao Liu, Shuiqi Cheng, Ji<span class="highlight-author">yang Gao</span>, <span class="highlight-author">Huazhe Xu</span>, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Galaxea Open-World Dataset, a large-scale, diverse collection of
robot behaviors recorded in authentic human living and working environments.
All demonstrations are gathered using a consistent robotic embodiment, paired
with precise subtask-level language annotations to facilitate both training and
evaluation. Building on this dataset, we introduce G0, a dual-system framework
that couples a Vision-Language Model (VLM) for multimodal planning with a
Vision-Language-Action (VLA) model for fine-grained execution. G0 is trained
using a three-stage curriculum: cross-embodiment pre-training,
single-embodiment pre-training, and task-specific post-training. A
comprehensive benchmark spanning tabletop manipulation, few-shot learning, and
long-horizon mobile manipulation, demonstrates the effectiveness of our
approach. In particular, we find that the single-embodiment pre-training stage,
together with the Galaxea Open-World Dataset, plays a critical role in
achieving strong performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://opengalaxea.github.io/G0/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Learning</span> Dolly-In Filming From Demonstration Using a Ground-Based <span class="highlight-title">Robot</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Lorimer, Alan Hunter, Wenbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cinematic camera control demands a balance of precision and artistry -
qualities that are difficult to encode through handcrafted reward functions.
While reinforcement learning (RL) has been applied to robotic filmmaking, its
reliance on bespoke rewards and extensive tuning limits creative usability. We
propose a Learning from Demonstration (LfD) approach using Generative
Adversarial Imitation Learning (GAIL) to automate dolly-in shots with a
free-roaming, ground-based filming robot. Expert trajectories are collected via
joystick teleoperation in simulation, capturing smooth, expressive motion
without explicit objective design.
  Trained exclusively on these demonstrations, our GAIL policy outperforms a
PPO baseline in simulation, achieving higher rewards, faster convergence, and
lower variance. Crucially, it transfers directly to a real-world robot without
fine-tuning, achieving more consistent framing and subject alignment than a
prior TD3-based method. These results show that LfD offers a robust,
reward-free alternative to RL in cinematic domains, enabling real-time
deployment with minimal technical effort. Our pipeline brings intuitive,
stylized camera control within reach of creative professionals, bridging the
gap between artistic intent and robotic autonomy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; under double-anonymous review. 6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gray-Box Computed Torque Control for Differential-Drive Mobile <span class="highlight-title">Robot</span>
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arman Javan Sekhavat Pishkhani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents a learning-based nonlinear algorithm for tracking control
of differential-drive mobile robots. The Computed Torque Method (CTM) suffers
from inaccurate knowledge of system parameters, while Deep Reinforcement
Learning (DRL) algorithms are known for sample inefficiency and weak stability
guarantees. The proposed method replaces the black-box policy network of a DRL
agent with a gray-box Computed Torque Controller (CTC) to improve sample
efficiency and ensure closed-loop stability. This approach enables finding an
optimal set of controller parameters for an arbitrary reward function using
only a few short learning episodes. The Twin-Delayed Deep Deterministic Policy
Gradient (TD3) algorithm is used for this purpose. Additionally, some
controller parameters are constrained to lie within known value ranges,
ensuring the RL agent learns physically plausible values. A technique is also
applied to enforce a critically damped closed-loop time response. The
controller's performance is evaluated on a differential-drive mobile robot
simulated in the MuJoCo physics engine and compared against the raw CTC and a
conventional kinematic controller.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConceptBot: Enhancing <span class="highlight-title">Robot</span>'s Autonomy through Task Decomposition with
  Large Language Models and Knowledge Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Leanza, Angelo Moroncelli, Giuseppe Vizzari, Francesco Braghin, Loris Roveda, Blerina Spahiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ConceptBot is a modular robotic planning framework that combines Large
Language Models and Knowledge Graphs to generate feasible and risk-aware plans
despite ambiguities in natural language instructions and correctly analyzing
the objects present in the environment - challenges that typically arise from a
lack of commonsense reasoning. To do that, ConceptBot integrates (i) an Object
Property Extraction (OPE) module that enriches scene understanding with
semantic concepts from ConceptNet, (ii) a User Request Processing (URP) module
that disambiguates and structures instructions, and (iii) a Planner that
generates context-aware, feasible pick-and-place policies. In comparative
evaluations against Google SayCan, ConceptBot achieved 100% success on explicit
tasks, maintained 87% accuracy on implicit tasks (versus 31% for SayCan),
reached 76% on risk-aware tasks (versus 15%), and outperformed SayCan in
application-specific scenarios, including material classification (70% vs. 20%)
and toxicity detection (86% vs. 36%). On SafeAgentBench, ConceptBot achieved an
overall score of 80% (versus 46% for the next-best baseline). These results,
validated in both simulation and laboratory experiments, demonstrate
ConceptBot's ability to generalize without domain-specific training and to
significantly improve the reliability of robotic policies in unstructured
environments. Website: https://sites.google.com/view/conceptbot
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement <span class="highlight-title">Learning</span> of Dolly-In Filming Using a Ground-Based <span class="highlight-title">Robot</span> <span class="chip">IROS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Lorimer, Jack Saunders, Alan Hunter, Wenbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Free-roaming dollies enhance filmmaking with dynamic movement, but challenges
in automated camera control remain unresolved. Our study advances this field by
applying Reinforcement Learning (RL) to automate dolly-in shots using
free-roaming ground-based filming robots, overcoming traditional control
hurdles. We demonstrate the effectiveness of combined control for precise film
tasks by comparing it to independent control strategies. Our robust RL pipeline
surpasses traditional Proportional-Derivative controller performance in
simulation and proves its efficacy in real-world tests on a modified ROSBot 2.0
platform equipped with a camera turret. This validates our approach's
practicality and sets the stage for further research in complex filming
scenarios, contributing significantly to the fusion of technology with
cinematic creativity. This work presents a leap forward in the field and opens
new avenues for research and development, effectively bridging the gap between
technological advancement and creative filmmaking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Authors' accepted manuscript (IROS 2024, Abu Dhabi, Oct 14-18, 2024).
  Please cite the version of record: DOI 10.1109/IROS58592.2024.10802717. 8
  pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Needle Biopsy And Fiber-Optic Compatible <span class="highlight-title">Robot</span>ic Insertion Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanxin Wang, Yikun Cheng, Chuyuan Tao, Rohit Bhargava, Thenkurussi Kesavadas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tissue biopsy is the gold standard for diagnosing many diseases, involving
the extraction of diseased tissue for histopathology analysis by expert
pathologists. However, this procedure has two main limitations: 1) Manual
sampling through tissue biopsy is prone to inaccuracies; 2) The extraction
process is followed by a time-consuming pathology test. To address these
limitations, we present a compact, accurate, and maneuverable robotic insertion
platform to overcome the limitations in traditional histopathology. Our
platform is capable of steering a variety of tools with different sizes,
including needle for tissue extraction and optical fibers for vibrational
spectroscopy applications. This system facilitates the guidance of end-effector
to the tissue and assists surgeons in navigating to the biopsy target area for
multi-modal diagnosis. In this paper, we outline the general concept of our
device, followed by a detailed description of its mechanical design and control
scheme. We conclude with the validation of the system through a series of
tests, including positioning accuracy, admittance performance, and tool
insertion efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented in EMBC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuralSVCD for Efficient Swept Volume Collision Detection <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwon Son, Hojin Jung, Beomjoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot manipulation in unstructured environments requires efficient and
reliable Swept Volume Collision Detection (SVCD) for safe motion planning.
Traditional discrete methods potentially miss collisions between these points,
whereas SVCD continuously checks for collisions along the entire trajectory.
Existing SVCD methods typically face a trade-off between efficiency and
accuracy, limiting practical use. In this paper, we introduce NeuralSVCD, a
novel neural encoder-decoder architecture tailored to overcome this trade-off.
Our approach leverages shape locality and temporal locality through distributed
geometric representations and temporal optimization. This enhances
computational efficiency without sacrificing accuracy. Comprehensive
experiments show that NeuralSVCD consistently outperforms existing
state-of-the-art SVCD methods in terms of both collision detection accuracy and
computational efficiency, demonstrating its robust applicability across diverse
robotic manipulation scenarios. Code and videos are available at
https://neuralsvcd.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoRL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLUID: A Fine-Grained Lightweight Urban Signalized-Intersection <span class="highlight-title">Dataset</span>
  of Dense Conflict Trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyang Chen, Zhigang Wu, Guohong Zheng, Xuesong Wu, Liwen Xu, Haoyuan Tang, Zhaocheng He, Haipeng Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The trajectory data of traffic participants (TPs) is a fundamental resource
for evaluating traffic conditions and optimizing policies, especially at urban
intersections. Although data acquisition using drones is efficient, existing
datasets still have limitations in scene representativeness, information
richness, and data fidelity. This study introduces FLUID, comprising a
fine-grained trajectory dataset that captures dense conflicts at typical urban
signalized intersections, and a lightweight, full-pipeline framework for
drone-based trajectory processing. FLUID covers three distinct intersection
types, with approximately 5 hours of recording time and featuring over 20,000
TPs across 8 categories. Notably, the dataset averages two vehicle conflicts
per minute, involving roughly 25% of all motor vehicles. FLUID provides
comprehensive data, including trajectories, traffic signals, maps, and raw
videos. Comparison with the DataFromSky platform and ground-truth measurements
validates its high spatio-temporal accuracy. Through a detailed classification
of motor vehicle conflicts and violations, FLUID reveals a diversity of
interactive behaviors, demonstrating its value for human preference mining,
traffic behavior modeling, and autonomous driving research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extended Diffeomorphism for Real-Time Motion Replication in Workspaces
  with Different Spatial Arrangements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masaki Saito, Shunki Itadera, Toshiyuki Murakami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents two types of extended diffeomorphism designs to
compensate for spatial placement differences between robot workspaces.
Teleoperation of multiple robots is attracting attention to expand the
utilization of the robot embodiment. Real-time reproduction of robot motion
would facilitate the efficient execution of similar tasks by multiple robots. A
challenge in the motion reproduction is compensating for the spatial
arrangement errors of target keypoints in robot workspaces. This paper proposes
a methodology for smooth mappings that transform primary robot poses into
follower robot poses based on the predefined key points in each workspace.
Through a picking task experiment using a dual-arm UR5 robot, this study
demonstrates that the proposed mapping generation method can balance lower
mapping errors for precise operation and lower mapping gradients for smooth
replicated movement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied Spatial Intelligence: from Implicit Scene Modeling to Spatial
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiading Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This thesis introduces "Embodied Spatial Intelligence" to address the
challenge of creating robots that can perceive and act in the real world based
on natural language instructions. To bridge the gap between Large Language
Models (LLMs) and physical embodiment, we present contributions on two fronts:
scene representation and spatial reasoning. For perception, we develop robust,
scalable, and accurate scene representations using implicit neural models, with
contributions in self-supervised camera calibration, high-fidelity depth field
generation, and large-scale reconstruction. For spatial reasoning, we enhance
the spatial capabilities of LLMs by introducing a novel navigation benchmark, a
method for grounding language in 3D, and a state-feedback mechanism to improve
long-horizon decision-making. This work lays a foundation for robots that can
robustly perceive their surroundings and intelligently act upon complex,
language-based commands.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AGS: Accelerating 3D Gaussian Splatting <span class="highlight-title">SLAM</span> via CODEC-Assisted Frame
  Covisibility Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houshu He, Naifeng Jing, Li Jiang, Xiaoyao Liang, Zhuoran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization and Mapping (SLAM) is a critical task that enables
autonomous vehicles to construct maps and localize themselves in unknown
environments. Recent breakthroughs combine SLAM with 3D Gaussian Splatting
(3DGS) to achieve exceptional reconstruction fidelity. However, existing
3DGS-SLAM systems provide insufficient throughput due to the need for multiple
training iterations per frame and the vast number of Gaussians.
  In this paper, we propose AGS, an algorithm-hardware co-design framework to
boost the efficiency of 3DGS-SLAM based on the intuition that SLAM systems
process frames in a streaming manner, where adjacent frames exhibit high
similarity that can be utilized for acceleration. On the software level: 1) We
propose a coarse-then-fine-grained pose tracking method with respect to the
robot's movement. 2) We avoid redundant computations of Gaussians by sharing
their contribution information across frames. On the hardware level, we propose
a frame covisibility detection engine to extract intermediate data from the
video CODEC. We also implement a pose tracking engine and a mapping engine with
workload schedulers to efficiently deploy the AGS algorithm. Our evaluation
shows that AGS achieves up to $17.12\times$, $6.71\times$, and $5.41\times$
speedups against the mobile and high-end GPUs, and a state-of-the-art 3DGS
accelerator, GSCore.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D
  <span class="highlight-title">Semantic</span> Segmentation <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialiang Kang, Jiawen Wang, Dingsheng Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of 3D LiDAR data plays a pivotal role in autonomous
driving. Traditional approaches rely on extensive annotated data for point
cloud analysis, incurring high costs and time investments. In contrast,
realworld image datasets offer abundant availability and substantial scale. To
mitigate the burden of annotating 3D LiDAR point clouds, we propose two
crossmodal knowledge distillation methods: Unsupervised Domain Adaptation
Knowledge Distillation (UDAKD) and Feature and Semantic-based Knowledge
Distillation (FSKD). Leveraging readily available spatio-temporally
synchronized data from cameras and LiDARs in autonomous driving scenarios, we
directly apply a pretrained 2D image model to unlabeled 2D data. Through
crossmodal knowledge distillation with known 2D-3D correspondence, we actively
align the output of the 3D network with the corresponding points of the 2D
network, thereby obviating the necessity for 3D annotations. Our focus is on
preserving modality-general information while filtering out modality-specific
details during crossmodal distillation. To achieve this, we deploy
self-calibrated convolution on 3D point clouds as the foundation of our domain
adaptation module. Rigorous experimentation validates the effectiveness of our
proposed methods, consistently surpassing the performance of state-of-the-art
approaches in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Generative</span> <span class="highlight-title">Visual</span> Foresight Meets Task-Agnostic Pose <span class="highlight-title">Estimation</span> in
  <span class="highlight-title">Robot</span>ic Table-Top <span class="highlight-title">Manipulation</span> <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuye Zhang, Xiaoxiong Zhang, Wei Pan, Linfang Zheng, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation in unstructured environments requires systems that can
generalize across diverse tasks while maintaining robust and reliable
performance. We introduce {GVF-TAPE}, a closed-loop framework that combines
generative visual foresight with task-agnostic pose estimation to enable
scalable robotic manipulation. GVF-TAPE employs a generative video model to
predict future RGB-D frames from a single side-view RGB image and a task
description, offering visual plans that guide robot actions. A decoupled pose
estimation model then extracts end-effector poses from the predicted frames,
translating them into executable commands via low-level controllers. By
iteratively integrating video foresight and pose estimation in a closed loop,
GVF-TAPE achieves real-time, adaptive manipulation across a broad range of
tasks. Extensive experiments in both simulation and real-world settings
demonstrate that our approach reduces reliance on task-specific action data and
generalizes effectively, providing a practical and scalable solution for
intelligent robotic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9th Conference on Robot Learning (CoRL 2025), Seoul, Korea</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Aggregate Sorting in Construction and Mining via Computer
  <span class="highlight-title">Vision</span>-Aided <span class="highlight-title">Robot</span>ic Arm Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Taherul Islam Shawon, Yuan Li, Yincai Cai, Junjie Niu, Ting Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional aggregate sorting methods, whether manual or mechanical, often
suffer from low precision, limited flexibility, and poor adaptability to
diverse material properties such as size, shape, and lithology. To address
these limitations, this study presents a computer vision-aided robotic arm
system designed for autonomous aggregate sorting in construction and mining
applications. The system integrates a six-degree-of-freedom robotic arm, a
binocular stereo camera for 3D perception, and a ROS-based control framework.
Core techniques include an attention-augmented YOLOv8 model for aggregate
detection, stereo matching for 3D localization, Denavit-Hartenberg kinematic
modeling for arm motion control, minimum enclosing rectangle analysis for size
estimation, and hand-eye calibration for precise coordinate alignment.
Experimental validation with four aggregate types achieved an average grasping
and sorting success rate of 97.5%, with comparable classification accuracy.
Remaining challenges include the reliable handling of small aggregates and
texture-based misclassification. Overall, the proposed system demonstrates
significant potential to enhance productivity, reduce operational costs, and
improve safety in aggregate handling, while providing a scalable framework for
advancing smart automation in construction, mining, and recycling industries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jacobian Exploratory Dual-Phase Reinforcement <span class="highlight-title">Learning</span> for <span class="highlight-title">Dynamic</span>
  Endoluminal <span class="highlight-title">Navigation</span> of Deformable Continuum <span class="highlight-title">Robots</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Tian, Chi Kit Ng, Hongliang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable continuum robots (DCRs) present unique planning challenges due to
nonlinear deformation mechanics and partial state observability, violating the
Markov assumptions of conventional reinforcement learning (RL) methods. While
Jacobian-based approaches offer theoretical foundations for rigid manipulators,
their direct application to DCRs remains limited by time-varying kinematics and
underactuated deformation dynamics. This paper proposes Jacobian Exploratory
Dual-Phase RL (JEDP-RL), a framework that decomposes planning into phased
Jacobian estimation and policy execution. During each training step, we first
perform small-scale local exploratory actions to estimate the deformation
Jacobian matrix, then augment the state representation with Jacobian features
to restore approximate Markovianity. Extensive SOFA surgical dynamic
simulations demonstrate JEDP-RL's three key advantages over proximal policy
optimization (PPO) baselines: 1) Convergence speed: 3.2x faster policy
convergence, 2) Navigation efficiency: requires 25% fewer steps to reach the
target, and 3) Generalization ability: achieve 92% success rate under material
property variations and achieve 83% (33% higher than PPO) success rate in the
unseen tissue environment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mechanistic interpretability for steering <span class="highlight-title">vision-language-action</span> models <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bear Häon, Kaylene Stocking, Ian Chuang, Claire Tomlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models are a promising path to realizing
generalist embodied agents that can quickly adapt to new tasks, modalities, and
environments. However, methods for interpreting and steering VLAs fall far
short of classical robotics pipelines, which are grounded in explicit models of
kinematics, dynamics, and control. This lack of mechanistic insight is a
central challenge for deploying learned policies in real-world robotics, where
robustness and explainability are critical. Motivated by advances in
mechanistic interpretability for large language models, we introduce the first
framework for interpreting and steering VLAs via their internal
representations, enabling direct intervention in model behavior at inference
time. We project feedforward activations within transformer layers onto the
token embedding basis, identifying sparse semantic directions - such as speed
and direction - that are causally linked to action selection. Leveraging these
findings, we introduce a general-purpose activation steering method that
modulates behavior in real time, without fine-tuning, reward signals, or
environment interaction. We evaluate this method on two recent open-source
VLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in
simulation (LIBERO) and on a physical robot (UR5). This work demonstrates that
interpretable components of embodied VLAs can be systematically harnessed for
control - establishing a new paradigm for transparent and steerable foundation
models in robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoRL 2025. Project website: https://vla-mech-interp.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contact-Aided <span class="highlight-title">Navigation</span> of Flexible <span class="highlight-title">Robot</span>ic Endoscope Using Deep
  Reinforcement <span class="highlight-title">Learning</span> in <span class="highlight-title">Dynamic</span> Stomach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Kit Ng, Huxin Gao, Tian-Ao Ren, Jiewen Lai, Hongliang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating a flexible robotic endoscope (FRE) through the gastrointestinal
tract is critical for surgical diagnosis and treatment. However, navigation in
the dynamic stomach is particularly challenging because the FRE must learn to
effectively use contact with the deformable stomach walls to reach target
locations. To address this, we introduce a deep reinforcement learning (DRL)
based Contact-Aided Navigation (CAN) strategy for FREs, leveraging contact
force feedback to enhance motion stability and navigation precision. The
training environment is established using a physics-based finite element method
(FEM) simulation of a deformable stomach. Trained with the Proximal Policy
Optimization (PPO) algorithm, our approach achieves high navigation success
rates (within 3 mm error between the FRE's end-effector and target) and
significantly outperforms baseline policies. In both static and dynamic stomach
environments, the CAN agent achieved a 100% success rate with 1.6 mm average
error, and it maintained an 85% success rate in challenging unseen scenarios
with stronger external disturbances. These results validate that the DRL-based
CAN strategy substantially enhances FRE navigation performance over prior
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Framework for Task and Motion <span class="highlight-title">Planning</span> based on Expanding AND/OR
  Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fulvio Mastrogiovanni, Antony Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot autonomy in space environments presents unique challenges, including
high perception and motion uncertainty, strict kinematic constraints, and
limited opportunities for human intervention. Therefore, Task and Motion
Planning (TMP) may be critical for autonomous servicing, surface operations, or
even in-orbit missions, just to name a few, as it models tasks as discrete
action sequencing integrated with continuous motion feasibility assessments. In
this paper, we introduce a TMP framework based on expanding AND/OR graphs,
referred to as TMP-EAOG, and demonstrate its adaptability to different
scenarios. TMP-EAOG encodes task-level abstractions within an AND/OR graph,
which expands iteratively as the plan is executed, and performs in-the-loop
motion planning assessments to ascertain their feasibility. As a consequence,
TMP-EAOG is characterised by the desirable properties of (i) robustness to a
certain degree of uncertainty, because AND/OR graph expansion can accommodate
for unpredictable information about the robot environment, (ii) controlled
autonomy, since an AND/OR graph can be validated by human experts, and (iii)
bounded flexibility, in that unexpected events, including the assessment of
unfeasible motions, can lead to different courses of action as alternative
paths in the AND/OR graph. We evaluate TMP-EAOG on two benchmark domains. We
use a simulated mobile manipulator as a proxy for space-grade autonomous
robots. Our evaluation shows that TMP-EAOG can deal with a wide range of
challenges in the benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for an oral presentation at ASTRA Conference, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TReF-6: Inferring Task-Relevant Frames from a Single Demonstration for
  One-Shot Skill Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Ding, Shuangge Wang, Tesca Fitzgerald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots often struggle to generalize from a single demonstration due to the
lack of a transferable and interpretable spatial representation. In this work,
we introduce TReF-6, a method that infers a simplified, abstracted 6DoF
Task-Relevant Frame from a single trajectory. Our approach identifies an
influence point purely from the trajectory geometry to define the origin for a
local frame, which serves as a reference for parameterizing a Dynamic Movement
Primitive (DMP). This influence point captures the task's spatial structure,
extending the standard DMP formulation beyond start-goal imitation. The
inferred frame is semantically grounded via a vision-language model and
localized in novel scenes by Grounded-SAM, enabling functionally consistent
skill generalization. We validate TReF-6 in simulation and demonstrate
robustness to trajectory noise. We further deploy an end-to-end pipeline on
real-world manipulation tasks, showing that TReF-6 supports one-shot imitation
learning that preserves task intent across diverse object configurations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Layered Control Perspective on <span class="highlight-title">Legged</span> <span class="highlight-title">Locomotion</span>: Embedding Reduced
  Order Models via Hybrid Zero <span class="highlight-title">Dynamic</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sergio A. Esteban, Max H. Cohen, Adrian B. Ghansah, Aaron D. Ames
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reduced-order models (ROMs) provide a powerful means of synthesizing dynamic
walking gaits on legged robots. Yet this approach lacks the formal guarantees
enjoyed by methods that utilize the full-order model (FOM) for gait synthesis,
e.g., hybrid zero dynamics. This paper aims to unify these approaches through a
layered control perspective. In particular, we establish conditions on when a
ROM of locomotion yields stable walking on the full-order hybrid dynamics. To
achieve this result, given an ROM we synthesize a zero dynamics manifold
encoding the behavior of the ROM -- controllers can be synthesized that drive
the FOM to this surface, yielding hybrid zero dynamics. We prove that a stable
periodic orbit in the ROM implies an input-to-state stable periodic orbit of
the FOM's hybrid zero dynamics, and hence the FOM dynamics. This result is
demonstrated in simulation on a linear inverted pendulum ROM and a 5-link
planar walking FOM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> LocoTouch: <span class="highlight-title">Learning</span> <span class="highlight-title">Dynamic</span> <span class="highlight-title">Quadrupedal</span> Transport with Tactile Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23175v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23175v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changyi Lin, Yuxin Ray Song, Boda Huo, Mingyang Yu, Yikai Wang, Shiqi Liu, Yuxiang Yang, Wenhao Yu, Tingnan Zhang, <span class="highlight-author">Jie Tan</span>, Yiyue Luo, <span class="highlight-author">Ding Zhao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quadrupedal robots have demonstrated remarkable agility and robustness in
traversing complex terrains. However, they struggle with dynamic object
interactions, where contact must be precisely sensed and controlled. To bridge
this gap, we present LocoTouch, a system that equips quadrupedal robots with
tactile sensing to address a particularly challenging task in this category:
long-distance transport of unsecured cylindrical objects, which typically
requires custom mounting or fastening mechanisms to maintain stability. For
efficient large-area tactile sensing, we design a high-density distributed
tactile sensor that covers the entire back of the robot. To effectively
leverage tactile feedback for robot control, we develop a simulation
environment with high-fidelity tactile signals, and train tactile-aware
transport policies using a two-stage learning pipeline. Furthermore, we design
a novel reward function to promote robust, symmetric, and frequency-adaptive
locomotion gaits. After training in simulation, LocoTouch transfers zero-shot
to the real world, reliably transporting a wide range of unsecured cylindrical
objects with diverse sizes, weights, and surface properties. Moreover, it
remains robust over long distances, on uneven terrain, and under severe
perturbations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://linchangyi1.github.io/LocoTouch</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Wheeled Lab: Modern Sim2Real for Low-cost, Open-source Wheeled <span class="highlight-title">Robot</span>ics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Han, Preet Shah, Sidharth Rajagopal, Yanda Bao, Sanghun Jung, Sidharth Talia, Gabriel Guo, Bryan Xu, Bhaumik Mehta, Emma Romig, Rosario Scalise, Byron Boots
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has been pivotal in recent robotics milestones
and is poised to play a prominent role in the future. However, these advances
can rely on proprietary simulators, expensive hardware, and a daunting range of
tools and skills. As a result, broader communities are disconnecting from the
state-of-the-art; education curricula are poorly equipped to teach
indispensable modern robotics skills involving hardware, deployment, and
iterative development. To address this gap between the broader and scientific
communities, we contribute Wheeled Lab, an ecosystem which integrates
accessible, open-source wheeled robots with Isaac Lab, an open-source robot
learning and simulation framework, that is widely adopted in the
state-of-the-art. To kickstart research and education, this work demonstrates
three state-of-the-art zero-shot policies for small-scale RC cars developed
through Wheeled Lab: controlled drifting, elevation traversal, and visual
navigation. The full stack, from hardware to software, is low-cost and
open-source. Videos and additional materials can be found at:
https://uwrobotlearning.github.io/WheeledLab/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at Conference on Robot Learning, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ManipBench: Benchmarking <span class="highlight-title">Vision</span>-Language Models for Low-Level <span class="highlight-title">Robot</span>
  <span class="highlight-title">Manipulation</span> <span class="chip">CoRL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.09698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.09698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enyu Zhao, Vedant Raval, Hejia Zhang, Jiageng Mao, Zeyu Shangguan, Stefanos Nikolaidis, Yue Wang, Daniel Seita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have revolutionized artificial intelligence and
robotics due to their commonsense reasoning capabilities. In robotic
manipulation, VLMs are used primarily as high-level planners, but recent work
has also studied their lower-level reasoning ability, which refers to making
decisions about precise robot movements. However, the community currently lacks
a clear and common benchmark that can evaluate how well VLMs can aid low-level
reasoning in robotics. Consequently, we propose a novel benchmark, ManipBench,
to evaluate the low-level robot manipulation reasoning capabilities of VLMs
across various dimensions, including how well they understand object-object
interactions and deformable object manipulation. We extensively test 33
representative VLMs across 10 model families on our benchmark, including
variants to test different model sizes. Our evaluation shows that the
performance of VLMs significantly varies across tasks, and there is a strong
correlation between this performance and trends in our real-world manipulation
tasks. It also shows that there remains a significant gap between these models
and human-level understanding. See our website at:
https://manipbench.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Robot Learning (CoRL) 2025. 50 pages and 30 figures. v2
  is the camera-ready and includes a few more new experiments compared to v1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Robot</span>xR1: Enabling Embodied <span class="highlight-title">Robot</span>ic Intelligence on Large Language
  Models through Closed-Loop Reinforcement <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03238v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03238v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liam Boyle, Nicolas Baumann, Paviththiren Sivasothilingam, Michele Magno, Luca Benini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Future robotic systems operating in real-world environments will require
on-board embodied intelligence without continuous cloud connection, balancing
capabilities with constraints on computational power and memory. This work
presents an extension of the R1-zero approach, which enables the usage of low
parameter-count Large Language Models (LLMs) in the robotic domain. The R1-Zero
approach was originally developed to enable mathematical reasoning in LLMs
using static datasets. We extend it to the robotics domain through integration
in a closed-loop Reinforcement Learning (RL) framework. This extension enhances
reasoning in Embodied Artificial Intelligence (Embodied AI) settings without
relying solely on distillation of large models through Supervised Fine-Tuning
(SFT). We show that small-scale LLMs can achieve effective reasoning
performance by learning through closed-loop interaction with their environment,
which enables tasks that previously required significantly larger models. In an
autonomous driving setting, a performance gain of 20.2%-points over the
SFT-based baseline is observed with a Qwen2.5-1.5B model. Using the proposed
training procedure, Qwen2.5-3B achieves a 63.3% control adaptability score,
surpassing the 58.5% obtained by the much larger, cloud-bound GPT-4o. These
results highlight that practical, on-board deployment of small LLMs is not only
feasible but can outperform larger models if trained through environmental
feedback, underscoring the importance of an interactive learning framework for
robotic Embodied AI, one grounded in practical experience rather than static
supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi Object Tracking for Predictive Collision Avoidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.02161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.02161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bruk Gebregziabher, Hadush Hailu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The safe and efficient operation of Autonomous Mobile Robots (AMRs) in
complex environments, such as manufacturing, logistics, and agriculture,
necessitates accurate multi-object tracking and predictive collision avoidance.
This paper presents algorithms and techniques for addressing these challenges
using Lidar sensor data, emphasizing ensemble Kalman filter. The developed
predictive collision avoidance algorithm employs the data provided by lidar
sensors to track multiple objects and predict their velocities and future
positions, enabling the AMR to navigate safely and effectively. A modification
to the dynamic windowing approach is introduced to enhance the performance of
the collision avoidance system. The overall system architecture encompasses
object detection, multi-object tracking, and predictive collision avoidance
control. The experimental results, obtained from both simulation and real-world
data, demonstrate the effectiveness of the proposed methods in various
scenarios, which lays the foundation for future research on global planners,
other controllers, and the integration of additional sensors. This thesis
contributes to the ongoing development of safe and efficient autonomous systems
in complex and dynamic environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPGrasp: Spatiotemporal Prompt-driven Grasp Synthesis in <span class="highlight-title">Dynamic</span> Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20547v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20547v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Mei, Hongjie Cao, Yinqiu Xia, Wei Xiao, Zhaohan Feng, Gang Wang, Jie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time interactive grasp synthesis for dynamic objects remains challenging
as existing methods fail to achieve low-latency inference while maintaining
promptability. To bridge this gap, we propose SPGrasp (spatiotemporal
prompt-driven dynamic grasp synthesis), a novel framework extending segment
anything model v2 (SAMv2) for video stream grasp estimation. Our core
innovation integrates user prompts with spatiotemporal context, enabling
real-time interaction with end-to-end latency as low as 59 ms while ensuring
temporal consistency for dynamic objects. In benchmark evaluations, SPGrasp
achieves instance-level grasp accuracies of 90.6% on OCID and 93.8% on
Jacquard. On the challenging GraspNet-1Billion dataset under continuous
tracking, SPGrasp achieves 92.0% accuracy with 73.1 ms per-frame latency,
representing a 58.5% reduction compared to the prior state-of-the-art
promptable method RoG-SAM while maintaining competitive accuracy. Real-world
experiments involving 13 moving objects demonstrate a 94.8% success rate in
interactive grasping scenarios. These results confirm SPGrasp effectively
resolves the latency-interactivity trade-off in dynamic grasp synthesis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLONE: Closed-Loop <span class="highlight-title">Whole-Body</span> <span class="highlight-title">Humanoid</span> Teleoperation for Long-Horizon
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.08931v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.08931v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Li, Yutang Lin, Jieming Cui, Tengyu Liu, Wei Liang, Yixin Zhu, Siyuan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid teleoperation plays a vital role in demonstrating and collecting
data for complex humanoid-scene interactions. However, current teleoperation
systems face critical limitations: they decouple upper- and lower-body control
to maintain stability, restricting natural coordination, and operate open-loop
without real-time position feedback, leading to accumulated drift. The
fundamental challenge is achieving precise, coordinated whole-body
teleoperation over extended durations while maintaining accurate global
positioning. Here we show that an MoE-based teleoperation system, CLONE, with
closed-loop error correction enables unprecedented whole-body teleoperation
fidelity, maintaining minimal positional drift over long-range trajectories
using only head and hand tracking from an MR headset. Unlike previous methods
that either sacrifice coordination for stability or suffer from unbounded
drift, CLONE learns diverse motion skills while preventing tracking error
accumulation through real-time feedback, enabling complex coordinated movements
such as ``picking up objects from the ground.'' These results establish a new
milestone for whole-body humanoid teleoperation for long-horizon humanoid-scene
interaction tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Toward Real-World Cooperative and Competitive Soccer with <span class="highlight-title">Quadrupedal</span>
  <span class="highlight-title">Robot</span> Teams <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.13834v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.13834v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi Su, Yuman Gao, Emily Lukas, Yunfei Li, Jiaze Cai, Faris Tulbah, Fei Gao, Chao Yu, Zhongyu Li, <span class="highlight-author">Yi Wu</span>, Koushil Sreenath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving coordinated teamwork among legged robots requires both fine-grained
locomotion control and long-horizon strategic decision-making. Robot soccer
offers a compelling testbed for this challenge, combining dynamic, competitive,
and multi-agent interactions. In this work, we present a hierarchical
multi-agent reinforcement learning (MARL) framework that enables fully
autonomous and decentralized quadruped robot soccer. First, a set of highly
dynamic low-level skills is trained for legged locomotion and ball
manipulation, such as walking, dribbling, and kicking. On top of these, a
high-level strategic planning policy is trained with Multi-Agent Proximal
Policy Optimization (MAPPO) via Fictitious Self-Play (FSP). This learning
framework allows agents to adapt to diverse opponent strategies and gives rise
to sophisticated team behaviors, including coordinated passing, interception,
and dynamic role allocation. With an extensive ablation study, the proposed
learning method shows significant advantages in the cooperative and competitive
multi-agent soccer game. We deploy the learned policies to real quadruped
robots relying solely on onboard proprioception and decentralized localization,
with the resulting system supporting autonomous robot-robot and robot-human
soccer matches on indoor and outdoor soccer courts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 12 figures, CoRL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-Conditioned Scene Graphs for State-Grounded Task <span class="highlight-title">Planning</span> <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2504.06661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2504.06661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Herzog, Jiangpin Liu, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent robotic task planning frameworks have integrated large multimodal
models (LMMs) such as GPT-4o. To address grounding issues of such models, it
has been suggested to split the pipeline into perceptional state grounding and
subsequent state-based planning. As we show in this work, the state grounding
ability of LMM-based approaches is still limited by weaknesses in granular,
structured, domain-specific scene understanding. To address this shortcoming,
we develop a more structured state grounding framework that features a
domain-conditioned scene graph as its scene representation. We show that such
representation is actionable in nature as it is directly mappable to a symbolic
state in planning languages such as the Planning Domain Definition Language
(PDDL). We provide an instantiation of our state grounding framework where the
domain-conditioned scene graph generation is implemented with a lightweight
vision-language approach that classifies domain-specific predicates on top of
domain-relevant object detections. Evaluated across three domains, our approach
achieves significantly higher state rounding accuracy and task planning success
rates compared to LMM-based approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-08-29T00:00:00Z">2025-08-29</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">42</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learn from What We HAVE: History-Aware VErifier that Reasons about Past
  Interactions Online <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yishu Li, Xinyi Mao, Ying Yuan, Kyutae Sim, Ben Eisner, David Held
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel History-Aware VErifier (HAVE) to disambiguate uncertain
scenarios online by leveraging past interactions. Robots frequently encounter
visually ambiguous objects whose manipulation outcomes remain uncertain until
physically interacted with. While generative models alone could theoretically
adapt to such ambiguity, in practice they obtain suboptimal performance in
ambiguous cases, even when conditioned on action history. To address this, we
propose explicitly decoupling action generation from verification: we use an
unconditional diffusion-based generator to propose multiple candidate actions
and employ our history-aware verifier to select the most promising action by
reasoning about past interactions. Through theoretical analysis, we demonstrate
that employing a verifier significantly improves expected action quality.
Empirical evaluations and analysis across multiple simulated and real-world
environments including articulated objects, multi-modal doors, and uneven
object pick-up confirm the effectiveness of our method and improvements over
baselines. Our project website is available at:
https://liy1shu.github.io/HAVE_CoRL25/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoRL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied AI in Social Spaces: Responsible and Adaptive <span class="highlight-title">Robots</span> in Complex
  Setting -- UKAIRS 2025 (Copy) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandra Landowska, Aislinn D Gomez Bergin, Ayodeji O. Abioye, Jayati Deshmukh, Andriana Bouadouki, Maria Wheadon, Athina Georgara, Dominic Price, Tuyen Nguyen, Shuang Ao, Lokesh Singh, Yi Long, Raffaele Miele, Joel E. Fischer, Sarvapali D. Ramchurn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces and overviews a multidisciplinary project aimed at
developing responsible and adaptive multi-human multi-robot (MHMR) systems for
complex, dynamic settings. The project integrates co-design, ethical
frameworks, and multimodal sensing to create AI-driven robots that are
emotionally responsive, context-aware, and aligned with the needs of diverse
users. We outline the project's vision, methodology, and early outcomes,
demonstrating how embodied AI can support sustainable, ethical, and
human-centred futures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Poke and Strike: <span class="highlight-title">Learning</span> Task-Informed Exploration Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Y. Aoyama, Joao Moura, Juan Del Aguila Ferrandis, Sethu Vijayakumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many dynamic robotic tasks, such as striking pucks into a goal outside the
reachable workspace, the robot must first identify the relevant physical
properties of the object for successful task execution, as it is unable to
recover from failure or retry without human intervention. To address this
challenge, we propose a task-informed exploration approach, based on
reinforcement learning, that trains an exploration policy using rewards
automatically generated from the sensitivity of a privileged task policy to
errors in estimated properties. We also introduce an uncertainty-based
mechanism to determine when to transition from exploration to task execution,
ensuring sufficient property estimation accuracy with minimal exploration time.
Our method achieves a 90% success rate on the striking task with an average
exploration time under 1.2 seconds, significantly outperforming baselines that
achieve at most 40% success or require inefficient querying and retraining in a
simulator at test time. Additionally, we demonstrate that our task-informed
rewards capture the relative importance of physical properties in both the
striking task and the classical CartPole example. Finally, we validate our
approach by demonstrating its ability to identify object properties and adjust
task execution in a physical setup using the KUKA iiwa robot arm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages (main paper), 27 pages (including references and appendices),
  6 figures (main paper), 21 figures (including appendices), Conference of
  Robot Learning 2025, For videos and the project website, see
  https://marina-aoyama.github.io/poke-and-strike/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tree-Guided Diffusion Planner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonseong Jeon, Cheolhong Min, Jaesik Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning with pretrained diffusion models has emerged as a promising approach
for solving test-time guided control problems. However, standard gradient
guidance typically performs optimally under convex and differentiable reward
landscapes, showing substantially reduced effectiveness in real-world scenarios
involving non-convex objectives, non-differentiable constraints, and
multi-reward structures. Furthermore, recent supervised planning approaches
require task-specific training or value estimators, which limits test-time
flexibility and zero-shot generalization. We propose a Tree-guided Diffusion
Planner (TDP), a zero-shot test-time planning framework that balances
exploration and exploitation through structured trajectory generation. We frame
test-time planning as a tree search problem using a bi-level sampling process:
(1) diverse parent trajectories are produced via training-free particle
guidance to encourage broad exploration, and (2) sub-trajectories are refined
through fast conditional denoising guided by task objectives. TDP addresses the
limitations of gradient guidance by exploring diverse trajectory regions and
harnessing gradient information across this expanded solution space using only
pretrained models and test-time reward signals. We evaluate TDP on three
diverse tasks: maze gold-picking, robot arm block manipulation, and AntMaze
multi-goal exploration. TDP consistently outperforms state-of-the-art
approaches on all tasks. The project page can be found at:
tree-diffusion-planner.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures, 14 tables (main paper + appendix) / under
  review / project page will be available after the paper becomes public in
  arxiv</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can a mobile <span class="highlight-title">robot</span> learn from a pedestrian model to prevent the side<span class="highlight-title">walk</span>
  salsa? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olger Siebinga, David Abbink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pedestrians approaching each other on a sidewalk sometimes end up in an
awkward interaction known as the "sidewalk salsa": they both (repeatedly)
deviate to the same side to avoid a collision. This provides an interesting use
case to study interactions between pedestrians and mobile robots because, in
the vast majority of cases, this phenomenon is avoided through a negotiation
based on implicit communication. Understanding how it goes wrong and how
pedestrians end up in the sidewalk salsa will therefore provide insight into
the implicit communication. This understanding can be used to design safe and
acceptable robotic behaviour. In a previous attempt to gain this understanding,
a model of pedestrian behaviour based on the Communication-Enabled Interaction
(CEI) framework was developed that can replicate the sidewalk salsa. However,
it is unclear how to leverage this model in robotic planning and
decision-making since it violates the assumptions of game theory, a much-used
framework in planning and decision-making. Here, we present a proof-of-concept
for an approach where a Reinforcement Learning (RL) agent leverages the model
to learn how to interact with pedestrians. The results show that a basic RL
agent successfully learned to interact with the CEI model. Furthermore, a
risk-averse RL agent that had access to the perceived risk of the CEI model
learned how to effectively communicate its intention through its motion and
thereby substantially lowered the perceived risk, and displayed effort by the
modelled pedestrian. These results show this is a promising approach and
encourage further exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Robust</span> Convex Model Predictive Control with collision avoidance
  guarantees for <span class="highlight-title">robot</span> manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bernhard Wullt, Johannes Köhler, Per Mattsson, Mikeal Norrlöf, Thomas B. Schön
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Industrial manipulators are normally operated in cluttered environments,
making safe motion planning important. Furthermore, the presence of
model-uncertainties make safe motion planning more difficult. Therefore, in
practice the speed is limited in order to reduce the effect of disturbances.
There is a need for control methods that can guarantee safe motions that can be
executed fast. We address this need by suggesting a novel model predictive
control (MPC) solution for manipulators, where our two main components are a
robust tube MPC and a corridor planning algorithm to obtain collision-free
motion. Our solution results in a convex MPC, which we can solve fast, making
our method practically useful. We demonstrate the efficacy of our method in a
simulated environment with a 6 DOF industrial robot operating in cluttered
environments with uncertainties in model parameters. We outperform benchmark
methods, both in terms of being able to work under higher levels of model
uncertainties, while also yielding faster motion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A-MHA*: Anytime Multi-Heuristic A* 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramkumar Natarajan, Muhammad Suhail Saleem, William Xiao, Sandip Aine, Howie Choset, Maxim Likhachev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing good heuristic functions for graph search requires adequate domain
knowledge. It is often easy to design heuristics that perform well and
correlate with the underlying true cost-to-go values in certain parts of the
search space but these may not be admissible throughout the domain thereby
affecting the optimality guarantees of the search. Bounded suboptimal search
using several such partially good but inadmissible heuristics was developed in
Multi-Heuristic A* (MHA*). Although MHA* leverages multiple inadmissible
heuristics to potentially generate a faster suboptimal solution, the original
version does not improve the solution over time. It is a one shot algorithm
that requires careful setting of inflation factors to obtain a desired one time
solution. In this work, we tackle this issue by extending MHA* to an anytime
version that finds a feasible suboptimal solution quickly and continually
improves it until time runs out. Our work is inspired from the Anytime
Repairing A* (ARA*) algorithm. We prove that our precise adaptation of ARA*
concepts in the MHA* framework preserves the original suboptimal and
completeness guarantees and enhances MHA* to perform in an anytime fashion.
Furthermore, we report the performance of A-MHA* in 3-D path planning domain
and sliding tiles puzzle and compare against MHA* and other anytime algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Rosario <span class="highlight-title">Dataset</span> v2: Multimodal <span class="highlight-title">Dataset</span> for Agricultural <span class="highlight-title">Robot</span>ics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Soncini, Javier Cremona, Erica Vidal, Maximiliano García, Gastón Castro, Taihú Pire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a multi-modal dataset collected in a soybean crop field,
comprising over two hours of recorded data from sensors such as stereo infrared
camera, color camera, accelerometer, gyroscope, magnetometer, GNSS (Single
Point Positioning, Real-Time Kinematic and Post-Processed Kinematic), and wheel
odometry. This dataset captures key challenges inherent to robotics in
agricultural environments, including variations in natural lighting, motion
blur, rough terrain, and long, perceptually aliased sequences. By addressing
these complexities, the dataset aims to support the development and
benchmarking of advanced algorithms for localization, mapping, perception, and
navigation in agricultural robotics. The platform and data collection system is
designed to meet the key requirements for evaluating multi-modal SLAM systems,
including hardware synchronization of sensors, 6-DOF ground truth and loops on
long trajectories.
  We run multimodal state-of-the art SLAM methods on the dataset, showcasing
the existing limitations in their application on agricultural settings. The
dataset and utilities to work with it are released on
https://cifasis.github.io/rosariov2/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>First published on The International Journal of Robotics Research:
  https://journals.sagepub.com/doi/10.1177/02783649251368909</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Learning</span> <span class="highlight-title">Agile</span> Gate Traversal via Analytical Optimal Policy <span class="highlight-title">Gradient</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchen Sun, Bingheng Wang, Longbin Tang, Yichao Gao, Lin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traversing narrow gates presents a significant challenge and has become a
standard benchmark for evaluating agile and precise quadrotor flight.
Traditional modularized autonomous flight stacks require extensive design and
parameter tuning, while end-to-end reinforcement learning (RL) methods often
suffer from low sample efficiency and limited interpretability. In this work,
we present a novel hybrid framework that adaptively fine-tunes model predictive
control (MPC) parameters online using outputs from a neural network (NN)
trained offline. The NN jointly predicts a reference pose and cost-function
weights, conditioned on the coordinates of the gate corners and the current
drone state. To achieve efficient training, we derive analytical policy
gradients not only for the MPC module but also for an optimization-based gate
traversal detection module. Furthermore, we introduce a new formulation of the
attitude tracking error that admits a simplified representation, facilitating
effective learning with bounded gradients. Hardware experiments demonstrate
that our method enables fast and accurate quadrotor traversal through narrow
gates in confined environments. It achieves several orders of magnitude
improvement in sample efficiency compared to naive end-to-end RL approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimated Informed Anytime Search for Sampling-Based <span class="highlight-title">Planning</span> via
  Adaptive Sampler 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liding Zhang, Kuanqi Cai, Yu Zhang, Zhenshan Bing, Chaoqun Wang, Fan Wu, Sami Haddadin, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path planning in robotics often involves solving continuously valued,
high-dimensional problems. Popular informed approaches include graph-based
searches, such as A*, and sampling-based methods, such as Informed RRT*, which
utilize informed set and anytime strategies to expedite path optimization
incrementally. Informed sampling-based planners define informed sets as subsets
of the problem domain based on the current best solution cost. However, when no
solution is found, these planners re-sample and explore the entire
configuration space, which is time-consuming and computationally expensive.
This article introduces Multi-Informed Trees (MIT*), a novel planner that
constructs estimated informed sets based on prior admissible solution costs
before finding the initial solution, thereby accelerating the initial
convergence rate. Moreover, MIT* employs an adaptive sampler that dynamically
adjusts the sampling strategy based on the exploration process. Furthermore,
MIT* utilizes length-related adaptive sparse collision checks to guide lazy
reverse search. These features enhance path cost efficiency and computation
times while ensuring high success rates in confined scenarios. Through a series
of simulations and real-world experiments, it is confirmed that MIT*
outperforms existing single-query, sampling-based planners for problems in R^4
to R^16 and has been successfully applied to real-world robot manipulation
tasks. A video showcasing our experimental results is available at:
https://youtu.be/30RsBIdexTU
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complete Gaussian Splats from a Single Image with Denoising Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Liao, Mohamed Sayed, Steven L. Waslander, Sara Vicente, Daniyar Turmukhambetov, Michael Firman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian splatting typically requires dense observations of the scene and can
fail to reconstruct occluded and unobserved areas. We propose a latent
diffusion model to reconstruct a complete 3D scene with Gaussian splats,
including the occluded parts, from only a single image during inference.
Completing the unobserved surfaces of a scene is challenging due to the
ambiguity of the plausible surfaces. Conventional methods use a
regression-based formulation to predict a single "mode" for occluded and
out-of-frustum surfaces, leading to blurriness, implausibility, and failure to
capture multiple possible explanations. Thus, they often address this problem
partially, focusing either on objects isolated from the background,
reconstructing only visible surfaces, or failing to extrapolate far from the
input views. In contrast, we propose a generative formulation to learn a
distribution of 3D representations of Gaussian splats conditioned on a single
input image. To address the lack of ground-truth training data, we propose a
Variational AutoReconstructor to learn a latent space only from 2D images in a
self-supervised manner, over which a diffusion model is trained. Our method
generates faithful reconstructions and diverse samples with the ability to
complete the occluded surfaces for high-quality 360-degree renderings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main paper: 11 pages; Supplementary materials: 7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-Shot Neuro-Symbolic Imitation <span class="highlight-title">Learning</span> for Long-Horizon <span class="highlight-title">Planning</span> and
  Acting <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierrick Lorang, Hong Lu, Johannes Huemer, Patrik Zips, Matthias Scheutz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning enables intelligent systems to acquire complex behaviors
with minimal supervision. However, existing methods often focus on
short-horizon skills, require large datasets, and struggle to solve
long-horizon tasks or generalize across task variations and distribution
shifts. We propose a novel neuro-symbolic framework that jointly learns
continuous control policies and symbolic domain abstractions from a few skill
demonstrations. Our method abstracts high-level task structures into a graph,
discovers symbolic rules via an Answer Set Programming solver, and trains
low-level controllers using diffusion policy imitation learning. A high-level
oracle filters task-relevant information to focus each controller on a minimal
observation and action space. Our graph-based neuro-symbolic framework enables
capturing complex state transitions, including non-spatial and temporal
relations, that data-driven learning or clustering techniques often fail to
discover in limited demonstration datasets. We validate our approach in six
domains that involve four robotic arms, Stacking, Kitchen, Assembly, and Towers
of Hanoi environments, and a distinct Automated Forklift domain with two
environments. The results demonstrate high data efficiency with as few as five
skill demonstrations, strong zero- and few-shot generalizations, and
interpretable decision making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CoRL 2025; to appear in PMLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing Human Cooperation for Enhancing Social <span class="highlight-title">Robot</span> <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21455v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21455v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hariharan Arunachalam, Phani Teja Singamaneni, Rachid Alami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Socially aware robot navigation is a planning paradigm where the robot
navigates in human environments and tries to adhere to social constraints while
interacting with the humans in the scene. These navigation strategies were
further improved using human prediction models, where the robot takes the
potential future trajectory of humans while computing its own. Though these
strategies significantly improve the robot's behavior, it faces difficulties
from time to time when the human behaves in an unexpected manner. This happens
as the robot fails to understand human intentions and cooperativeness, and the
human does not have a clear idea of what the robot is planning to do. In this
paper, we aim to address this gap through effective communication at an
appropriate time based on a geometric analysis of the context and human
cooperativeness in head-on crossing scenarios. We provide an assessment
methodology and propose some evaluation metrics that could distinguish a
cooperative human from a non-cooperative one. Further, we also show how
geometric reasoning can be used to generate appropriate verbal responses or
robot actions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoboInspector: Unveiling the Unreliability of Policy Code for
  LLM-enabled <span class="highlight-title">Robot</span>ic <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenduo Ying, Linkang Du, Peng Cheng, Yuanchao Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate remarkable capabilities in reasoning
and code generation, enabling robotic manipulation to be initiated with just a
single instruction. The LLM carries out various tasks by generating policy code
required to control the robot. Despite advances in LLMs, achieving reliable
policy code generation remains a significant challenge due to the diverse
requirements of real-world tasks and the inherent complexity of user
instructions. In practice, different users may provide distinct instructions to
drive the robot for the same task, which may cause the unreliability of policy
code generation. To bridge this gap, we design RoboInspector, a pipeline to
unveil and characterize the unreliability of the policy code for LLM-enabled
robotic manipulation from two perspectives: the complexity of the manipulation
task and the granularity of the instruction. We perform comprehensive
experiments with 168 distinct combinations of tasks, instructions, and LLMs in
two prominent frameworks. The RoboInspector identifies four main unreliable
behaviors that lead to manipulation failure. We provide a detailed
characterization of these behaviors and their underlying causes, giving insight
for practical development to reduce unreliability. Furthermore, we introduce a
refinement approach guided by failure policy code feedback that improves the
reliability of policy code generation by up to 35% in LLM-enabled robotic
manipulation, evaluated in both simulation and real-world environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Dynamic</span>s-Compliant Trajectory Diffusion for Super-Nominal Payload
  <span class="highlight-title">Manipulation</span> <span class="chip">CoRL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuj Pasricha, Joewie Koh, Jay Vakil, Alessandro Roncone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nominal payload ratings for articulated robots are typically derived from
worst-case configurations, resulting in uniform payload constraints across the
entire workspace. This conservative approach severely underutilizes the robot's
inherent capabilities -- our analysis demonstrates that manipulators can safely
handle payloads well above nominal capacity across broad regions of their
workspace while staying within joint angle, velocity, acceleration, and torque
limits. To address this gap between assumed and actual capability, we propose a
novel trajectory generation approach using denoising diffusion models that
explicitly incorporates payload constraints into the planning process. Unlike
traditional sampling-based methods that rely on inefficient trial-and-error,
optimization-based methods that are prohibitively slow, or kinodynamic planners
that struggle with problem dimensionality, our approach generates dynamically
feasible joint-space trajectories in constant time that can be directly
executed on physical hardware without post-processing. Experimental validation
on a 7 DoF Franka Emika Panda robot demonstrates that up to 67.6% of the
workspace remains accessible even with payloads exceeding 3 times the nominal
capacity. This expanded operational envelope highlights the importance of a
more nuanced consideration of payload dynamics in motion planning algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2025 Conference on Robot Learning [CoRL]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi-Modal</span> Model Predictive Path Integral Control for Collision
  Avoidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21364v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21364v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alberto Bertipaglia, Dariu M. Gavrila, Barys Shyrokau
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel approach to motion planning and decision-making
for automated vehicles, using a multi-modal Model Predictive Path Integral
control algorithm. The method samples with Sobol sequences around the prior
input and incorporates analytical solutions for collision avoidance. By
leveraging multiple modes, the multi-modal control algorithm explores diverse
trajectories, such as manoeuvring around obstacles or stopping safely before
them, mitigating the risk of sub-optimal solutions. A non-linear single-track
vehicle model with a Fiala tyre serves as the prediction model, and tyre force
constraints within the friction circle are enforced to ensure vehicle stability
during evasive manoeuvres. The optimised steering angle and longitudinal
acceleration are computed to generate a collision-free trajectory and to
control the vehicle. In a high-fidelity simulation environment, we demonstrate
that the proposed algorithm can successfully avoid obstacles, keeping the
vehicle stable while driving a double lane change manoeuvre on high and
low-friction road surfaces and occlusion scenarios with moving obstacles,
outperforming a standard Model Predictive Path Integral approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as an oral presentation at the 29th IAVSD. August 18-22,
  2025. Shanghai, China</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Robust</span> Real-Time Coordination of CAVs: A Distributed <span class="highlight-title">Optimization</span>
  Framework under Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Bai, Yang Wang, Cong Guo, Xiongwei Zhao, Hai Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving both safety guarantees and real-time performance in cooperative
vehicle coordination remains a fundamental challenge, particularly in dynamic
and uncertain environments. This paper presents a novel coordination framework
that resolves this challenge through three key innovations: 1) direct control
of vehicles' trajectory distributions during coordination, formulated as a
robust cooperative planning problem with adaptive enhanced safety constraints,
ensuring a specified level of safety regarding the uncertainty of the
interactive trajectory, 2) a fully parallel ADMM-based distributed trajectory
negotiation (ADMM-DTN) algorithm that efficiently solves the optimization
problem while allowing configurable negotiation rounds to balance solution
quality and computational resources, and 3) an interactive attention mechanism
that selectively focuses on critical interactive participants to further
enhance computational efficiency. Both simulation results and practical
experiments demonstrate that our framework achieves significant advantages in
safety (reducing collision rates by up to 40.79\% in various scenarios) and
real-time performance compared to state-of-the-art methods, while maintaining
strong scalability with increasing vehicle numbers. The proposed interactive
attention mechanism further reduces the computational demand by 14.1\%. The
framework's effectiveness is further validated through real-world experiments
with unexpected dynamic obstacles, demonstrating robust coordination in complex
environments. The experiment demo could be found at
https://youtu.be/4PZwBnCsb6Q.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cooperative Sensing Enhanced UAV Path-Following and Obstacle Avoidance
  with Variable Formation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changheng Wang, Zhiqing Wei, Wangjun Jiang, Haoyue Jiang, Zhiyong Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high mobility of unmanned aerial vehicles (UAVs) enables them to be used
in various civilian fields, such as rescue and cargo transport. Path-following
is a crucial way to perform these tasks while sensing and collision avoidance
are essential for safe flight. In this paper, we investigate how to efficiently
and accurately achieve path-following, obstacle sensing and avoidance subtasks,
as well as their conflict-free fusion scheduling. Firstly, a high precision
deep reinforcement learning (DRL)-based UAV formation path-following model is
developed, and the reward function with adaptive weights is designed from the
perspective of distance and velocity errors. Then, we use integrated sensing
and communication (ISAC) signals to detect the obstacle and derive the
Cramer-Rao lower bound (CRLB) for obstacle sensing by information-level fusion,
based on which we propose the variable formation enhanced obstacle position
estimation (VFEO) algorithm. In addition, an online obstacle avoidance scheme
without pretraining is designed to solve the sparse reward. Finally, with the
aid of null space based (NSB) behavioral method, we present a hierarchical
subtasks fusion strategy. Simulation results demonstrate the effectiveness and
superiority of the subtask algorithms and the hierarchical fusion strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Observability-driven Assignment of Heterogeneous Sensors for
  Multi-Target Tracking <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Ali Rakhshan, Mehdi Golestani, He Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of assigning heterogeneous sensors (i.e.,
robots with varying sensing capabilities) for multi-target tracking. We
classify robots into two categories: (1) sufficient sensing robots, equipped
with range and bearing sensors, capable of independently tracking targets, and
(2) limited sensing robots, which are equipped with only range or bearing
sensors and need to at least form a pair to collaboratively track a target. Our
objective is to optimize tracking quality by minimizing uncertainty in target
state estimation through efficient robot-to-target assignment. By leveraging
matroid theory, we propose a greedy assignment algorithm that dynamically
allocates robots to targets to maximize tracking quality. The algorithm
guarantees constant-factor approximation bounds of 1/3 for arbitrary tracking
quality functions and 1/2 for submodular functions, while maintaining
polynomial-time complexity. Extensive simulations demonstrate the algorithm's
effectiveness in accurately estimating and tracking targets over extended
periods. Furthermore, numerical results confirm that the algorithm's
performance is close to that of the optimal assignment, highlighting its
robustness and practical applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to the 2025 IEEE/RSJ IROS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Domain Shifts in Myoelectric Activations: Challenges and
  Opportunities in Stream <span class="highlight-title">Learning</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Sun, Nick Lim, Guilherme Weigert Cassales, Heitor Murilo Gomes, Bernhard Pfahringer, Albert Bifet, Anany Dwivedi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting domain shifts in myoelectric activations poses a significant
challenge due to the inherent non-stationarity of electromyography (EMG)
signals. This paper explores the detection of domain shifts using data stream
(DS) learning techniques, focusing on the DB6 dataset from the Ninapro
database. We define domains as distinct time-series segments based on different
subjects and recording sessions, applying Kernel Principal Component Analysis
(KPCA) with a cosine kernel to pre-process and highlight these shifts. By
evaluating multiple drift detection methods such as CUSUM, Page-Hinckley, and
ADWIN, we reveal the limitations of current techniques in achieving high
performance for real-time domain shift detection in EMG signals. Our results
underscore the potential of streaming-based approaches for maintaining stable
EMG decoding models, while highlighting areas for further research to enhance
robustness and accuracy in real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures, 1 table, PRICAI25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Learning</span> to Assemble the Soma Cube with Legal-Action Masked DQN and Safe
  ZYZ Regrasp on a Doosan M0609 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehong Oh, Seungjun Jung, Sawoong Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first comprehensive application of legal-action
masked Deep Q-Networks with safe ZYZ regrasp strategies to an underactuated
gripper-equipped 6-DOF collaborative robot for autonomous Soma cube assembly
learning. Our approach represents the first systematic integration of
constraint-aware reinforcement learning with singularity-safe motion planning
on a Doosan M0609 collaborative robot. We address critical challenges in
robotic manipulation: combinatorial action space explosion, unsafe motion
planning, and systematic assembly strategy learning. Our system integrates a
legal-action masked DQN with hierarchical architecture that decomposes
Q-function estimation into orientation and position components, reducing
computational complexity from $O(3,132)$ to $O(116) + O(27)$ while maintaining
solution completeness. The robot-friendly reward function encourages
ground-first, vertically accessible assembly sequences aligned with
manipulation constraints. Curriculum learning across three progressive
difficulty levels (2-piece, 3-piece, 7-piece) achieves remarkable training
efficiency: 100\% success rate for Level 1 within 500 episodes, 92.9\% for
Level 2, and 39.9\% for Level 3 over 105,300 total training episodes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 figures, 17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mini Autonomous Car <span class="highlight-title">Driving</span> based on 3D Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Moraes, Monica Rodriguez, Kristofer S. Kappel, Hiago Sodre, Santiago Fernandez, Igor Nunes, Bruna Guterres, Ricardo Grando
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving applications have become increasingly relevant in the
automotive industry due to their potential to enhance vehicle safety,
efficiency, and user experience, thereby meeting the growing demand for
sophisticated driving assistance features. However, the development of reliable
and trustworthy autonomous systems poses challenges such as high complexity,
prolonged training periods, and intrinsic levels of uncertainty. Mini
Autonomous Cars (MACs) are used as a practical testbed, enabling validation of
autonomous control methodologies on small-scale setups. This simplified and
cost-effective environment facilitates rapid evaluation and comparison of
machine learning models, which is particularly useful for algorithms requiring
online training. To address these challenges, this work presents a methodology
based on RGB-D information and three-dimensional convolutional neural networks
(3D CNNs) for MAC autonomous driving in simulated environments. We evaluate the
proposed approach against recurrent neural networks (RNNs), with architectures
trained and tested on two simulated tracks with distinct environmental
features. Performance was assessed using task completion success, lap-time
metrics, and driving consistency. Results highlight how architectural
modifications and track complexity influence the models' generalization
capability and vehicle control performance. The proposed 3D CNN demonstrated
promising results when compared with RNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Diffusion <span class="highlight-title">Dynamic</span>s Models with <span class="highlight-title">Generative</span> State <span class="highlight-title">Estimation</span> for Cloth
  <span class="highlight-title">Manipulation</span> <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.11999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.11999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongxuan Tian, Haoyang Li, Bo Ai, Xiaodi Yuan, Zhiao Huang, <span class="highlight-author">Hao Su</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cloth manipulation is challenging due to its highly complex dynamics,
near-infinite degrees of freedom, and frequent self-occlusions, which
complicate both state estimation and dynamics modeling. Inspired by recent
advances in generative models, we hypothesize that these expressive models can
effectively capture intricate cloth configurations and deformation patterns
from data. Therefore, we propose a diffusion-based generative approach for both
perception and dynamics modeling. Specifically, we formulate state estimation
as reconstructing full cloth states from partial observations and dynamics
modeling as predicting future states given the current state and robot actions.
Leveraging a transformer-based diffusion model, our method achieves accurate
state reconstruction and reduces long-horizon dynamics prediction errors by an
order of magnitude compared to prior approaches. We integrate our dynamics
models with model predictive control and show that our framework enables
effective cloth folding on real robotic systems, demonstrating the potential of
generative models for deformable object manipulation under partial
observability and complex dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CoRL 2025. Project website: https://uniclothdiff.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DexFruit: Dexterous <span class="highlight-title">Manipulation</span> and Gaussian Splatting Inspection of
  Fruit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.07118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.07118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiden Swann, Alex Qiu, Matthew Strong, Angelina Zhang, Samuel Morstein, Kai Rayle, Monroe Kennedy III
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DexFruit is a robotic manipulation framework that enables gentle, autonomous
handling of fragile fruit and precise evaluation of damage. Many fruits are
fragile and prone to bruising, thus requiring humans to manually harvest them
with care. In this work, we demonstrate by using optical tactile sensing,
autonomous manipulation of fruit with minimal damage can be achieved. We show
that our tactile informed diffusion policies outperform baselines in both
reduced bruising and pick-and-place success rate across three fruits:
strawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,
a novel technique to represent and quantify visual damage in high-resolution 3D
representation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring
damage lack quantitative rigor or require expensive equipment. With FruitSplat,
we distill a 2D strawberry mask as well as a 2D bruise segmentation mask into
the 3DGS representation. Furthermore, this representation is modular and
general, compatible with any relevant 2D model. Overall, we demonstrate a 92%
grasping policy success rate, up to a 20% reduction in visual bruising, and up
to an 31% improvement in grasp success rate on challenging fruit compared to
our baselines across our three tested fruits. We rigorously evaluate this
result with over 630 trials. Please checkout our website at
https://dex-fruit.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Energy-Aware Lane <span class="highlight-title">Planning</span> for Connected Electric Vehicles in Urban
  Traffic: Design and Vehicle-in-the-Loop Validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.23228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.23228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansung Kim, Eric Yongkeun Choi, Eunhyek Joa, Hotae Lee, Linda Lim, Scott Moura, Francesco Borrelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban driving with connected and automated vehicles (CAVs) offers potential
for energy savings, yet most eco-driving strategies focus solely on
longitudinal speed control within a single lane. This neglects the significant
impact of lateral decisions, such as lane changes, on overall energy
efficiency, especially in environments with traffic signals and heterogeneous
traffic flow. To address this gap, we propose a novel energy-aware motion
planning framework that jointly optimizes longitudinal speed and lateral
lane-change decisions using vehicle-to-infrastructure (V2I) communication. Our
approach estimates long-term energy costs using a graph-based approximation and
solves short-horizon optimal control problems under traffic constraints. Using
a data-driven energy model calibrated to an actual battery electric vehicle, we
demonstrate with vehicle-in-the-loop experiments that our method reduces motion
energy consumption by up to 24 percent compared to a human driver, highlighting
the potential of connectivity-enabled planning for sustainable urban autonomy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2025 IEEE Conference on Decision and Control (CDC25')</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Physics-Based Continuum Model for Versatile, Scalable, and Fast
  Terramechanics Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.05643v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.05643v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huzaifa Unjhawala, Luning Bakke, Harry Zhang, Michael Taylor, Ganesh Arivoli, Radu Serban, Dan Negrut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper discusses Chrono's Continuous Representation Model (called herein
Chrono::CRM), a general-purpose, scalable, and efficient simulation solution
for terramechanics problems. Built on Chrono's Smoothed Particle Hydrodynamics
(SPH) framework, Chrono::CRM moves beyond semi-empirical terramechanics
approaches, e.g., Bekker-Wong/Janosi-Hanamoto, to provide a physics-based model
able to address complex tasks such as digging, grading, as well as interaction
with deformable wheels and complex grouser/lug patterns. The terramechanics
model is versatile in that it allows the terrain to interact with both rigid
and flexible implements simulated via the Chrono dynamics engine. We validate
Chrono::CRM against experimental data from three physical tests, including one
involving NASA's MGRU3 rover. In addition, the simulator is benchmarked against
a high-fidelity Discrete Element Method (DEM) simulation of a digging scenario
involving the Regolith Advanced Surface Systems Operations Robot (RASSOR).
Being GPU-accelerated, Chrono::CRM achieves computational efficiency comparable
to that of semi-empirical simulation approaches for terramechanics problems.
Through an ``active domains'' implementation, Chrono::CRM can handle terrain
stretches up to 10 km long with 100 million SPH particles at near interactive
rates, making high-fidelity off-road simulations at large scales feasible. As a
component of the Chrono package, the CRM model is open source and released
under a BSD-3 license. All models and simulations used in this contribution are
available in a public GitHub repository for reproducibility studies and further
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 21 figures, Submitted to Journal of Terramechanics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COBRA-PPM: A Causal Bayesian Reasoning Architecture Using Probabilistic
  Programming for <span class="highlight-title">Robot</span> <span class="highlight-title">Manipulation</span> Under Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.14488v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.14488v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Cannizzaro, Michael Groom, Jonathan Routley, Robert Osazuwa Ness, Lars Kunze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulation tasks require robots to reason about cause and effect when
interacting with objects. Yet, many data-driven approaches lack causal
semantics and thus only consider correlations. We introduce COBRA-PPM, a novel
causal Bayesian reasoning architecture that combines causal Bayesian networks
and probabilistic programming to perform interventional inference for robot
manipulation under uncertainty. We demonstrate its capabilities through
high-fidelity Gazebo-based experiments on an exemplar block stacking task,
where it predicts manipulation outcomes with high accuracy (Pred Acc: 88.6%)
and performs greedy next-best action selection with a 94.2% task success rate.
We further demonstrate sim2real transfer on a domestic robot, showing
effectiveness in handling real-world uncertainty from sensor noise and
stochastic actions. Our generalised and extensible framework supports a wide
range of manipulation scenarios and lays a foundation for future work at the
intersection of robotics and causality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, accepted to the 2025 IEEE European Conference on
  Mobile Robots (ECMR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Centralization vs. decentralization in multi-<span class="highlight-title">robot</span> sweep coverage with
  ground <span class="highlight-title">robots</span> and UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06553v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06553v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryo Jamshidpey, Mostafa Wahby, Michael Allwright, Weixu Zhu, Marco Dorigo, Mary Katherine Heinrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In swarm robotics, decentralized control is often proposed as a more scalable
and fault-tolerant alternative to centralized control. However, centralized
behaviors are often faster and more efficient than their decentralized
counterparts. In any given application, the goals and constraints of the task
being solved should guide the choice to use centralized control, decentralized
control, or a combination of the two. Currently, the exact trade-offs that
exist between centralization and decentralization are not well defined. In this
paper, we compare the performance of centralization and decentralization in the
example task of sweep coverage, across five different types of multi-robot
control structures: random walk, decentralized with beacons, hybrid formation
control using self-organizing hierarchy, centralized formation control, and
predetermined. In all five approaches, the coverage task is completed by a
group of ground robots. In each approach, except for the random walk, the
ground robots are assisted by UAVs, acting as supervisors or beacons. We
compare the approaches in terms of three performance metrics for which
centralized approaches are expected to have an advantage -- coverage
completeness, coverage uniformity, and sweep completion time -- and two metrics
for which decentralized approaches are expected to have an advantage --
scalability (4, 8, or 16 ground robots) and fault tolerance (0%, 25%, 50%, or
75% ground robot failure).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IRIDIA, Universite Libre de Bruxelles, Brussels, Belgium, 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Merging and Disentangling Views in <span class="highlight-title">Visual</span> Reinforcement <span class="highlight-title">Learning</span> for
  <span class="highlight-title">Robot</span>ic <span class="highlight-title">Manipulation</span> <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.04619v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.04619v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdulaziz Almuzairee, Rohan Patil, Dwait Bhatt, Henrik I. Christensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision is well-known for its use in manipulation, especially using visual
servoing. Due to the 3D nature of the world, using multiple camera views and
merging them creates better representations for Q-learning and in turn, trains
more sample efficient policies. Nevertheless, these multi-view policies are
sensitive to failing cameras and can be burdensome to deploy. To mitigate these
issues, we introduce a Merge And Disentanglement (MAD) algorithm that
efficiently merges views to increase sample efficiency while simultaneously
disentangling views by augmenting multi-view feature inputs with single-view
features. This produces robust policies and allows lightweight deployment. We
demonstrate the efficiency and robustness of our approach using Meta-World and
ManiSkill3. For project website and code, see https://aalmuzairee.github.io/mad
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CoRL 2025. For project website and code, see
  https://aalmuzairee.github.io/mad</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ontology Neural Network and ORTSF: A Framework for Topological Reasoning
  and Delay-<span class="highlight-title">Robust</span> Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2506.19277v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2506.19277v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehong Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of autonomous robotic systems has led to impressive
capabilities in perception, localization, mapping, and control. Yet, a
fundamental gap remains: existing frameworks excel at geometric reasoning and
dynamic stability but fall short in representing and preserving relational
semantics, contextual reasoning, and cognitive transparency essential for
collaboration in dynamic, human-centric environments. This paper introduces a
unified architecture comprising the Ontology Neural Network (ONN) and the
Ontological Real-Time Semantic Fabric (ORTSF) to address this gap. The ONN
formalizes relational semantic reasoning as a dynamic topological process. By
embedding Forman-Ricci curvature, persistent homology, and semantic tensor
structures within a unified loss formulation, ONN ensures that relational
integrity and topological coherence are preserved as scenes evolve over time.
The ORTSF transforms reasoning traces into actionable control commands while
compensating for system delays. It integrates predictive and delay-aware
operators that ensure phase margin preservation and continuity of control
signals, even under significant latency conditions. Empirical studies
demonstrate the ONN + ORTSF framework's ability to unify semantic cognition and
robust control, providing a mathematically principled and practically viable
solution for cognitive robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, includes theoretical proofs and simulation
  results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge in multi-<span class="highlight-title">robot</span> systems: an interplay of <span class="highlight-title">dynamic</span>s, computation
  and communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giorgio Cignarale, Stephan Felber, Eric Goubault, Bernardo Hummes Flores, Hugo Rincon Galeana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we provide a framework integrating distributed multi-robot
systems and temporal epistemic logic. We show that continuous-discrete hybrid
systems are compatible with logical models of knowledge already used in
distributed computing, and demonstrate its usefulness by deriving sufficient
epistemic conditions for exploration and gathering robot tasks to be solvable.
We provide a separation of the physical and computational aspects of a robotic
system, allowing us to decouple the problems related to each and directly use
methods from control theory and distributed computing, fields that are
traditionally distant in the literature. Finally, we demonstrate a novel
approach for reasoning about the knowledge in multi-robot systems through a
principled method of converting a switched hybrid dynamical system into a
temporal-epistemic logic model, passing through an abstract state machine
representation. This creates space for methods and results to be exchanged
across the fields of control theory, distributed computing and
temporal-epistemic logic, while reasoning about multi-robot systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoRI: Communication of <span class="highlight-title">Robot</span> Intent for Physical Human-<span class="highlight-title">Robot</span> Interaction <span class="chip">CoRL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.20537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.20537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junxiang Wang, Emek Barış Küçüktabak, Rana Soltani Zarrin, Zackory Erickson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clear communication of robot intent fosters transparency and interpretability
in physical human-robot interaction (pHRI), particularly during assistive tasks
involving direct human-robot contact. We introduce CoRI, a pipeline that
automatically generates natural language communication of a robot's upcoming
actions directly from its motion plan and visual perception. Our pipeline first
processes the robot's image view to identify human poses and key environmental
features. It then encodes the planned 3D spatial trajectory (including velocity
and force) onto this view, visually grounding the path and its dynamics. CoRI
queries a vision-language model with this visual representation to interpret
the planned action within the visual context before generating concise,
user-directed statements, without relying on task-specific information. Results
from a user study involving robot-assisted feeding, bathing, and shaving tasks
across two different robots indicate that CoRI leads to statistically
significant difference in communication clarity compared to a baseline
communication strategy. Specifically, CoRI effectively conveys not only the
robot's high-level intentions but also crucial details about its motion and any
collaborative user action needed. Video and code of our project can be found on
our project website: https://cori-phri.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proceedings of the 9th Conference on Robot
  Learning (CoRL). 34 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Traversing the Narrow Path: A Two-Stage Reinforcement <span class="highlight-title">Learning</span> Framework
  for <span class="highlight-title">Humanoid</span> Beam <span class="highlight-title">Walk</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        TianChen Huang, Wei Gao, Runchen Xu, Shiwu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traversing narrow beams is challenging for humanoids due to sparse,
safety-critical contacts and the fragility of purely learned policies. We
propose a physically grounded, two-stage framework that couples an XCoM/LIPM
footstep template with a lightweight residual planner and a simple low-level
tracker. Stage-1 is trained on flat ground: the tracker learns to robustly
follow footstep targets by adding small random perturbations to heuristic
footsteps, without any hand-crafted centerline locking, so it acquires stable
contact scheduling and strong target-tracking robustness. Stage-2 is trained in
simulation on a beam: a high-level planner predicts a body-frame residual
(Delta x, Delta y, Delta psi) for the swing foot only, refining the template
step to prioritize safe, precise placement under narrow support while
preserving interpretability. To ease deployment, sensing is kept minimal and
consistent between simulation and hardware: the planner consumes compact,
forward-facing elevation cues together with onboard IMU and joint signals. On a
Unitree G1, our system reliably traverses a 0.2 m-wide, 3 m-long beam. Across
simulation and real-world studies, residual refinement consistently outperforms
template-only and monolithic baselines in success rate, centerline adherence,
and safety margins, while the structured footstep interface enables transparent
analysis and low-friction sim-to-real transfer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website:
  https://huangtc233.github.io/Traversing-the-Narrow-Path/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Soft <span class="highlight-title">Manipulation</span> Surface With Reduced Actuator Density For
  Heterogeneous Object <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14290v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14290v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratik Ingle, Kasper Støy, Andres Faiña
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object manipulation in robotics faces challenges due to diverse object
shapes, sizes, and fragility. Gripper-based methods offer precision and low
degrees of freedom (DOF) but the gripper limits the kind of objects to grasp.
On the other hand, surface-based approaches provide flexibility for handling
fragile and heterogeneous objects but require numerous actuators, increasing
complexity. We propose new manipulation hardware that utilizes equally spaced
linear actuators placed vertically and connected by a soft surface. In this
setup, object manipulation occurs on the soft surface through coordinated
movements of the surrounding actuators. This approach requires fewer actuators
to cover a large manipulation area, offering a cost-effective solution with a
lower DOF compared to dense actuator arrays. It also effectively handles
heterogeneous objects of varying shapes and weights, even when they are
significantly smaller than the distance between actuators. This method is
particularly suitable for managing highly fragile objects in the food industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Path Planner with Adaptive Safety and Optimality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.23197v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.23197v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jatin Kumar Arora, Soutrik Bandyopadhyay, Shubhendu Bhasin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path planning for autonomous robots presents a fundamental trade-off between
optimality and safety. While conventional algorithms typically prioritize one
of these objectives, we introduce the Unified Path Planner (UPP), a unified
framework that simultaneously addresses both. UPP is a graph-search-based
algorithm that employs a modified heuristic function incorporating a dynamic
safety cost, enabling an adaptive balance between path length and obstacle
clearance. We establish theoretical sub-optimality bounds for the planner and
demonstrate that its safety-to-optimality ratio can be tuned via adjustable
parameters, with a trade-off in computational complexity. Extensive simulations
show that UPP achieves a high success rate, generating near-optimal paths with
only a negligible increase in cost over traditional A*, while ensuring safety
margins that closely approach those of the classical Voronoi planner. Finally,
the practical efficacy of UPP is validated through a hardware implementation on
a TurtleBot, confirming its ability to navigate cluttered environments by
generating safe, sub-optimal paths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages,4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Adaptive Planner for <span class="highlight-title">Dynamic</span> <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghun Noh, Deqian Kong, Minglu Zhao, Andrew Lizarraga, Jianwen Xie, Ying Nian Wu, Dennis Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Latent Adaptive Planner (LAP), a trajectory-level
latent-variable policy for dynamic nonprehensile manipulation (e.g., box
catching) that formulates planning as inference in a low-dimensional latent
space and is learned effectively from human demonstration videos. During
execution, LAP achieves real-time adaptation by maintaining a posterior over
the latent plan and performing variational replanning as new observations
arrive. To bridge the embodiment gap between humans and robots, we introduce a
model-based proportional mapping that regenerates accurate kinematic-dynamic
joint states and object positions from human demonstrations. Through
challenging box catching experiments with varying object properties, LAP
demonstrates superior success rates, trajectory smoothness, and energy
efficiency by learning human-like compliant motions and adaptive behaviors.
Overall, LAP enables dynamic manipulation with real-time adaptation and
successfully transfer across heterogeneous robot platforms using the same human
demonstration videos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SignLoc: <span class="highlight-title">Robust</span> <span class="highlight-title">Localization</span> using <span class="highlight-title">Navigation</span> Signs and Public Maps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.18606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.18606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicky Zimmerman, Joel Loo, Ayush Agrawal, David Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigation signs and maps, such as floor plans and street maps, are widely
available and serve as ubiquitous aids for way-finding in human environments.
Yet, they are rarely used by robot systems. This paper presents SignLoc, a
global localization method that leverages navigation signs to localize the
robot on publicly available maps -- specifically floor plans and OpenStreetMap
(OSM) graphs -- without prior sensor-based mapping. SignLoc first extracts a
navigation graph from the input map. It then employs a probabilistic
observation model to match directional and locational cues from the detected
signs to the graph, enabling robust topo-semantic localization within a Monte
Carlo framework. We evaluated SignLoc in diverse large-scale environments: part
of a university campus, a shopping mall, and a hospital complex. Experimental
results show that SignLoc reliably localizes the robot after observing only one
to two signs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Visual</span> Imitation Enables Contextual <span class="highlight-title">Humanoid</span> Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.03729v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.03729v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Allshire, Hongsuk Choi, Junyi Zhang, David McAllister, Anthony Zhang, Chung Min Kim, Trevor Darrell, Pieter Abbeel, Jitendra Malik, Angjoo Kanazawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can we teach humanoids to climb staircases and sit on chairs using the
surrounding environment context? Arguably, the simplest way is to just show
them-casually capture a human motion video and feed it to humanoids. We
introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday
videos, jointly reconstructs the humans and the environment, and produces
whole-body control policies for humanoid robots that perform the corresponding
skills. We demonstrate the results of our pipeline on real humanoid robots,
showing robust, repeatable contextual control such as staircase ascents and
descents, sitting and standing from chairs and benches, as well as other
dynamic whole-body skills-all from a single policy, conditioned on the
environment and global root commands. VIDEOMIMIC offers a scalable path towards
teaching humanoids to operate in diverse real-world environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://www.videomimic.net/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QuaDreamer: Controllable Panoramic Video Generation for Quadruped <span class="highlight-title">Robots</span> <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.02512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.02512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Wu, Fei Teng, Hao Shi, Qi Jiang, Kai Luo, Kaiwei Wang, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoramic cameras, capturing comprehensive 360-degree environmental data, are
suitable for quadruped robots in surrounding perception and interaction with
complex environments. However, the scarcity of high-quality panoramic training
data-caused by inherent kinematic constraints and complex sensor calibration
challenges-fundamentally limits the development of robust perception systems
tailored to these embodied platforms. To address this issue, we propose
QuaDreamer-the first panoramic data generation engine specifically designed for
quadruped robots. QuaDreamer focuses on mimicking the motion paradigm of
quadruped robots to generate highly controllable, realistic panoramic videos,
providing a data source for downstream tasks. Specifically, to effectively
capture the unique vertical vibration characteristics exhibited during
quadruped locomotion, we introduce Vertical Jitter Encoding (VJE). VJE extracts
controllable vertical signals through frequency-domain feature filtering and
provides high-quality prompts. To facilitate high-quality panoramic video
generation under jitter signal control, we propose a Scene-Object Controller
(SOC) that effectively manages object motion and boosts background jitter
control through the attention mechanism. To address panoramic distortions in
wide-FoV video generation, we propose the Panoramic Enhancer (PE)-a dual-stream
architecture that synergizes frequency-texture refinement for local detail
enhancement with spatial-structure correction for global geometric consistency.
We further demonstrate that the generated video sequences can serve as training
data for the quadruped robot's panoramic visual perception model, enhancing the
performance of multi-object tracking in 360-degree scenes. The source code and
model weights will be publicly available at
https://github.com/losehu/QuaDreamer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CoRL 2025. The source code and model weights will be
  publicly available at \url{https://github.com/losehu/QuaDreamer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UltraTac: Integrated Ultrasound-Augmented Visuotactile Sensor for
  Enhanced <span class="highlight-title">Robot</span>ic Perception <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Gong, Kit-Wa Sou, Shoujie Li, Changqing Guo, Yan Huang, Chuqiao Lyu, Ziwu Song, Wenbo Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visuotactile sensors provide high-resolution tactile information but are
incapable of perceiving the material features of objects. We present UltraTac,
an integrated sensor that combines visuotactile imaging with ultrasound sensing
through a coaxial optoacoustic architecture. The design shares structural
components and achieves consistent sensing regions for both modalities.
Additionally, we incorporate acoustic matching into the traditional
visuotactile sensor structure, enabling integration of the ultrasound sensing
modality without compromising visuotactile performance. Through tactile
feedback, we dynamically adjust the operating state of the ultrasound module to
achieve flexible functional coordination. Systematic experiments demonstrate
three key capabilities: proximity sensing in the 3-8 cm range ($R^2=0.90$),
material classification (average accuracy: 99.20%), and texture-material
dual-mode object recognition achieving 92.11% accuracy on a 15-class task.
Finally, we integrate the sensor into a robotic manipulation system to
concurrently detect container surface patterns and internal content, which
verifies its potential for advanced human-machine interaction and precise
robotic manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Towards Embodiment Scaling Laws in <span class="highlight-title">Robot</span> <span class="highlight-title">Locomotion</span> <span class="chip">CoRL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.05753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.05753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Ai, Liu Dai, Nico Bohlinger, Dichen Li, Tongzhou Mu, Zhanxin Wu, K. Fay, Henrik I. Christensen, Jan Peters, <span class="highlight-author">Hao Su</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-embodiment generalization underpins the vision of building generalist
embodied agents for any robot, yet its enabling factors remain poorly
understood. We investigate embodiment scaling laws, the hypothesis that
increasing the number of training embodiments improves generalization to unseen
ones, using robot locomotion as a test bed. We procedurally generate ~1,000
embodiments with topological, geometric, and joint-level kinematic variations,
and train policies on random subsets. We observe positive scaling trends
supporting the hypothesis, and find that embodiment scaling enables
substantially broader generalization than data scaling on fixed embodiments.
Our best policy, trained on the full dataset, transfers zero-shot to novel
embodiments in simulation and the real world, including the Unitree Go2 and H1.
These results represent a step toward general embodied intelligence, with
relevance to adaptive control for configurable robots, morphology co-design,
and beyond.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Robot Learning (CoRL), 2025. Project website:
  https://embodiment-scaling-laws.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Motion Priors Reimagined: Adapting Flat-<span class="highlight-title">Terrain</span> Skills for Complex
  Quadruped Mobility <span class="chip">CoRL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2505.16084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2505.16084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zewei Zhang, Chenhao Li, Takahiro Miki, <span class="highlight-author">Marco Hutter</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL)-based motion imitation methods trained on
demonstration data can effectively learn natural and expressive motions with
minimal reward engineering but often struggle to generalize to novel
environments. We address this by proposing a hierarchical RL framework in which
a low-level policy is first pre-trained to imitate animal motions on flat
ground, thereby establishing motion priors. A subsequent high-level,
goal-conditioned policy then builds on these priors, learning residual
corrections that enable perceptive locomotion, local obstacle avoidance, and
goal-directed navigation across diverse and rugged terrains. Simulation
experiments illustrate the effectiveness of learned residuals in adapting to
progressively challenging uneven terrains while still preserving the locomotion
characteristics provided by the motion priors. Furthermore, our results
demonstrate improvements in motion regularization over baseline models trained
without motion priors under similar reward setups. Real-world experiments with
an ANYmal-D quadruped robot confirm our policy's capability to generalize
animal-like locomotion skills to complex terrains, demonstrating smooth and
efficient locomotion and local navigation performance amidst challenging
terrains with obstacles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference on Robot Learning (CoRL)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-08-28T00:00:00Z">2025-08-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Remarks on stochastic cloning and delayed-state filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21260v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21260v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tara Mina, Lindsey Marinello, John Christian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many estimation problems in robotics and navigation involve measurements that
depend on prior states. A prominent example is odometry, which measures the
relative change between states over time. Accurately handling these
delayed-state measurements requires capturing their correlations with prior
state estimates, and a widely used approach is stochastic cloning (SC), which
augments the state vector to account for these correlations.
  This work revisits a long-established but often overlooked alternative--the
delayed-state Kalman filter--and demonstrates that a properly derived filter
yields exactly the same state and covariance update as SC, without requiring
state augmentation. Moreover, the generalized Kalman filter formulation
provides computational advantages, while also reducing memory requirements for
higher-dimensional states.
  Our findings clarify a common misconception that Kalman filter variants are
inherently unable to handle correlated delayed-state measurements,
demonstrating that an alternative formulation achieves the same results more
efficiently.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty-Aware Ankle Exoskeleton Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatima Mumtaza Tourk, Bishoy Galoaa, Sanat Shajan, Aaron J. Young, Michael Everett, Max K. Shepherd
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lower limb exoskeletons show promise to assist human movement, but their
utility is limited by controllers designed for discrete, predefined actions in
controlled environments, restricting their real-world applicability. We present
an uncertainty-aware control framework that enables ankle exoskeletons to
operate safely across diverse scenarios by automatically disengaging when
encountering unfamiliar movements. Our approach uses an uncertainty estimator
to classify movements as similar (in-distribution) or different
(out-of-distribution) relative to actions in the training set. We evaluated
three architectures (model ensembles, autoencoders, and generative adversarial
networks) on an offline dataset and tested the strongest performing
architecture (ensemble of gait phase estimators) online. The online test
demonstrated the ability of our uncertainty estimator to turn assistance on and
off as the user transitioned between in-distribution and out-of-distribution
tasks (F1: 89.2). This new framework provides a path for exoskeletons to safely
and autonomously support human movement in unstructured, everyday environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-<span class="highlight-title">robot</span> Path <span class="highlight-title">Planning</span> and Scheduling via Model Predictive Optimal
  Transport (<span class="highlight-title">MPC</span>-OT) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Usman A. Khan, Mouhacine Benosman, Wenliang Liu, Federico Pecora, Joseph W. Durham
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel methodology for path planning and
scheduling for multi-robot navigation that is based on optimal transport theory
and model predictive control. We consider a setup where $N$ robots are tasked
to navigate to $M$ targets in a common space with obstacles. Mapping robots to
targets first and then planning paths can result in overlapping paths that lead
to deadlocks. We derive a strategy based on optimal transport that not only
provides minimum cost paths from robots to targets but also guarantees
non-overlapping trajectories. We achieve this by discretizing the space of
interest into $K$ cells and by imposing a ${K\times K}$ cost structure that
describes the cost of transitioning from one cell to another. Optimal transport
then provides \textit{optimal and non-overlapping} cell transitions for the
robots to reach the targets that can be readily deployed without any scheduling
considerations. The proposed solution requires $\unicode{x1D4AA}(K^3\log K)$
computations in the worst-case and $\unicode{x1D4AA}(K^2\log K)$ for
well-behaved problems. To further accommodate potentially overlapping
trajectories (unavoidable in certain situations) as well as robot dynamics, we
show that a temporal structure can be integrated into optimal transport with
the help of \textit{replans} and \textit{model predictive control}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE Conference on Decision and Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Observer Design for Optical Flow-Based <span class="highlight-title">Visual</span>-<span class="highlight-title">Inertial</span> Odometry with
  Almost-Global Convergence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tarek Bouazza, Soulaimane Berkane, Minh-Duc Hua, Tarek Hamel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel cascaded observer architecture that combines
optical flow and IMU measurements to perform continuous monocular
visual-inertial odometry (VIO). The proposed solution estimates body-frame
velocity and gravity direction simultaneously by fusing velocity direction
information from optical flow measurements with gyro and accelerometer data.
This fusion is achieved using a globally exponentially stable Riccati observer,
which operates under persistently exciting translational motion conditions. The
estimated gravity direction in the body frame is then employed, along with an
optional magnetometer measurement, to design a complementary observer on
$\mathbf{SO}(3)$ for attitude estimation. The resulting interconnected observer
architecture is shown to be almost globally asymptotically stable. To extract
the velocity direction from sparse optical flow data, a gradient descent
algorithm is developed to solve a constrained minimization problem on the unit
sphere. The effectiveness of the proposed algorithms is validated through
simulation results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures. To appear in IEEE CDC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comparative Study of Spline-Based Trajectory Reconstruction Methods
  Across Varying Automatic Vehicle Location Data Densities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2509.00119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2509.00119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jake Robbennolt, Sirajum Munira, Stephen D. Boyles
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic vehicle location (AVL) data offers insights into transit dynamics,
but its effectiveness is often hampered by inconsistent update frequencies,
necessitating trajectory reconstruction. This research evaluates 13 trajectory
reconstruction methods, including several novel approaches, using
high-resolution AVL data from Austin, Texas. We examine the interplay of four
critical factors -- velocity, position, smoothing, and data density -- on
reconstruction performance. A key contribution of this study is evaluation of
these methods across sparse and dense datasets, providing insights into the
trade-off between accuracy and resource allocation. Our evaluation framework
combines traditional mathematical error metrics for positional and velocity
with practical considerations, such as physical realism (e.g., aligning
velocity and acceleration with stopped states, deceleration rates, and speed
variability). In addition, we provide insight into the relative value of each
method in calculating realistic metrics for infrastructure evaluations. Our
findings indicate that velocity-aware methods consistently outperform
position-only approaches. Interestingly, we discovered that smoothing-based
methods can degrade overall performance in complex, congested urban
environments, although enforcing monotonicity remains critical. The velocity
constrained Hermite interpolation with monotonicity enforcement (VCHIP-ME)
yields optimal results, offering a balance between high accuracy and
computational efficiency. Its minimal overhead makes it suitable for both
historical analysis and real-time applications, providing significant
predictive power when combined with dense datasets. These findings offer
practical guidance for researchers and practitioners implementing trajectory
reconstruction systems and emphasize the importance of investing in
higher-frequency AVL data collection for improved analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Learning</span> on the Fly: Rapid Policy Adaptation via Differentiable
  Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahe Pan, Jiaxu Xing, Rudolf Reiter, Yifan Zhai, Elie Aljalbout, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning control policies in simulation enables rapid, safe, and
cost-effective development of advanced robotic capabilities. However,
transferring these policies to the real world remains difficult due to the
sim-to-real gap, where unmodeled dynamics and environmental disturbances can
degrade policy performance. Existing approaches, such as domain randomization
and Real2Sim2Real pipelines, can improve policy robustness, but either struggle
under out-of-distribution conditions or require costly offline retraining. In
this work, we approach these problems from a different perspective. Instead of
relying on diverse training conditions before deployment, we focus on rapidly
adapting the learned policy in the real world in an online fashion. To achieve
this, we propose a novel online adaptive learning framework that unifies
residual dynamics learning with real-time policy adaptation inside a
differentiable simulation. Starting from a simple dynamics model, our framework
refines the model continuously with real-world data to capture unmodeled
effects and disturbances such as payload changes and wind. The refined dynamics
model is embedded in a differentiable simulation framework, enabling gradient
backpropagation through the dynamics and thus rapid, sample-efficient policy
updates beyond the reach of classical RL methods like PPO. All components of
our system are designed for rapid adaptation, enabling the policy to adjust to
unseen disturbances within 5 seconds of training. We validate the approach on
agile quadrotor control under various disturbances in both simulation and the
real world. Our framework reduces hovering error by up to 81% compared to
L1-MPC and 55% compared to DATT, while also demonstrating robustness in
vision-based control without explicit state estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prompt-to-Product: <span class="highlight-title">Generative</span> Assembly via Bimanual <span class="highlight-title">Manipulation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixuan Liu, Philip Huang, Ava Pun, Kangle Deng, Shobhit Aggarwal, Kevin Tang, Michelle Liu, Deva Ramanan, Jun-Yan Zhu, Jiaoyang Li, Changliu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating assembly products demands significant manual effort and expert
knowledge in 1) designing the assembly and 2) constructing the product. This
paper introduces Prompt-to-Product, an automated pipeline that generates
real-world assembly products from natural language prompts. Specifically, we
leverage LEGO bricks as the assembly platform and automate the process of
creating brick assembly structures. Given the user design requirements,
Prompt-to-Product generates physically buildable brick designs, and then
leverages a bimanual robotic system to construct the real assembly products,
bringing user imaginations into the real world. We conduct a comprehensive user
study, and the results demonstrate that Prompt-to-Product significantly lowers
the barrier and reduces manual effort in creating assembly products from
imaginative ideas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cog<span class="highlight-title">VLA</span>: Cognition-Aligned <span class="highlight-title">Vision-Language-Action</span> Model via
  Instruction-Driven Routing & Sparsification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Li, Renshan Zhang, Rui Shao, Jie He, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Vision-Language-Action (VLA) models built on pre-trained
Vision-Language Models (VLMs) require extensive post-training, resulting in
high computational overhead that limits scalability and deployment.We propose
CogVLA, a Cognition-Aligned Vision-Language-Action framework that leverages
instruction-driven routing and sparsification to improve both efficiency and
performance. CogVLA draws inspiration from human multimodal coordination and
introduces a 3-stage progressive architecture. 1) Encoder-FiLM based
Aggregation Routing (EFA-Routing) injects instruction information into the
vision encoder to selectively aggregate and compress dual-stream visual tokens,
forming a instruction-aware latent representation. 2) Building upon this
compact visual encoding, LLM-FiLM based Pruning Routing (LFP-Routing)
introduces action intent into the language model by pruning
instruction-irrelevant visually grounded tokens, thereby achieving token-level
sparsity. 3) To ensure that compressed perception inputs can still support
accurate and coherent action generation, we introduce V-L-A Coupled Attention
(CAtten), which combines causal vision-language attention with bidirectional
action parallel decoding. Extensive experiments on the LIBERO benchmark and
real-world robotic tasks demonstrate that CogVLA achieves state-of-the-art
performance with success rates of 97.4% and 70.0%, respectively, while reducing
training costs by 2.5-fold and decreasing inference latency by 2.8-fold
compared to OpenVLA. CogVLA is open-sourced and publicly available at
https://github.com/JiuTian-VL/CogVLA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures, Project Page:
  https://jiutian-vl.github.io/CogVLA-page</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rapid Mismatch <span class="highlight-title">Estimation</span> via Neural Network Informed Variational
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Jaszczuk, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With robots increasingly operating in human-centric environments, ensuring
soft and safe physical interactions, whether with humans, surroundings, or
other machines, is essential. While compliant hardware can facilitate such
interactions, this work focuses on impedance controllers that allow
torque-controlled robots to safely and passively respond to contact while
accurately executing tasks. From inverse dynamics to quadratic
programming-based controllers, the effectiveness of these methods relies on
accurate dynamics models of the robot and the object it manipulates. Any model
mismatch results in task failures and unsafe behaviors. Thus, we introduce
Rapid Mismatch Estimation (RME), an adaptive, controller-agnostic,
probabilistic framework that estimates end-effector dynamics mismatches online,
without relying on external force-torque sensors. From the robot's
proprioceptive feedback, a Neural Network Model Mismatch Estimator generates a
prior for a Variational Inference solver, which rapidly converges to the
unknown parameters while quantifying uncertainty. With a real 7-DoF manipulator
driven by a state-of-the-art passive impedance controller, RME adapts to sudden
changes in mass and center of mass at the end-effector in $\sim400$ ms, in
static and dynamic settings. We demonstrate RME in a collaborative scenario
where a human attaches an unknown basket to the robot's end-effector and
dynamically adds/removes heavy items, showcasing fast and safe adaptation to
changing dynamics during physical interaction without any external sensory
system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 9th Annual Conference on Robot Learning. Project Website
  - https://mateusz-jaszczuk.github.io/rme/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Train-Once Plan-Anywhere Kino<span class="highlight-title">dynamic</span> Motion <span class="highlight-title">Planning</span> via Diffusion Trees <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.21001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.21001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaniv Hassidof, Tom Jurgenson, Kiril Solovey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kinodynamic motion planning is concerned with computing collision-free
trajectories while abiding by the robot's dynamic constraints. This critical
problem is often tackled using sampling-based planners (SBPs) that explore the
robot's high-dimensional state space by constructing a search tree via action
propagations. Although SBPs can offer global guarantees on completeness and
solution quality, their performance is often hindered by slow exploration due
to uninformed action sampling. Learning-based approaches can yield
significantly faster runtimes, yet they fail to generalize to
out-of-distribution (OOD) scenarios and lack critical guarantees, e.g., safety,
thus limiting their deployment on physical robots. We present Diffusion Tree
(DiTree): a \emph{provably-generalizable} framework leveraging diffusion
policies (DPs) as informed samplers to efficiently guide state-space search
within SBPs. DiTree combines DP's ability to model complex distributions of
expert trajectories, conditioned on local observations, with the completeness
of SBPs to yield \emph{provably-safe} solutions within a few action propagation
iterations for complex dynamical systems. We demonstrate DiTree's power with an
implementation combining the popular RRT planner with a DP action sampler
trained on a \emph{single environment}. In comprehensive evaluations on OOD
scenarios, % DiTree has comparable runtimes to a standalone DP (3x faster than
classical SBPs), while improving the average success rate over DP and SBPs.
DiTree is on average 3x faster than classical SBPs, and outperforms all other
approaches by achieving roughly 30\% higher success rate. Project webpage:
https://sites.google.com/view/ditree.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CoRL 2025. Project page:
  https://sites.google.com/view/ditree</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ActLoc: <span class="highlight-title">Learning</span> to Localize on the Move via Active Viewpoint Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20981v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20981v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajie Li, Boyang Sun, Luca Di Giammarino, Hermann Blum, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable localization is critical for robot navigation, yet most existing
systems implicitly assume that all viewing directions at a location are equally
informative. In practice, localization becomes unreliable when the robot
observes unmapped, ambiguous, or uninformative regions. To address this, we
present ActLoc, an active viewpoint-aware planning framework for enhancing
localization accuracy for general robot navigation tasks. At its core, ActLoc
employs a largescale trained attention-based model for viewpoint selection. The
model encodes a metric map and the camera poses used during map construction,
and predicts localization accuracy across yaw and pitch directions at arbitrary
3D locations. These per-point accuracy distributions are incorporated into a
path planner, enabling the robot to actively select camera orientations that
maximize localization robustness while respecting task and motion constraints.
ActLoc achieves stateof-the-art results on single-viewpoint selection and
generalizes effectively to fulltrajectory planning. Its modular design makes it
readily applicable to diverse robot navigation and inspection tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Fabric-Based Piezoresistive Sensor Arrays for <span class="highlight-title">Whole-Body</span> Tactile
  Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Curtis C. Johnson, Daniel Webb, David Hill, Marc D. Killpack
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling tactile sensing for robust whole-body manipulation is a significant
challenge, often limited by wiring complexity, data throughput, and system
reliability. This paper presents a complete architecture designed to overcome
these barriers. Our approach pairs open-source, fabric-based sensors with
custom readout electronics that reduce signal crosstalk to less than 3.3%
through hardware-based mitigation. Critically, we introduce a novel,
daisy-chained SPI bus topology that avoids the practical limitations of common
wireless protocols and the prohibitive wiring complexity of USB hub-based
systems. This architecture streams synchronized data from over 8,000 taxels
across 1 square meter of sensing area at update rates exceeding 50 FPS,
confirming its suitability for real-time control. We validate the system's
efficacy in a whole-body grasping task where, without feedback, the robot's
open-loop trajectory results in an uncontrolled application of force that
slowly crushes a deformable cardboard box. With real-time tactile feedback, the
robot transforms this motion into a gentle, stable grasp, successfully
manipulating the object without causing structural damage. This work provides a
robust and well-characterized platform to enable future research in advanced
whole-body control and physical human-robot interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In submission to IEEE Sensors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PLUME: Procedural Layer Underground Modeling Engine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Manuel Garcia, Antoine Richard, Miguel Olivares-Mendez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As space exploration advances, underground environments are becoming
increasingly attractive due to their potential to provide shelter, easier
access to resources, and enhanced scientific opportunities. Although such
environments exist on Earth, they are often not easily accessible and do not
accurately represent the diversity of underground environments found throughout
the solar system. This paper presents PLUME, a procedural generation framework
aimed at easily creating 3D underground environments. Its flexible structure
allows for the continuous enhancement of various underground features, aligning
with our expanding understanding of the solar system. The environments
generated using PLUME can be used for AI training, evaluating robotics
algorithms, 3D rendering, and facilitating rapid iteration on developed
exploration algorithms. In this paper, it is demonstrated that PLUME has been
used along with a robotic simulator. PLUME is open source and has been released
on Github. https://github.com/Gabryss/P.L.U.M.E
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COMETH: Convex <span class="highlight-title">Optimization</span> for Multiview <span class="highlight-title">Estimation</span> and Tracking of
  Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrico Martini, Ho Jin Choi, Nadia Figueroa, Nicola Bombieri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of Industry 5.0, monitoring human activity is essential for
ensuring both ergonomic safety and overall well-being. While multi-camera
centralized setups improve pose estimation accuracy, they often suffer from
high computational costs and bandwidth requirements, limiting scalability and
real-time applicability. Distributing processing across edge devices can reduce
network bandwidth and computational load. On the other hand, the constrained
resources of edge devices lead to accuracy degradation, and the distribution of
computation leads to temporal and spatial inconsistencies. We address this
challenge by proposing COMETH (Convex Optimization for Multiview Estimation and
Tracking of Humans), a lightweight algorithm for real-time multi-view human
pose fusion that relies on three concepts: it integrates kinematic and
biomechanical constraints to increase the joint positioning accuracy; it
employs convex optimization-based inverse kinematics for spatial fusion; and it
implements a state observer to improve temporal consistency. We evaluate COMETH
on both public and industrial datasets, where it outperforms state-of-the-art
methods in localization, detection, and tracking accuracy. The proposed fusion
pipeline enables accurate and scalable human motion tracking, making it
well-suited for industrial and safety-critical applications. The code is
publicly available at https://github.com/PARCO-LAB/COMETH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Information Fusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language-Enhanced Mobile <span class="highlight-title">Manipulation</span> for Efficient Object Search in
  Indoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.20899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.20899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liding Zhang, Zeqi Li, Kuanqi Cai, Qian Huang, Zhenshan Bing, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enabling robots to efficiently search for and identify objects in complex,
unstructured environments is critical for diverse applications ranging from
household assistance to industrial automation. However, traditional scene
representations typically capture only static semantics and lack interpretable
contextual reasoning, limiting their ability to guide object search in
completely unfamiliar settings. To address this challenge, we propose a
language-enhanced hierarchical navigation framework that tightly integrates
semantic perception and spatial reasoning. Our method, Goal-Oriented
Dynamically Heuristic-Guided Hierarchical Search (GODHS), leverages large
language models (LLMs) to infer scene semantics and guide the search process
through a multi-level decision hierarchy. Reliability in reasoning is achieved
through the use of structured prompts and logical constraints applied at each
stage of the hierarchy. For the specific challenges of mobile manipulation, we
introduce a heuristic-based motion planner that combines polar angle sorting
with distance prioritization to efficiently generate exploration paths.
Comprehensive evaluations in Isaac Sim demonstrate the feasibility of our
framework, showing that GODHS can locate target objects with higher search
efficiency compared to conventional, non-semantic search strategies. Website
and Video are available at: https://drapandiger.github.io/GODHS
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocoMamba: <span class="highlight-title">Vision</span>-Driven <span class="highlight-title">Locomotion</span> via End-to-End Deep Reinforcement
  <span class="highlight-title">Learning</span> with Mamba 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.11849v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.11849v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yinuo Wang, Gavin Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LocoMamba, a vision-driven cross-modal DRL framework built on
selective state-space models, specifically leveraging Mamba, that achieves
near-linear-time sequence modeling, effectively captures long-range
dependencies, and enables efficient training with longer sequences. First, we
embed proprioceptive states with a multilayer perceptron and patchify depth
images with a lightweight convolutional neural network, producing compact
tokens that improve state representation. Second, stacked Mamba layers fuse
these tokens via near-linear-time selective scanning, reducing latency and
memory footprint, remaining robust to token length and image resolution, and
providing an inductive bias that mitigates overfitting. Third, we train the
policy end-to-end with Proximal Policy Optimization under terrain and
appearance randomization and an obstacle-density curriculum, using a compact
state-centric reward that balances progress, smoothness, and safety. We
evaluate our method in challenging simulated environments with static and
moving obstacles as well as uneven terrain. Compared with state-of-the-art
baselines, our method achieves higher returns and success rates with fewer
collisions, exhibits stronger generalization to unseen terrains and obstacle
densities, and improves training efficiency by converging in fewer updates
under the same compute budget.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pellet-based 3D Printing of Soft Thermoplastic Elastomeric Membranes for
  Soft <span class="highlight-title">Robot</span>ic Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.20957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.20957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nick Willemstein, Mohammad Ebrahim Imanian, Herman van der Kooij, Ali Sadeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Additive Manufacturing (AM) is a promising solution for handling the
complexity of fabricating soft robots. However, the AM of hyperelastic
materials is still challenging with a limited material range. Within this work,
pellet-based 3D printing of very soft thermoplastic elastomers (TPEs) was
explored (down to Shore Hardness 00-30). Our results show that TPEs can have
similar engineering stress and maximum elongation as Ecoflex OO-10. In
addition, we 3D-printed airtight thin membranes (0.2-1.2 mm), which could
inflate up to a stretch of 1320%. Combining the membrane's large expansion and
softness with the 3D printing of hollow structures simplified the design of a
bending actuator that can bend 180 degrees and reach a blocked force of 238
times its weight. In addition, by 3D printing TPE pellets and rigid filaments,
the soft membrane could grasp objects by enveloping an object or as a
sensorized sucker, which relied on the TPE's softness to conform to the object
or act as a seal. In addition, the membrane of the sucker acted as a tactile
sensor to detect an object before adhesion. These results suggest the
feasibility of AM of soft robots using soft TPEs and membranes as a promising
class of materials and sensorized actuators, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PUB: A Plasma-Propelled Ultra-Quiet Blimp with Two-DOF Vector Thrusting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.12395v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.12395v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study presents the design and control of a Plasma-propelled
Ultra-silence Blimp (PUB), a novel aerial robot employing plasma vector
propulsion for ultra-quiet flight without mechanical propellers. The system
utilizes a helium-lift platform for extended endurance and a four-layer ring
asymmetric capacitor to generate ionic wind thrust. The modular propulsion
units allow flexible configuration to meet mission-specific requirements, while
a two-degree-of-freedom (DOF) head enables thrust vector control. A closed-loop
slip control scheme is implemented for stable maneuvering. Flight experiments
demonstrate full-envelope capability, including take-off, climb, hover,
descent, and smooth landing, confirming the feasibility of plasma vector
propulsion, the effectiveness of DOF vector control, and the stability of the
control system. Owing to its low acoustic signature, structural simplicity, and
high maneuverability, PUB is well suited for noise-sensitive, enclosed, and
near-space applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Multi-critic <span class="highlight-title">Learning</span> for <span class="highlight-title">Whole-body</span> End-effector Twist Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2507.08656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2507.08656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aravind Elanjimattathil Vijayan, Andrei Cramariuc, Mattia Risiglione, Christian Gehring, <span class="highlight-author">Marco Hutter</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning whole-body control for locomotion and arm motions in a single policy
has challenges, as the two tasks have conflicting goals. For instance,
efficient locomotion typically favors a horizontal base orientation, while
end-effector tracking may benefit from base tilting to extend reachability.
Additionally, current Reinforcement Learning (RL) approaches using a pose-based
task specification lack the ability to directly control the end-effector
velocity, making smoothly executing trajectories very challenging. To address
these limitations, we propose an RL-based framework that allows for dynamic,
velocity-aware whole-body end-effector control. Our method introduces a
multi-critic actor architecture that decouples the reward signals for
locomotion and manipulation, simplifying reward tuning and allowing the policy
to resolve task conflicts more effectively. Furthermore, we design a
twist-based end-effector task formulation that can track both discrete poses
and motion trajectories. We validate our approach through a set of simulation
and hardware experiments using a quadruped robot equipped with a robotic arm.
The resulting controller can simultaneously walk and move its end-effector and
shows emergent whole-body behaviors, where the base assists the arm in
extending the workspace, despite a lack of explicit formulations. Videos and
supplementary material can be found at multi-critic-locomanipulation.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Divide, Discover, Deploy: Factorized Skill <span class="highlight-title">Learning</span> with Symmetry and
  Style Priors <span class="chip">CoRL 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2508.19953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2508.19953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Cathomen, Mayank Mittal, Marin Vlastelica, <span class="highlight-author">Marco Hutter</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Skill Discovery (USD) allows agents to autonomously learn
diverse behaviors without task-specific rewards. While recent USD methods have
shown promise, their application to real-world robotics remains underexplored.
In this paper, we propose a modular USD framework to address the challenges in
the safety, interpretability, and deployability of the learned skills. Our
approach employs user-defined factorization of the state space to learn
disentangled skill representations. It assigns different skill discovery
algorithms to each factor based on the desired intrinsic reward function. To
encourage structured morphology-aware skills, we introduce symmetry-based
inductive biases tailored to individual factors. We also incorporate a style
factor and regularization penalties to promote safe and robust behaviors. We
evaluate our framework in simulation using a quadrupedal robot and demonstrate
zero-shot transfer of the learned skills to real hardware. Our results show
that factorization and symmetry lead to the discovery of structured
human-interpretable behaviors, while the style factor and penalties enhance
safety and diversity. Additionally, we show that the learned skills can be used
for downstream tasks and perform on par with oracle policies trained with
hand-crafted rewards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CoRL 2025. For code and videos, please check:
  https://leggedrobotics.github.io/d3-skill-discovery/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-09-05T20:00:50.177138366Z">
            2025-09-05 20:00:50 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
